<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2511.11410.pdf' target='_blank'>https://arxiv.org/pdf/2511.11410.pdf</a></span>   <span><a href='https://github.com/cydxf/Q-Doc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Huang, Dongxu Wu, Hanwei Zhu, Lingyu Zhu, Jun Xing, Xu Wang, Baoliang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11410">Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at: https://github.com/cydxf/Q-Doc.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2511.08155.pdf' target='_blank'>https://arxiv.org/pdf/2511.08155.pdf</a></span>   <span><a href='https://stootaghaj.github.io/nova-project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijay Ghildyal, Rajesh Sureddi, Nabajeet Barman, Saman Zadtootaghaj, Alan Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08155">Non-Aligned Reference Image Quality Assessment for Novel View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2511.08032.pdf' target='_blank'>https://arxiv.org/pdf/2511.08032.pdf</a></span>   <span><a href='https://github.com/diaoyn/3DGSQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolin Wan, Yining Diao, Jingqi Xu, Hao Wang, Zhiyang Li, Xiaopeng Fan, Wangmeng Zuo, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08032">Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2511.07298.pdf' target='_blank'>https://arxiv.org/pdf/2511.07298.pdf</a></span>   <span><a href='https://github.com/itu-biai/lmms_ldct_iqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kagan Celik, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07298">LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-dose computed tomography (CT) represents a significant improvement in patient safety through lower radiation doses, but increased noise, blur, and contrast loss can diminish diagnostic quality. Therefore, consistency and robustness in image quality assessment become essential for clinical applications. In this study, we propose an LLM-based quality assessment system that generates both numerical scores and textual descriptions of degradations such as noise, blur, and contrast loss. Furthermore, various inference strategies - from the zero-shot approach to metadata integration and error feedback - are systematically examined, demonstrating the progressive contribution of each method to overall performance. The resultant assessments yield not only highly correlated scores but also interpretable output, thereby adding value to clinical workflows. The source codes of our study are available at https://github.com/itu-biai/lmms_ldct_iqa.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2511.07290.pdf' target='_blank'>https://arxiv.org/pdf/2511.07290.pdf</a></span>   <span><a href='https://github.com/xinyiW915/CAMP-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Angeliki Katsenou, Junxiao Shen, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07290">CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prevalence of user-generated content (UGC) on platforms such as YouTube and TikTok has rendered no-reference (NR) perceptual video quality assessment (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of non-professional acquisition and the subsequent transcoding of UGC video on sharing platforms present significant challenges for NR-VQA. Although NR-VQA models attempt to infer mean opinion scores (MOS), their modeling of subjective scores for compressed content remains limited due to the absence of fine-grained perceptual annotations of artifact types. To address these challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the semantic understanding capabilities of large vision-language models. Our approach introduces a quality-aware prompting mechanism that integrates video metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted from inter-frame variations to guide the BLIP-2 pretraining approach in generating fine-grained quality captions. A unified architecture has been designed to model perceptual quality across three dimensions: semantic alignment, temporal characteristics, and spatial characteristics. These multimodal features are extracted and fused, then regressed to video quality scores. Extensive experiments on a wide variety of UGC datasets demonstrate that our model consistently outperforms existing NR-VQA methods, achieving improved accuracy without the need for costly manual fine-grained annotations. Our method achieves the best performance in terms of average rank and linear correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods. The source code and trained models, along with a user-friendly demo, are available at: https://github.com/xinyiW915/CAMP-VQA.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2511.06890.pdf' target='_blank'>https://arxiv.org/pdf/2511.06890.pdf</a></span>   <span><a href='https://github.com/YL1N/EduGuardBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Jiang, Mingzi Zhang, Xuanyu Yin, Sheng Jin, Suyu Lu, Zuocan Ying, Zengyi Yu, Xiangjie Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06890">EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2510.26661.pdf' target='_blank'>https://arxiv.org/pdf/2510.26661.pdf</a></span>   <span><a href='https://github.com/BioMedIA-MBZUAI/BRIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alya Almsouti, Ainur Khamitova, Darya Taratynova, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26661">BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric Brain MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.25238.pdf' target='_blank'>https://arxiv.org/pdf/2510.25238.pdf</a></span>   <span><a href='https://github.com/BestiVictory/VADB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianqian Qiao, DanDan Zheng, Yihang Bo, Bao Peng, Heng Huang, Longteng Jiang, Huaye Wang, Jingdong Chen, Jun Zhou, Xin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25238">VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video aesthetic assessment, a vital area in multimedia computing, integrates computer vision with human cognition. Its progress is limited by the lack of standardized datasets and robust models, as the temporal dynamics of video and multimodal fusion challenges hinder direct application of image-based methods. This study introduces VADB, the largest video aesthetic database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, including overall and attribute-specific aesthetic scores, rich language comments and objective tags. We propose VADB-Net, a dual-modal pre-training framework with a two-stage training strategy, which outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks. The dataset and source code are available at https://github.com/BestiVictory/VADB.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2510.23538.pdf' target='_blank'>https://arxiv.org/pdf/2510.23538.pdf</a></span>   <span><a href='https://github.com/InternLM/JanusCoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23538">JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.22373.pdf' target='_blank'>https://arxiv.org/pdf/2510.22373.pdf</a></span>   <span><a href='https://github.com/HKUSTDial/VisJudgeBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Xie, Zhiyang Zhang, Yifan Wu, Sirong Lu, Jiayi Zhang, Zhaoyang Yu, Jinlin Wang, Sirui Hong, Bang Liu, Chenglin Wu, Yuyu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22373">VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.09879.pdf' target='_blank'>https://arxiv.org/pdf/2510.09879.pdf</a></span>   <span><a href='https://shreshthsaini.github.io/CHUG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreshth Saini, Alan C. Bovik, Neil Birkbeck, Yilin Wang, Balu Adsumilli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09879">CHUG: Crowdsourced User-Generated HDR Video Quality Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High Dynamic Range (HDR) videos enhance visual experiences with superior brightness, contrast, and color depth. The surge of User-Generated Content (UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR video quality assessment (VQA) due to diverse capture conditions, editing artifacts, and compression distortions. Existing HDR-VQA datasets primarily focus on professionally generated content (PGC), leaving a gap in understanding real-world UGC-HDR degradations. To address this, we introduce CHUG: Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos, transcoded across multiple resolutions and bitrates to simulate real-world scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse, and real-world UGC dataset. The dataset is publicly available at: https://shreshthsaini.github.io/CHUG/.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.06842.pdf' target='_blank'>https://arxiv.org/pdf/2510.06842.pdf</a></span>   <span><a href='https://github.com/ZhouKanglei/MAGRPP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06842">Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) quantifies human actions in videos, supporting applications in sports scoring, rehabilitation, and skill evaluation. A major challenge lies in the non-stationary nature of quality distributions in real-world scenarios, which limits the generalization ability of conventional methods. We introduce Continual AQA (CAQA), which equips AQA with Continual Learning (CL) capabilities to handle evolving distributions while mitigating catastrophic forgetting. Although parameter-efficient fine-tuning of pretrained models has shown promise in CL for image classification, we find it insufficient for CAQA. Our empirical and theoretical analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is necessary for effective representation learning; yet (ii) uncontrolled FPFT induces overfitting and feature manifold shift, thereby aggravating forgetting. To address this, we propose Adaptive Manifold-Aligned Graph Regularization (MAGR++), which couples backbone fine-tuning that stabilizes shallow layers while adapting deeper ones with a two-step feature rectification pipeline: a manifold projector to translate deviated historical features into the current representation space, and a graph regularizer to align local and global distributions. We construct four CAQA benchmarks from three datasets with tailored evaluation protocols and strong baselines, enabling systematic cross-dataset comparison. Extensive experiments show that MAGR++ achieves state-of-the-art performance, with average correlation gains of 3.6% offline and 12.2% online over the strongest baseline, confirming its robustness and effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.02876.pdf' target='_blank'>https://arxiv.org/pdf/2510.02876.pdf</a></span>   <span><a href='https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Zahim Hassan, Md. Osama, Muhammad Ashad Kabir, Md. Saiful Islam, Zannatul Naim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02876">ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate, non-destructive assessment of egg quality is critical for ensuring food safety, maintaining product standards, and operational efficiency in commercial poultry production. This paper introduces ELMF4EggQ, an ensemble learning framework that employs multimodal feature fusion to classify egg grade and freshness using only external attributes - image, shape, and weight. A novel, publicly available dataset of 186 brown-shelled eggs was constructed, with egg grade and freshness levels determined through laboratory-based expert assessments involving internal quality measurements, such as yolk index and Haugh unit. To the best of our knowledge, this is the first study to apply machine learning methods for internal egg quality assessment using only external, non-invasive features, and the first to release a corresponding labeled dataset. The proposed framework integrates deep features extracted from external egg images with structural characteristics such as egg shape and weight, enabling a comprehensive representation of each egg. Image feature extraction is performed using top-performing pre-trained CNN models (ResNet152, DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction, SMOTE augmentation, and classification using multiple machine learning algorithms. An ensemble voting mechanism combines predictions from the best-performing classifiers to enhance overall accuracy. Experimental results demonstrate that the multimodal approach significantly outperforms image-only and tabular (shape and weight) only baselines, with the multimodal ensemble approach achieving 86.57% accuracy in grade classification and 70.83% in freshness prediction. All code and data are publicly available at https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting transparency, reproducibility, and further research in this domain.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.01469.pdf' target='_blank'>https://arxiv.org/pdf/2510.01469.pdf</a></span>   <span><a href='https://github.com/pnyxai/a-vert,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolás Aguirre, Ramiro Caso, Ramiro Rodríguez Colmeiro, Mauro Santelli, Joaquín Toranzo Calderón
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01469">A-VERT: Agnostic Verification with Embedding Ranking Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automatic evaluation of Language Model (LM) responses is a critical piece in the development of benchmarks and metrics, both for model training and quality assessment of production model endpoints. The current approaches to response classification relies on methods that are too expensive (i.e. LLM-as-a-Judge) or that are far from real-world conditions (string-matching, logprob). In this paper, a structure-free evaluation method is presented. The method makes use of semantic embedding distances to match target candidates with arbitrary LM-generated text, resulting in a robust classification of the response at a relatively low compute cost (embedding models of less than $10B$ parameters). The results show a regression score of ~0.97 and an accuracy of ~96% against human annotators, tested over 3 data sets and 3 different LM architectures.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2509.23841.pdf' target='_blank'>https://arxiv.org/pdf/2509.23841.pdf</a></span>   <span><a href='https://cbysjtu.github.io/Rank2Score/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyang Cui, Yujie Zhang, Qi Yang, Zhu Li, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23841">Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Text-to-3D (T23D) generative models have enabled the synthesis of diverse, high-fidelity 3D assets from textual prompts. However, existing challenges restrict the development of reliable T23D quality assessment (T23DQA). First, existing benchmarks are outdated, fragmented, and coarse-grained, making fine-grained metric training infeasible. Moreover, current objective metrics exhibit inherent design limitations, resulting in non-representative feature extraction and diminished metric robustness. To address these limitations, we introduce T23D-CompBench, a comprehensive benchmark for compositional T23D generation. We define five components with twelve sub-components for compositional prompts, which are used to generate 3,600 textured meshes from ten state-of-the-art generative models. A large-scale subjective experiment is conducted to collect 129,600 reliable human ratings across different perspectives. Based on T23D-CompBench, we further propose Rank2Score, an effective evaluator with two-stage training for T23DQA. Rank2Score enhances pairwise training via supervised contrastive regression and curriculum learning in the first stage, and subsequently refines predictions using mean opinion scores to achieve closer alignment with human judgments in the second stage. Extensive experiments and downstream applications demonstrate that Rank2Score consistently outperforms existing metrics across multiple dimensions and can additionally serve as a reward function to optimize generative models. The project is available at https://cbysjtu.github.io/Rank2Score/.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2509.23770.pdf' target='_blank'>https://arxiv.org/pdf/2509.23770.pdf</a></span>   <span><a href='https://github.com/xiaojieli0903/GenViewPlusPlus' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xiaojieli0903/GenViewPlusPlus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojie Li, Bei Wang, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23770">GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair's semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%. The code is available at https://github.com/xiaojieli0903/GenViewPlusPlus.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2509.22799.pdf' target='_blank'>https://arxiv.org/pdf/2509.22799.pdf</a></span>   <span><a href='https://tiger-ai-lab.github.io/VideoScore2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Ge Zhang, Wenhao Huang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22799">VideoScore2: Think before You Score in Generative Video Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2509.22261.pdf' target='_blank'>https://arxiv.org/pdf/2509.22261.pdf</a></span>   <span><a href='https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanghao Zhu, Zhitian Hou, Zeyu Liu, Zhijie Sang, Congkai Xie, Hongxia Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22261">InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have shown remarkable potential in various domains, yet their application in the medical field is hindered by several challenges. General-purpose MLLMs often lack the specialized knowledge required for medical tasks, leading to uncertain or hallucinatory responses. Knowledge distillation from advanced models struggles to capture domain-specific expertise in radiology and pharmacology. Additionally, the computational cost of continual pretraining with large-scale medical data poses significant efficiency challenges. To address these issues, we propose InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs designed to deliver state-of-the-art performance in medical applications. We combined high-quality general-purpose and medical multimodal data and proposed a novel five-dimensional quality assessment framework to curate high-quality multimodal medical datasets. We employ low-to-high image resolution and multimodal sequence packing to enhance training efficiency, enabling the integration of extensive medical data. Furthermore, a three-stage supervised fine-tuning process ensures effective knowledge extraction for complex medical tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B and MedGemma-27B-IT, demonstrating superior performance in medical visual question answering and diagnostic tasks. By addressing key challenges in data quality, training efficiency, and domain-specific knowledge extraction, our work paves the way for more reliable and effective AI-driven solutions in healthcare. InfiMed-Foundation-4B model is available at \href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2509.21359.pdf' target='_blank'>https://arxiv.org/pdf/2509.21359.pdf</a></span>   <span><a href='https://github.com/SJTU-DMTai/RAG-CSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21359">Influence Guided Context Selection for Effective Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2509.17628.pdf' target='_blank'>https://arxiv.org/pdf/2509.17628.pdf</a></span>   <span><a href='https://github.com/D3E0-source/MSCoRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17628">MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.17321.pdf' target='_blank'>https://arxiv.org/pdf/2509.17321.pdf</a></span>   <span><a href='github.com/budzianowski/opengvl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ Budzianowski, Emilia WiÅnios, Gracjan GÃ³ral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17321">OpenGVL -- Benchmarking Visual Temporal Progress for Data Curation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2509.16975.pdf' target='_blank'>https://arxiv.org/pdf/2509.16975.pdf</a></span>   <span><a href='https://github.com/NKU-HLT/Eval_Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Jia, Xu Zhang, Yang Chen, Hui Wang, Enzhi Wang, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16975">Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at https://github.com/NKU-HLT/Eval_Reasoning.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2509.16609.pdf' target='_blank'>https://arxiv.org/pdf/2509.16609.pdf</a></span>   <span><a href='https://github.com/xauat-liushipeng/D2S' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, Zhonglin Zhang, Dengfeng Chen, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16609">Describe-to-Score: Text-Guided Efficient Image Complexity Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately assessing image complexity (IC) is critical for computer vision, yet most existing methods rely solely on visual features and often neglect high-level semantic information, limiting their accuracy and generalization. We introduce vision-text fusion for IC modeling. This approach integrates visual and textual semantic features, increasing representational diversity. It also reduces the complexity of the hypothesis space, which enhances both accuracy and generalization in complexity assessment. We propose the D2S (Describe-to-Score) framework, which generates image captions with a pre-trained vision-language model. We propose the feature alignment and entropy distribution alignment mechanisms, D2S guides semantic information to inform complexity assessment while bridging the gap between vision and text modalities. D2S utilizes multi-modal information during training but requires only the vision branch during inference, thereby avoiding multi-modal computational overhead and enabling efficient assessment. Experimental results demonstrate that D2S outperforms existing methods on the IC9600 dataset and maintains competitiveness on no-reference image quality assessment (NR-IQA) benchmark, validating the effectiveness and efficiency of multi-modal fusion in complexity-related tasks. Code is available at: https://github.com/xauat-liushipeng/D2S
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2509.11589.pdf' target='_blank'>https://arxiv.org/pdf/2509.11589.pdf</a></span>   <span><a href='https://github.com/Controller01-ai/MVQA-68K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyun Pu, Kehan Li, Zeyi Huang, Zhijie Zhong, Kaixiang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11589">MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: https://github.com/Controller01-ai/MVQA-68K
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2509.06413.pdf' target='_blank'>https://arxiv.org/pdf/2509.06413.pdf</a></span>   <span><a href='https://github.com/Lighting-YXLI/ISRGen-QA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Li, Xin Li, Chris Wei Zhou, Shuo Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan Zhao, Weide Liu, Xiaoyuan Yang, Zhengzhong Tu, Xinyu Li, Chuanbiao Song, Chenqi Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Xiaoyan Sun, Shishun Tian, Dongyang Yan, Weixia Zhang, Junlin Chen, Wei Sun, Zhihua Wang, Zhuohang Shi, Zhizun Luo, Hang Ouyang, Tianxin Xiao, Fan Yang, Zhaowang Wu, Kaixin Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06413">VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.01183.pdf' target='_blank'>https://arxiv.org/pdf/2509.01183.pdf</a></span>   <span><a href='https://github.com/Yangbn97/SegAssess' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, Jianya Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01183">SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments across 32 datasets derived from 6 sources demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks, establishing PQM via SegAssess as a robust and transferable solution for unsupervised SQA. The code is available at https://github.com/Yangbn97/SegAssess.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2508.19850.pdf' target='_blank'>https://arxiv.org/pdf/2508.19850.pdf</a></span>   <span><a href='https://github.com/XiaoqiWang/MIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Wang, Yun Zhang, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19850">Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine vision systems (MVS) are intrinsically vulnerable to performance degradation under adverse visual conditions. To address this, we propose a machine-centric image quality assessment (MIQA) framework that quantifies the impact of image degradations on MVS performance. We establish an MIQA paradigm encompassing the end-to-end assessment workflow. To support this, we construct a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million samples that capture distinctive degradation responses in both consistency and accuracy metrics, spanning 75 vision models, 250 degradation types, and three representative vision tasks. We further propose a region-aware MIQA (RA-MIQA) model to evaluate MVS visual quality through fine-grained spatial degradation analysis. Extensive experiments benchmark the proposed RA-MIQA against seven human visual system (HVS)-based IQA metrics and five retrained classical backbones. Results demonstrate RA-MIQA's superior performance in multiple dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, while also revealing task-specific degradation sensitivities. Critically, HVS-based metrics prove inadequate for MVS quality prediction, while even specialized MIQA models struggle with background degradations, accuracy-oriented estimation, and subtle distortions. This study can advance MVS reliability and establish foundations for machine-centric image processing and optimization. The model and code are available at: https://github.com/XiaoqiWang/MIQA.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.19808.pdf' target='_blank'>https://arxiv.org/pdf/2508.19808.pdf</a></span>   <span><a href='https://github.com/wcbup/AutoQ-VIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixuan Lu, Mehmet Onurcan Kaya, Dim P. Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19808">AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. The source code of our method is available at https://github.com/wcbup/AutoQ-VIS.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.16887.pdf' target='_blank'>https://arxiv.org/pdf/2508.16887.pdf</a></span>   <span><a href='https://github.com/YaoShunyu19/MDIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunyu Yao, Ming Liu, Zhilu Zhang, Zhaolin Wan, Zhilong Ji, Jinfeng Bai, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16887">MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in image quality assessment (IQA), driven by sophisticated deep neural network designs, have significantly improved the ability to approach human perceptions. However, most existing methods are obsessed with fitting the overall score, neglecting the fact that humans typically evaluate image quality from different dimensions before arriving at an overall quality assessment. To overcome this problem, we propose a multi-dimensional image quality assessment (MDIQA) framework. Specifically, we model image quality across various perceptual dimensions, including five technical and four aesthetic dimensions, to capture the multifaceted nature of human visual perception within distinct branches. Each branch of our MDIQA is initially trained under the guidance of a separate dimension, and the respective features are then amalgamated to generate the final IQA score. Additionally, when the MDIQA model is ready, we can deploy it for a flexible training of image restoration (IR) models, enabling the restoration results to better align with varying user preferences through the adjustment of perceptual dimension weights. Extensive experiments demonstrate that our MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.16291.pdf' target='_blank'>https://arxiv.org/pdf/2508.16291.pdf</a></span>   <span><a href='https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengshun Wang, Qiurui Wang, Peilin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16291">Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Technical Element Score (TES) and Program Component Score (PCS) evaluations in figure skating demand precise assessment of athletic actions and artistic interpretation, respectively. Existing methods face three major challenges. Firstly, video and audio cues are regarded as common features for both TES and PCS predictions in previous works without considering the prior evaluation criterion of figure skating. Secondly, action elements in competitions are separated in time, TES should be derived from each element's score, but existing methods try to give an overall TES prediction without evaluating each action element. Thirdly, lengthy competition videos make it difficult and inefficient to handle long-range contexts. To address these challenges, we propose a two-stream Mamba pyramid network that aligns with actual judging criteria to predict TES and PCS by separating visual-feature based TES evaluation stream from audio-visual-feature based PCS evaluation stream. In the PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee that video-based features remain unaffected when assessing TES, and enhance PCS estimation by fusing visual and auditory cues across each contextual level of the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and TES head we proposed effectively address the challenges of localizing and evaluating action elements with various temporal scales and give score predictions. With Mamba's superior ability to capture long-range dependencies and its linear computational complexity, our method is ideal for handling lengthy figure skating videos. Comprehensive experimentation demonstrates that our framework attains state-of-the-art performance on the FineFS benchmark. Our source code is available at https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.15215.pdf' target='_blank'>https://arxiv.org/pdf/2508.15215.pdf</a></span>   <span><a href='https://github.com/Ben1001409/SleepDIFFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15215">SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification of sleep stages is essential for assessing sleep quality and diagnosing sleep disorders. However, manual inspection of EEG characteristics for each stage is time-consuming and prone to human error. Although machine learning and deep learning methods have been actively developed, they continue to face challenges from the non-stationarity and variability of electroencephalography (EEG) and electrooculography (EOG) signals across different domains (i.e., datasets), often leading to poor generalization. This work proposed a Sleep Stage Classification method by developing Multivariate Differential Transformer (SleepDIFFormer) for joint EEG and EOG representation learning. Specifically, SleepDIFFormer was developed to process EEG and EOG signals using our Multivariate Differential Transformer Architecture (MDTA) for time series, trained with cross-domain alignment. Our method mitigated spatial and temporal attention noise while learning a domain-invariant joint EEG-EOG representation through feature distribution alignment, thereby enabling generalization to unseen target datasets. Empirically, we evaluated our method on five different sleep staging datasets and compared it with existing approaches, achieving state-of-the-art performance. We also conducted a thorough ablation analysis of SleepDIFFormer and interpreted the differential attention weights, highlighting their relevance to characteristic sleep EEG patterns. These findings have implications for advancing automated sleep stage classification and its application to sleep quality assessment. Our source code is publicly available at https://github.com/Ben1001409/SleepDIFFormer
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2508.14475.pdf' target='_blank'>https://arxiv.org/pdf/2508.14475.pdf</a></span>   <span><a href='https://pxf0429.github.io/FGResQ/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangfei Sheng, Xiaofeng Pan, Zhichao Yang, Pengfei Chen, Leida Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14475">Fine-grained Image Quality Assessment for Perceptual Image Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed remarkable achievements in perceptual image restoration (IR), creating an urgent demand for accurate image quality assessment (IQA), which is essential for both performance comparison and algorithm optimization. Unfortunately, the existing IQA metrics exhibit inherent weakness for IR task, particularly when distinguishing fine-grained quality differences among restored images. To address this dilemma, we contribute the first-of-its-kind fine-grained image quality assessment dataset for image restoration, termed FGRestore, comprising 18,408 restored images across six common IR tasks. Beyond conventional scalar quality scores, FGRestore was also annotated with 30,886 fine-grained pairwise preferences. Based on FGRestore, a comprehensive benchmark was conducted on the existing IQA metrics, which reveal significant inconsistencies between score-based IQA evaluations and the fine-grained restoration quality. Motivated by these findings, we further propose FGResQ, a new IQA model specifically designed for image restoration, which features both coarse-grained score regression and fine-grained quality ranking. Extensive experiments and comparisons demonstrate that FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and model weights have been released in https://pxf0429.github.io/FGResQ/
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2508.10605.pdf' target='_blank'>https://arxiv.org/pdf/2508.10605.pdf</a></span>   <span><a href='https://github.com/xinyiW915/DIVA-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Angeliki Katsenou, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10605">DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of user-generated (video) content (UGC) has driven increased demand for research on no-reference (NR) perceptual video quality assessment (VQA). NR-VQA is a key component for large-scale video quality monitoring in social media and streaming applications where a pristine reference is not available. This paper proposes a novel NR-VQA model based on spatio-temporal fragmentation driven by inter-frame variations. By leveraging these inter-frame differences, the model progressively analyses quality-sensitive regions at multiple levels: frames, patches, and fragmented frames. It integrates frames, fragmented residuals, and fragmented frames aligned with residuals to effectively capture global and local information. The model extracts both 2D and 3D features in order to characterize these spatio-temporal variations. Experiments conducted on five UGC datasets and against state-of-the-art models ranked our proposed method among the top 2 in terms of average rank correlation (DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered at a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on average compared to the fastest existing NR-VQA method. Code and models are publicly available at: https://github.com/xinyiW915/DIVA-VQA.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2508.08700.pdf' target='_blank'>https://arxiv.org/pdf/2508.08700.pdf</a></span>   <span><a href='https://github.com/uniqzheng/CBAND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zheng, Li-Heng Chen, Chenlong He, Neil Berkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik, Yibo Fan, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08700">Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although there have been notable advancements in video compression technologies in recent years, banding artifacts remain a serious issue affecting the quality of compressed videos, particularly on smooth regions of high-definition videos. Noticeable banding artifacts can severely impact the perceptual quality of videos viewed on a high-end HDTV or high-resolution screen. Hence, there is a pressing need for a systematic investigation of the banding video quality assessment problem for advanced video codecs. Given that the existing publicly available datasets for studying banding artifacts are limited to still picture data only, which cannot account for temporal banding dynamics, we have created a first-of-a-kind open video dataset, dubbed LIVE-YT-Banding, which consists of 160 videos generated by four different compression parameters using the AV1 video codec. A total of 7,200 subjective opinions are collected from a cohort of 45 human subjects. To demonstrate the value of this new resources, we tested and compared a variety of models that detect banding occurrences, and measure their impact on perceived quality. Among these, we introduce an effective and efficient new no-reference (NR) video quality evaluator which we call CBAND. CBAND leverages the properties of the learned statistics of natural images expressed in the embeddings of deep neural networks. Our experimental results show that the perceptual banding prediction performance of CBAND significantly exceeds that of previous state-of-the-art models, and is also orders of magnitude faster. Moreover, CBAND can be employed as a differentiable loss function to optimize video debanding models. The LIVE-YT-Banding database, code, and pre-trained model are all publically available at https://github.com/uniqzheng/CBAND.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2508.07038.pdf' target='_blank'>https://arxiv.org/pdf/2508.07038.pdf</a></span>   <span><a href='https://github.com/YukeXing/3DGS-VBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuke Xing, William Gordon, Qi Yang, Kaifa Yang, Jiarui Wang, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07038">3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high visual fidelity, but its substantial storage requirements hinder practical deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate compression modules. However, these 3DGS generative compression techniques introduce unique distortions lacking systematic quality assessment research. To this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment (VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences generated from 11 scenes across 6 SOTA 3DGS compression algorithms with systematically designed parameter levels. With annotations from 50 participants, we obtained MOS scores with outlier removal and validated dataset reliability. We benchmark 6 3DGS compression algorithms on storage efficiency and visual quality, and evaluate 15 quality assessment metrics across multiple paradigms. Our work enables specialized VQA model training for 3DGS, serving as a catalyst for compression and quality assessment research. The dataset is available at https://github.com/YukeXing/3DGS-VBench.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2508.05609.pdf' target='_blank'>https://arxiv.org/pdf/2508.05609.pdf</a></span>   <span><a href='https://zyh482.github.io/Hi3DEval/' target='_blank'>  GitHub</a></span> <span><a href='https://zyh482.github.io/Hi3DEval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05609">Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2508.05016.pdf' target='_blank'>https://arxiv.org/pdf/2508.05016.pdf</a></span>   <span><a href='https://github.com/WNNGGU/AU-IQA-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shushi Wang, Chunyi Li, Zicheng Zhang, Han Zhou, Wei Dong, Jun Chen, Guangtao Zhai, Xiaohong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05016">AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-based image enhancement techniques have been widely adopted in various visual applications, significantly improving the perceptual quality of user-generated content (UGC). However, the lack of specialized quality assessment models has become a significant limiting factor in this field, limiting user experience and hindering the advancement of enhancement methods. While perceptual quality assessment methods have shown strong performance on UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC) which blends features from both, remains largely unexplored. To address this gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images produced by three representative enhancement types which include super-resolution, low-light enhancement, and denoising. On this dataset, we further evaluate a range of existing quality assessment models, including traditional IQA methods and large multimodal models. Finally, we provide a comprehensive analysis of how well current approaches perform in assessing the perceptual quality of AI-UGC. The access link to the AU-IQA is https://github.com/WNNGGU/AU-IQA-Dataset.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2507.23343.pdf' target='_blank'>https://arxiv.org/pdf/2507.23343.pdf</a></span>   <span><a href='https://github.com/zyj-2000/Talker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Jiezhang Cao, Zicheng Zhang, Farong Wen, Yanwei Jiang, Jun Jia, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23343">Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech-driven methods for portraits are figuratively known as "Talkers" because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at https://github.com/zyj-2000/Talker.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2507.22802.pdf' target='_blank'>https://arxiv.org/pdf/2507.22802.pdf</a></span>   <span><a href='https://github.com/donglihe-hub/FetalCLIP-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongli He, Hu Wang, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22802">Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.19165.pdf' target='_blank'>https://arxiv.org/pdf/2507.19165.pdf</a></span>   <span><a href='https://github.com/CMRxMotion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang Wang, Chen Qin, Zhang Shi, Haoran Wang, Xiwen Zhang, Chen Chen, Cheng Ouyang, Chengliang Dai, Yuanhan Mo, Chenchen Dai, Xutong Kuang, Ruizhe Li, Xin Chen, Xiuzheng Yue, Song Tian, Alejandro Mora-Rubio, Kumaradevan Punithakumar, Shizhan Gong, Qi Dou, Sina Amirrajab, Yasmina Al Khalil, Cian M. Scannell, Lexiaozi Fan, Huili Yang, Xiaowu Sun, Rob van der Geest, Tewodros Weldebirhan Arega, Fabrice Meriaudeau, Caner Ãzer, Amin Ranem, John Kalkhof, Ä°lkay ÃksÃ¼z, Anirban Mukhopadhyay, Abdul Qayyum, Moona Mazher, Steven A Niederer, Carles Garcia-Cabrera, Eric Arazo, Michal K. Grzeszczyk, Szymon PÅotka, Wanqin Ma, Xiaomeng Li, Rongjun Ge, Yongqing Kou, Xinrong Chen, He Wang, Chengyan Wang, Wenjia Bai, Shuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19165">Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have achieved state-of-the-art performance in automated Cardiac Magnetic Resonance (CMR) analysis. However, the efficacy of these models is highly dependent on the availability of high-quality, artifact-free images. In clinical practice, CMR acquisitions are frequently degraded by respiratory motion, yet the robustness of deep learning models against such artifacts remains an underexplored problem. To promote research in this domain, we organized the MICCAI CMRxMotion challenge. We curated and publicly released a dataset of 320 CMR cine series from 40 healthy volunteers who performed specific breathing protocols to induce a controlled spectrum of motion artifacts. The challenge comprised two tasks: 1) automated image quality assessment to classify images based on motion severity, and 2) robust myocardial segmentation in the presence of motion artifacts. A total of 22 algorithms were submitted and evaluated on the two designated tasks. This paper presents a comprehensive overview of the challenge design and dataset, reports the evaluation results for the top-performing methods, and further investigates the impact of motion artifacts on five clinically relevant biomarkers. All resources and code are publicly available at: https://github.com/CMRxMotion
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.15709.pdf' target='_blank'>https://arxiv.org/pdf/2507.15709.pdf</a></span>   <span><a href='https://github.com/sunwei925/Efficient-FIQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Weixia Zhang, Linhan Cao, Jun Jia, Xiangyang Zhu, Dandan Zhu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15709">Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face image quality assessment (FIQA) is essential for various face-related applications. Although FIQA has been extensively studied and achieved significant progress, the computational complexity of FIQA algorithms remains a key concern for ensuring scalability and practical deployment in real-world systems. In this paper, we aim to develop a computationally efficient FIQA method that can be easily deployed in real-world applications. Specifically, our method consists of two stages: training a powerful teacher model and distilling a lightweight student model from it. To build a strong teacher model, we adopt a self-training strategy to improve its capacity. We first train the teacher model using labeled face images, then use it to generate pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are used in two ways: (1) to distill knowledge into the student model, and (2) to combine with the original labeled images to further enhance the teacher model through self-training. The enhanced teacher model is used to further pseudo-label another set of unlabeled images for distilling the student models. The student model is trained using a combination of labeled images, pseudo-labeled images from the original teacher model, and pseudo-labeled images from the enhanced teacher model. Experimental results demonstrate that our student model achieves comparable performance to the teacher model with an extremely low computational overhead. Moreover, our method achieved first place in the ICCV 2025 VQualA FIQA Challenge. The code is available at https://github.com/sunwei925/Efficient-FIQA.git.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.12796.pdf' target='_blank'>https://arxiv.org/pdf/2507.12796.pdf</a></span>   <span><a href='https://github.com/Junjie-Gao19/DeQA-Doc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Gao, Runze Liu, Yingzhe Peng, Shujian Yang, Jin Zhang, Kai Yang, Zhiyuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12796">DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document quality assessment is critical for a wide range of applications including document digitization, OCR, and archival. However, existing approaches often struggle to provide accurate and robust quality scores, limiting their applicability in practical scenarios. With the rapid progress in Multi-modal Large Language Models (MLLMs), recent MLLM-based methods have achieved remarkable performance in image quality assessment. In this work, we extend this success to the document domain by adapting DeQA-Score, a state-of-the-art MLLM-based image quality scorer, for document quality assessment. We propose DeQA-Doc, a framework that leverages the visual language capabilities of MLLMs and a soft label strategy to regress continuous document quality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary solutions to construct soft labels without the variance information. Also, we relax the resolution constrains to support the large resolution of document images. Finally, we introduce ensemble methods to further enhance the performance. Extensive experiments demonstrate that DeQA-Doc significantly outperforms existing baselines, offering accurate and generalizable document quality assessment across diverse degradation types. Codes and model weights are available in https://github.com/Junjie-Gao19/DeQA-Doc.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.12687.pdf' target='_blank'>https://arxiv.org/pdf/2507.12687.pdf</a></span>   <span><a href='https://github.com/rajeshsureddi/triqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajesh Sureddi, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12687">TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) models aim to predict perceptual image quality in alignment with human judgments. No-Reference (NR) IQA remains particularly challenging due to the absence of a reference image. While deep learning has significantly advanced this field, a major hurdle in developing NR-IQA models is the limited availability of subjectively labeled data. Most existing deep learning-based NR-IQA approaches rely on pre-training on large-scale datasets before fine-tuning for IQA tasks. To further advance progress in this area, we propose a novel approach that constructs a custom dataset using a limited number of reference content images and introduces a no-reference IQA model that incorporates both content and quality features for perceptual quality prediction. Specifically, we train a quality-aware model using contrastive triplet-based learning, enabling efficient training with fewer samples while achieving strong generalization performance across publicly available datasets. Our repository is available at https://github.com/rajeshsureddi/triqa.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2507.11900.pdf' target='_blank'>https://arxiv.org/pdf/2507.11900.pdf</a></span>   <span><a href='https://github.com/sunwei925/CompressedVQA-HDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Linhan Cao, Kang Fu, Dandan Zhu, Jun Jia, Menghan Hu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11900">CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video compression is a standard procedure applied to all videos to minimize storage and transmission demands while preserving visual quality as much as possible. Therefore, evaluating the visual quality of compressed videos is crucial for guiding the practical usage and further development of video compression algorithms. Although numerous compressed video quality assessment (VQA) methods have been proposed, they often lack the generalization capability needed to handle the increasing diversity of video types, particularly high dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an effective VQA framework designed to address the challenges of HDR video quality assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the backbone networks for the proposed full-reference (FR) and no-reference (NR) VQA models, respectively. For the FR model, we compute deep structural and textural similarities between reference and distorted frames using intermediate-layer features extracted from the Swin Transformer as its quality-aware feature representation. For the NR model, we extract the global mean of the final-layer feature maps from SigLip 2 as its quality-aware representation. To mitigate the issue of limited HDR training data, we pre-train the FR model on a large-scale standard dynamic range (SDR) VQA dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ an iterative mixed-dataset training strategy across multiple compressed VQA datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental results show that our models achieve state-of-the-art performance compared to existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE ICME 2025. The code is available at https://github.com/sunwei925/CompressedVQA-HDR.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2507.10432.pdf' target='_blank'>https://arxiv.org/pdf/2507.10432.pdf</a></span>   <span><a href='https://github.com/mozhu1/SC-AGIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Li, Qingsen Yan, Haojian Huang, Peng Wu, Haokui Zhang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10432">Text-Visual Semantic Constrained AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements in Artificial Intelligence Generated Image (AGI) technology, the accurate assessment of their quality has become an increasingly vital requirement. Prevailing methods typically rely on cross-modal models like CLIP or BLIP to evaluate text-image alignment and visual quality. However, when applied to AGIs, these methods encounter two primary challenges: semantic misalignment and details perception missing. To address these limitations, we propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment (SC-AGIQA), a unified framework that leverages text-visual semantic constraints to significantly enhance the comprehensive evaluation of both text-image consistency and perceptual distortion in AI-generated images. Our approach integrates key capabilities from multiple models and tackles the aforementioned challenges by introducing two core modules: the Text-assisted Semantic Alignment Module (TSAM), which leverages Multimodal Large Language Models (MLLMs) to bridge the semantic gap by generating an image description and comparing it against the original prompt for a refined consistency check, and the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which draws inspiration from Human Visual System (HVS) properties by employing frequency domain analysis combined with perceptual sensitivity weighting to better quantify subtle visual distortions and enhance the capture of fine-grained visual quality details in images. Extensive experiments conducted on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing state-of-the-art methods. The code is publicly available at https://github.com/mozhu1/SC-AGIQA.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.06971.pdf' target='_blank'>https://arxiv.org/pdf/2507.06971.pdf</a></span>   <span><a href='https://github.com/Bryant-Teng/Percep360' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Bryant-Teng/Percep360' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Teng, Kai Luo, Sheng Wu, Siyu Li, Pujun Guo, Jiale Wei, Kunyu Peng, Jiaming Zhang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06971">Hallucinating 360Â°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360Â° surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot achieve high-quality, controllable panoramic generation. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at https://github.com/Bryant-Teng/Percep360.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2507.03990.pdf' target='_blank'>https://arxiv.org/pdf/2507.03990.pdf</a></span>   <span><a href='https://aleksandrgushchin.github.io/lcvqad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandr Gushchin, Maksim Smirnov, Dmitriy Vatolin, Anastasia Antsiferova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03990">LEHA-CVQAD: Dataset To Enable Generalized Video Quality Assessment of Compression Artifacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the LEHA-CVQAD (Large-scale Enriched Human-Annotated Compressed Video Quality Assessment) dataset, which comprises 6,240 clips for compression-oriented video quality assessment. 59 source videos are encoded with 186 codec-preset variants, 1.8M pairwise, and 1.5k MOS ratings are fused into a single quality scale; part of the videos remains hidden for blind evaluation. We also propose Rate-Distortion Alignment Error (RDAE), a novel evaluation metric that quantifies how well VQA models preserve bitrate-quality ordering, directly supporting codec parameter tuning. Testing IQA/VQA methods reveals that popular VQA metrics exhibit high RDAE and lower correlations, underscoring the dataset challenges and utility. The open part and the results of LEHA-CVQAD are available at https://aleksandrgushchin.github.io/lcvqad/
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2507.02316.pdf' target='_blank'>https://arxiv.org/pdf/2507.02316.pdf</a></span>   <span><a href='https://jasoncodemaker.github.io/SynTVA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zecheng Zhao, Selena Song, Tong Chen, Zhi Chen, Shazia Sadiq, Yadan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02316">Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation metrics primarily capture visual quality and temporal consistency, offering limited insight into how synthetic videos perform in downstream tasks such as text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset and benchmark designed to evaluate the utility of synthetic videos for building retrieval models. Based on 800 diverse user queries derived from MSRVTT training split, we generate synthetic videos using state-of-the-art T2V models and annotate each video-text pair along four key semantic alignment dimensions: Object \& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation framework correlates general video quality assessment (VQA) metrics with these alignment scores, and examines their predictive power for downstream TVR performance. To explore pathways of scaling up, we further develop an Auto-Evaluator to estimate alignment quality from existing metrics. Beyond benchmarking, our results show that SynTVA is a valuable asset for dataset augmentation, enabling the selection of high-utility synthetic samples that measurably improve TVR outcomes. Project page and dataset can be found at https://jasoncodemaker.github.io/SynTVA/.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2507.00377.pdf' target='_blank'>https://arxiv.org/pdf/2507.00377.pdf</a></span>   <span><a href='https://github.com/JianhaoXie1/MedDiff-FT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhao Xie, Ziang Zhang, Zhenyu Weng, Yuesheng Zhu, Guibo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00377">MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data.While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at https://github.com/JianhaoXie1/MedDiff-FT.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2506.23852.pdf' target='_blank'>https://arxiv.org/pdf/2506.23852.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/RGC-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Jin, Jiangyong Ying, Huiyu Duan, Liu Yang, Sijing Wu, Yunhao Li, Yushuo Zheng, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23852">RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As camera-equipped robotic platforms become increasingly integrated into daily life, robotic-generated videos have begun to appear on streaming media platforms, enabling us to envision a future where humans and robots coexist. We innovatively propose the concept of Robotic-Generated Content (RGC) to term these videos generated from egocentric perspective of robots. The perceptual quality of RGC videos is critical in human-robot interaction scenarios, and RGC videos exhibit unique distortions and visual requirements that differ markedly from those of professionally-generated content (PGC) videos and user-generated content (UGC) videos. However, dedicated research on quality assessment of RGC videos is still lacking. To address this gap and to support broader robotic applications, we establish the first Robotic-Generated Content Database (RGCD), which contains a total of 2,100 videos drawn from three robot categories and sourced from diverse platforms. A subjective VQA experiment is conducted subsequently to assess human visual perception of robotic-generated videos. Finally, we conduct a benchmark experiment to evaluate the performance of 11 state-of-the-art VQA models on our database. Experimental results reveal significant limitations in existing VQA models when applied to complex, robotic-generated content, highlighting a critical need for RGC-specific VQA models. Our RGCD is publicly available at: https://github.com/IntMeGroup/RGC-VQA.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.21925.pdf' target='_blank'>https://arxiv.org/pdf/2506.21925.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/AIGCOIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Yang, Huiyu Duan, Jiarui Wang, Jing Liu, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21925">Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques, AI generated images (AIGIs) have attracted widespread attention, among which AI generated omnidirectional images (AIGODIs) hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications. AI generated omnidirectional images exhibit unique quality issues, however, research on the quality assessment and optimization of AI-generated omnidirectional images is still lacking. To this end, this work first studies the quality assessment and distortion-aware saliency prediction problems for AIGODIs, and further presents a corresponding optimization process. Specifically, we first establish a comprehensive database to reflect human feedback for AI-generated omnidirectionals, termed OHF2024, which includes both subjective quality ratings evaluated from three perspectives and distortion-aware salient regions. Based on the constructed OHF2024 database, we propose two models with shared encoders based on the BLIP-2 model to evaluate the human visual experience and predict distortion-aware saliency for AI-generated omnidirectional images, which are named as BLIP2OIQA and BLIP2OISal, respectively. Finally, based on the proposed models, we present an automatic optimization process that utilizes the predicted visual experience scores and distortion regions to further enhance the visual quality of an AI-generated omnidirectional image. Extensive experiments show that our BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in the human visual experience evaluation task and the distortion-aware saliency prediction task for AI generated omnidirectional images, and can be effectively used in the optimization process. The database and codes will be released on https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.20978.pdf' target='_blank'>https://arxiv.org/pdf/2506.20978.pdf</a></span>   <span><a href='https://github.com/n4feng/ResponseQualityAssessment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20978">Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing research on Retrieval-Augmented Generation (RAG) primarily focuses on improving overall question-answering accuracy, often overlooking the quality of sub-claims within generated responses. Recent methods that attempt to improve RAG trustworthiness, such as through auto-evaluation metrics, lack probabilistic guarantees or require ground truth answers. To address these limitations, we propose Conformal-RAG, a novel framework inspired by recent applications of conformal prediction (CP) on large language models (LLMs). Conformal-RAG leverages CP and internal information from the RAG mechanism to offer statistical guarantees on response quality. It ensures group-conditional coverage spanning multiple sub-domains without requiring manual labelling of conformal sets, making it suitable for complex RAG applications. Compared to existing RAG auto-evaluation methods, Conformal-RAG offers statistical guarantees on the quality of refined sub-claims, ensuring response reliability without the need for ground truth answers. Additionally, our experiments demonstrate that by leveraging information from the RAG system, Conformal-RAG retains up to 60\% more high-quality sub-claims from the response compared to direct applications of CP to LLMs, while maintaining the same reliability guarantee.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2506.20200.pdf' target='_blank'>https://arxiv.org/pdf/2506.20200.pdf</a></span>   <span><a href='https://github.com/MS-IQA/MS-IQA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqiao Li, Chen Hui, Wei Zhang, Rui Liang, Chenyue Song, Feng Jiang, Haiqi Zhu, Zhixuan Li, Hong Huang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20200">MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical role in medical imaging, combining functional and anatomical information to aid in accurate diagnosis. However, image quality degradation due to noise, compression and other factors could potentially lead to diagnostic uncertainty and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT image, both low-level features like distortions and high-level features like organ anatomical structures affect the diagnostic value of the image. However, existing medical image quality assessment (IQA) methods are unable to account for both feature types simultaneously. In this work, we propose MS-IQA, a novel multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale features from various intermediate layers of ResNet and Swin Transformer, enhancing its ability of perceiving both local and global information. In addition, a multi-scale feature fusion module is also introduced to effectively combine high-level and low-level information through a dynamically weighted channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset, we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT images with quality scores assigned by radiologists. Experiments on our dataset and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed model has achieved superior performance against existing state-of-the-art methods in various IQA metrics. This work provides an accurate and efficient IQA method for PET/CT. Our code and dataset are available at https://github.com/MS-IQA/MS-IQA/.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2506.18102.pdf' target='_blank'>https://arxiv.org/pdf/2506.18102.pdf</a></span>   <span><a href='https://github.com/fywang12/InspireDebate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18102">InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at https://github.com/fywang12/InspireDebate.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2506.14642.pdf' target='_blank'>https://arxiv.org/pdf/2506.14642.pdf</a></span>   <span><a href='https://github.com/YukeXing/3DGS-IEval-15K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuke Xing, Jiarui Wang, Peizhi Niu, Wenjie Huang, Guangtao Zhai, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14642">3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2506.13827.pdf' target='_blank'>https://arxiv.org/pdf/2506.13827.pdf</a></span>   <span><a href='https://joyli-x.github.io/BPM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoying Li, Zhu Xu, Yuxin Peng, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13827">Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2506.12260.pdf' target='_blank'>https://arxiv.org/pdf/2506.12260.pdf</a></span>   <span><a href='https://github.com/urgent-challenge/urgent2026_challenge_track2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Wangyou Zhang, Chenda Li, Jiatong Shi, Shinji Watanabe, Yanmin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12260">Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment (SQA) aims to predict the perceived quality of speech signals under a wide range of distortions. It is inherently connected to speech enhancement (SE), which seeks to improve speech quality by removing unwanted signal components. While SQA models are widely used to evaluate SE performance, their potential to guide SE training remains underexplored. In this work, we investigate a training framework that leverages a SQA model, trained to predict multiple evaluation metrics from a public SE leaderboard, as a supervisory signal for SE. This approach addresses a key limitation of conventional SE objectives, such as SI-SNR, which often fail to align with perceptual quality and generalize poorly across evaluation metrics. Moreover, it enables training on real-world data where clean references are unavailable. Experiments on both simulated and real-world test sets show that SQA-guided training consistently improves performance across a range of quality metrics. Code and checkpoints are available at https://github.com/urgent-challenge/urgent2026_challenge_track2
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2506.07412.pdf' target='_blank'>https://arxiv.org/pdf/2506.07412.pdf</a></span>   <span><a href='https://github.com/chansongoal/Compressed-Feature-Quality-Assessment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changsheng Gao, Wei Zhou, Guosheng Lin, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07412">Compressed Feature Quality Assessment: Dataset and Baselines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread deployment of large models in resource-constrained environments has underscored the need for efficient transmission of intermediate feature representations. In this context, feature coding, which compresses features into compact bitstreams, becomes a critical component for scenarios involving feature transmission, storage, and reuse. However, this compression process inevitably introduces semantic degradation that is difficult to quantify with traditional metrics. To address this, we formalize the research problem of Compressed Feature Quality Assessment (CFQA), aiming to evaluate the semantic fidelity of compressed features. To advance CFQA research, we propose the first benchmark dataset, comprising 300 original features and 12000 compressed features derived from three vision tasks and four feature codecs. Task-specific performance degradation is provided as true semantic distortion for evaluating CFQA metrics. We systematically assess three widely used metrics -- MSE, cosine similarity, and Centered Kernel Alignment (CKA) -- in terms of their ability to capture semantic degradation. Our findings demonstrate the representativeness of the proposed dataset while underscoring the need for more sophisticated metrics capable of measuring semantic distortion in compressed features. This work advances the field by establishing a foundational benchmark and providing a critical resource for the community to explore CFQA. To foster further research, we release the dataset and all associated source code at https://github.com/chansongoal/Compressed-Feature-Quality-Assessment.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2506.04715.pdf' target='_blank'>https://arxiv.org/pdf/2506.04715.pdf</a></span>   <span><a href='https://github.com/QiZelu/AIGVEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelu Qi, Ping Shi, Chaoyang Zhang, Shuqi Wang, Fei Zhao, Da Pan, Zefeng Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04715">Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of AI-Generated Video (AIGV) technology has been remarkable in recent years, significantly transforming the paradigm of video content production. However, AIGVs still suffer from noticeable visual quality defects, such as noise, blurriness, frame jitter and low dynamic degree, which severely impact the user's viewing experience. Therefore, an effective automatic visual quality assessment is of great importance for AIGV content regulation and generative model improvement. In this work, we decompose the visual quality of AIGVs into three dimensions: technical quality, motion quality, and video semantics. For each dimension, we design corresponding encoder to achieve effective feature representation. Moreover, considering the outstanding performance of large language models (LLMs) in various vision and language tasks, we introduce a LLM as the quality regression module. To better enable the LLM to establish reasoning associations between multi-dimensional features and visual quality, we propose a specially designed multi-modal prompt engineering framework. Additionally, we incorporate LoRA fine-tuning technology during the training phase, allowing the LLM to better adapt to specific tasks. Our proposed method achieved \textbf{second place} in the NTIRE 2025 Quality Assessment of AI-Generated Content Challenge: Track 2 AI Generated video, demonstrating its effectiveness. Codes can be obtained at https://github.com/QiZelu/AIGVEval.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2506.03198.pdf' target='_blank'>https://arxiv.org/pdf/2506.03198.pdf</a></span>   <span><a href='https://haoyin116.github.io/FLEX_Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yin, Lijun Gu, Paritosh Parmar, Lin Xu, Tianxiao Guo, Weiwei Fu, Yang Zhang, Tianyou Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03198">FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing awareness of health and the growing desire for aesthetic physique, fitness has become a prevailing trend. However, the potential risks associated with fitness training, especially with weight-loaded fitness actions, cannot be overlooked. Action Quality Assessment (AQA), a technology that quantifies the quality of human action and provides feedback, holds the potential to assist fitness enthusiasts of varying skill levels in achieving better training outcomes. Nevertheless, current AQA methodologies and datasets are limited to single-view competitive sports scenarios and RGB modality and lack professional assessment and guidance of fitness actions. To address this gap, we propose the FLEX dataset, the first multi-modal, multi-action, large-scale dataset that incorporates surface electromyography (sEMG) signals into AQA. FLEX utilizes high-precision MoCap to collect 20 different weight-loaded actions performed by 38 subjects across 3 different skill levels for 10 repetitions each, containing 5 different views of the RGB video, 3D pose, sEMG, and physiological information. Additionally, FLEX incorporates knowledge graphs into AQA, constructing annotation rules in the form of penalty functions that map weight-loaded actions, action keysteps, error types, and feedback. We conducted various baseline methodologies on FLEX, demonstrating that multimodal data, multiview data, and fine-grained annotations significantly enhance model performance. FLEX not only advances AQA methodologies and datasets towards multi-modal and multi-action scenarios but also fosters the integration of artificial intelligence within the fitness domain. Dataset and code are available at https://haoyin116.github.io/FLEX_Dataset.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2506.01789.pdf' target='_blank'>https://arxiv.org/pdf/2506.01789.pdf</a></span>   <span><a href='https://github.com/datarubrics/datarubrics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, Niklas Muennighoff, Seungone Kim, Hanyang Zhao, Sudipta Kar, Kezia Erina Suryoraharjo, M. Farid Adilazuarda, En-Shiun Annie Lee, Ayu Purwarianti, Derry Tanti Wijaya, Monojit Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01789">Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2506.01738.pdf' target='_blank'>https://arxiv.org/pdf/2506.01738.pdf</a></span>   <span><a href='https://storm-bench.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhong Wang, Shuo Tong, Jian liu, Dongqi Tang, Jintai Chen, Haochao Ying, Hongxia Xu, Danny Chen, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01738">STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual rating is an essential capability of artificial intelligence (AI) for multi-dimensional quantification of visual content, primarily applied in ordinal regression (OR) tasks such as image quality assessment, facial age estimation, and medical image grading. However, current multi-modal large language models (MLLMs) under-perform in such visual rating ability while also suffering the lack of relevant datasets and benchmarks. In this work, we collect and present STORM, a data collection and benchmark for Stimulating Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating. STORM encompasses 14 ordinal regression datasets across five common visual rating domains, comprising 655K image-level pairs and the corresponding carefully curated VQAs. Importantly, we also propose a coarse-to-fine processing pipeline that dynamically considers label candidates and provides interpretable thoughts, providing MLLMs with a general and trustworthy ordinal thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot performance of MLLMs in scenarios requiring understanding of the essential common ordinal relationships of rating labels. Extensive experiments demonstrate the effectiveness of our framework and shed light on better fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models are available on the following webpage to support further research in this area. Datasets and codes are released on the project page: https://storm-bench.github.io/.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.24787.pdf' target='_blank'>https://arxiv.org/pdf/2505.24787.pdf</a></span>   <span><a href='https://github.com/yczhou001/LongBench-T2I' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Zhou, Jiahao Yuan, Qianning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24787">Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model's ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at https://github.com/yczhou001/LongBench-T2I.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.19065.pdf' target='_blank'>https://arxiv.org/pdf/2505.19065.pdf</a></span>   <span><a href='https://github.com/Future-IQA/MMP-2k' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Chang, Zhengyi Li, Jianxun Lou, Zhen Qiu, Hanhe Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19065">MMP-2K: A Benchmark Multi-Labeled Macro Photography Image Quality Assessment Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Macro photography (MP) is a specialized field of photography that captures objects at an extremely close range, revealing tiny details. Although an accurate macro photography image quality assessment (MPIQA) metric can benefit macro photograph capturing, which is vital in some domains such as scientific research and medical applications, the lack of MPIQA data limits the development of MPIQA metrics. To address this limitation, we conducted a large-scale MPIQA study. Specifically, to ensure diversity both in content and quality, we sampled 2,000 MP images from 15,700 MP images, collected from three public image websites. For each MP image, 17 (out of 21 after outlier removal) quality ratings and a detailed quality report of distortion magnitudes, types, and positions are gathered by a lab study. The images, quality ratings, and quality reports form our novel multi-labeled MPIQA database, MMP-2k. Experimental results showed that the state-of-the-art generic IQA metrics underperform on MP images. The database and supplementary materials are available at https://github.com/Future-IQA/MMP-2k.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.16815.pdf' target='_blank'>https://arxiv.org/pdf/2505.16815.pdf</a></span>   <span><a href='https://github.com/lcysyzxdxc/EmbodiedIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16815">Perceptual Quality Assessment for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: https://github.com/lcysyzxdxc/EmbodiedIQA
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.15061.pdf' target='_blank'>https://arxiv.org/pdf/2505.15061.pdf</a></span>   <span><a href='https://github.com/unilight/sheet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Chin Huang, Erica Cooper, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15061">SHEET: A Multi-purpose Open-source Speech Human Evaluation Estimation Toolkit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SHEET, a multi-purpose open-source toolkit designed to accelerate subjective speech quality assessment (SSQA) research. SHEET stands for the Speech Human Evaluation Estimation Toolkit, which focuses on data-driven deep neural network-based models trained to predict human-labeled quality scores of speech samples. SHEET provides comprehensive training and evaluation scripts, multi-dataset and multi-model support, as well as pre-trained models accessible via Torch Hub and HuggingFace Spaces. To demonstrate its capabilities, we re-evaluated SSL-MOS, a speech self-supervised learning (SSL)-based SSQA model widely used in recent scientific papers, on an extensive list of speech SSL models. Experiments were conducted on two representative SSQA datasets named BVCC and NISQA, and we identified the optimal speech SSL model, whose performance surpassed the original SSL-MOS implementation and was comparable to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2505.10824.pdf' target='_blank'>https://arxiv.org/pdf/2505.10824.pdf</a></span>   <span><a href='https://github.com/yyyykf/FMQM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaifa Yang, Qi Yang, Zhu Li, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10824">Textured mesh Quality Assessment using Geometry and Color Field Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Textured mesh quality assessment (TMQA) is critical for various 3D mesh applications. However, existing TMQA methods often struggle to provide accurate and robust evaluations. Motivated by the effectiveness of fields in representing both 3D geometry and color information, we propose a novel point-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes signed distance fields and a newly proposed color field named nearest surface point color field to realize effective mesh feature description. Four features related to visual perception are extracted from the geometry and color fields: geometry similarity, geometry gradient similarity, space color distribution similarity, and space color gradient similarity. Experimental results on three benchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA) TMQA metrics. Furthermore, FMQM exhibits low computational complexity, making it a practical and efficient solution for real-world applications in 3D graphics and visualization. Our code is publicly available at: https://github.com/yyyykf/FMQM.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2505.05423.pdf' target='_blank'>https://arxiv.org/pdf/2505.05423.pdf</a></span>   <span><a href='https://github.com/zhangr2021/TransProQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05423">LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impact of Large Language Models (LLMs) has extended into literary domains. However, existing evaluation metrics prioritize mechanical accuracy over artistic expression and tend to overrate machine translation as being superior to human translation from experienced professionals. In the long run, this bias could result in an irreversible decline in translation quality and cultural authenticity. In response to the urgent need for a specialized literary evaluation metric, we introduce LiTransProQA, a novel, reference-free, LLM-based question-answering framework designed for literary translation evaluation. LiTransProQA uniquely integrates insights from professional literary translators and researchers, focusing on critical elements in literary quality assessment such as literary devices, cultural understanding, and authorial voice. Our extensive evaluation shows that while literary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially outperforms current metrics, achieving up to 0.07 gain in correlation and surpassing the best state-of-the-art metrics by over 15 points in adequacy assessments. Incorporating professional translator insights as weights further improves performance, highlighting the value of translator inputs. Notably, LiTransProQA reaches human-level evaluation performance comparable to trained student evaluators. It shows broad applicability to open-source models like LLaMa3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and training-free tool for evaluating literary translations that require local processing due to copyright or ethical considerations. The code and datasets are available under: https://github.com/zhangr2021/TransProQA.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2505.03007.pdf' target='_blank'>https://arxiv.org/pdf/2505.03007.pdf</a></span>   <span><a href='https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolay Safonov, Alexey Bryncev, Andrey Moskalenko, Dmitry Kulikov, Dmitry Vatolin, Radu Timofte, Haibo Lei, Qifan Gao, Qing Luo, Yaqing Li, Jie Song, Shaozhe Hao, Meisong Zheng, Jingyi Xu, Chengbin Wu, Jiahui Liu, Ying Chen, Xin Deng, Mai Xu, Peipei Liang, Jie Ma, Junjie Jin, Yingxue Pang, Fangzhou Luo, Kai Chen, Shijie Zhao, Mingyang Wu, Renjie Li, Yushen Zuo, Shengyun Zhong, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03007">NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an overview of the NTIRE 2025 Challenge on UGC Video Enhancement. The challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. The goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. Given the widespread use of UGC on short-form video platforms, this task holds substantial practical importance. The evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. The challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. The outcomes may provide insights into the state-of-the-art in UGC video enhancement and highlight emerging trends and effective strategies in this evolving research area. All data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2505.02134.pdf' target='_blank'>https://arxiv.org/pdf/2505.02134.pdf</a></span>   <span><a href='https://github.com/LabShuHangGU/HiLLIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaorui Zhao, Xinyue Zhou, Peibei Cao, Junyu Lou, Shuhang Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02134">HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (LLIE). In this paper, we propose a human-in-the-loop LLIE training framework that improves the visual quality of unsupervised LLIE model outputs through iterative training stages, named HiLLIE. At each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. Subsequently, we employ a tailored image quality assessment (IQA) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. With only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the IQA model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing LLIE results. Extensive experiments demonstrate that our approach significantly improves unsupervised LLIE model performance in terms of both quantitative and qualitative performance. The code and collected ranking dataset will be available at https://github.com/LabShuHangGU/HiLLIE.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2504.21682.pdf' target='_blank'>https://arxiv.org/pdf/2504.21682.pdf</a></span>   <span><a href='https://github.com/shuyansy/Visual-Text-Processing-survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21682">Visual Text Processing: A Comprehensive Review and Unified Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at https://github.com/shuyansy/Visual-Text-Processing-survey.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.15003.pdf' target='_blank'>https://arxiv.org/pdf/2504.15003.pdf</a></span>   <span><a href='https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Xijun Wang, Bingchen Li, Kun Yuan, Yizhen Shao, Suhang Yao, Ming Sun, Chao Zhou, Radu Timofte, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15003">NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we build the first benchmark dataset for short-form UGC Image Super-resolution in the wild, termed KwaiSR, intending to advance the research on developing image super-resolution algorithms for short-form UGC platforms. This dataset is collected from the Kwai Platform, which is composed of two parts, i.e., synthetic and wild parts. Among them, the synthetic dataset, including 1,900 image pairs, is produced by simulating the degradation following the distribution of real-world low-quality short-form UGC images, aiming to provide the ground truth for training and objective comparison in the validation/testing. The wild dataset contains low-quality images collected directly from the Kwai Platform, which are filtered using the quality assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset contains 1800 synthetic image pairs and 1900 wild images, which are divided into training, validation, and testing parts with a ratio of 8:1:1. Based on the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form UGC Video quality assessment and enhancement, which attracts lots of researchers to develop the algorithm for it. The results of this competition have revealed that our KwaiSR dataset is pretty challenging for existing Image SR methods, which is expected to lead to a new direction in the image super-resolution field. The dataset can be found from https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2504.14600.pdf' target='_blank'>https://arxiv.org/pdf/2504.14600.pdf</a></span>   <span><a href='https://github.com/zhengchen1999/NTIRE2025_RealWorld_Face_Restoration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Chen, Jingkai Wang, Kai Liu, Jue Gong, Lei Sun, Zongwei Wu, Radu Timofte, Yulun Zhang, Jianxing Zhang, Jinlong Wu, Jun Wang, Zheng Xie, Hakjae Jeon, Suejin Han, Hyung-Ju Chun, Hyunhee Park, Zhicun Yin, Junjie Chen, Ming Liu, Xiaoming Li, Chao Zhou, Wangmeng Zuo, Weixia Zhang, Dingquan Li, Kede Ma, Yun Zhang, Zhuofan Zheng, Yuyue Liu, Shizhen Tang, Zihao Zhang, Yi Ning, Hao Jiang, Wenjie An, Kangmeng Yu, Chenyang Wang, Kui Jiang, Xianming Liu, Junjun Jiang, Yingfu Zhang, Gang He, Siqi Wang, Kepeng Xu, Zhenyang Liu, Changxin Zhou, Shanlan Shen, Yubo Duan, Yiang Chen, Jin Guo, Mengru Yang, Jen-Wei Lee, Chia-Ming Lee, Chih-Chung Hsu, Hu Peng, Chunming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14600">NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides a review of the NTIRE 2025 challenge on real-world face restoration, highlighting the proposed solutions and the resulting outcomes. The challenge focuses on generating natural, realistic outputs while maintaining identity consistency. Its goal is to advance state-of-the-art solutions for perceptual quality and realism, without imposing constraints on computational resources or training data. The track of the challenge evaluates performance using a weighted image quality assessment (IQA) score and employs the AdaFace model as an identity checker. The competition attracted 141 registrants, with 13 teams submitting valid models, and ultimately, 10 teams achieved a valid score in the final ranking. This collaborative effort advances the performance of real-world face restoration while offering an in-depth overview of the latest trends in the field.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2504.13131.pdf' target='_blank'>https://arxiv.org/pdf/2504.13131.pdf</a></span>   <span><a href='https://github.com/lixinustc/KVQE-' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Kun Yuan, Bingchen Li, Fengbin Guan, Yizhen Shao, Zihao Yu, Xijun Wang, Yiting Lu, Wei Luo, Suhang Yao, Ming Sun, Chao Zhou, Zhibo Chen, Radu Timofte, Yabin Zhang, Ao-Xiang Zhang, Tianwu Zhi, Jianzhao Liu, Yang Li, Jingwen Xu, Yiting Liao, Yushen Zuo, Mingyang Wu, Renjie Li, Shengyun Zhong, Zhengzhong Tu, Yufan Liu, Xiangguang Chen, Zuowei Cao, Minhao Tang, Shan Liu, Kexin Zhang, Jingfen Xie, Yan Wang, Kai Chen, Shijie Zhao, Yunchen Zhang, Xiangkai Xu, Hong Gao, Ji Shi, Yiming Bao, Xiugang Dong, Xiangsheng Zhou, Yaofeng Tu, Ying Liang, Yiwen Wang, Xinning Chai, Yuxuan Zhang, Zhengxue Cheng, Yingsheng Qin, Yucai Yang, Rong Xie, Li Song, Wei Sun, Kang Fu, Linhan Cao, Dandan Zhu, Kaiwei Zhang, Yucheng Zhu, Zicheng Zhang, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Zhi Jin, Jiawei Wu, Wei Wang, Wenjian Zhang, Yuhai Lan, Gaoxiong Yi, Hengyuan Na, Wang Luo, Di Wu, MingYin Bai, Jiawang Du, Zilong Lu, Zhenyu Jiang, Hui Zeng, Ziguan Cui, Zongliang Gan, Guijin Tang, Xinglin Xie, Kehuan Song, Xiaoqiang Lu, Licheng Jiao, Fang Liu, Xu Liu, Puhua Chen, Ha Thu Nguyen, Katrien De Moor, Seyed Ali Amirshahi, Mohamed-Chaker Larabi, Qi Tang, Linfeng He, Zhiyong Gao, Zixuan Gao, Guohua Zhang, Zhiye Huang, Yi Deng, Qingmiao Jiang, Lu Chen, Yi Yang, Xi Liao, Nourine Mohammed Nadir, Yuxuan Jiang, Qiang Zhu, Siyue Teng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull, Meiqin Liu, Chao Yao, Yao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13131">NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement. The challenge comprises two tracks: (i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image Super-Resolution (KwaiSR). Track 1 aims to advance the development of lightweight and efficient video quality assessment (VQA) models, with an emphasis on eliminating reliance on model ensembles, redundant weights, and other computationally expensive components in the previous IQA/VQA competitions. Track 2 introduces a new short-form UGC dataset tailored for single image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800 synthetically generated S-UGC image pairs and 1,900 real-world S-UGC images, which are split into training, validation, and test sets using a ratio of 8:1:1. The primary objective of the challenge is to drive research that benefits the user experience of short-form UGC platforms such as Kwai and TikTok. This challenge attracted 266 participants and received 18 valid final submissions with corresponding fact sheets, significantly contributing to the progress of short-form UGC VQA and image superresolution. The project is publicly available at https://github.com/lixinustc/KVQE- ChallengeCVPR-NTIRE2025.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2504.09530.pdf' target='_blank'>https://arxiv.org/pdf/2504.09530.pdf</a></span>   <span><a href='https://github.com/shuchaoduan/TraMP-Former' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchao Duan, Amirhossein Dadashzadeh, Alan Whone, Majid Mirmehdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09530">Trajectory-guided Motion Perception for Facial Expression Quality Assessment in Neurological Disorders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated facial expression quality assessment (FEQA) in neurological disorders is critical for enhancing diagnostic accuracy and improving patient care, yet effectively capturing the subtle motions and nuances of facial muscle movements remains a challenge. We propose to analyse facial landmark trajectories, a compact yet informative representation, that encodes these subtle motions from a high-level structural perspective. Hence, we introduce Trajectory-guided Motion Perception Transformer (TraMP-Former), a novel FEQA framework that fuses landmark trajectory features for fine-grained motion capture with visual semantic cues from RGB frames, ultimately regressing the combined features into a quality score. Extensive experiments demonstrate that TraMP-Former achieves new state-of-the-art performance on benchmark datasets with neurological disorders, including PFED5 (up by 6.51%) and an augmented Toronto NeuroFace (up by 7.62%). Our ablation studies further validate the efficiency and effectiveness of landmark trajectories in FEQA. Our code is available at https://github.com/shuchaoduan/TraMP-Former.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2504.08125.pdf' target='_blank'>https://arxiv.org/pdf/2504.08125.pdf</a></span>   <span><a href='https://shalini-maiti.github.io/gen3deval.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://shalini-maiti.github.io/gen3deval.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalini Maiti, Lourdes Agapito, Filippos Kokkinos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08125">Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in text-to-3D generation require robust and scalable evaluation metrics that align closely with human judgment, a need unmet by current metrics such as PSNR and CLIP, which require ground-truth data or focus only on prompt fidelity. To address this, we introduce Gen3DEval, a novel evaluation framework that leverages vision large language models (vLLMs) specifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates text fidelity, appearance, and surface quality by analyzing 3D surface normals, without requiring ground-truth comparisons, bridging the gap between automated metrics and user preferences. Compared to state-of-the-art task-agnostic models, Gen3DEval demonstrates superior performance in user-aligned evaluations, placing it as a comprehensive and accessible benchmark for future research on text-to-3D generation. The project page can be found here: \href{https://shalini-maiti.github.io/gen3deval.github.io/}{https://shalini-maiti.github.io/gen3deval.github.io/}.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2504.06301.pdf' target='_blank'>https://arxiv.org/pdf/2504.06301.pdf</a></span>   <span><a href='https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Jenadeleh, Jon Sneyers, Panqi Jia, Shima Mohammadi, Joao Ascenso, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06301">Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based image compression methods have recently emerged as promising alternatives to traditional codecs, offering improved rate-distortion performance and perceptual quality. JPEG AI represents the latest standardized framework in this domain, leveraging deep neural networks for high-fidelity image reconstruction. In this study, we present a comprehensive subjective visual quality assessment of JPEG AI-compressed images using the JPEG AIC-3 methodology, which quantifies perceptual differences in terms of Just Noticeable Difference (JND) units. We generated a dataset of 50 compressed images with fine-grained distortion levels from five diverse sources. A large-scale crowdsourced experiment collected 96,200 triplet responses from 459 participants. We reconstructed JND-based quality scales using a unified model based on boosted and plain triplet comparisons. Additionally, we evaluated the alignment of multiple objective image quality metrics with human perception in the high-fidelity range. The CVVDP metric achieved the overall highest performance; however, most metrics including CVVDP were overly optimistic in predicting the quality of JPEG AI-compressed images. These findings emphasize the necessity for rigorous subjective evaluations in the development and benchmarking of modern image codecs, particularly in the high-fidelity range. Another technical contribution is the introduction of the well-known Meng-Rosenthal-Rubin statistical test to the field of Quality of Experience research. This test can reliably assess the significance of difference in performance of quality metrics in terms of correlation between metrics and ground truth. The complete dataset, including all subjective scores, is publicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2504.02522.pdf' target='_blank'>https://arxiv.org/pdf/2504.02522.pdf</a></span>   <span><a href='https://github.com/FBehrad/Charm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02522">Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capacity of Vision transformers (ViTs) to handle variable-sized inputs is often constrained by computational complexity and batch processing limitations. Consequently, ViTs are typically trained on small, fixed-size images obtained through downscaling or cropping. While reducing computational burden, these methods result in significant information loss, negatively affecting tasks like image aesthetic assessment. We introduce Charm, a novel tokenization approach that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale information simultaneously. Charm prioritizes high-resolution details in specific regions while downscaling others, enabling shorter fixed-size input sequences for ViTs while incorporating essential information. Charm is designed to be compatible with pre-trained ViTs and their learned positional embeddings. By providing multiscale input and introducing variety to input tokens, Charm improves ViT performance and generalizability for image aesthetic assessment. We avoid cropping or changing the aspect ratio to further preserve information. Extensive experiments demonstrate significant performance improvements on various image aesthetic and quality assessment datasets (up to 8.1 %) using a lightweight ViT backbone. Code and pre-trained models are available at https://github.com/FBehrad/Charm.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2504.01655.pdf' target='_blank'>https://arxiv.org/pdf/2504.01655.pdf</a></span>   <span><a href='https://github.com/yeppp27/Q-Adapt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiting Lu, Xin Li, Haoning Wu, Bingchen Li, Weisi Lin, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01655">Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality evaluator, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases. The source code is publicly available at https://github.com/yeppp27/Q-Adapt.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2503.23911.pdf' target='_blank'>https://arxiv.org/pdf/2503.23911.pdf</a></span>   <span><a href='https://github.com/Harrison21/FineCausal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisheng Han, Kanglei Zhou, Amir Atapour-Abarghouei, Xiaohui Liang, Hubert P. H. Shum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23911">FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action quality assessment (AQA) is critical for evaluating athletic performance, informing training strategies, and ensuring safety in competitive sports. However, existing deep learning approaches often operate as black boxes and are vulnerable to spurious correlations, limiting both their reliability and interpretability. In this paper, we introduce FineCausal, a novel causal-based framework that achieves state-of-the-art performance on the FineDiving-HM dataset. Our approach leverages a Graph Attention Network-based causal intervention module to disentangle human-centric foreground cues from background confounders, and incorporates a temporal causal attention module to capture fine-grained temporal dependencies across action stages. This dual-module strategy enables FineCausal to generate detailed spatio-temporal representations that not only achieve state-of-the-art scoring performance but also provide transparent, interpretable feedback on which features drive the assessment. Despite its strong performance, FineCausal requires extensive expert knowledge to define causal structures and depends on high-quality annotations, challenges that we discuss and address as future research directions. Code is available at https://github.com/Harrison21/FineCausal.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2503.22679.pdf' target='_blank'>https://arxiv.org/pdf/2503.22679.pdf</a></span>   <span><a href='https://github.com/lwq20020127/Q-Insight' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22679">Q-Insight: Understanding Image Quality via Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at https://github.com/lwq20020127/Q-Insight.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2503.18559.pdf' target='_blank'>https://arxiv.org/pdf/2503.18559.pdf</a></span>   <span><a href='https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18559">AMD-Hummingbird: Towards an Efficient Text-to-Video Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2503.13073.pdf' target='_blank'>https://arxiv.org/pdf/2503.13073.pdf</a></span>   <span><a href='https://github.com/mmic-lcl/Datasets-and-benchmark-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhao, Jinquan Yan, Chenglong Li, Xiao Wang, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13073">DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical remote sensing image dehazing presents significant challenges due to its extensive spatial scale and highly non-uniform haze distribution, which traditional single-image dehazing methods struggle to address effectively. While Synthetic Aperture Radar (SAR) imagery offers inherently haze-free reference information for large-scale scenes, existing SAR-guided dehazing approaches face two critical limitations: the integration of SAR information often diminishes the quality of haze-free regions, and the instability of feature quality further exacerbates cross-modal domain shift. To overcome these challenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built on a progressive haze decoupling fusion strategy. Our approach incorporates two key innovations: a Haze Perception and Decoupling Module (HPDM) that dynamically identifies haze-affected regions through optical-SAR difference analysis, and a Progressive Fusion Module (PFM) that mitigates domain shift through a two-stage fusion process based on feature quality assessment. To facilitate research in this domain, we present MRSHaze, a large-scale benchmark dataset comprising 8,000 pairs of temporally synchronized, precisely geo-registered SAR-optical images with high resolution and diverse haze conditions. Extensive experiments demonstrate that DehazeMamba significantly outperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR and substantial enhancements in downstream tasks such as semantic segmentation. The dataset is available at https://github.com/mmic-lcl/Datasets-and-benchmark-code.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2503.11221.pdf' target='_blank'>https://arxiv.org/pdf/2503.11221.pdf</a></span>   <span><a href='https://tianhewu.github.io/A-FINE-page.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Du Chen, Tianhe Wu, Kede Ma, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11221">Toward Generalized Image Quality Assessment: Relaxing the Perfect Reference Quality Assumption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-reference image quality assessment (FR-IQA) generally assumes that reference images are of perfect quality. However, this assumption is flawed due to the sensor and optical limitations of modern imaging systems. Moreover, recent generative enhancement methods are capable of producing images of higher quality than their original. All of these challenge the effectiveness and applicability of current FR-IQA models. To relax the assumption of perfect reference image quality, we build a large-scale IQA database, namely DiffIQA, containing approximately 180,000 images generated by a diffusion-based image enhancer with adjustable hyper-parameters. Each image is annotated by human subjects as either worse, similar, or better quality compared to its reference. Building on this, we present a generalized FR-IQA model, namely Adaptive Fidelity-Naturalness Evaluator (A-FINE), to accurately assess and adaptively combine the fidelity and naturalness of a test image. A-FINE aligns well with standard FR-IQA when the reference image is much more natural than the test image. We demonstrate by extensive experiments that A-FINE surpasses standard FR-IQA models on well-established IQA datasets and our newly created DiffIQA. To further validate A-FINE, we additionally construct a super-resolution IQA benchmark (SRIQA-Bench), encompassing test images derived from ten state-of-the-art SR methods with reliable human quality annotations. Tests on SRIQA-Bench re-affirm the advantages of A-FINE. The code and dataset are available at https://tianhewu.github.io/A-FINE-page.github.io/.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2503.10259.pdf' target='_blank'>https://arxiv.org/pdf/2503.10259.pdf</a></span>   <span><a href='https://github.com/qyp2000/KVQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Qu, Kun Yuan, Qizhi Xie, Ming Sun, Chao Zhou, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10259">KVQ: Boosting Video Quality Assessment via Saliency-guided Local Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA), which intends to predict the perceptual quality of videos, has attracted increasing attention. Due to factors like motion blur or specific distortions, the quality of different regions in a video varies. Recognizing the region-wise local quality within a video is beneficial for assessing global quality and can guide us in adopting fine-grained enhancement or transcoding strategies. Due to the heavy cost of annotating region-wise quality, the lack of ground truth constraints from relevant datasets further complicates the utilization of local perception. Inspired by the Human Visual System (HVS) that links global quality to the local texture of different regions and their visual saliency, we propose a Kaleidoscope Video Quality Assessment (KVQ) framework, which aims to effectively assess both saliency and local texture, thereby facilitating the assessment of global quality. Our framework extracts visual saliency and allocates attention using Fusion-Window Attention (FWA) while incorporating a Local Perception Constraint (LPC) to mitigate the reliance of regional texture perception on neighboring areas. KVQ obtains significant improvements across multiple scenarios on five VQA benchmarks compared to SOTA methods. Furthermore, to assess local perception, we establish a new Local Perception Visual Quality (LPVQ) dataset with region-wise annotations. Experimental results demonstrate the capability of KVQ in perceiving local distortions. KVQ models and the LPVQ dataset will be available at https://github.com/qyp2000/KVQ.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2503.10078.pdf' target='_blank'>https://arxiv.org/pdf/2503.10078.pdf</a></span>   <span><a href='https://github.com/lcysyzxdxc/MPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Yuan Tian, Xiaoyue Ling, Zicheng Zhang, Haodong Duan, Haoning Wu, Ziheng Jia, Xiaohong Liu, Xiongkuo Min, Guo Lu, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10078">Image Quality Assessment: From Human to Machine Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) based on human subjective preferences has undergone extensive research in the past decades. However, with the development of communication protocols, the visual data consumption volume of machines has gradually surpassed that of humans. For machines, the preference depends on downstream tasks such as segmentation and detection, rather than visual appeal. Considering the huge gap between human and machine visual systems, this paper proposes the topic: Image Quality Assessment for Machine Vision for the first time. Specifically, we (1) defined the subjective preferences of machines, including downstream tasks, test models, and evaluation metrics; (2) established the Machine Preference Database (MPD), which contains 2.25M fine-grained annotations and 30k reference/distorted image pair instances; (3) verified the performance of mainstream IQA algorithms on MPD. Experiments show that current IQA metrics are human-centric and cannot accurately characterize machine preferences. We sincerely hope that MPD can promote the evolution of IQA from human to machine preferences. Project page is on: https://github.com/lcysyzxdxc/MPD.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2503.09197.pdf' target='_blank'>https://arxiv.org/pdf/2503.09197.pdf</a></span>   <span><a href='https://github.com/Q-Future/Q-SiT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Haoning Wu, Ziheng Jia, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09197">Teaching LMMs for Image Quality Scoring and Interpreting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality scoring and interpreting are two fundamental components of Image Quality Assessment (IQA). The former quantifies image quality, while the latter enables descriptive question answering about image quality. Traditionally, these two tasks have been addressed independently. However, from the perspective of the Human Visual System (HVS) and the Perception-Decision Integration Model, they are inherently interconnected: interpreting serves as the foundation for scoring, while scoring provides an abstract summary of interpreting. Thus, unifying these capabilities within a single model is both intuitive and logically coherent. In this paper, we propose Q-SiT (Quality Scoring and Interpreting joint Teaching), a unified framework that enables large multimodal models (LMMs) to learn both image quality scoring and interpreting simultaneously. We achieve this by transforming conventional IQA datasets into learnable question-answering datasets and incorporating human-annotated quality interpreting data for training. Furthermore, we introduce an efficient scoring & interpreting balance strategy, which first determines the optimal data mix ratio on lightweight LMMs and then maps this ratio to primary LMMs for fine-tuning adjustment. This strategy not only mitigates task interference and enhances cross-task knowledge transfer but also significantly reduces computational costs compared to direct optimization on full-scale LMMs. With this joint learning framework and corresponding training strategy, we develop Q-SiT, the first model capable of simultaneously performing image quality scoring and interpreting tasks, along with its lightweight variant, Q-SiT-mini. Experimental results demonstrate that Q-SiT achieves strong performance in both tasks with superior generalization IQA abilities.Project page at https://github.com/Q-Future/Q-SiT.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2503.02330.pdf' target='_blank'>https://arxiv.org/pdf/2503.02330.pdf</a></span>   <span><a href='https://github.com/srcn-ivl/SiamVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guotao Shen, Ziheng Yan, Xin Jin, Longhai Wu, Jie Chen, Ilhyun Cho, Cheul-Hee Hahm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02330">Exploring Simple Siamese Network for High-Resolution Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the research of video quality assessment (VQA), two-branch network has emerged as a promising solution. It decouples VQA with separate technical and aesthetic branches to measure the perception of low-level distortions and high-level semantics respectively. However, we argue that while technical and aesthetic perspectives are complementary, the technical perspective itself should be measured in semantic-aware manner. We hypothesize that existing technical branch struggles to perceive the semantics of high-resolution videos, as it is trained on local mini-patches sampled from videos. This issue can be hidden by apparently good results on low-resolution videos, but indeed becomes critical for high-resolution VQA. This work introduces SiamVQA, a simple but effective Siamese network for highre-solution VQA. SiamVQA shares weights between technical and aesthetic branches, enhancing the semantic perception ability of technical branch to facilitate technical-quality representation learning. Furthermore, it integrates a dual cross-attention layer for fusing technical and aesthetic features. SiamVQA achieves state-of-the-art accuracy on high-resolution benchmarks, and competitive results on lower-resolution benchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2502.19917.pdf' target='_blank'>https://arxiv.org/pdf/2502.19917.pdf</a></span>   <span><a href='https://github.com/HITsz-TMG/ViSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19917">Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To improve Multimodal Large Language Models' (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach via \textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5\% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at https://github.com/HITsz-TMG/ViSA.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2502.19644.pdf' target='_blank'>https://arxiv.org/pdf/2502.19644.pdf</a></span>   <span><a href='https://github.com/ZhouKanglei/ASAL_CVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Zikai Hao, Liyuan Wang, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19644">Adaptive Score Alignment Learning for Continual Perceptual Quality Assessment of 360-Degree Videos in Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Reality Video Quality Assessment (VR-VQA) aims to evaluate the perceptual quality of 360-degree videos, which is crucial for ensuring a distortion-free user experience. Traditional VR-VQA methods trained on static datasets with limited distortion diversity struggle to balance correlation and precision. This becomes particularly critical when generalizing to diverse VR content and continually adapting to dynamic and evolving video distribution variations. To address these challenges, we propose a novel approach for assessing the perceptual quality of VR videos, Adaptive Score Alignment Learning (ASAL). ASAL integrates correlation loss with error loss to enhance alignment with human subjective ratings and precision in predicting perceptual quality. In particular, ASAL can naturally adapt to continually changing distributions through a feature space smoothing process that enhances generalization to unseen content. To further improve continual adaptation to dynamic VR environments, we extend ASAL with adaptive memory replay as a novel Continul Learning (CL) framework. Unlike traditional CL models, ASAL utilizes key frame extraction and feature adaptation to address the unique challenges of non-stationary variations with both the computation and storage restrictions of VR devices. We establish a comprehensive benchmark for VR-VQA and its CL counterpart, introducing new data splits and evaluation metrics. Our experiments demonstrate that ASAL outperforms recent strong baseline models, achieving overall correlation gains of up to 4.78\% in the static joint training setting and 12.19\% in the dynamic CL setting on various datasets. This validates the effectiveness of ASAL in addressing the inherent challenges of VR-VQA.Our code is available at https://github.com/ZhouKanglei/ASAL_CVQA.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2502.19046.pdf' target='_blank'>https://arxiv.org/pdf/2502.19046.pdf</a></span>   <span><a href='https://github.com/WenJuing/Max360IQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Ziwen Tan, Yuming Fang, Jiale Rao, Yifan Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19046">Max360IQ: Blind Omnidirectional Image Quality Assessment with Multi-axis Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional image, also called 360-degree image, is able to capture the entire 360-degree scene, thereby providing more realistic immersive feelings for users than general 2D image and stereoscopic image. Meanwhile, this feature brings great challenges to measuring the perceptual quality of omnidirectional images, which is closely related to users' quality of experience, especially when the omnidirectional images suffer from non-uniform distortion. In this paper, we propose a novel and effective blind omnidirectional image quality assessment (BOIQA) model with multi-axis attention (Max360IQ), which can proficiently measure not only the quality of uniformly distorted omnidirectional images but also the quality of non-uniformly distorted omnidirectional images. Specifically, the proposed Max360IQ is mainly composed of a backbone with stacked multi-axis attention modules for capturing both global and local spatial interactions of extracted viewports, a multi-scale feature integration (MSFI) module to fuse multi-scale features and a quality regression module with deep semantic guidance for predicting the quality of omnidirectional images. Experimental results demonstrate that the proposed Max360IQ outperforms the state-of-the-art Assessor360 by 3.6\% in terms of SRCC on the JUFE database with non-uniform distortion, and gains improvement of 0.4\% and 0.8\% in terms of SRCC on the OIQA and CVIQ databases, respectively. The source code is available at https://github.com/WenJuing/Max360IQ.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2502.15271.pdf' target='_blank'>https://arxiv.org/pdf/2502.15271.pdf</a></span>   <span><a href='https://github.com/WenJuing/IQCaption360' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Ziwen Tan, Yuming Fang, Junjie Chen, Wenhui Jiang, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15271">Omnidirectional Image Quality Captioning: A Large-scale Database and A New Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fast growing application of omnidirectional images calls for effective approaches for omnidirectional image quality assessment (OIQA). Existing OIQA methods have been developed and tested on homogeneously distorted omnidirectional images, but it is hard to transfer their success directly to the heterogeneously distorted omnidirectional images. In this paper, we conduct the largest study so far on OIQA, where we establish a large-scale database called OIQ-10K containing 10,000 omnidirectional images with both homogeneous and heterogeneous distortions. A comprehensive psychophysical study is elaborated to collect human opinions for each omnidirectional image, together with the spatial distributions (within local regions or globally) of distortions, and the head and eye movements of the subjects. Furthermore, we propose a novel multitask-derived adaptive feature-tailoring OIQA model named IQCaption360, which is capable of generating a quality caption for an omnidirectional image in a manner of textual template. Extensive experiments demonstrate the effectiveness of IQCaption360, which outperforms state-of-the-art methods by a significant margin on the proposed OIQ-10K database. The OIQ-10K database and the related source codes are available at https://github.com/WenJuing/IQCaption360.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2502.15167.pdf' target='_blank'>https://arxiv.org/pdf/2502.15167.pdf</a></span>   <span><a href='https://github.com/strawhatboy/M3-AGIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuan Cui, Kejiang Chen, Zhihua Wei, Wen Shen, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15167">M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of AI-generated image (AIGI) models presents new challenges for evaluating image quality, particularly across three aspects: perceptual quality, prompt correspondence, and authenticity. To address these challenges, we introduce M3-AGIQA, a comprehensive framework that leverages Multimodal Large Language Models (MLLMs) to enable more human-aligned, holistic evaluation of AI-generated images across both visual and textual domains. Besides, our framework features a structured multi-round evaluation process, generating and analyzing intermediate image descriptions to provide deeper insight into these three aspects. By aligning model outputs more closely with human judgment, M3-AGIQA delivers robust and interpretable quality scores. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance on tested datasets and aspects, and exhibits strong generalizability in most cross-dataset settings. Code is available at https://github.com/strawhatboy/M3-AGIQA.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2502.10263.pdf' target='_blank'>https://arxiv.org/pdf/2502.10263.pdf</a></span>   <span><a href='https://github.com/worldbank/ai4data-use' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aivin V. Solatorio, Rafael Macalaba, James Liounis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10263">Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2502.06476.pdf' target='_blank'>https://arxiv.org/pdf/2502.06476.pdf</a></span>   <span><a href='https://github.com/SonyResearch/IISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vlad Hosu, Lorenzo Agnolucci, Daisuke Iso, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06476">Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) measures and predicts perceived image quality by human observers. Although recent studies have highlighted the critical influence that variations in the scale of an image have on its perceived quality, this relationship has not been systematically quantified. To bridge this gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest scale where an image exhibits its highest perceived quality. We also present the Image Intrinsic Scale Assessment (IISA) task, which involves subjectively measuring and predicting the IIS based on human judgments. We develop a subjective annotation methodology and create the IISA-DB dataset, comprising 785 image-IIS pairs annotated by experts in a rigorously controlled crowdsourcing study. Furthermore, we propose WIISA (Weak-labeling for Image Intrinsic Scale Assessment), a strategy that leverages how the IIS of an image varies with downscaling to generate weak labels. Experiments show that applying WIISA during the training of several IQA methods adapted for IISA consistently improves the performance compared to using only ground-truth labels. The code, dataset, and pre-trained models are available at https://github.com/SonyResearch/IISA.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2502.05139.pdf' target='_blank'>https://arxiv.org/pdf/2502.05139.pdf</a></span>   <span><a href='https://github.com/facebookresearch/audiobox-aesthetics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, Carleigh Wood, Ann Lee, Wei-Ning Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05139">Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for Speech, Music, and Sound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quantification of audio aesthetics remains a complex challenge in audio processing, primarily due to its subjective nature, which is influenced by human perception and cultural context. Traditional methods often depend on human listeners for evaluation, leading to inconsistencies and high resource demands. This paper addresses the growing need for automated systems capable of predicting audio aesthetics without human intervention. Such systems are crucial for applications like data filtering, pseudo-labeling large datasets, and evaluating generative audio models, especially as these models become more sophisticated. In this work, we introduce a novel approach to audio aesthetic evaluation by proposing new annotation guidelines that decompose human listening perspectives into four distinct axes. We develop and train no-reference, per-item prediction models that offer a more nuanced assessment of audio quality. Our models are evaluated against human mean opinion scores (MOS) and existing methods, demonstrating comparable or superior performance. This research not only advances the field of audio aesthetics but also provides open-source models and datasets to facilitate future work and benchmarking. We release our code and pre-trained model at: https://github.com/facebookresearch/audiobox-aesthetics
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2502.04476.pdf' target='_blank'>https://arxiv.org/pdf/2502.04476.pdf</a></span>   <span><a href='https://github.com/soham97/ADIFF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Deshmukh, Shuo Han, Rita Singh, Bhiksha Raj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04476">ADIFF: Explaining audio difference using natural language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model's ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2502.04076.pdf' target='_blank'>https://arxiv.org/pdf/2502.04076.pdf</a></span>   <span><a href='https://github.com/littlespray/CRAVE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangkun Sun, Xiaoyu Liang, Bowen Qu, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04076">Content-Rich AIGC Video Quality Assessment via Intricate Text Alignment and Motion-Aware Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of next-generation video generation models like \textit{Sora} poses challenges for AI-generated content (AIGC) video quality assessment (VQA). These models substantially mitigate flickering artifacts prevalent in prior models, enable longer and complex text prompts and generate longer videos with intricate, diverse motion patterns. Conventional VQA methods designed for simple text and basic motion patterns struggle to evaluate these content-rich videos. To this end, we propose \textbf{CRAVE} (\underline{C}ontent-\underline{R}ich \underline{A}IGC \underline{V}ideo \underline{E}valuator), specifically for the evaluation of Sora-era AIGC videos. CRAVE proposes the multi-granularity text-temporal fusion that aligns long-form complex textual semantics with video dynamics. Additionally, CRAVE leverages the hybrid motion-fidelity modeling to assess temporal artifacts. Furthermore, given the straightforward prompts and content in current AIGC VQA datasets, we introduce \textbf{CRAVE-DB}, a benchmark featuring content-rich videos from next-generation models paired with elaborate prompts. Extensive experiments have shown that the proposed CRAVE achieves excellent results on multiple AIGC VQA benchmarks, demonstrating a high degree of alignment with human perception. All data and code will be publicly available at https://github.com/littlespray/CRAVE.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2502.02817.pdf' target='_blank'>https://arxiv.org/pdf/2502.02817.pdf</a></span>   <span><a href='https://haoyin116.github.io/Survey_of_AQA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yin, Paritosh Parmar, Daoliang Xu, Yang Zhang, Tianyou Zheng, Weiwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02817">A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) -- the ability to quantify the quality of human motion, actions, or skill levels and provide feedback -- has far-reaching implications in areas such as low-cost physiotherapy, sports training, and workforce development. As such, it has become a critical field in computer vision & video understanding over the past decade. Significant progress has been made in AQA methodologies, datasets, & applications, yet a pressing need remains for a comprehensive synthesis of this rapidly evolving field. In this paper, we present a thorough survey of the AQA landscape, systematically reviewing over 200 research papers using the preferred reporting items for systematic reviews & meta-analyses (PRISMA) framework. We begin by covering foundational concepts & definitions, then move to general frameworks & performance metrics, & finally discuss the latest advances in methodologies & datasets. This survey provides a detailed analysis of research trends, performance comparisons, challenges, & future directions. Through this work, we aim to offer a valuable resource for both newcomers & experienced researchers, promoting further exploration & progress in AQA. Data are available at https://haoyin116.github.io/Survey_of_AQA/
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2502.01707.pdf' target='_blank'>https://arxiv.org/pdf/2502.01707.pdf</a></span>   <span><a href='https://github.com/JunFu1995/CLIP-DQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yirui Zeng, Jun Fu, Hadi Amirpour, Huasheng Wang, Guanghui Yue, Hantao Liu, Ying Chen, Wei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01707">CLIP-DQA: Blindly Evaluating Dehazed Images from Global and Local Perspectives Using CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind dehazed image quality assessment (BDQA), which aims to accurately predict the visual quality of dehazed images without any reference information, is essential for the evaluation, comparison, and optimization of image dehazing algorithms. Existing learning-based BDQA methods have achieved remarkable success, while the small scale of DQA datasets limits their performance. To address this issue, in this paper, we propose to adapt Contrastive Language-Image Pre-Training (CLIP), pre-trained on large-scale image-text pairs, to the BDQA task. Specifically, inspired by the fact that the human visual system understands images based on hierarchical features, we take global and local information of the dehazed image as the input of CLIP. To accurately map the input hierarchical information of dehazed images into the quality score, we tune both the vision branch and language branch of CLIP with prompt learning. Experimental results on two authentic DQA datasets demonstrate that our proposed approach, named CLIP-DQA, achieves more accurate quality predictions over existing BDQA methods. The code is available at https://github.com/JunFu1995/CLIP-DQA.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2501.18314.pdf' target='_blank'>https://arxiv.org/pdf/2501.18314.pdf</a></span>   <span><a href='https://github.com/charlotte9524/AGAV-Rater' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18314">AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many video-to-audio (VTA) methods have been proposed for dubbing silent AI-generated videos. An efficient quality assessment method for AI-generated audio-visual content (AGAV) is crucial for ensuring audio-visual quality. Existing audio-visual quality assessment methods struggle with unique distortions in AGAVs, such as unrealistic and inconsistent elements. To address this, we introduce AGAVQA-3k, the first large-scale AGAV quality assessment dataset, comprising $3,382$ AGAVs from $16$ VTA methods. AGAVQA-3k includes two subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality, content consistency, and overall quality, and AGAVQA-Pair, designed for optimal AGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can score AGAVs, as well as audio and music generated from text, across multiple dimensions, and selects the best AGAV generated by VTA methods to present to the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA-3k, Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that AGAV-Rater enhances VTA performance and user experience. The dataset and code is available at https://github.com/charlotte9524/AGAV-Rater.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2501.13107.pdf' target='_blank'>https://arxiv.org/pdf/2501.13107.pdf</a></span>   <span><a href='https://mgwillia.github.io/ilf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13107">Accelerate High-Quality Diffusion Models with Inner Loop Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion models' inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. Project information is available at https://mgwillia.github.io/ilf.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2501.11561.pdf' target='_blank'>https://arxiv.org/pdf/2501.11561.pdf</a></span>   <span><a href='https://depictqa.github.io/deqa-score/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, Chao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11561">Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of Multi-modal Large Language Models (MLLMs), MLLM-based Image Quality Assessment (IQA) methods have shown promising performance in linguistic quality description. However, current methods still fall short in accurately scoring image quality. In this work, we aim to leverage MLLMs to regress accurate quality scores. A key challenge is that the quality score is inherently continuous, typically modeled as a Gaussian distribution, whereas MLLMs generate discrete token outputs. This mismatch necessitates score discretization. Previous approaches discretize the mean score into a one-hot label, resulting in information loss and failing to capture inter-image relationships. We propose a distribution-based approach that discretizes the score distribution into a soft label. This method preserves the characteristics of the score distribution, achieving high accuracy and maintaining inter-image relationships. Moreover, to address dataset variation, where different IQA datasets exhibit various distributions, we introduce a fidelity loss based on Thurstone's model. This loss captures intra-dataset relationships, facilitating co-training across multiple IQA datasets. With these designs, we develop the distribution-based Depicted image Quality Assessment model for Score regression (DeQA-Score). Experiments across multiple benchmarks show that DeQA-Score stably outperforms baselines in score regression. Also, DeQA-Score can predict the score distribution that closely aligns with human annotations. Codes and model weights have been released in https://depictqa.github.io/deqa-score/.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2501.11512.pdf' target='_blank'>https://arxiv.org/pdf/2501.11512.pdf</a></span>   <span><a href='https://github.com/RJL2000/MTAOIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Jiale Rao, Junjie Chen, Ziwen Tan, Weide Liu, Yuming Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11512">Multitask Auxiliary Network for Perceptual Quality Assessment of Non-Uniformly Distorted Omnidirectional Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional image quality assessment (OIQA) has been widely investigated in the past few years and achieved much success. However, most of existing studies are dedicated to solve the uniform distortion problem in OIQA, which has a natural gap with the non-uniform distortion problem, and their ability in capturing non-uniform distortion is far from satisfactory. To narrow this gap, in this paper, we propose a multitask auxiliary network for non-uniformly distorted omnidirectional images, where the parameters are optimized by jointly training the main task and other auxiliary tasks. The proposed network mainly consists of three parts: a backbone for extracting multiscale features from the viewport sequence, a multitask feature selection module for dynamically allocating specific features to different tasks, and auxiliary sub-networks for guiding the proposed model to capture local distortion and global quality change. Extensive experiments conducted on two large-scale OIQA databases demonstrate that the proposed model outperforms other state-of-the-art OIQA metrics, and these auxiliary sub-networks contribute to improve the performance of the proposed model. The source code is available at https://github.com/RJL2000/MTAOIQA.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2501.11511.pdf' target='_blank'>https://arxiv.org/pdf/2501.11511.pdf</a></span>   <span><a href='https://github.com/RJL2000/OIQAND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Jiale Rao, Xuelin Liu, Yuming Fang, Yifan Zuo, Weide Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11511">Subjective and Objective Quality Assessment of Non-Uniformly Distorted Omnidirectional Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional image quality assessment (OIQA) has been one of the hot topics in IQA with the continuous development of VR techniques, and achieved much success in the past few years. However, most studies devote themselves to the uniform distortion issue, i.e., all regions of an omnidirectional image are perturbed by the ``same amount'' of noise, while ignoring the non-uniform distortion issue, i.e., partial regions undergo ``different amount'' of perturbation with the other regions in the same omnidirectional image. Additionally, nearly all OIQA models are verified on the platforms containing a limited number of samples, which largely increases the over-fitting risk and therefore impedes the development of OIQA. To alleviate these issues, we elaborately explore this topic from both subjective and objective perspectives. Specifically, we construct a large OIQA database containing 10,320 non-uniformly distorted omnidirectional images, each of which is generated by considering quality impairments on one or two camera len(s). Then we meticulously conduct psychophysical experiments and delve into the influence of both holistic and individual factors (i.e., distortion range and viewing condition) on omnidirectional image quality. Furthermore, we propose a perception-guided OIQA model for non-uniform distortion by adaptively simulating users' viewing behavior. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods. The source code is available at https://github.com/RJL2000/OIQAND.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2501.03674.pdf' target='_blank'>https://arxiv.org/pdf/2501.03674.pdf</a></span>   <span><a href='https://github.com/Lumos0507/HP-MCoRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengshi Qi, Hao Ye, Jiaxuan Peng, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03674">Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA), which aims at automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains different durations. However, existing methods focus on segmenting the video into fixed frames, which disrupts the temporal continuity of sub-actions resulting in unavoidable prediction errors. To address these challenges, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. Firstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture fine-grained spatio-temporal visual and skeletal features. Then, a procedure segmentation network is introduced to separate different sub-actions and obtain segmented features. Afterwards, the segmented visual and skeletal features are both fed into a multi-modal fusion module as physics structural priors, to guide the model in learning refined activity similarities and variances. Finally, a multi-stage contrastive learning regression approach is employed to learn discriminative representations and output prediction results. In addition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the current low-quality human pose labels. In experiments, the results on FineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority of our proposed approach. Our source code and dataset are available at https://github.com/Lumos0507/HP-MCoRe.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2501.02552.pdf' target='_blank'>https://arxiv.org/pdf/2501.02552.pdf</a></span>   <span><a href='https://github.com/teamreboott/MLBCAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeyoung Kim, Jongho Lee, Hong-Jun Choi, Ting-Yao Hsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, Clyde Lee Giles, Ting-Hao 'Kenneth' Huang, Sungchul Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02552">Multi-LLM Collaborative Caption Generation in Scientific Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at https://github.com/teamreboott/MLBCAP
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2412.20423.pdf' target='_blank'>https://arxiv.org/pdf/2412.20423.pdf</a></span>   <span><a href='https://github.com/iamazxl/ESVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilei Zhu, Huiyu Duan, Liu Yang, Yucheng Zhu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20423">ESVQA: Perceptual Quality Assessment of Egocentric Spatial Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of eXtended Reality (XR), egocentric spatial shooting and display technologies have further enhanced immersion and engagement for users, delivering more captivating and interactive experiences. Assessing the quality of experience (QoE) of egocentric spatial videos is crucial to ensure a high-quality viewing experience. However, the corresponding research is still lacking. In this paper, we use the concept of embodied experience to highlight this more immersive experience and study the new problem, i.e., embodied perceptual quality assessment for egocentric spatial videos. Specifically, we introduce the first Egocentric Spatial Video Quality Assessment Database (ESVQAD), which comprises 600 egocentric spatial videos captured using the Apple Vision Pro and their corresponding mean opinion scores (MOSs). Furthermore, we propose a novel multi-dimensional binocular feature fusion model, termed ESVQAnet, which integrates binocular spatial, motion, and semantic features to predict the overall perceptual quality. Experimental results demonstrate the ESVQAnet significantly outperforms 16 state-of-the-art VQA models on the embodied perceptual quality assessment task, and exhibits strong generalization capability on traditional VQA tasks. The database and code are available at https://github.com/iamazxl/ESVQA.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2412.18933.pdf' target='_blank'>https://arxiv.org/pdf/2412.18933.pdf</a></span>   <span><a href='https://github.com/Lighting-YXLI/TINQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Li, Xiaoyuan Yang, Weide Liu, Xin Jin, Xu Jia, Yukun Lai, Haotao Liu, Paul L Rosin, Wei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18933">TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind video quality assessment (BVQA) has been actively researched for user-generated content (UGC) videos. Recently, super-resolution (SR) techniques have been widely applied in UGC. Therefore, an effective BVQA method for both UGC and SR scenarios is essential. Temporal inconsistency, referring to irregularities between consecutive frames, is relevant to video quality. Current BVQA approaches typically model temporal relationships in UGC videos using statistics of motion information, but inconsistencies remain unexplored. Additionally, different from temporal inconsistency in UGC videos, such inconsistency in SR videos is amplified due to upscaling algorithms. In this paper, we introduce the Temporal Inconsistency Guided Blind Video Quality Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency is crucial for effective BVQA. Since temporal inconsistencies vary between UGC and SR videos, they are calculated in different ways. Based on this, a spatial module highlights inconsistent areas across consecutive frames at coarse and fine granularities. In addition, a temporal module aggregates features over time in two stages. The first stage employs a visual memory capacity block to adaptively segment the time dimension based on estimated complexity, while the second stage focuses on selecting key features. The stages work together through Consistency-aware Fusion Units to regress cross-time-scale video quality. Extensive experiments on UGC and SR video quality datasets show that our method outperforms existing state-of-the-art BVQA methods. Code is available at https://github.com/Lighting-YXLI/TINQ.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2412.18774.pdf' target='_blank'>https://arxiv.org/pdf/2412.18774.pdf</a></span>   <span><a href='https://github.com/Jianbo-maker/EPD_benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Zhang, Chunyi Li, Jie Hao, Jun Jia, Huiyu Duan, Guoquan Zheng, Liang Yuan, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18774">Embodied Image Quality Assessment for Robotic Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) of User-Generated Content (UGC) is a critical technique for human Quality of Experience (QoE). However, does the the image quality of Robot-Generated Content (RGC) demonstrate traits consistent with the Moravec paradox, potentially conflicting with human perceptual norms? Human subjective scoring is more based on the attractiveness of the image. Embodied agent are required to interact and perceive in the environment, and finally perform specific tasks. Visual images as inputs directly influence downstream tasks. In this paper, we explore the perception mechanism of embodied robots for image quality. We propose the first Embodied Preference Database (EPD), which contains 12,500 distorted image annotations. We establish assessment metrics based on the downstream tasks of robot. In addition, there is a gap between UGC and RGC. To address this, we propose a novel Multi-scale Attention Embodied Image Quality Assessment called MA-EIQA. For the proposed EPD dataset, this is the first no-reference IQA model designed for embodied robot. Finally, the performance of mainstream IQA algorithms on EPD dataset is verified. The experiments demonstrate that quality assessment of embodied images is different from that of humans. We sincerely hope that the EPD can contribute to the development of embodied AI by focusing on image quality assessment. The benchmark is available at https://github.com/Jianbo-maker/EPD_benchmark.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2412.17504.pdf' target='_blank'>https://arxiv.org/pdf/2412.17504.pdf</a></span>   <span><a href='https://github.com/created-Bi/background_inpainting_products_dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Liang, Jun Luo, Xiaoxi Guo, Jianqi Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17504">An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In product advertising applications, the automated inpainting of backgrounds utilizing AI techniques in product images has emerged as a significant task. However, the techniques still suffer from issues such as inappropriate background and inconsistent product in generated product images, and existing approaches for evaluating the quality of generated product images are mostly inconsistent with human feedback causing the evaluation for this task to depend on manual annotation. To relieve the issues above, this paper proposes Human Feedback and Product Consistency (HFPC), which can automatically assess the generated product images based on two modules. Firstly, to solve inappropriate backgrounds, human feedback on 44,000 automated inpainting product images is collected to train a reward model based on multi-modal features extracted from BLIP and comparative learning. Secondly, to filter generated product images containing inconsistent products, a fine-tuned segmentation model is employed to segment the product of the original and generated product images and then compare the differences between the above two. Extensive experiments have demonstrated that HFPC can effectively evaluate the quality of generated product images and significantly reduce the expense of manual annotation. Moreover, HFPC achieves state-of-the-art(96.4% in precision) in comparison to other open-source visual-quality-assessment models. Dataset and code are available at: https://github.com/created-Bi/background_inpainting_products_dataset
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2412.15677.pdf' target='_blank'>https://arxiv.org/pdf/2412.15677.pdf</a></span>   <span><a href='https://github.com/ytian73/AIGI-VC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Tian, Yixuan Li, Baoliang Chen, Hanwei Zhu, Shiqi Wang, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15677">AI-generated Image Quality Assessment in Visual Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the quality of artificial intelligence-generated images (AIGIs) plays a crucial role in their application in real-world scenarios. However, traditional image quality assessment (IQA) algorithms primarily focus on low-level visual perception, while existing IQA works on AIGIs overemphasize the generated content itself, neglecting its effectiveness in real-world applications. To bridge this gap, we propose AIGI-VC, a quality assessment database for AI-Generated Images in Visual Communication, which studies the communicability of AIGIs in the advertising field from the perspectives of information clarity and emotional interaction. The dataset consists of 2,500 images spanning 14 advertisement topics and 8 emotion types. It provides coarse-grained human preference annotations and fine-grained preference descriptions, benchmarking the abilities of IQA methods in preference prediction, interpretation, and reasoning. We conduct an empirical study of existing representative IQA methods and large multi-modal models on the AIGI-VC dataset, uncovering their strengths and weaknesses.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2412.12667.pdf' target='_blank'>https://arxiv.org/pdf/2412.12667.pdf</a></span>   <span><a href='https://github.com/sendjasni/patch-selection-360-image-quality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abderrezzaq Sendjasni, Seif-Eddine Benkabou, Mohamed-Chaker Larabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12667">A Two-Fold Patch Selection Approach for Improved 360-Degree Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents a novel approach to improving the accuracy of 360-degree perceptual image quality assessment (IQA) through a two-fold patch selection process. Our methodology combines visual patch selection with embedding similarity-based refinement. The first stage focuses on selecting patches from 360-degree images using three distinct sampling methods to ensure comprehensive coverage of visual content for IQA. The second stage, which is the core of our approach, employs an embedding similarity-based selection process to filter and prioritize the most informative patches based on their embeddings similarity distances. This dual selection mechanism ensures that the training data is both relevant and informative, enhancing the model's learning efficiency. Extensive experiments and statistical analyses using three distance metrics across three benchmark datasets validate the effectiveness of our selection algorithm. The results highlight its potential to deliver robust and accurate 360-degree IQA, with performance gains of up to 4.5% in accuracy and monotonicity of quality score prediction, while using only 40% to 50% of the training patches. These improvements are consistent across various configurations and evaluation metrics, demonstrating the strength of the proposed method. The code for the selection process is available at: https://github.com/sendjasni/patch-selection-360-image-quality.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2412.11170.pdf' target='_blank'>https://arxiv.org/pdf/2412.11170.pdf</a></span>   <span><a href='https://mate-3d.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhang, Bingyang Cui, Qi Yang, Zhu Li, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11170">Benchmarking and Learning Multi-Dimensional Quality Evaluator for Text-to-3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-3D generation has achieved remarkable progress in recent years, yet evaluating these methods remains challenging for two reasons: i) Existing benchmarks lack fine-grained evaluation on different prompt categories and evaluation dimensions. ii) Previous evaluation metrics only focus on a single aspect (e.g., text-3D alignment) and fail to perform multi-dimensional quality assessment. To address these problems, we first propose a comprehensive benchmark named MATE-3D. The benchmark contains eight well-designed prompt categories that cover single and multiple object generation, resulting in 1,280 generated textured meshes. We have conducted a large-scale subjective experiment from four different evaluation dimensions and collected 107,520 annotations, followed by detailed analyses of the results. Based on MATE-3D, we propose a novel quality evaluator named HyperScore. Utilizing hypernetwork to generate specified mapping functions for each evaluation dimension, our metric can effectively perform multi-dimensional quality assessment. HyperScore presents superior performance over existing metrics on MATE-3D, making it a promising metric for assessing and improving text-to-3D generation. The project is available at https://mate-3d.github.io/.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2412.11149.pdf' target='_blank'>https://arxiv.org/pdf/2412.11149.pdf</a></span>   <span><a href='https://ZhouKanglei.github.io/AQA-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Ruizhi Cai, Liyuan Wang, Hubert P. H. Shum, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11149">A Comprehensive Survey of Action Quality Assessment: Method and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) quantitatively evaluates the quality of human actions, providing automated assessments that reduce biases in human judgment. Its applications span domains such as sports analysis, skill assessment, and medical care. Recent advances in AQA have introduced innovative methodologies, but similar methods often intertwine across different domains, highlighting the fragmented nature that hinders systematic reviews. In addition, the lack of a unified benchmark and limited computational comparisons hinder consistent evaluation and fair assessment of AQA approaches. In this work, we address these gaps by systematically analyzing over 150 AQA-related papers to develop a hierarchical taxonomy, construct a unified benchmark, and provide an in-depth analysis of current trends, challenges, and future directions. Our hierarchical taxonomy categorizes AQA methods based on input modalities (video, skeleton, multi-modal) and their specific characteristics, highlighting the evolution and interrelations across various approaches. To promote standardization, we present a unified benchmark, integrating diverse datasets to evaluate the assessment precision and computational efficiency. Finally, we review emerging task-specific applications and identify under-explored challenges in AQA, providing actionable insights into future research directions. This survey aims to deepen understanding of AQA progress, facilitate method comparison, and guide future innovations. The project web page can be found at https://ZhouKanglei.github.io/AQA-Survey.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2412.08029.pdf' target='_blank'>https://arxiv.org/pdf/2412.08029.pdf</a></span>   <span><a href='https://github.com/VincentQQu/NeRF-NQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08029">NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF and Neural View Synthesis Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural View Synthesis (NVS) has demonstrated efficacy in generating high-fidelity dense viewpoint videos using a image set with sparse views. However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not tailored for the scenes with dense viewpoints synthesized by NVS and NeRF variants, thus, they often fall short in capturing the perceptual quality, including spatial and angular aspects of NVS-synthesized scenes. Furthermore, the lack of dense ground truth views makes the full reference quality assessment on NVS-synthesized scenes challenging. For instance, datasets such as LLFF provide only sparse images, insufficient for complete full-reference assessments. To address the issues above, we propose NeRF-NQA, the first no-reference quality assessment method for densely-observed scenes synthesized from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment strategy, integrating both viewwise and pointwise approaches, to evaluate the quality of NVS-generated scenes. The viewwise approach assesses the spatial quality of each individual synthesized view and the overall inter-views consistency, while the pointwise approach focuses on the angular qualities of scene surface points and their compound inter-point quality. Extensive evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality assessment methods (from fields of image, video, and light-field assessment). The results demonstrate NeRF-NQA outperforms the existing assessment methods significantly and it shows substantial superiority on assessing NVS-synthesized scenes without references. An implementation of this paper are available at https://github.com/VincentQQu/NeRF-NQA.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2412.07277.pdf' target='_blank'>https://arxiv.org/pdf/2412.07277.pdf</a></span>   <span><a href='https://github.com/yuyi-sd/BAIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yu, Song Xia, Xun Lin, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07277">Backdoor Attacks against No-Reference Image Quality Assessment Models via a Scalable Trigger</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the quality of a single input image without using any reference, plays a critical role in evaluating and optimizing computer vision systems, e.g., low-light enhancement. Recent research indicates that NR-IQA models are susceptible to adversarial attacks, which can significantly alter predicted scores with visually imperceptible perturbations. Despite revealing vulnerabilities, these attack methods have limitations, including high computational demands, untargeted manipulation, limited practical utility in white-box scenarios, and reduced effectiveness in black-box scenarios. To address these challenges, we shift our focus to another significant threat and present a novel poisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker to manipulate the IQA model's output to any desired target value by simply adjusting a scaling coefficient $Î±$ for the trigger. We propose to inject the trigger in the discrete cosine transform (DCT) domain to improve the local invariance of the trigger for countering trigger diminishment in NR-IQA models due to widely adopted data augmentations. Furthermore, the universal adversarial perturbations (UAP) in the DCT space are designed as the trigger, to increase IQA model susceptibility to manipulation and improve attack effectiveness. In addition to the heuristic method for poison-label BAIQA (P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on $Î±$ sampling and image data refinement, driven by theoretical insights we reveal. Extensive experiments on diverse datasets and various NR-IQA models demonstrate the effectiveness of our attacks. Code can be found at https://github.com/yuyi-sd/BAIQA.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2412.04814.pdf' target='_blank'>https://arxiv.org/pdf/2412.04814.pdf</a></span>   <span><a href='https://codegoat24.github.io/LiFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04814">LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are subjective and challenging to formalize as objective functions. Existing studies train video quality assessment models that rely on human-annotated ratings for video evaluation but overlook the reasoning behind evaluations, limiting their ability to capture nuanced human criteria. Moreover, aligning T2V model using video-based human feedback remains unexplored. Therefore, this paper proposes LiFT, the first method designed to leverage human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2412.04508.pdf' target='_blank'>https://arxiv.org/pdf/2412.04508.pdf</a></span>   <span><a href='https://github.com/taco-group/Video-Quality-Assessment-A-Comprehensive-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zheng, Yibo Fan, Leilei Huang, Tianyu Zhu, Jiaming Liu, Zhijian Hao, Shuo Xing, Chia-Ju Chen, Xiongkuo Min, Alan C. Bovik, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04508">Video Quality Assessment: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is an important processing task, aiming at predicting the quality of videos in a manner highly consistent with human judgments of perceived quality. Traditional VQA models based on natural image and/or video statistics, which are inspired both by models of projected images of the real world and by dual models of the human visual system, deliver only limited prediction performances on real-world user-generated content (UGC), as exemplified in recent large-scale VQA databases containing large numbers of diverse video contents crawled from the web. Fortunately, recent advances in deep neural networks and Large Multimodality Models (LMMs) have enabled significant progress in solving this problem, yielding better results than prior handcrafted models. Numerous deep learning-based VQA models have been developed, with progress in this direction driven by the creation of content-diverse, large-scale human-labeled databases that supply ground truth psychometric video quality data. Here, we present a comprehensive survey of recent progress in the development of VQA algorithms and the benchmarking studies and databases that make them possible. We also analyze open research directions on study design and VQA algorithm architectures. Github link: https://github.com/taco-group/Video-Quality-Assessment-A-Comprehensive-Survey.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2412.01794.pdf' target='_blank'>https://arxiv.org/pdf/2412.01794.pdf</a></span>   <span><a href='https://github.com/X1716/IQA-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khaled Abud, Sergey Lavrushkin, Alexey Kirillov, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01794">IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based models have recently revolutionized image generation, achieving unprecedented levels of fidelity. However, consistent generation of high-quality images remains challenging partly due to the lack of conditioning mechanisms for perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. We show that diffusion models can learn complex qualitative relationships from both IQA models' outputs and internal activations. First, we experiment with gradient-based guidance to optimize image quality directly and show this method has limited generalizability. To address this, we introduce IQA-Adapter, a novel framework that conditions generation on target quality levels by learning the implicit relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter can shift the distribution of generated images towards a higher-quality subdomain, and, inversely, it can be used as a degradation model, generating progressively more distorted images when provided with a lower-quality signal. Under high-quality condition, IQA-Adapter achieves up to a 10% improvement across multiple objective metrics, as confirmed by a user preference study, while preserving generative diversity and content. Furthermore, we extend IQA-Adapter to a reference-based conditioning scenario, utilizing the rich activation space of IQA models to transfer highly specific, content-agnostic qualitative features between images.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2411.17928.pdf' target='_blank'>https://arxiv.org/pdf/2411.17928.pdf</a></span>   <span><a href='https://github.com/JokerJohn/Cloud\_Map\_Evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangcheng Hu, Jin Wu, Mingkai Jia, Hongyu Yan, Yi Jiang, Binqian Jiang, Wei Zhang, Wei He, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17928">MapEval: Towards Unified, Robust and Efficient SLAM Map Evaluation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating massive-scale point cloud maps in Simultaneous Localization and Mapping (SLAM) remains challenging, primarily due to the absence of unified, robust and efficient evaluation frameworks. We present MapEval, an open-source framework for comprehensive quality assessment of point cloud maps, specifically addressing SLAM scenarios where ground truth map is inherently sparse compared to the mapped environment. Through systematic analysis of existing evaluation metrics in SLAM applications, we identify their fundamental limitations and establish clear guidelines for consistent map quality assessment. Building upon these insights, we propose a novel Gaussian-approximated Wasserstein distance in voxelized space, enabling two complementary metrics under the same error standard: Voxelized Average Wasserstein Distance (AWD) for global geometric accuracy and Spatial Consistency Score (SCS) for local consistency evaluation. This theoretical foundation leads to significant improvements in both robustness against noise and computational efficiency compared to conventional metrics. Extensive experiments on both simulated and real-world datasets demonstrate that MapEval achieves at least \SI{100}{}-\SI{500}{} times faster while maintaining evaluation integrity. The MapEval library\footnote{\texttt{https://github.com/JokerJohn/Cloud\_Map\_Evaluation}} will be publicly available to promote standardized map evaluation practices in the robotics community.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2411.17237.pdf' target='_blank'>https://arxiv.org/pdf/2411.17237.pdf</a></span>   <span><a href='https://github.com/zhengchen1999/Grounding-IQA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhengchen1999/Grounding-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Chen, Xun Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiongkuo Min, Xiaohong Liu, Xin Yuan, Yong Guo, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17237">Grounding-IQA: Multimodal Language Grounding Model for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of multimodal large language models (MLLMs) enables the evaluation of image quality through natural language descriptions. This advancement allows for more detailed assessments. However, these MLLM-based IQA methods primarily rely on general contextual descriptions, sometimes limiting fine-grained quality assessment. To address this limitation, we introduce a new image quality assessment (IQA) task paradigm, grounding-IQA. This paradigm integrates multimodal referring and grounding with IQA to realize more fine-grained quality perception. Specifically, grounding-IQA comprises two subtasks: grounding-IQA-description (GIQA-DES) and visual question answering (GIQA-VQA). GIQA-DES involves detailed descriptions with precise locations (e.g., bounding boxes), while GIQA-VQA focuses on quality QA for local regions. To realize grounding-IQA, we construct a corresponding dataset, GIQA-160K, through our proposed automated annotation pipeline. Furthermore, we develop a well-designed benchmark, GIQA-Bench. The benchmark comprehensively evaluates the model grounding-IQA performance from three perspectives: description quality, VQA accuracy, and grounding precision. Experiments demonstrate that our proposed task paradigm, dataset, and benchmark facilitate the more fine-grained IQA application. Code: https://github.com/zhengchen1999/Grounding-IQA.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2411.16619.pdf' target='_blank'>https://arxiv.org/pdf/2411.16619.pdf</a></span>   <span><a href='https://github.com/zczhang-sjtu/GHVQ.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Zhang, Wei Sun, Xinyue Li, Yunhao Li, Qihang Ge, Jun Jia, Zicheng Zhang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16619">Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-driven video generation techniques have made significant progress in recent years. However, AI-generated videos (AGVs) involving human activities often exhibit substantial visual and semantic distortions, hindering the practical application of video generation technologies in real-world scenarios. To address this challenge, we conduct a pioneering study on human activity AGV quality assessment, focusing on visual quality evaluation and the identification of semantic distortions. First, we construct the AI-Generated Human activity Video Quality Assessment (Human-AGVQA) dataset, consisting of 6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text prompts that describe diverse human activities. We conduct a subjective study to evaluate the human appearance quality, action continuity quality, and overall video quality of AGVs, and identify semantic issues of human body parts. Based on Human-AGVQA, we benchmark the performance of T2V models and analyze their strengths and weaknesses in generating different categories of human activities. Second, we develop an objective evaluation metric, named AI-Generated Human activity Video Quality metric (GHVQ), to automatically analyze the quality of human activity AGVs. GHVQ systematically extracts human-focused quality features, AI-generated content-aware quality features, and temporal continuity features, making it a comprehensive and explainable quality metric for human activity AGVs. The extensive experimental results show that GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a large margin, demonstrating its efficacy in assessing the quality of human activity AGVs. The Human-AGVQA dataset and GHVQ metric will be released at https://github.com/zczhang-sjtu/GHVQ.git.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2411.11199.pdf' target='_blank'>https://arxiv.org/pdf/2411.11199.pdf</a></span>   <span><a href='https://github.com/fan-aaron-zhang/bvi-cr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Gao, Adrian Azzarelli, Ho Man Kwan, Nantheera Anantrasirichai, Fan Zhang, Oliver Moolan-Feroze, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11199">BVI-CR: A Multi-View Human Dataset for Volumetric Video Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advances in immersive technologies and 3D reconstruction have enabled the creation of digital replicas of real-world objects and environments with fine details. These processes generate vast amounts of 3D data, requiring more efficient compression methods to satisfy the memory and bandwidth constraints associated with data storage and transmission. However, the development and validation of efficient 3D data compression methods are constrained by the lack of comprehensive and high-quality volumetric video datasets, which typically require much more effort to acquire and consume increased resources compared to 2D image and video databases. To bridge this gap, we present an open multi-view volumetric human dataset, denoted BVI-CR, which contains 18 multi-view RGB-D captures and their corresponding textured polygonal meshes, depicting a range of diverse human actions. Each video sequence contains 10 views in 1080p resolution with durations between 10-15 seconds at 30FPS. Using BVI-CR, we benchmarked three conventional and neural coordinate-based multi-view video compression methods, following the MPEG MIV Common Test Conditions, and reported their rate quality performance based on various quality metrics. The results show the great potential of neural representation based methods in volumetric video compression compared to conventional video coding methods (with an up to 38\% average coding gain in PSNR). This dataset provides a development and validation platform for a variety of tasks including volumetric reconstruction, compression, and quality assessment. The database will be shared publicly at \url{https://github.com/fan-aaron-zhang/bvi-cr}.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2411.10161.pdf' target='_blank'>https://arxiv.org/pdf/2411.10161.pdf</a></span>   <span><a href='https://github.com/chencn2020/Seagull' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Chen, Juan Wang, Wen Wang, Sunhan Xu, Hang Xiong, Yun Zeng, Jian Guo, Shuxun Wang, Chunfeng Yuan, Bing Li, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10161">SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2411.07728.pdf' target='_blank'>https://arxiv.org/pdf/2411.07728.pdf</a></span>   <span><a href='https://github.com/chenwuwq/GC-PCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wu Chen, Qiuping Jiang, Wei Zhou, Feng Shao, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07728">No-Reference Point Cloud Quality Assessment via Graph Convolutional Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2411.02441.pdf' target='_blank'>https://arxiv.org/pdf/2411.02441.pdf</a></span>   <span><a href='https://github.com/convergedmachine/Cross-D-Conv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Can Yavuz, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02441">Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier Shifting Operation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In biomedical imaging analysis, the dichotomy between 2D and 3D data presents a significant challenge. While 3D volumes offer superior real-world applicability, they are less available for each modality and not easy to train in large scale, whereas 2D samples are abundant but less comprehensive. This paper introduces Cross-D Conv operation, a novel approach that bridges the dimensional gap by learning the phase shifting in the Fourier domain. Our method enables seamless weight transfer between 2D and 3D convolution operations, effectively facilitating cross-dimensional learning. The proposed architecture leverages the abundance of 2D training data to enhance 3D model performance, offering a practical solution to the multimodal data scarcity challenge in 3D medical model pretraining. Experimental validation on the RadImagenet (2D) and multimodal volumetric sets demonstrates that our approach achieves comparable or superior performance in feature quality assessment. The enhanced convolution operation presents new opportunities for developing efficient classification and segmentation models in medical imaging. This work represents an advancement in cross-dimensional and multimodal medical image analysis, offering a robust framework for utilizing 2D priors in 3D model pretraining while maintaining computational efficiency of 2D training. The code is available on https://github.com/convergedmachine/Cross-D-Conv.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2410.09501.pdf' target='_blank'>https://arxiv.org/pdf/2410.09501.pdf</a></span>   <span><a href='https://github.com/jpeg-aic/dataset-BTC-PTC-24' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michela Testolina, Mohsen Jenadeleh, Shima Mohammadi, Shaolin Su, Joao Ascenso, Touradj Ebrahimi, Jon Sneyers, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09501">Fine-grained subjective visual quality assessment for high-fidelity compressed images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in image compression, storage, and display technologies have made high-quality images and videos widely accessible. At this level of quality, distinguishing between compressed and original content becomes difficult, highlighting the need for assessment methodologies that are sensitive to even the smallest visual quality differences. Conventional subjective visual quality assessments often use absolute category rating scales, ranging from ``excellent'' to ``bad''. While suitable for evaluating more pronounced distortions, these scales are inadequate for detecting subtle visual differences. The JPEG standardization project AIC is currently developing a subjective image quality assessment methodology for high-fidelity images. This paper presents the proposed assessment methods, a dataset of high-quality compressed images, and their corresponding crowdsourced visual quality ratings. It also outlines a data analysis approach that reconstructs quality scale values in just noticeable difference (JND) units. The assessment method uses boosting techniques on visual stimuli to help observers detect compression artifacts more clearly. This is followed by a rescaling process that adjusts the boosted quality values back to the original perceptual scale. This reconstruction yields a fine-grained, high-precision quality scale in JND units, providing more informative results for practical applications. The dataset and code to reproduce the results will be available at https://github.com/jpeg-aic/dataset-BTC-PTC-24.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2410.06729.pdf' target='_blank'>https://arxiv.org/pdf/2410.06729.pdf</a></span>   <span><a href='https://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongshuai Duan, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06729">Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference bitstream-layer point cloud quality assessment (PCQA) can be deployed without full decoding at any network node to achieve real-time quality monitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT encoding mode. First, to address the issue that existing PCQA databases have a small scale and limited distortion levels, we establish the WPC5.0 database which is the first one dedicated to Octree-RAHT encoding mode with a scale of 400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude distortion levels. Then, we propose the first PCQA model dedicated to Octree-RAHT encoding mode by parsing PC bitstreams without full decoding. The model introduces texture bitrate (TBPP) to predict texture complexity (TC) and further derives the texture distortion factor. In addition, the Geometric Quantization Parameter (PQS) is used to estimate the geometric distortion factor, which is then integrated into the model along with the texture distortion factor to obtain the proposed PCQA model named streamPCQ-OR. The proposed model has been compared with other advanced PCQA methods on the WPC5.0, BASICS and M-PCCD databases, and experimental results show that our model has excellent performance while having very low computational complexity, providing a reliable choice for time-critical applications. To facilitate subsequent research, the database and source code will be publicly released at https://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2410.06689.pdf' target='_blank'>https://arxiv.org/pdf/2410.06689.pdf</a></span>   <span><a href='https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Long, Honglei Su, Qi Liu, Hui Yuan, Wei Gao, Jiarun Song, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06689">Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference bitstream-layer point cloud quality assessment (PCQA) can be deployed without full decoding at any network node to achieve real-time quality monitoring. In this work, we develop the first PCQA model dedicated to Trisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full decoding. Specifically, we investigate the relationship among texture bitrate per point (TBPP), texture complexity (TC) and texture quantization parameter (TQP) while geometry encoding is lossless. Subsequently, we estimate TC by utilizing TQP and TBPP. Then, we establish a texture distortion evaluation model based on TC, TBPP and TQP. Ultimately, by integrating this texture distortion model with a geometry attenuation factor, a function of trisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA model named streamPCQ-TL. In addition, this work establishes a database named WPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting encoding mode, encompassing 400 distorted point clouds with both 4 geometric multiplied by 5 texture distortion levels. Experiment results on M-PCCD, ICIP2020 and the proposed WPC6.0 database suggest that the proposed streamPCQ-TL model exhibits robust and notable performance in contrast to existing advanced PCQA metrics, particularly in terms of computational cost. The dataset and source code will be publicly released at https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2410.02505.pdf' target='_blank'>https://arxiv.org/pdf/2410.02505.pdf</a></span>   <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Liu, Ziqing Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiaohong Liu, Linghe Kong, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02505">Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) serves as the golden standard for all models' performance in nearly all computer vision fields. However, it still suffers from poor out-of-distribution generalization ability and expensive training costs. To address these problems, we propose Dog-IQA, a standard-guided zero-shot mix-grained IQA method, which is training-free and utilizes the exceptional prior knowledge of multimodal large language models (MLLMs). To obtain accurate IQA scores, namely scores consistent with humans, we design an MLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA applies two techniques. First, Dog-IQA objectively scores with specific standards that utilize MLLM's behavior pattern and minimize the influence of subjective factors. Second, Dog-IQA comprehensively takes local semantic objects and the whole image as input and aggregates their scores, leveraging local and global information. Our proposed Dog-IQA achieves state-of-the-art (SOTA) performance compared with training-free methods, and competitive performance compared with training-based methods in cross-dataset scenarios. Our code will be available at https://github.com/Kai-Liu001/Dog-IQA.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2410.01411.pdf' target='_blank'>https://arxiv.org/pdf/2410.01411.pdf</a></span>   <span><a href='https://github.com/safouaneelg/copulasimilarity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Safouane El Ghazouali, Umberto Michelucci, Yassin El Hillali, Hichem Nouira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01411">CSIM: A Copula-based similarity index sensitive to local changes for Image quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image similarity metrics play an important role in computer vision applications, as they are used in image processing, computer vision and machine learning. Furthermore, those metrics enable tasks such as image retrieval, object recognition and quality assessment, essential in fields like healthcare, astronomy and surveillance. Existing metrics, such as PSNR, MSE, SSIM, ISSM and FSIM, often face limitations in terms of either speed, complexity or sensitivity to small changes in images. To address these challenges, a novel image similarity metric, namely CSIM, that combines real-time while being sensitive to subtle image variations is investigated in this paper. The novel metric uses Gaussian Copula from probability theory to transform an image into vectors of pixel distribution associated to local image patches. These vectors contain, in addition to intensities and pixel positions, information on the dependencies between pixel values, capturing the structural relationships within the image. By leveraging the properties of Copulas, CSIM effectively models the joint distribution of pixel intensities, enabling a more nuanced comparison of image patches making it more sensitive to local changes compared to other metrics. Experimental results demonstrate that CSIM outperforms existing similarity metrics in various image distortion scenarios, including noise, compression artifacts and blur. The metric's ability to detect subtle differences makes it suitable for applications requiring high precision, such as medical imaging, where the detection of minor anomalies can be of a high importance. The results obtained in this work can be reproduced from this Github repository: https://github.com/safouaneelg/copulasimilarity.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2410.00289.pdf' target='_blank'>https://arxiv.org/pdf/2410.00289.pdf</a></span>   <span><a href='https://github.com/dasongli1/SnapUGC_Engagement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dasong Li, Wenjie Li, Baili Lu, Hongsheng Li, Sizhuo Ma, Gurunandan Krishnan, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00289">Delving Deep into Engagement Prediction of Short Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and modeling the popularity of User Generated Content (UGC) short videos on social media platforms presents a critical challenge with broad implications for content creators and recommendation systems. This study delves deep into the intricacies of predicting engagement for newly published videos with limited user interactions. Surprisingly, our findings reveal that Mean Opinion Scores from previous video quality assessment datasets do not strongly correlate with video engagement levels. To address this, we introduce a substantial dataset comprising 90,000 real-world UGC short videos from Snapchat. Rather than relying on view count, average watch time, or rate of likes, we propose two metrics: normalized average watch percentage (NAWP) and engagement continuation rate (ECR) to describe the engagement levels of short videos. Comprehensive multi-modal features, including visual content, background music, and text data, are investigated to enhance engagement prediction. With the proposed dataset and two key metrics, our method demonstrates its ability to predict engagements of short videos purely from video content.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2409.18541.pdf' target='_blank'>https://arxiv.org/pdf/2409.18541.pdf</a></span>   <span><a href='https://github.com/DCDmllm/Align2LLaVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongzhe Huang, Jiang Liu, Zhewen Yu, Li Cai, Dian Jiao, Wenqiao Zhang, Siliang Tang, Juncheng Li, Hao Jiang, Haoyuan Li, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18541">Align$^2$LLaVA: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multi-modal Large Language Models (MLLMs), such as LLaVA-series models, are driven by massive machine-generated instruction-following data tuning. Such automatic instruction collection pipelines, however, inadvertently introduce significant variability in data quality. This paper introduces a novel instruction curation algorithm, derived from two unique perspectives, human and LLM preference alignment, to compress this vast corpus of machine-generated multimodal instructions to a compact and high-quality form: (i) For human preference alignment, we have collected a machine-generated multimodal instruction dataset and established a comprehensive set of both subjective and objective criteria to guide the data quality assessment critically from human experts. By doing so, a reward model was trained on the annotated dataset to internalize the nuanced human understanding of instruction alignment. (ii) For LLM preference alignment, given the instruction selected by the reward model, we propose leveraging the inner LLM used in MLLM to align the writing style of visual instructions with that of the inner LLM itself, resulting in LLM-aligned instruction improvement. Extensive experiments demonstrate that we can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%. Impressively, by aggressively reducing the training instructions from 158k to 14k (9$\times$ smaller), our model consistently outperforms its full-size dataset counterpart across various MLLM benchmarks. Our project is available at https://github.com/DCDmllm/Align2LLaVA.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2409.16644.pdf' target='_blank'>https://arxiv.org/pdf/2409.16644.pdf</a></span>   <span><a href='https://github.com/bytedance/SALMONN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16644">Enabling Auditory Large Language Models for Automatic Speech Quality Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment typically requires evaluating audio from multiple aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc., which can be challenging to cover using one small model designed for a single task. In this paper, we propose leveraging recently introduced auditory large language models (LLMs) for automatic speech quality assessment. By employing task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B testing results, which are commonly used for evaluating text-to-speech systems. Additionally, the finetuned auditory LLM is able to generate natural language descriptions assessing aspects like noisiness, distortion, discontinuity, and overall quality, providing more interpretable outputs. Extensive experiments have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and Qwen2-Audio. For the natural language descriptions task, a commercial model Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory LLMs achieve competitive performance compared to state-of-the-art task-specific small models in predicting MOS and SIM, while also delivering promising results in A/B testing and natural language descriptions. Our data processing scripts and finetuned model checkpoints can be found at https://github.com/bytedance/SALMONN.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2409.14847.pdf' target='_blank'>https://arxiv.org/pdf/2409.14847.pdf</a></span>   <span><a href='https://github.com/XinliYue/VQA-Generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinli Yue, Jianhui Sun, Liangchao Yao, Fan Xia, Yuetang Deng, Tianyi Wang, Lei Li, Fengyun Rao, Jing Lv, Qian Wang, Lingchen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14847">Revisiting Video Quality Assessment from the Perspective of Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing popularity of short video platforms such as YouTube Shorts, TikTok, and Kwai has led to a surge in User-Generated Content (UGC), which presents significant challenges for the generalization performance of Video Quality Assessment (VQA) tasks. These challenges not only affect performance on test sets but also impact the ability to generalize across different datasets. While prior research has primarily focused on enhancing feature extractors, sampling methods, and network branches, it has largely overlooked the generalization capabilities of VQA tasks. In this work, we reevaluate the VQA task from a generalization standpoint. We begin by analyzing the weight loss landscape of VQA models, identifying a strong correlation between this landscape and the generalization gaps. We then investigate various techniques to regularize the weight loss landscape. Our results reveal that adversarial weight perturbations can effectively smooth this landscape, significantly improving the generalization performance, with cross-dataset generalization and fine-tuning performance enhanced by up to 1.8% and 3%, respectively. Through extensive experiments across various VQA methods and datasets, we validate the effectiveness of our approach. Furthermore, by leveraging our insights, we achieve state-of-the-art performance in Image Quality Assessment (IQA) tasks. Our code is available at https://github.com/XinliYue/VQA-Generalization.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2409.08374.pdf' target='_blank'>https://arxiv.org/pdf/2409.08374.pdf</a></span>   <span><a href='https://github.com/JozefColdenhoff/OpenACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jozef Coldenhoff, Niclas Granqvist, Milos Cernak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08374">OpenACE: An Open Benchmark for Evaluating Audio Coding Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio and speech coding lack unified evaluation and open-source testing. Many candidate systems were evaluated on proprietary, non-reproducible, or small data, and machine learning-based codecs are often tested on datasets with similar distributions as trained on, which is unfairly compared to digital signal processing-based codecs that usually work well with unseen data. This paper presents a full-band audio and speech coding quality benchmark with more variable content types, including traditional open test vectors. An example use case of audio coding quality assessment is presented with open-source Opus, 3GPP's EVS, and recent ETSI's LC3 with LC3+ used in Bluetooth LE Audio profiles. Besides, quality variations of emotional speech encoding at 16 kbps are shown. The proposed open-source benchmark contributes to audio and speech coding democratization and is available at https://github.com/JozefColdenhoff/OpenACE.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2409.07650.pdf' target='_blank'>https://arxiv.org/pdf/2409.07650.pdf</a></span>   <span><a href='https://github.com/abhijay9/ZS-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07650">Foundation Models Boost Low-Level Perceptual Similarity Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For full-reference image quality assessment (FR-IQA) using deep-learning approaches, the perceptual similarity score between a distorted image and a reference image is typically computed as a distance measure between features extracted from a pretrained CNN or more recently, a Transformer network. Often, these intermediate features require further fine-tuning or processing with additional neural network layers to align the final similarity scores with human judgments. So far, most IQA models based on foundation models have primarily relied on the final layer or the embedding for the quality score estimation. In contrast, this work explores the potential of utilizing the intermediate features of these foundation models, which have largely been unexplored so far in the design of low-level perceptual similarity metrics. We demonstrate that the intermediate features are comparatively more effective. Moreover, without requiring any training, these metrics can outperform both traditional and state-of-the-art learned metrics by utilizing distance measures between the features.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2409.07236.pdf' target='_blank'>https://arxiv.org/pdf/2409.07236.pdf</a></span>   <span><a href='https://github.com/zyj-2000/3DGCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Zicheng Zhang, Farong Wen, Jun Jia, Yanwei Jiang, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07236">3DGCQA: A Quality Assessment Database for 3D AI-Generated Contents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although 3D generated content (3DGC) offers advantages in reducing production costs and accelerating design timelines, its quality often falls short when compared to 3D professionally generated content. Common quality issues frequently affect 3DGC, highlighting the importance of timely and effective quality assessment. Such evaluations not only ensure a higher standard of 3DGCs for end-users but also provide critical insights for advancing generative technologies. To address existing gaps in this domain, this paper introduces a novel 3DGC quality assessment dataset, 3DGCQA, built using 7 representative Text-to-3D generation methods. During the dataset's construction, 50 fixed prompts are utilized to generate contents across all methods, resulting in the creation of 313 textured meshes that constitute the 3DGCQA dataset. The visualization intuitively reveals the presence of 6 common distortion categories in the generated 3DGCs. To further explore the quality of the 3DGCs, subjective quality assessment is conducted by evaluators, whose ratings reveal significant variation in quality across different generation methods. Additionally, several objective quality assessment algorithms are tested on the 3DGCQA dataset. The results expose limitations in the performance of existing algorithms and underscore the need for developing more specialized quality assessment methods. To provide a valuable resource for future research and development in 3D content generation and quality assessment, the dataset has been open-sourced in https://github.com/zyj-2000/3DGCQA.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2409.07115.pdf' target='_blank'>https://arxiv.org/pdf/2409.07115.pdf</a></span>   <span><a href='https://github.com/mas94/ADTRS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/mas94/ADTRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Alsaafin, Musab Alsheikh, Saeed Anwar, Muhammad Usman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07115">Attention Down-Sampling Transformer, Relative Ranking and Self-Consistency for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The no-reference image quality assessment is a challenging domain that addresses estimating image quality without the original reference. We introduce an improved mechanism to extract local and non-local information from images via different transformer encoders and CNNs. The utilization of Transformer encoders aims to mitigate locality bias and generate a non-local representation by sequentially processing CNN features, which inherently capture local visual structures. Establishing a stronger connection between subjective and objective assessments is achieved through sorting within batches of images based on relative distance information. A self-consistency approach to self-supervision is presented, explicitly addressing the degradation of no-reference image quality assessment (NR-IQA) models under equivariant transformations. Our approach ensures model robustness by maintaining consistency between an image and its horizontally flipped equivalent. Through empirical evaluation of five popular image quality assessment datasets, the proposed model outperforms alternative algorithms in the context of no-reference image quality assessment datasets, especially on smaller datasets. Codes are available at \href{https://github.com/mas94/ADTRS}{https://github.com/mas94/ADTRS}
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2409.05151.pdf' target='_blank'>https://arxiv.org/pdf/2409.05151.pdf</a></span>   <span><a href='https://github.com/lszhuhaichao/ultron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05151">Ultron: Enabling Temporal Geometry Compression of 3D Mesh Sequences using Temporal Correspondence and Mesh Deformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of computer vision, dynamic 3D reconstruction techniques have seen significant progress and found applications in various fields. However, these techniques generate large amounts of 3D data sequences, necessitating efficient storage and transmission methods. Existing 3D model compression methods primarily focus on static models and do not consider inter-frame information, limiting their ability to reduce data size. Temporal mesh compression, which has received less attention, often requires all input meshes to have the same topology, a condition rarely met in real-world applications. This research proposes a method to compress mesh sequences with arbitrary topology using temporal correspondence and mesh deformation. The method establishes temporal correspondence between consecutive frames, applies a deformation model to transform the mesh from one frame to subsequent frames, and replaces the original meshes with deformed ones if the quality meets a tolerance threshold. Extensive experiments demonstrate that this method can achieve state-of-the-art performance in terms of compression performance. The contributions of this paper include a geometry and motion-based model for establishing temporal correspondence between meshes, a mesh quality assessment for temporal mesh sequences, an entropy-based encoding and corner table-based method for compressing mesh sequences, and extensive experiments showing the effectiveness of the proposed method. All the code will be open-sourced at https://github.com/lszhuhaichao/ultron.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2409.03470.pdf' target='_blank'>https://arxiv.org/pdf/2409.03470.pdf</a></span>   <span><a href='https://github.com/prerakmody/bayesuncertainty-error-correspondence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03470">Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Increased usage of automated tools like deep learning in medical image segmentation has alleviated the bottleneck of manual contouring. This has shifted manual labour to quality assessment (QA) of automated contours which involves detecting errors and correcting them. A potential solution to semi-automated QA is to use deep Bayesian uncertainty to recommend potentially erroneous regions, thus reducing time spent on error detection. Previous work has investigated the correspondence between uncertainty and error, however, no work has been done on improving the "utility" of Bayesian uncertainty maps such that it is only present in inaccurate regions and not in the accurate ones. Our work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which promotes uncertainty to be present only in inaccurate regions. We apply this method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated against voxel inaccuracies using Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves. Numerical results show that when compared to the Bayesian baseline the proposed method successfully suppresses uncertainty for accurate voxels, with similar presence of uncertainty for inaccurate voxels. Code to reproduce experiments is available at https://github.com/prerakmody/bayesuncertainty-error-correspondence
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2409.03179.pdf' target='_blank'>https://arxiv.org/pdf/2409.03179.pdf</a></span>   <span><a href='https://github.com/ZhuKeven/MOBOSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiwen Zhu, Yanjie Wang, Shilv Cai, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03179">Perceptual-Distortion Balanced Image Super-Resolution is a Multi-Objective Optimization Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training Single-Image Super-Resolution (SISR) models using pixel-based regression losses can achieve high distortion metrics scores (e.g., PSNR and SSIM), but often results in blurry images due to insufficient recovery of high-frequency details. Conversely, using GAN or perceptual losses can produce sharp images with high perceptual metric scores (e.g., LPIPS), but may introduce artifacts and incorrect textures. Balancing these two types of losses can help achieve a trade-off between distortion and perception, but the challenge lies in tuning the loss function weights. To address this issue, we propose a novel method that incorporates Multi-Objective Optimization (MOO) into the training process of SISR models to balance perceptual quality and distortion. We conceptualize the relationship between loss weights and image quality assessment (IQA) metrics as black-box objective functions to be optimized within our Multi-Objective Bayesian Optimization Super-Resolution (MOBOSR) framework. This approach automates the hyperparameter tuning process, reduces overall computational cost, and enables the use of numerous loss functions simultaneously. Extensive experiments demonstrate that MOBOSR outperforms state-of-the-art methods in terms of both perceptual quality and distortion, significantly advancing the perception-distortion Pareto frontier. Our work points towards a new direction for future research on balancing perceptual quality and fidelity in nearly all image restoration tasks. The source code and pretrained models are available at: https://github.com/ZhuKeven/MOBOSR.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2409.01212.pdf' target='_blank'>https://arxiv.org/pdf/2409.01212.pdf</a></span>   <span><a href='https://github.com/chencn2020/MobileIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Chen, Sunhan Xu, Yun Zeng, Haochen Guo, Jian Guo, Shuai Liu, Juan Wang, Bing Li, Weiming Hu, Dehua Liu, Hesong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01212">MobileIQA: Exploiting Mobile-level Diverse Opinion Network For No-Reference Image Quality Assessment Using Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rising demand for high-resolution (HR) images, No-Reference Image Quality Assessment (NR-IQA) gains more attention, as it can ecaluate image quality in real-time on mobile devices and enhance user experience. However, existing NR-IQA methods often resize or crop the HR images into small resolution, which leads to a loss of important details. And most of them are of high computational complexity, which hinders their application on mobile devices due to limited computational resources. To address these challenges, we propose MobileIQA, a novel approach that utilizes lightweight backbones to efficiently assess image quality while preserving image details through high-resolution input. MobileIQA employs the proposed multi-view attention learning (MAL) module to capture diverse opinions, simulating subjective opinions provided by different annotators during the dataset annotation process. The model uses a teacher model to guide the learning of a student model through knowledge distillation. This method significantly reduces computational complexity while maintaining high performance. Experiments demonstrate that MobileIQA outperforms novel IQA methods on evaluation metrics and computational efficiency. The code is available at https://github.com/chencn2020/MobileIQA.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2409.00749.pdf' target='_blank'>https://arxiv.org/pdf/2409.00749.pdf</a></span>   <span><a href='https://github.com/sunwei925/UIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Weixia Zhang, Yuqin Cao, Linhan Cao, Jun Jia, Zijian Chen, Zicheng Zhang, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00749">Assessing UHD Image Quality from Aesthetics, Distortions, and Saliency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UHD images, typically with resolutions equal to or higher than 4K, pose a significant challenge for efficient image quality assessment (IQA) algorithms, as adopting full-resolution images as inputs leads to overwhelming computational complexity and commonly used pre-processing methods like resizing or cropping may cause substantial loss of detail. To address this problem, we design a multi-branch deep neural network (DNN) to assess the quality of UHD images from three perspectives: global aesthetic characteristics, local technical distortions, and salient content perception. Specifically, aesthetic features are extracted from low-resolution images downsampled from the UHD ones, which lose high-frequency texture information but still preserve the global aesthetics characteristics. Technical distortions are measured using a fragment image composed of mini-patches cropped from UHD images based on the grid mini-patch sampling strategy. The salient content of UHD images is detected and cropped to extract quality-aware features from the salient regions. We adopt the Swin Transformer Tiny as the backbone networks to extract features from these three perspectives. The extracted features are concatenated and regressed into quality scores by a two-layer multi-layer perceptron (MLP) network. We employ the mean square error (MSE) loss to optimize prediction accuracy and the fidelity loss to optimize prediction monotonicity. Experimental results show that the proposed model achieves the best performance on the UHD-IQA dataset while maintaining the lowest computational complexity, demonstrating its effectiveness and efficiency. Moreover, the proposed model won first prize in ECCV AIM 2024 UHD-IQA Challenge. The code is available at https://github.com/sunwei925/UIQA.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2408.17057.pdf' target='_blank'>https://arxiv.org/pdf/2408.17057.pdf</a></span>   <span><a href='https://github.com/nasimjamshidi/LAR-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17057">LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality Assessment Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in the field of No-Reference Image Quality Assessment (NR-IQA) using deep learning techniques demonstrate high performance across multiple open-source datasets. However, such models are typically very large and complex making them not so suitable for real-world deployment, especially on resource- and battery-constrained mobile devices. To address this limitation, we propose a compact, lightweight NR-IQA model that achieves state-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation and test datasets while being also nearly 5.7 times faster than the fastest SOTA model. Our model features a dual-branch architecture, with each branch separately trained on synthetically and authentically distorted images which enhances the model's generalizability across different distortion types. To improve robustness under diverse real-world visual conditions, we additionally incorporate multiple color spaces during the training process. We also demonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks (KANs) for final quality regression as compared to the conventional Multi-Layer Perceptrons (MLPs). Our evaluation considering various open-source datasets highlights the practical, high-accuracy, and robust performance of our proposed lightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2408.14008.pdf' target='_blank'>https://arxiv.org/pdf/2408.14008.pdf</a></span>   <span><a href='https://github.com/Sueqk/LMM-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Ge, Wei Sun, Yu Zhang, Yunhao Li, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14008">LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of videos on streaming media platforms has underscored the urgent need for effective video quality assessment (VQA) algorithms to monitor and perceptually optimize the quality of streaming videos. However, VQA remains an extremely challenging task due to the diverse video content and the complex spatial and temporal distortions, thus necessitating more advanced methods to address these issues. Nowadays, large multimodal models (LMMs), such as GPT-4V, have exhibited strong capabilities for various visual understanding tasks, motivating us to leverage the powerful multimodal representation ability of LMMs to solve the VQA task. Therefore, we propose the first Large Multi-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel spatiotemporal visual modeling strategy for quality-aware feature extraction. Specifically, we first reformulate the quality regression problem into a question and answering (Q&A) task and construct Q&A prompts for VQA instruction tuning. Then, we design a spatiotemporal vision encoder to extract spatial and temporal features to represent the quality characteristics of videos, which are subsequently mapped into the language space by the spatiotemporal projector for modality alignment. Finally, the aligned visual tokens and the quality-inquired text tokens are aggregated as inputs for the large language model (LLM) to generate the quality score and level. Extensive experiments demonstrate that LMM-VQA achieves state-of-the-art performance across five VQA benchmarks, exhibiting an average improvement of $5\%$ in generalization ability over existing methods. Furthermore, due to the advanced design of the spatiotemporal encoder and projector, LMM-VQA also performs exceptionally well on general video understanding tasks, further validating its effectiveness. Our code will be released at https://github.com/Sueqk/LMM-VQA.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2408.11687.pdf' target='_blank'>https://arxiv.org/pdf/2408.11687.pdf</a></span>   <span><a href='https://github.com/dx199771/Interpretability-AQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Dong, Xinran Liu, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11687">Interpretable Long-term Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term Action Quality Assessment (AQA) evaluates the execution of activities in videos. However, the length presents challenges in fine-grained interpretability, with current AQA methods typically producing a single score by averaging clip features, lacking detailed semantic meanings of individual clips. Long-term videos pose additional difficulty due to the complexity and diversity of actions, exacerbating interpretability challenges. While query-based transformer networks offer promising long-term modeling capabilities, their interpretability in AQA remains unsatisfactory due to a phenomenon we term Temporal Skipping, where the model skips self-attention layers to prevent output degradation. To address this, we propose an attention loss function and a query initialization method to enhance performance and interpretability. Additionally, we introduce a weight-score regression module designed to approximate the scoring patterns observed in human judgments and replace conventional single-score regression, improving the rationality of interpretability. Our approach achieves state-of-the-art results on three real-world, long-term AQA benchmarks. Our code is available at: https://github.com/dx199771/Interpretability-AQA
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2408.11481.pdf' target='_blank'>https://arxiv.org/pdf/2408.11481.pdf</a></span>   <span><a href='https://github.com/littlespray/VE-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11481">VE-Bench: Subjective-Aligned Benchmark Suite for Text-Driven Video Editing Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven video editing has recently experienced rapid development. Despite this, evaluating edited videos remains a considerable challenge. Current metrics tend to fail to align with human perceptions, and effective quantitative metrics for video editing are still notably absent. To address this, we introduce VE-Bench, a benchmark suite tailored to the assessment of text-driven video editing. This suite includes VE-Bench DB, a video quality assessment (VQA) database for video editing. VE-Bench DB encompasses a diverse set of source videos featuring various motions and subjects, along with multiple distinct editing prompts, editing results from 8 different models, and the corresponding Mean Opinion Scores (MOS) from 24 human annotators. Based on VE-Bench DB, we further propose VE-Bench QA, a quantitative human-aligned measurement for the text-driven video editing task. In addition to the aesthetic, distortion, and other visual quality indicators that traditional VQA methods emphasize, VE-Bench QA focuses on the text-video alignment and the relevance modeling between source and edited videos. It proposes a new assessment network for video editing that attains superior performance in alignment with human preferences. To the best of our knowledge, VE-Bench introduces the first quality assessment dataset for video editing and an effective subjective-aligned quantitative metric for this domain. All data and code will be publicly available at https://github.com/littlespray/VE-Bench.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2408.09920.pdf' target='_blank'>https://arxiv.org/pdf/2408.09920.pdf</a></span>   <span><a href='https://github.com/KANGX99/SMIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang Xiao, Xu Wang, Yulin He, Baoliang Chen, Xuelin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09920">Sliced Maximal Information Coefficient: A Training-Free Approach for Image Quality Assessment Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-reference image quality assessment (FR-IQA) models generally operate by measuring the visual differences between a degraded image and its reference. However, existing FR-IQA models including both the classical ones (eg, PSNR and SSIM) and deep-learning based measures (eg, LPIPS and DISTS) still exhibit limitations in capturing the full perception characteristics of the human visual system (HVS). In this paper, instead of designing a new FR-IQA measure, we aim to explore a generalized human visual attention estimation strategy to mimic the process of human quality rating and enhance existing IQA models. In particular, we model human attention generation by measuring the statistical dependency between the degraded image and the reference image. The dependency is captured in a training-free manner by our proposed sliced maximal information coefficient and exhibits surprising generalization in different IQA measures. Experimental results verify the performance of existing IQA models can be consistently improved when our attention module is incorporated. The source code is available at https://github.com/KANGX99/SMIC.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2408.02138.pdf' target='_blank'>https://arxiv.org/pdf/2408.02138.pdf</a></span>   <span><a href='https://abrarmajeedi.github.io/rica2_aqa/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi GNVV, Yin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02138">RICA2: Rubric-Informed, Calibrated Assessment of Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to quantify how well an action is carried out, also known as action quality assessment (AQA), has attracted recent interest in the vision community. Unfortunately, prior methods often ignore the score rubric used by human experts and fall short of quantifying the uncertainty of the model prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model that integrates score rubric and accounts for prediction uncertainty for AQA. Central to our method lies in stochastic embeddings of action steps, defined on a graph structure that encodes the score rubric. The embeddings spread probabilistic density in the latent space and allow our method to represent model uncertainty. The graph encodes the scoring criteria, based on which the quality scores can be decoded. We demonstrate that our method establishes new state of the art on public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS, with superior performance in score prediction and uncertainty calibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2407.21408.pdf' target='_blank'>https://arxiv.org/pdf/2407.21408.pdf</a></span>   <span><a href='https://github.com/zczhang-sjtu/UGVQ.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Zhang, Wei Sun, Xinyue Li, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Fengyu Sun, Shangling Jui, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21408">Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, artificial intelligence (AI)-driven video generation has gained significant attention. Consequently, there is a growing need for accurate video quality assessment (VQA) metrics to evaluate the perceptual quality of AI-generated content (AIGC) videos and optimize video generation models. However, assessing the quality of AIGC videos remains a significant challenge because these videos often exhibit highly complex distortions, such as unnatural actions and irrational objects. To address this challenge, we systematically investigate the AIGC-VQA problem, considering both subjective and objective quality assessment perspectives. For the subjective perspective, we construct the Large-scale Generated Video Quality assessment (LGVQ) dataset, consisting of 2,808 AIGC videos generated by 6 video generation models using 468 carefully curated text prompts. We evaluate the perceptual quality of AIGC videos from three critical dimensions: spatial quality, temporal quality, and text-video alignment. For the objective perspective, we establish a benchmark for evaluating existing quality assessment metrics on the LGVQ dataset. Our findings show that current metrics perform poorly on this dataset, highlighting a gap in effective evaluation tools. To bridge this gap, we propose the Unify Generated Video Quality assessment (UGVQ) model, designed to accurately evaluate the multi-dimensional quality of AIGC videos. The UGVQ model integrates the visual and motion features of videos with the textual features of their corresponding prompts, forming a unified quality-aware feature representation tailored to AIGC videos. Experimental results demonstrate that UGVQ achieves state-of-the-art performance on the LGVQ dataset across all three quality dimensions. Both the LGVQ dataset and the UGVQ model are publicly available on https://github.com/zczhang-sjtu/UGVQ.git.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2407.21363.pdf' target='_blank'>https://arxiv.org/pdf/2407.21363.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/ESIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilei Zhu, Liu Yang, Huiyu Duan, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21363">ESIQA: Perceptual Quality Assessment of Vision-Pro-based Egocentric Spatial Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of eXtended Reality (XR), photo capturing and display technology based on head-mounted displays (HMDs) have experienced significant advancements and gained considerable attention. Egocentric spatial images and videos are emerging as a compelling form of stereoscopic XR content. The assessment for the Quality of Experience (QoE) of XR content is important to ensure a high-quality viewing experience. Different from traditional 2D images, egocentric spatial images present challenges for perceptual quality assessment due to their special shooting, processing methods, and stereoscopic characteristics. However, the corresponding image quality assessment (IQA) research for egocentric spatial images is still lacking. In this paper, we establish the Egocentric Spatial Images Quality Assessment Database (ESIQAD), the first IQA database dedicated for egocentric spatial images as far as we know. Our ESIQAD includes 500 egocentric spatial images and the corresponding mean opinion scores (MOSs) under three display modes, including 2D display, 3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a novel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which predicts the perceptual quality of egocentric spatial images under the three display modes. Specifically, we first extract features from multiple visual state space duality (VSSD) blocks, then apply cross attention to fuse binocular view information and use transposed attention to further refine the features. The multi-stage features are finally concatenated and fed into a quality regression network to predict the quality score. Extensive experimental results demonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the ESIQAD under all three display modes. The database and code are available at https://github.com/IntMeGroup/ESIQA.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2407.17035.pdf' target='_blank'>https://arxiv.org/pdf/2407.17035.pdf</a></span>   <span><a href='https://github.com/Q-Future/Q-Ground' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofeng Chen, Sensen Yang, Haoning Wu, Liang Liao, Zicheng Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17035">Q-Ground: Image Quality Grounding with Large Multi-modality Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances of large multi-modality models (LMM) have greatly improved the ability of image quality assessment (IQA) method to evaluate and explain the quality of visual content. However, these advancements are mostly focused on overall quality assessment, and the detailed examination of local quality, which is crucial for comprehensive visual understanding, is still largely unexplored. In this work, we introduce Q-Ground, the first framework aimed at tackling fine-scale visual quality grounding by combining large multi-modality models with detailed visual quality analysis. Central to our contribution is the introduction of the QGround-100K dataset, a novel resource containing 100k triplets of (image, quality text, distortion segmentation) to facilitate deep investigations into visual quality. The dataset comprises two parts: one with human-labeled annotations for accurate quality assessment, and another labeled automatically by LMMs such as GPT4V, which helps improve the robustness of model training while also reducing the costs of data collection. With the QGround-100K dataset, we propose a LMM-based method equipped with multi-scale feature learning to learn models capable of performing both image quality answering and distortion segmentation based on text prompts. This dual-capability approach not only refines the model's understanding of region-aware image quality but also enables it to interactively respond to complex, text-based queries about image quality and specific distortions. Q-Ground takes a step towards sophisticated visual quality analysis in a finer scale, establishing a new benchmark for future research in the area. Codes and dataset are available at https://github.com/Q-Future/Q-Ground.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2407.16541.pdf' target='_blank'>https://arxiv.org/pdf/2407.16541.pdf</a></span>   <span><a href='https://github.com/KeiChiTse/QPT-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhi Xie, Kun Yuan, Yunpeng Qu, Mingda Wu, Ming Sun, Chao Zhou, Jihong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16541">QPT V2: Masked Image Modeling Advances Visual Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment and aesthetics assessment aim to evaluate the perceived quality and aesthetics of visual content. Current learning-based methods suffer greatly from the scarcity of labeled data and usually perform sub-optimally in terms of generalization. Although masked image modeling (MIM) has achieved noteworthy advancements across various high-level tasks (e.g., classification, detection etc.). In this work, we take on a novel perspective to investigate its capabilities in terms of quality- and aesthetics-awareness. To this end, we propose Quality- and aesthetics-aware pretraining (QPT V2), the first pretraining framework based on MIM that offers a unified solution to quality and aesthetics assessment. To perceive the high-level semantics and fine-grained details, pretraining data is curated. To comprehensively encompass quality- and aesthetics-related factors, degradation is introduced. To capture multi-scale quality and aesthetic information, model structure is modified. Extensive experimental results on 11 downstream benchmarks clearly show the superior performance of QPT V2 in comparison with current state-of-the-art approaches and other pretraining paradigms. Code and models will be released at \url{https://github.com/KeiChiTse/QPT-V2}.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2407.16124.pdf' target='_blank'>https://arxiv.org/pdf/2407.16124.pdf</a></span>   <span><a href='https://github.com/ljh0v0/FMD-frechet-motion-distance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16124">FrÃ©chet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant advancements have been made in video generative models recently. Unlike image generation, video generation presents greater challenges, requiring not only generating high-quality frames but also ensuring temporal consistency across these frames. Despite the impressive progress, research on metrics for evaluating the quality of generated videos, especially concerning temporal and motion consistency, remains underexplored. To bridge this research gap, we propose FrÃ©chet Video Motion Distance (FVMD) metric, which focuses on evaluating motion consistency in video generation. Specifically, we design explicit motion features based on key point tracking, and then measure the similarity between these features via the FrÃ©chet distance. We conduct sensitivity analysis by injecting noise into real videos to verify the effectiveness of FVMD. Further, we carry out a large-scale human study, demonstrating that our metric effectively detects temporal noise and aligns better with human perceptions of generated video quality than existing metrics. Additionally, our motion features can consistently improve the performance of Video Quality Assessment (VQA) models, indicating that our approach is also applicable to unary video quality evaluation. Code is available at https://github.com/ljh0v0/FMD-frechet-motion-distance.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2407.15171.pdf' target='_blank'>https://arxiv.org/pdf/2407.15171.pdf</a></span>   <span><a href='https://github.com/cvlab-stonybrook/LS-sample-quality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Xu, Hieu Le, Dimitris Samaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15171">Assessing Sample Quality via the Latent Space of Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in generative models increase the need for sample quality assessment. To do so, previous methods rely on a pre-trained feature extractor to embed the generated samples and real samples into a common space for comparison. However, different feature extractors might lead to inconsistent assessment outcomes. Moreover, these methods are not applicable for domains where a robust, universal feature extractor does not yet exist, such as medical images or 3D assets. In this paper, we propose to directly examine the latent space of the trained generative model to infer generated sample quality. This is feasible because the quality a generated sample directly relates to the amount of training data resembling it, and we can infer this information by examining the density of the latent space. Accordingly, we use a latent density score function to quantify sample quality. We show that the proposed score correlates highly with the sample quality for various generative models including VAEs, GANs and Latent Diffusion Models. Compared with previous quality assessment methods, our method has the following advantages: 1) pre-generation quality estimation with reduced computational cost, 2) generalizability to various domains and modalities, and 3) applicability to latent-based image editing and generation methods. Extensive experiments demonstrate that our proposed methods can benefit downstream tasks such as few-shot image classification and latent face image editing. Code is available at https://github.com/cvlab-stonybrook/LS-sample-quality.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2407.14197.pdf' target='_blank'>https://arxiv.org/pdf/2407.14197.pdf</a></span>   <span><a href='https://github.com/Qi-Yangsjtu/GGSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Yang, Kaifa Yang, Yuke Xing, Yiling Xu, Zhu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14197">A Benchmark for Gaussian Splatting Compression and Quality Assessment Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To fill the gap of traditional GS compression method, in this paper, we first propose a simple and effective GS data compression anchor called Graph-based GS Compression (GGSC). GGSC is inspired by graph signal processing theory and uses two branches to compress the primitive center and attributes. We split the whole GS sample via KDTree and clip the high-frequency components after the graph Fourier transform. Followed by quantization, G-PCC and adaptive arithmetic coding are used to compress the primitive center and attribute residual matrix to generate the bitrate file. GGSS is the first work to explore traditional GS compression, with advantages that can reveal the GS distortion characteristics corresponding to typical compression operation, such as high-frequency clipping and quantization. Second, based on GGSC, we create a GS Quality Assessment dataset (GSQA) with 120 samples. A subjective experiment is conducted in a laboratory environment to collect subjective scores after rendering GS into Processed Video Sequences (PVS). We analyze the characteristics of different GS distortions based on Mean Opinion Scores (MOS), demonstrating the sensitivity of different attributes distortion to visual quality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are made publicly available at https://github.com/Qi-Yangsjtu/GGSC.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2407.13719.pdf' target='_blank'>https://arxiv.org/pdf/2407.13719.pdf</a></span>   <span><a href='https://github.com/Troivyn/HazeCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyi Wang, Wenhao Li, Xiaohong Liu, Chunyi Li, Zicheng Zhang, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13719">HazeCLIP: Towards Language Guided Real-World Image Dehazing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods have achieved remarkable performance in image dehazing, particularly on synthetic datasets. However, they often struggle with real-world hazy images due to domain shift, limiting their practical applicability. This paper introduces HazeCLIP, a language-guided adaptation framework designed to enhance the real-world performance of pre-trained dehazing networks. Inspired by the Contrastive Language-Image Pre-training (CLIP) model's ability to distinguish between hazy and clean images, we leverage it to evaluate dehazing results. Combined with a region-specific dehazing technique and tailored prompt sets, the CLIP model accurately identifies hazy areas, providing a high-quality, human-like prior that guides the fine-tuning process of pre-trained networks. Extensive experiments demonstrate that HazeCLIP achieves state-of-the-art performance in real-word image dehazing, evaluated through both visual quality and image quality assessment metrics. Codes are available at https://github.com/Troivyn/HazeCLIP.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2407.11496.pdf' target='_blank'>https://arxiv.org/pdf/2407.11496.pdf</a></span>   <span><a href='https://github.com/xinyiW915/ReLaX-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Angeliki Katsenou, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11496">ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of User-Generated Content (UGC) exchanged between users and sharing platforms, the need for video quality assessment in the wild is increasingly evident. UGC is typically acquired using consumer devices and undergoes multiple rounds of compression (transcoding) before reaching the end user. Therefore, traditional quality metrics that employ the original content as a reference are not suitable. In this paper, we propose ReLaX-VQA, a novel No-Reference Video Quality Assessment (NR-VQA) model that aims to address the challenges of evaluating the quality of diverse video content without reference to the original uncompressed videos. ReLaX-VQA uses frame differences to select spatio-temporal fragments intelligently together with different expressions of spatial features associated with the sampled frames. These are then used to better capture spatial and temporal variabilities in the quality of neighbouring frames. Furthermore, the model enhances abstraction by employing layer-stacking techniques in deep neural network features from Residual Networks and Vision Transformers. Extensive testing across four UGC datasets demonstrates that ReLaX-VQA consistently outperforms existing NR-VQA methods, achieving an average SRCC of 0.8658 and PLCC of 0.8873. Open-source code and trained models that will facilitate further research and applications of NR-VQA can be found at https://github.com/xinyiW915/ReLaX-VQA.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2407.09806.pdf' target='_blank'>https://arxiv.org/pdf/2407.09806.pdf</a></span>   <span><a href='https://github.com/zhangyujie-1998/AFQ-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhang, Qi Yang, Ziyu Shan, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09806">Asynchronous Feedback Network for Perceptual Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed the success of the deep learning-based technique in research of no-reference point cloud quality assessment (NR-PCQA). For a more accurate quality prediction, many previous studies have attempted to capture global and local features in a bottom-up manner, but ignored the interaction and promotion between them. To solve this problem, we propose a novel asynchronous feedback quality prediction network (AFQ-Net). Motivated by human visual perception mechanisms, AFQ-Net employs a dual-branch structure to deal with global and local features, simulating the left and right hemispheres of the human brain, and constructs a feedback module between them. Specifically, the input point clouds are first fed into a transformer-based global encoder to generate the attention maps that highlight these semantically rich regions, followed by being merged into the global feature. Then, we utilize the generated attention maps to perform dynamic convolution for different semantic regions and obtain the local feature. Finally, a coarse-to-fine strategy is adopted to merge the two features into the final quality score. We conduct comprehensive experiments on three datasets and achieve superior performance over the state-of-the-art approaches on all of these datasets. The code will be available at The code will be available at https://github.com/zhangyujie-1998/AFQ-Net.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2407.08165.pdf' target='_blank'>https://arxiv.org/pdf/2407.08165.pdf</a></span>   <span><a href='https://github.com/YukeXing/Explicit-NeRF-QA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuke Xing, Qi Yang, Kaifa Yang, Yilin Xu, Zhu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08165">Explicit-NeRF-QA: A Quality Assessment Database for Explicit NeRF Model Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Neural Radiance Fields (NeRF) have demonstrated significant advantages in representing and synthesizing 3D scenes. Explicit NeRF models facilitate the practical NeRF applications with faster rendering speed, and also attract considerable attention in NeRF compression due to its huge storage cost. To address the challenge of the NeRF compression study, in this paper, we construct a new dataset, called Explicit-NeRF-QA. We use 22 3D objects with diverse geometries, textures, and material complexities to train four typical explicit NeRF models across five parameter levels. Lossy compression is introduced during the model generation, pivoting the selection of key parameters such as hash table size for InstantNGP and voxel grid resolution for Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a large scale subjective experiment with lab environment is conducted to collect subjective scores from 21 viewers. The diversity of content, accuracy of mean opinion scores (MOS), and characteristics of NeRF distortion are comprehensively presented, establishing the heterogeneity of the proposed dataset. The state-of-the-art objective metrics are tested in the new dataset. Best Person correlation, which is around 0.85, is collected from the full-reference objective metric. All tested no-reference metrics report very poor results with 0.4 to 0.6 correlations, demonstrating the need for further development of more robust no-reference metrics. The dataset, including NeRF samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is made publicly available at the following location: https://github.com/YukeXing/Explicit-NeRF-QA.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2407.07254.pdf' target='_blank'>https://arxiv.org/pdf/2407.07254.pdf</a></span>   <span><a href='https://github.com/arf111/HAMIL-QA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/arf111/HAMIL-QA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>K M Arefeen Sultan, Md Hasibul Husain Hisham, Benjamin Orkild, Alan Morris, Eugene Kholmovski, Erik Bieging, Eugene Kwan, Ravi Ranjan, Ed DiBella, Shireen Elhabian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07254">HAMIL-QA: Hierarchical Approach to Multiple Instance Learning for Atrial LGE MRI Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accurate evaluation of left atrial fibrosis via high-quality 3D Late Gadolinium Enhancement (LGE) MRI is crucial for atrial fibrillation management but is hindered by factors like patient movement and imaging variability. The pursuit of automated LGE MRI quality assessment is critical for enhancing diagnostic accuracy, standardizing evaluations, and improving patient outcomes. The deep learning models aimed at automating this process face significant challenges due to the scarcity of expert annotations, high computational costs, and the need to capture subtle diagnostic details in highly variable images. This study introduces HAMIL-QA, a multiple instance learning (MIL) framework, designed to overcome these obstacles. HAMIL-QA employs a hierarchical bag and sub-bag structure that allows for targeted analysis within sub-bags and aggregates insights at the volume level. This hierarchical MIL approach reduces reliance on extensive annotations, lessens computational load, and ensures clinically relevant quality predictions by focusing on diagnostically critical image features. Our experiments show that HAMIL-QA surpasses existing MIL methods and traditional supervised approaches in accuracy, AUROC, and F1-Score on an LGE MRI scan dataset, demonstrating its potential as a scalable solution for LGE MRI quality assessment automation. The code is available at: $\href{https://github.com/arf111/HAMIL-QA}{\text{this https URL}}$
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2407.07176.pdf' target='_blank'>https://arxiv.org/pdf/2407.07176.pdf</a></span>   <span><a href='https://yeolj00.github.io/personal-projects/personalized-aesthetics/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jooyeol Yun, Jaegul Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07176">Scaling Up Personalized Image Aesthetic Assessment via Task Vector Customization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of personalized image aesthetic assessment seeks to tailor aesthetic score prediction models to match individual preferences with just a few user-provided inputs. However, the scalability and generalization capabilities of current approaches are considerably restricted by their reliance on an expensive curated database. To overcome this long-standing scalability challenge, we present a unique approach that leverages readily available databases for general image aesthetic assessment and image quality assessment. Specifically, we view each database as a distinct image score regression task that exhibits varying degrees of personalization potential. By determining optimal combinations of task vectors, known to represent specific traits of each database, we successfully create personalized models for individuals. This approach of integrating multiple models allows us to harness a substantial amount of data. Our extensive experiments demonstrate the effectiveness of our approach in generalizing to previously unseen domains-a challenge previous approaches have struggled to achieve-making it highly applicable to real-world scenarios. Our novel approach significantly advances the field by offering scalable solutions for personalized aesthetic assessment and establishing high standards for future research. https://yeolj00.github.io/personal-projects/personalized-aesthetics/
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2407.06189.pdf' target='_blank'>https://arxiv.org/pdf/2407.06189.pdf</a></span>   <span><a href='https://orrzohar.github.io/projects/video-star/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, Serena Yeung-Levy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06189">Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of Large Vision Language Models (LVLMs) is dependent on the size and quality of their training datasets. Existing video instruction tuning datasets lack diversity as they are derived by prompting large language models with video captions to generate question-answer pairs, and are therefore mostly descriptive. Meanwhile, many labeled video datasets with diverse labels and supervision exist - however, we find that their integration into LVLMs is non-trivial. Herein, we present Video Self-Training with augmented Reasoning (Video-STaR), the first video self-training approach. Video-STaR allows the utilization of any labeled video dataset for video instruction tuning. In Video-STaR, an LVLM cycles between instruction generation and finetuning, which we show (I) improves general video understanding and (II) adapts LVLMs to novel downstream tasks with existing supervision. During generation, an LVLM is prompted to propose an answer. The answers are then filtered only to those that contain the original video labels, and the LVLM is then re-trained on the generated dataset. By only training on generated answers that contain the correct video labels, Video-STaR utilizes these existing video labels as weak supervision for video instruction tuning. Our results demonstrate that Video-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA, where TempCompass performance improved by 10%, and (II) on downstream tasks, where Video-STaR improved Kinetics700-QA accuracy by 20% and action quality assessment on FineDiving by 15%.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2407.03886.pdf' target='_blank'>https://arxiv.org/pdf/2407.03886.pdf</a></span>   <span><a href='https://github.com/I2-Multimedia-Lab/DSMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsong Shi, Pan Gao, Xiaojiang Peng, Jie Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03886">DSMix: Distortion-Induced Sensitivity Map Based Pre-training for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) has long been a fundamental challenge in image understanding. In recent years, deep learning-based IQA methods have shown promising performance. However, the lack of large amounts of labeled data in the IQA field has hindered further advancements in these methods. This paper introduces DSMix, a novel data augmentation technique specifically designed for IQA tasks, aiming to overcome this limitation. DSMix leverages the distortion-induced sensitivity map (DSM) of an image as prior knowledge. It applies cut and mix operations to diverse categories of synthetic distorted images, assigning confidence scores to class labels based on the aforementioned prior knowledge. In the pre-training phase using DSMix-augmented data, knowledge distillation is employed to enhance the model's ability to extract semantic features. Experimental results on both synthetic and authentic IQA datasets demonstrate the significant predictive and generalization performance achieved by DSMix, without requiring fine-tuning of the full model. Code is available at \url{https://github.com/I2-Multimedia-Lab/DSMix}.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2407.03885.pdf' target='_blank'>https://arxiv.org/pdf/2407.03885.pdf</a></span>   <span><a href='https://github.com/zhangyujie-1998/PHM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhang, Qi Yang, Yiling Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03885">Perception-Guided Quality Metric of 3D Point Clouds Using Hybrid Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Full-reference point cloud quality assessment (FR-PCQA) aims to infer the quality of distorted point clouds with available references. Most of the existing FR-PCQA metrics ignore the fact that the human visual system (HVS) dynamically tackles visual information according to different distortion levels (i.e., distortion detection for high-quality samples and appearance perception for low-quality samples) and measure point cloud quality using unified features. To bridge the gap, in this paper, we propose a perception-guided hybrid metric (PHM) that adaptively leverages two visual strategies with respect to distortion degree to predict point cloud quality: to measure visible difference in high-quality samples, PHM takes into account the masking effect and employs texture complexity as an effective compensatory factor for absolute difference; on the other hand, PHM leverages spectral graph theory to evaluate appearance degradation in low-quality samples. Variations in geometric signals on graphs and changes in the spectral graph wavelet coefficients are utilized to characterize geometry and texture appearance degradation, respectively. Finally, the results obtained from the two components are combined in a non-linear method to produce an overall quality score of the tested point cloud. The results of the experiment on five independent databases show that PHM achieves state-of-the-art (SOTA) performance and offers significant performance improvement in multiple distortion environments. The code is publicly available at https://github.com/zhangyujie-1998/PHM.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2406.19393.pdf' target='_blank'>https://arxiv.org/pdf/2406.19393.pdf</a></span>   <span><a href='https://github.com/VICO-UoE/Looking3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankan Bhunia, Changjian Li, Hakan Bilen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19393">Looking 3D: Anomaly Detection with 2D-3D Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic anomaly detection based on visual cues holds practical significance in various domains, such as manufacturing and product quality assessment. This paper introduces a new conditional anomaly detection problem, which involves identifying anomalies in a query image by comparing it to a reference shape. To address this challenge, we have created a large dataset, BrokenChairs-180K, consisting of around 180K images, with diverse anomalies, geometries, and textures paired with 8,143 reference 3D shapes. To tackle this task, we have proposed a novel transformer-based approach that explicitly learns the correspondence between the query image and reference 3D shape via feature alignment and leverages a customized attention mechanism for anomaly detection. Our approach has been rigorously evaluated through comprehensive experiments, serving as a benchmark for future research in this domain.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2406.16641.pdf' target='_blank'>https://arxiv.org/pdf/2406.16641.pdf</a></span>   <span><a href='https://github.com/JunFu1995/CLIP-AGIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Fu, Wei Zhou, Qiuping Jiang, Hantao Liu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16641">Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, textual prompt tuning has shown inspirational performance in adapting Contrastive Language-Image Pre-training (CLIP) models to natural image quality assessment. However, such uni-modal prompt learning method only tunes the language branch of CLIP models. This is not enough for adapting CLIP models to AI generated image quality assessment (AGIQA) since AGIs visually differ from natural images. In addition, the consistency between AGIs and user input text prompts, which correlates with the perceptual quality of AGIs, is not investigated to guide AGIQA. In this letter, we propose vision-language consistency guided multi-modal prompt learning for blind AGIQA, dubbed CLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in language and vision branches of CLIP models, respectively. Moreover, we design a text-to-image alignment quality prediction task, whose learned vision-language consistency knowledge is used to guide the optimization of the above multi-modal prompts. Experimental results on two public AGIQA datasets demonstrate that the proposed method outperforms state-of-the-art quality assessment models. The source code is available at https://github.com/JunFu1995/CLIP-AGIQA.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2406.15848.pdf' target='_blank'>https://arxiv.org/pdf/2406.15848.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/quality-guided-enhancement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Gao, Huiyu Duan, Xinyue Li, Kang Fu, Yicong Peng, Qihang Xu, Yuanyuan Chang, Jia Wang, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15848">Quality-guided Skin Tone Enhancement for Portrait Photography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, learning-based color and tone enhancement methods for photos have become increasingly popular. However, most learning-based image enhancement methods just learn a mapping from one distribution to another based on one dataset, lacking the ability to adjust images continuously and controllably. It is important to enable the learning-based enhancement models to adjust an image continuously, since in many cases we may want to get a slighter or stronger enhancement effect rather than one fixed adjusted result. In this paper, we propose a quality-guided image enhancement paradigm that enables image enhancement models to learn the distribution of images with various quality ratings. By learning this distribution, image enhancement models can associate image features with their corresponding perceptual qualities, which can be used to adjust images continuously according to different quality scores. To validate the effectiveness of our proposed method, a subjective quality assessment experiment is first conducted, focusing on skin tone adjustment in portrait photography. Guided by the subjective quality ratings obtained from this experiment, our method can adjust the skin tone corresponding to different quality requirements. Furthermore, an experiment conducted on 10 natural raw images corroborates the effectiveness of our model in situations with fewer subjects and fewer shots, and also demonstrates its general applicability to natural images. Our project page is https://github.com/IntMeGroup/quality-guided-enhancement .
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2406.10520.pdf' target='_blank'>https://arxiv.org/pdf/2406.10520.pdf</a></span>   <span><a href='https://github.com/STAC-USC/FRSVR-PCQA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/STAC-USC/FRSVR-PCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryosuke Watanabe, Shashank N. Sridhara, Haoran Hong, Eduardo Pavez, Keisuke Nonaka, Tatsuya Kobayashi, Antonio Ortega
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10520">Full reference point cloud quality assessment using support vector regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point clouds are a general format for representing realistic 3D objects in diverse 3D applications. Since point clouds have large data sizes, developing efficient point cloud compression methods is crucial. However, excessive compression leads to various distortions, which deteriorates the point cloud quality perceived by end users. Thus, establishing reliable point cloud quality assessment (PCQA) methods is essential as a benchmark to develop efficient compression methods. This paper presents an accurate full-reference point cloud quality assessment (FR-PCQA) method called full-reference quality assessment using support vector regression (FRSVR) for various types of degradations such as compression distortion, Gaussian noise, and down-sampling. The proposed method demonstrates accurate PCQA by integrating five FR-based metrics covering various types of errors (e.g., considering geometric distortion, color distortion, and point count) using support vector regression (SVR). Moreover, the proposed method achieves a superior trade-off between accuracy and calculation speed because it includes only the calculation of these five simple metrics and SVR, which can perform fast prediction. Experimental results with three types of open datasets show that the proposed method is more accurate than conventional FR-PCQA methods. In addition, the proposed method is faster than state-of-the-art methods that utilize complicated features such as curvature and multi-scale features. Thus, the proposed method provides excellent performance in terms of the accuracy of PCQA and processing speed. Our method is available from https://github.com/STAC-USC/FRSVR-PCQA.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2406.09546.pdf' target='_blank'>https://arxiv.org/pdf/2406.09546.pdf</a></span>   <span><a href='https://github.com/bingo-G/QMamba.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengbin Guan, Xin Li, Zihao Yu, Yiting Lu, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09546">QMamba: On First Exploration of Vision Mamba for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we take the first exploration of the recently popular foundation model, i.e., State Space Model/Mamba, in image quality assessment (IQA), aiming at observing and excavating the perception potential in vision Mamba. A series of works on Mamba has shown its significant potential in various fields, e.g., segmentation and classification. However, the perception capability of Mamba remains under-explored. Consequently, we propose QMamba by revisiting and adapting the Mamba model for three crucial IQA tasks, i.e., task-specific, universal, and transferable IQA, which reveals its clear advantages over existing foundational models, e.g., Swin Transformer, ViT, and CNNs, in terms of perception and computational cost. To improve the transferability of QMamba, we propose the StylePrompt tuning paradigm, where lightweight mean and variance prompts are injected to assist task-adaptive transfer learning of pre-trained QMamba for different downstream IQA tasks. Compared with existing prompt tuning strategies, our StylePrompt enables better perceptual transfer with lower computational cost. Extensive experiments on multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate the effectiveness of our proposed QMamba. The code will be available at: https://github.com/bingo-G/QMamba.git
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2406.08377.pdf' target='_blank'>https://arxiv.org/pdf/2406.08377.pdf</a></span>   <span><a href='https://github.com/eezkni/DDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Wu, Zhangkai Ni, Hanli Wang, Wenhan Yang, Yuyin Zhou, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08377">DDR: Exploiting Deep Degradation Response as Flexible Image Descriptor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image deep features extracted by pre-trained networks are known to contain rich and informative representations. In this paper, we present Deep Degradation Response (DDR), a method to quantify changes in image deep features under varying degradation conditions. Specifically, our approach facilitates flexible and adaptive degradation, enabling the controlled synthesis of image degradation through text-driven prompts. Extensive evaluations demonstrate the versatility of DDR as an image descriptor, with strong correlations observed with key image attributes such as complexity, colorfulness, sharpness, and overall quality. Moreover, we demonstrate the efficacy of DDR across a spectrum of applications. It excels as a blind image quality assessment metric, outperforming existing methodologies across multiple datasets. Additionally, DDR serves as an effective unsupervised learning objective in image restoration tasks, yielding notable advancements in image deblurring and single-image super-resolution. Our code is available at: https://github.com/eezkni/DDR
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2406.01069.pdf' target='_blank'>https://arxiv.org/pdf/2406.01069.pdf</a></span>   <span><a href='https://github.com/zht8506/UniQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hantao Zhou, Longxiang Tang, Rui Yang, Guanyi Qin, Yan Zhang, Yutao Li, Xiu Li, Runze Hu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01069">UniQA: Unified Vision-Language Pre-training for Image Quality and Aesthetic Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to simulate human subjective perception of image visual quality and aesthetic appeal. Despite distinct learning objectives, they have underlying interconnectedness due to consistent human assessment perception. In this paper, we propose Unified vision-language pre-training of Quality and Aesthetics (UniQA}), to extract useful and common representations from two tasks, thereby benefiting them simultaneously. However, the lack of text in the IQA datasets and the textual noise in the IAA datasets pose severe challenges for multimodal pre-training. To address this, we (1) utilize multimodal large language models (MLLMs) to generate high-quality text descriptions; (2) use the generated text for IAA as metadata to purify noisy IAA data. To effectively adapt the pre-trained UniQA to downstream tasks, we further propose a lightweight adapter that utilizes versatile cues to fully exploit the extensive knowledge of the pre-trained model. UniQA demonstrates high competitiveness in various image assessment tasks, including classical IQA and IAA tasks, few-label IQA, and other downstream tasks, showing promise as a foundational assessment model. Codes are available at https://github.com/zht8506/UniQA.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2406.00212.pdf' target='_blank'>https://arxiv.org/pdf/2406.00212.pdf</a></span>   <span><a href='https://chenfeng-bristol.github.io/MVAD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Duolikun Danier, Fan Zhang, Alex Mackin, Andrew Collins, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00212">MVAD: A Multiple Visual Artifact Detector for Video Streaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual artifacts are often introduced into streamed video content, due to prevailing conditions during content production and delivery. Since these can degrade the quality of the user's experience, it is important to automatically and accurately detect them in order to enable effective quality measurement and enhancement. Existing detection methods often focus on a single type of artifact and/or determine the presence of an artifact through thresholding objective quality indices. Such approaches have been reported to offer inconsistent prediction performance and are also impractical for real-world applications where multiple artifacts co-exist and interact. In this paper, we propose a Multiple Visual Artifact Detector, MVAD, for video streaming which, for the first time, is able to detect multiple artifacts using a single framework that is not reliant on video quality assessment models. Our approach employs a new Artifact-aware Dynamic Feature Extractor (ADFE) to obtain artifact-relevant spatial features within each frame for multiple artifact types. The extracted features are further processed by a Recurrent Memory Vision Transformer (RMViT) module, which captures both short-term and long-term temporal information within the input video. The proposed network architecture is optimized in an end-to-end manner based on a new, large and diverse training database that is generated by simulating the video streaming pipeline and based on Adversarial Data Augmentation. This model has been evaluated on two video artifact databases, Maxwell and BVI-Artifact, and achieves consistent and improved prediction results for ten target visual artifacts when compared to seven existing single and multiple artifact detectors. The source code and training database will be available at https://chenfeng-bristol.github.io/MVAD/.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2405.19996.pdf' target='_blank'>https://arxiv.org/pdf/2405.19996.pdf</a></span>   <span><a href='https://github.com/RomGai/DP-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghao Fu, Yufei Wang, Wenhan Yang, Alex C. Kot, Bihan Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19996">DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (IQA) in the wild, which assesses the quality of images with complex authentic distortions and no reference images, presents significant challenges. Given the difficulty in collecting large-scale training data, leveraging limited data to develop a model with strong generalization remains an open problem. Motivated by the robust image perception capabilities of pre-trained text-to-image (T2I) diffusion models, we propose a novel IQA method, diffusion priors-based IQA (DP-IQA), to utilize the T2I model's prior for improved performance and generalization ability. Specifically, we utilize pre-trained Stable Diffusion as the backbone, extracting multi-level features from the denoising U-Net guided by prompt embeddings through a tunable text adapter. Simultaneously, an image adapter compensates for information loss introduced by the lossy pre-trained encoder. Unlike T2I models that require full image distribution modeling, our approach targets image quality assessment, which inherently requires fewer parameters. To improve applicability, we distill the knowledge into a lightweight CNN-based student model, significantly reducing parameters while maintaining or even enhancing generalization performance. Experimental results demonstrate that DP-IQA achieves state-of-the-art performance on various in-the-wild datasets, highlighting the superior generalization capability of T2I priors in blind IQA tasks. To our knowledge, DP-IQA is the first method to apply pre-trained diffusion priors in blind IQA. Codes and checkpoints are available at https://github.com/RomGai/DP-IQA.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2405.18842.pdf' target='_blank'>https://arxiv.org/pdf/2405.18842.pdf</a></span>   <span><a href='https://depictqa.github.io/depictqa-wild/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, Tianfan Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18842">Descriptive Image Quality Assessment in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2405.18790.pdf' target='_blank'>https://arxiv.org/pdf/2405.18790.pdf</a></span>   <span><a href='https://github.com/eezkni/MDFS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangkai Ni, Yue Liu, Keyan Ding, Wenhan Yang, Hanli Wang, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18790">Opinion-Unaware Blind Image Quality Assessment using Multi-Scale Deep Feature Statistics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based methods have significantly influenced the blind image quality assessment (BIQA) field, however, these methods often require training using large amounts of human rating data. In contrast, traditional knowledge-based methods are cost-effective for training but face challenges in effectively extracting features aligned with human visual perception. To bridge these gaps, we propose integrating deep features from pre-trained visual models with a statistical analysis model into a Multi-scale Deep Feature Statistics (MDFS) model for achieving opinion-unaware BIQA (OU-BIQA), thereby eliminating the reliance on human rating data and significantly improving training efficiency. Specifically, we extract patch-wise multi-scale features from pre-trained vision models, which are subsequently fitted into a multivariate Gaussian (MVG) model. The final quality score is determined by quantifying the distance between the MVG model derived from the test image and the benchmark MVG model derived from the high-quality image set. A comprehensive series of experiments conducted on various datasets show that our proposed model exhibits superior consistency with human visual perception compared to state-of-the-art BIQA models. Furthermore, it shows improved generalizability across diverse target-specific BIQA tasks. Our code is available at: https://github.com/eezkni/MDFS
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2405.15395.pdf' target='_blank'>https://arxiv.org/pdf/2405.15395.pdf</a></span>   <span><a href='https://github.com/hyeonjaegil/fieldscale' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonjae Gil, Myung-Hwan Jeon, Ayoung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15395">Fieldscale: Locality-Aware Field-based Adaptive Rescaling for Thermal Infrared Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal infrared (TIR) cameras are emerging as promising sensors in safety-related fields due to their robustness against external illumination. However, RAW TIR image has 14 bits of pixel depth and needs to be rescaled into 8 bits for general applications. Previous works utilize a global 1D look-up table to compute pixel-wise gain solely based on its intensity, which degrades image quality by failing to consider the local nature of the heat. We propose Fieldscale, a rescaling based on locality-aware 2D fields where both the intensity value and spatial context of each pixel within an image are embedded. It can adaptively determine the pixel gain for each region and produce spatially consistent 8-bit rescaled images with minimal information loss and high visibility. Consistent performance improvement on image quality assessment and two other downstream tasks support the effectiveness and usability of Fieldscale. All the codes are publicly opened to facilitate research advancements in this field. https://github.com/hyeonjaegil/fieldscale
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2405.08745.pdf' target='_blank'>https://arxiv.org/pdf/2405.08745.pdf</a></span>   <span><a href='https://github.com/sunwei925/RQ-VQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Haoning Wu, Zicheng Zhang, Jun Jia, Zhichao Zhang, Linhan Cao, Qiubo Chen, Xiongkuo Min, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08745">Enhancing Blind Video Quality Assessment with Rich Quality-aware Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a simple but effective method to enhance blind video quality assessment (BVQA) models for social media videos. Motivated by previous researches that leverage pre-trained features extracted from various computer vision models as the feature representation for BVQA, we further explore rich quality-aware features from pre-trained blind image quality assessment (BIQA) and BVQA models as auxiliary features to help the BVQA model to handle complex distortions and diverse content of social media videos. Specifically, we use SimpleVQA, a BVQA model that consists of a trainable Swin Transformer-B and a fixed SlowFast, as our base model. The Swin Transformer-B and SlowFast components are responsible for extracting spatial and motion features, respectively. Then, we extract three kinds of features from Q-Align, LIQE, and FAST-VQA to capture frame-level quality-aware features, frame-level quality-aware along with scene-specific features, and spatiotemporal quality-aware features, respectively. Through concatenating these features, we employ a multi-layer perceptron (MLP) network to regress them into quality scores. Experimental results demonstrate that the proposed model achieves the best performance on three public social media VQA datasets. Moreover, the proposed model won first place in the CVPR NTIRE 2024 Short-form UGC Video Quality Assessment Challenge. The code is available at \url{https://github.com/sunwei925/RQ-VQA.git}.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2405.08555.pdf' target='_blank'>https://arxiv.org/pdf/2405.08555.pdf</a></span>   <span><a href='https://github.com/sunwei925/DN-PIQA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Weixia Zhang, Yanwei Jiang, Haoning Wu, Zicheng Zhang, Jun Jia, Yingjie Zhou, Zhongpeng Ji, Xiongkuo Min, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08555">Dual-Branch Network for Portrait Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Portrait images typically consist of a salient person against diverse backgrounds. With the development of mobile devices and image processing techniques, users can conveniently capture portrait images anytime and anywhere. However, the quality of these portraits may suffer from the degradation caused by unfavorable environmental conditions, subpar photography techniques, and inferior capturing devices. In this paper, we introduce a dual-branch network for portrait image quality assessment (PIQA), which can effectively address how the salient person and the background of a portrait image influence its visual quality. Specifically, we utilize two backbone networks (\textit{i.e.,} Swin Transformer-B) to extract the quality-aware features from the entire portrait image and the facial image cropped from it. To enhance the quality-aware feature representation of the backbones, we pre-train them on the large-scale video quality assessment dataset LSVQ and the large-scale facial image quality assessment dataset GFIQA. Additionally, we leverage LIQE, an image scene classification and quality assessment model, to capture the quality-aware and scene-specific features as the auxiliary features. Finally, we concatenate these features and regress them into quality scores via a multi-perception layer (MLP). We employ the fidelity loss to train the model via a learning-to-rank manner to mitigate inconsistencies in quality scores in the portrait image quality assessment dataset PIQ. Experimental results demonstrate that the proposed model achieves superior performance in the PIQ dataset, validating its effectiveness. The code is available at \url{https://github.com/sunwei925/DN-PIQA.git}.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2405.07346.pdf' target='_blank'>https://arxiv.org/pdf/2405.07346.pdf</a></span>   <span><a href='https://github.com/IntMeGroup/MINT-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Wang, Huiyu Duan, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07346">Quality Assessment for AI Generated Images with Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability. However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs. To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence. Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities. Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models. The AIGCIQA2023+ database and MINT-IQA model are available at: https://github.com/IntMeGroup/MINT-IQA.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2405.06887.pdf' target='_blank'>https://arxiv.org/pdf/2405.06887.pdf</a></span>   <span><a href='https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinglin Xu, Sibo Yin, Guohao Zhao, Zishuo Wang, Yuxin Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06887">FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and interpretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named \textbf{FineParser}. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of invalid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric foreground action masks for the FineDiving dataset, called \textbf{FineDiving-HM}. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at \url{https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024}.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2405.06143.pdf' target='_blank'>https://arxiv.org/pdf/2405.06143.pdf</a></span>   <span><a href='https://github.com/arshafiee/crack-detection-VVM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Shafiee Sarvestani, Wei Zhou, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06143">Perceptual Crack Detection for Rendered 3D Textured Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed many advancements in the applications of 3D textured meshes. As the demand continues to rise, evaluating the perceptual quality of this new type of media content becomes crucial for quality assurance and optimization purposes. Different from traditional image quality assessment, crack is an annoying artifact specific to rendered 3D meshes that severely affects their perceptual quality. In this work, we make one of the first attempts to propose a novel Perceptual Crack Detection (PCD) method for detecting and localizing crack artifacts in rendered meshes. Specifically, motivated by the characteristics of the human visual system (HVS), we adopt contrast and Laplacian measurement modules to characterize crack artifacts and differentiate them from other undesired artifacts. Extensive experiments on large-scale public datasets of 3D textured meshes demonstrate effectiveness and efficiency of the proposed PCD method in correct localization and detection of crack artifacts. %Specifically, We propose a full-reference crack artifact localization method that operates on a pair of input snapshots of distorted and reference 3D objects to generate a final crack map. Moreover, to quantify the performance of the proposed detection method and validate its effectiveness, we propose a simple yet effective weighting mechanism to incorporate the resulting crack map into classical quality assessment (QA) models, which creates significant performance improvement in predicting the perceptual image quality when tested on public datasets of static 3D textured meshes. A software release of the proposed method is publicly available at: https://github.com/arshafiee/crack-detection-VVM
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2405.00451.pdf' target='_blank'>https://arxiv.org/pdf/2405.00451.pdf</a></span>   <span><a href='https://github.com/YuxiXie/MCTS-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, Michael Shieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00451">Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\%$ (+$5.9\%$), $34.7\%$ (+$5.8\%$), and $76.4\%$ (+$15.8\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2404.19500.pdf' target='_blank'>https://arxiv.org/pdf/2404.19500.pdf</a></span>   <span><a href='https://ziyannchen.github.io/projects/VFRxBenchmark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Chen, Jingwen He, Xinqi Lin, Yu Qiao, Chao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19500">Towards Real-world Video Face Restoration: A New Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind face restoration (BFR) on images has significantly progressed over the last several years, while real-world video face restoration (VFR), which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved, remains unsolved. Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images, which are limited in their coverage of real-world video frames. In this work, we introduced new real-world datasets named FOS with a taxonomy of "Full, Occluded, and Side" faces from mainly video frames to study the applicability of current methods on videos. Compared with existing test datasets, FOS datasets cover more diverse degradations and involve face samples from more complex scenarios, which helps to revisit current face restoration approaches more comprehensively. Given the established datasets, we benchmarked both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches, identifying their potential and limitations in VFR tasks. In addition, we studied the effectiveness of the commonly used image quality assessment (IQA) metrics and face IQA (FIQA) metrics by leveraging a subjective user study. With extensive experimental results and detailed analysis provided, we gained insights from the successes and failures of both current BFR and VSR methods. These results also pose challenges to current face restoration approaches, which we hope stimulate future advances in VFR research.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2404.18203.pdf' target='_blank'>https://arxiv.org/pdf/2404.18203.pdf</a></span>   <span><a href='https://github.com/zzc-1998/LMM-PCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Haoning Wu, Yingjie Zhou, Chunyi Li, Wei Sun, Chaofeng Chen, Xiongkuo Min, Xiaohong Liu, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18203">LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model understanding and assessment accuracy. We hope our contributions can inspire subsequent investigations into the fusion of LMMs with PCQA, fostering advancements in 3D visual quality analysis and beyond. The code is available at https://github.com/zzc-1998/LMM-PCQA.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2404.17762.pdf' target='_blank'>https://arxiv.org/pdf/2404.17762.pdf</a></span>   <span><a href='https://github.com/wangpuyi/MA-AGIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Puyi Wang, Wei Sun, Zicheng Zhang, Jun Jia, Yanwei Jiang, Zhichao Zhang, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17762">Large Multi-modality Model Assisted AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional deep neural network (DNN)-based image quality assessment (IQA) models leverage convolutional neural networks (CNN) or Transformer to learn the quality-aware feature representation, achieving commendable performance on natural scene images. However, when applied to AI-Generated images (AGIs), these DNN-based IQA models exhibit subpar performance. This situation is largely due to the semantic inaccuracies inherent in certain AGIs caused by uncontrollable nature of the generation process. Thus, the capability to discern semantic content becomes crucial for assessing the quality of AGIs. Traditional DNN-based IQA models, constrained by limited parameter complexity and training data, struggle to capture complex fine-grained semantic features, making it challenging to grasp the existence and coherence of semantic content of the entire image. To address the shortfall in semantic content perception of current IQA models, we introduce a large Multi-modality model Assisted AI-Generated Image Quality Assessment (MA-AGIQA) model, which utilizes semantically informed guidance to sense semantic information and extract semantic vectors through carefully designed text prompts. Moreover, it employs a mixture of experts (MoE) structure to dynamically integrate the semantic information with the quality-aware features extracted by traditional DNN-based IQA models. Comprehensive experiments conducted on two AI-generated content datasets, AIGCQA-20k and AGIQA-3k show that MA-AGIQA achieves state-of-the-art performance, and demonstrate its superior generalization capabilities on assessing the quality of AGIs. Code is available at https://github.com/wangpuyi/MA-AGIQA.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2404.14471.pdf' target='_blank'>https://arxiv.org/pdf/2404.14471.pdf</a></span>   <span><a href='https://github.com/shiyi-zh0408/NAE_CVPR2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14471">Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate a new problem called narrative action evaluation (NAE). NAE aims to generate professional commentary that evaluates the execution of an action. Unlike traditional tasks such as score-based action quality assessment and video captioning involving superficial sentences, NAE focuses on creating detailed narratives in natural language. These narratives provide intricate descriptions of actions along with objective evaluations. NAE is a more challenging task because it requires both narrative flexibility and evaluation rigor. One existing possible solution is to use multi-task learning, where narrative language and evaluative information are predicted separately. However, this approach results in reduced performance for individual tasks because of variations between tasks and differences in modality between language information and evaluation information. To address this, we propose a prompt-guided multimodal interaction framework. This framework utilizes a pair of transformers to facilitate the interaction between different modalities of information. It also uses prompts to transform the score regression task into a video-text matching task, thus enabling task interactivity. To support further research in this field, we re-annotate the MTL-AQA and FineGym datasets with high-quality and comprehensive action narration. Additionally, we establish benchmarks for NAE. Extensive experiment results prove that our method outperforms separate learning methods and naive multi-task learning methods. Data and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2404.13999.pdf' target='_blank'>https://arxiv.org/pdf/2404.13999.pdf</a></span>   <span><a href='https://github.com/ZhouKanglei/CoFInAl_AQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Junlin Li, Ruizhi Cai, Liyuan Wang, Xingxing Zhang, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13999">CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) is pivotal for quantifying actions across domains like sports and medical care. Existing methods often rely on pre-trained backbones from large-scale action recognition datasets to boost performance on smaller AQA datasets. However, this common strategy yields suboptimal results due to the inherent struggle of these backbones to capture the subtle cues essential for AQA. Moreover, fine-tuning on smaller datasets risks overfitting. To address these issues, we propose Coarse-to-Fine Instruction Alignment (CoFInAl). Inspired by recent advances in large language model tuning, CoFInAl aligns AQA with broader pre-trained tasks by reformulating it as a coarse-to-fine classification task. Initially, it learns grade prototypes for coarse assessment and then utilizes fixed sub-grade prototypes for fine-grained assessment. This hierarchical approach mirrors the judging process, enhancing interpretability within the AQA framework. Experimental results on two long-term AQA datasets demonstrate CoFInAl achieves state-of-the-art performance with significant correlation gains of 5.49% and 3.55% on Rhythmic Gymnastics and Fis-V, respectively. Our code is available at https://github.com/ZhouKanglei/CoFInAl_AQA.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2404.13573.pdf' target='_blank'>https://arxiv.org/pdf/2404.13573.pdf</a></span>   <span><a href='https://github.com/Coobiw/TriVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Qu, Xiaoyu Liang, Shangkun Sun, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13573">Exploring AIGC Video Quality: A Focus on Visual Harmony, Video-Text Consistency and Domain Distribution Gap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in Text-to-Video Artificial Intelligence Generated Content (AIGC) have been remarkable. Compared with traditional videos, the assessment of AIGC videos encounters various challenges: visual inconsistency that defy common sense, discrepancies between content and the textual prompt, and distribution gap between various generative models, etc. Target at these challenges, in this work, we categorize the assessment of AIGC video quality into three dimensions: visual harmony, video-text consistency, and domain distribution gap. For each dimension, we design specific modules to provide a comprehensive quality assessment of AIGC videos. Furthermore, our research identifies significant variations in visual quality, fluidity, and style among videos generated by different text-to-video models. Predicting the source generative model can make the AIGC video features more discriminative, which enhances the quality assessment performance. The proposed method was used in the third-place winner of the NTIRE 2024 Quality Assessment for AI-Generated Content - Track 2 Video, demonstrating its effectiveness. Code will be available at https://github.com/Coobiw/TriVQA.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2404.11429.pdf' target='_blank'>https://arxiv.org/pdf/2404.11429.pdf</a></span>   <span><a href='https://github.com/UARK-AICV/CarcassFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Tran, Sang Truong, Arthur F. A. Fernandes, Michael T. Kidd, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11429">CarcassFormer: An End-to-end Transformer-based Framework for Simultaneous Localization, Segmentation and Classification of Poultry Carcass Defect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the food industry, assessing the quality of poultry carcasses during processing is a crucial step. This study proposes an effective approach for automating the assessment of carcass quality without requiring skilled labor or inspector involvement. The proposed system is based on machine learning (ML) and computer vision (CV) techniques, enabling automated defect detection and carcass quality assessment. To this end, an end-to-end framework called CarcassFormer is introduced. It is built upon a Transformer-based architecture designed to effectively extract visual representations while simultaneously detecting, segmenting, and classifying poultry carcass defects. Our proposed framework is capable of analyzing imperfections resulting from production and transport welfare issues, as well as processing plant stunner, scalder, picker, and other equipment malfunctions. To benchmark the framework, a dataset of 7,321 images was initially acquired, which contained both single and multiple carcasses per image. In this study, the performance of the CarcassFormer system is compared with other state-of-the-art (SOTA) approaches for both classification, detection, and segmentation tasks. Through extensive quantitative experiments, our framework consistently outperforms existing methods, demonstrating remarkable improvements across various evaluation metrics such as AP, AP@50, and AP@75. Furthermore, the qualitative results highlight the strengths of CarcassFormer in capturing fine details, including feathers, and accurately localizing and segmenting carcasses with high precision. To facilitate further research and collaboration, the pre-trained model and source code of CarcassFormer is available for research purposes at: \url{https://github.com/UARK-AICV/CarcassFormer}.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2404.11313.pdf' target='_blank'>https://arxiv.org/pdf/2404.11313.pdf</a></span>   <span><a href='https://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Kun Yuan, Yajing Pei, Yiting Lu, Ming Sun, Chao Zhou, Zhibo Chen, Radu Timofte, Wei Sun, Haoning Wu, Zicheng Zhang, Jun Jia, Zhichao Zhang, Linhan Cao, Qiubo Chen, Xiongkuo Min, Weisi Lin, Guangtao Zhai, Jianhui Sun, Tianyi Wang, Lei Li, Han Kong, Wenxuan Wang, Bing Li, Cheng Luo, Haiqiang Wang, Xiangguang Chen, Wenhui Meng, Xiang Pan, Huiying Shi, Han Zhu, Xiaozhong Xu, Lei Sun, Zhenzhong Chen, Shan Liu, Fangyuan Kong, Haotian Fan, Yifang Xu, Haoran Xu, Mengduo Yang, Jie Zhou, Jiaze Li, Shijie Wen, Mai Xu, Da Li, Shunyu Yao, Jiazhi Du, Wangmeng Zuo, Zhibo Li, Shuai He, Anlong Ming, Huiyuan Fu, Huadong Ma, Yong Wu, Fie Xue, Guozhi Zhao, Lina Du, Jie Guo, Yu Zhang, Huimin Zheng, Junhao Chen, Yue Liu, Dulan Zhou, Kele Xu, Qisheng Xu, Tao Sun, Zhixiang Ding, Yuhang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11313">NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reviews the NTIRE 2024 Challenge on Shortform UGC Video Quality Assessment (S-UGC VQA), where various excellent solutions are submitted and evaluated on the collected dataset KVQ from popular short-form video platform, i.e., Kuaishou/Kwai Platform. The KVQ database is divided into three parts, including 2926 videos for training, 420 videos for validation, and 854 videos for testing. The purpose is to build new benchmarks and advance the development of S-UGC VQA. The competition had 200 participants and 13 teams submitted valid solutions for the final testing phase. The proposed solutions achieved state-of-the-art performances for S-UGC VQA. The project can be found at https://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2404.09555.pdf' target='_blank'>https://arxiv.org/pdf/2404.09555.pdf</a></span>   <span><a href='https://github.com/LSIbabnikz/AI-KD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Å½iga Babnik, Fadi Boutros, Naser Damer, Peter Peer, Vitomir Å truc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09555">AI-KD: Towards Alignment Invariant Face Image Quality Assessment Using Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Image Quality Assessment (FIQA) techniques have seen steady improvements over recent years, but their performance still deteriorates if the input face samples are not properly aligned. This alignment sensitivity comes from the fact that most FIQA techniques are trained or designed using a specific face alignment procedure. If the alignment technique changes, the performance of most existing FIQA techniques quickly becomes suboptimal. To address this problem, we present in this paper a novel knowledge distillation approach, termed AI-KD that can extend on any existing FIQA technique, improving its robustness to alignment variations and, in turn, performance with different alignment procedures. To validate the proposed distillation approach, we conduct comprehensive experiments on 6 face datasets with 4 recent face recognition models and in comparison to 7 state-of-the-art FIQA techniques. Our results show that AI-KD consistently improves performance of the initial FIQA techniques not only with misaligned samples, but also with properly aligned facial images. Furthermore, it leads to a new state-of-the-art, when used with a competitive initial FIQA approach. The code for AI-KD is made publicly available from: https://github.com/LSIbabnikz/AI-KD.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2404.09003.pdf' target='_blank'>https://arxiv.org/pdf/2404.09003.pdf</a></span>   <span><a href='https://github.com/zyj-2000/THQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09003">THQA: A Perceptual Quality Assessment Database for Talking Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. To tackle this issue, this paper introduces the Talking Head Quality Assessment (THQA) database, featuring 800 TH videos generated through 8 diverse speech-driven methods. Extensive experiments affirm the THQA database's richness in character and speech features. Subsequent subjective quality assessment experiments analyze correlations between scoring results and speech-driven methods, ages, and genders. In addition, experimental results show that mainstream image and video quality assessment methods have limitations for the THQA database, underscoring the imperative for further research to enhance TH video quality assessment. The THQA database is publicly accessible at https://github.com/zyj-2000/THQA.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2404.05029.pdf' target='_blank'>https://arxiv.org/pdf/2404.05029.pdf</a></span>   <span><a href='https://github.com/shiyi-zh0408/LOGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05029">LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To benchmark LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at \url{https://github.com/shiyi-zh0408/LOGO }.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2403.18714.pdf' target='_blank'>https://arxiv.org/pdf/2403.18714.pdf</a></span>   <span><a href='https://github.com/Coobiw/IP-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Qu, Haohui Li, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18714">Bringing Textual Prompt to AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-Generated Images (AGIs) have inherent multimodal nature. Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual prompt into consideration. This is coupled in the ground truth score, which confuses the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via corresponding image and prompt incorporation. Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual prompts. An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied. Both are plug-and-play and beneficial for the cooperation of image and its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available at https://github.com/Coobiw/IP-IQA.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2403.17708.pdf' target='_blank'>https://arxiv.org/pdf/2403.17708.pdf</a></span>   <span><a href='https://dianvrlab.github.io/Panonut360/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17708">Panonut360: A Head and Eye Tracking Dataset for Panoramic Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2403.13798.pdf' target='_blank'>https://arxiv.org/pdf/2403.13798.pdf</a></span>   <span><a href='https://github.com/laurenok24/NSAQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Okamoto, Paritosh Parmar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13798">Hierarchical NeuroSymbolic Approach for Comprehensive and Explainable Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action. Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth. To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols. We take diving as the case study. We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving. Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence. As verified by a group of domain experts, this report may be used to assist judges in scoring, help train judges, and provide feedback to divers. Annotated training data and code: https://github.com/laurenok24/NSAQA.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2403.11956.pdf' target='_blank'>https://arxiv.org/pdf/2403.11956.pdf</a></span>   <span><a href='https://github.com/QMME/T2VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, Ning Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11956">Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language model to give the prediction score. Experimental results show that T2VQA outperforms existing T2V metrics and SOTA video quality assessment models. Quantitative analysis indicates that T2VQA is capable of giving subjective-align predictions, validating its effectiveness. The dataset and code will be released at https://github.com/QMME/T2VQA.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2403.11176.pdf' target='_blank'>https://arxiv.org/pdf/2403.11176.pdf</a></span>   <span><a href='https://github.com/miccunifi/QualiCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11176">Quality-Aware Image-Text Alignment for Opinion-Unaware Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. Most state-of-the-art NR-IQA approaches are opinion-aware, i.e. they require human annotations for training. This dependency limits their scalability and broad applicability. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware approach that does not require human opinions. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate quality-aware image representations. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts. At the same time, we force CLIP to generate consistent representations for images with similar content and the same level of degradation. Our experiments show that the proposed method improves over existing opinion-unaware approaches across multiple datasets with diverse distortion types. Moreover, despite not requiring human annotations, QualiCLIP achieves excellent performance against supervised opinion-aware methods in cross-dataset experiments, thus demonstrating remarkable generalization capabilities. The code and the model are publicly available at https://github.com/miccunifi/QualiCLIP.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2403.04398.pdf' target='_blank'>https://arxiv.org/pdf/2403.04398.pdf</a></span>   <span><a href='https://github.com/ZhouKanglei/MAGR_CAQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Liyuan Wang, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Jianguo Li, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04398">MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data. We propose Continual AQA (CAQA) to refine models using sparse new data. Feature replay preserves memory without storing raw inputs. However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting. To address this novel problem, we propose Manifold-Aligned Graph Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency. It then constructs a graph jointly arranging old and new features aligned with quality scores. Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively. This validates MAGR for continual assessment challenges arising from non-stationary skill variations. Code is available at https://github.com/ZhouKanglei/MAGR_CAQA}{https://github.com/ZhouKanglei/MAGR_CAQA.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2402.09178.pdf' target='_blank'>https://arxiv.org/pdf/2402.09178.pdf</a></span>   <span><a href='https://github.com/DXOMARK-Research/PIQ2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09178">Generalized Portrait Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2402.07116.pdf' target='_blank'>https://arxiv.org/pdf/2402.07116.pdf</a></span>   <span><a href='https://github.com/Q-Future/Q-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07116">Q-Bench+: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in low-level visual perception and understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception (A1) via visual question answering related to low-level attributes (e.g. clarity, lighting); and the low-level visual description (A2), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at https://github.com/Q-Future/Q-Bench.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2402.01162.pdf' target='_blank'>https://arxiv.org/pdf/2402.01162.pdf</a></span>   <span><a href='https://github.com/h4nwei/2AFC-LMMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwei Zhu, Xiangjie Sui, Baoliang Chen, Xuelin Liu, Peilin Chen, Yuming Fang, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01162">2AFC Prompting of Large Multimodal Models for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While abundant research has been conducted on improving high-level visual understanding and reasoning capabilities of large multimodal models~(LMMs), their visual quality assessment~(IQA) ability has been relatively under-explored. Here we take initial steps towards this goal by employing the two-alternative forced choice~(2AFC) prompting, as 2AFC is widely regarded as the most reliable way of collecting human opinions of visual quality. Subsequently, the global quality score of each image estimated by a particular LMM can be efficiently aggregated using the maximum a posterior estimation. Meanwhile, we introduce three evaluation criteria: consistency, accuracy, and correlation, to provide comprehensive quantifications and deeper insights into the IQA capability of five LMMs. Extensive experiments show that existing LMMs exhibit remarkable IQA ability on coarse-grained quality comparison, but there is room for improvement on fine-grained quality discrimination. The proposed dataset sheds light on the future development of IQA models based on LMMs. The codes will be made publicly available at https://github.com/h4nwei/2AFC-LMMs.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2401.17736.pdf' target='_blank'>https://arxiv.org/pdf/2401.17736.pdf</a></span>   <span><a href='https://github.com/esla/Multilabelfy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Esla Timothy Anzaku, Hyesoo Hong, Jin-Woo Park, Wonjun Yang, Kangmin Kim, JongBum Won, Deshika Vinoshani Kumari Herath, Arnout Van Messem, Wesley De Neve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17736">Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale datasets for single-label multi-class classification, such as \emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision. However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors. In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement. We term this novel framework \emph{Multilabelfy}. Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets. Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection. Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions. This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation. Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development. The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.
  \keywords{Computer Vision \and Dataset Quality Enhancement \and Dataset Validation \and Human-Computer Interaction \and Multi-label Annotation.}
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2401.16087.pdf' target='_blank'>https://arxiv.org/pdf/2401.16087.pdf</a></span>   <span><a href='https://github.com/jarikorhonen/hriq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huang Huang, Qiang Wan, Jari Korhonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16087">High Resolution Image Quality Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With technology for digital photography and high resolution displays rapidly evolving and gaining popularity, there is a growing demand for blind image quality assessment (BIQA) models for high resolution images. Unfortunately, the publicly available large scale image quality databases used for training BIQA models contain mostly low or general resolution images. Since image resizing affects image quality, we assume that the accuracy of BIQA models trained on low resolution images would not be optimal for high resolution images. Therefore, we created a new high resolution image quality database (HRIQ), consisting of 1120 images with resolution of 2880x2160 pixels. We conducted a subjective study to collect the subjective quality ratings for HRIQ in a controlled laboratory setting, resulting in accurate MOS at high resolution. To demonstrate the importance of a high resolution image quality database for training BIQA models to predict mean opinion scores (MOS) of high resolution images accurately, we trained and tested several traditional and deep learning based BIQA methods on different resolution versions of our database. The database is publicly available in https://github.com/jarikorhonen/hriq.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2401.13531.pdf' target='_blank'>https://arxiv.org/pdf/2401.13531.pdf</a></span>   <span><a href='https://github.com/wzb-bupt/QAGait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengbin Wang, Saihui Hou, Man Zhang, Xu Liu, Chunshui Cao, Yongzhen Huang, Peipei Li, Shibiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13531">QAGait: Revisit Gait Recognition from a Quality Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2401.02614.pdf' target='_blank'>https://arxiv.org/pdf/2401.02614.pdf</a></span>   <span><a href='https://github.com/Sissuire/SAMA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Sissuire/SAMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongxu Liu, Yinghui Quan, Guoyao Xiao, Aobo Li, Jinjian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02614">Scaling and Masking: A New Paradigm of Data Sampling for Image and Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment of images and videos emphasizes both local details and global semantics, whereas general data sampling methods (e.g., resizing, cropping or grid-based fragment) fail to catch them simultaneously. To address the deficiency, current approaches have to adopt multi-branch models and take as input the multi-resolution data, which burdens the model complexity. In this work, instead of stacking up models, a more elegant data sampling method (named as SAMA, scaling and masking) is explored, which compacts both the local and global content in a regular input size. The basic idea is to scale the data into a pyramid first, and reduce the pyramid into a regular data dimension with a masking strategy. Benefiting from the spatial and temporal redundancy in images and videos, the processed data maintains the multi-scale characteristics with a regular input size, thus can be processed by a single-branch model. We verify the sampling method in image and video quality assessment. Experiments show that our sampling method can improve the performance of current single-branch models significantly, and achieves competitive performance to the multi-branch models without extra model complexity. The source code will be available at https://github.com/Sissuire/SAMA.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2312.17090.pdf' target='_blank'>https://arxiv.org/pdf/2312.17090.pdf</a></span>   <span><a href='https://github.com/Q-Future/Q-Align' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17090">Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosion of visual content available online underscores the requirement for an accurate machine assessor to robustly evaluate scores across diverse types of visual contents. While recent studies have demonstrated the exceptional potentials of large multi-modality models (LMMs) on a wide range of related fields, in this work, we explore how to teach them for visual rating aligned with human opinions. Observing that human raters only learn and judge discrete text-defined levels in subjective studies, we propose to emulate this subjective process and teach LMMs with text-defined rating levels instead of scores. The proposed Q-Align achieves state-of-the-art performance on image quality assessment (IQA), image aesthetic assessment (IAA), as well as video quality assessment (VQA) tasks under the original LMM structure. With the syllabus, we further unify the three tasks into one model, termed the OneAlign. In our experiments, we demonstrate the advantage of the discrete-level-based syllabus over direct-score-based variants for LMMs. Our code and the pre-trained weights are released at https://github.com/Q-Future/Q-Align.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2312.15425.pdf' target='_blank'>https://arxiv.org/pdf/2312.15425.pdf</a></span>   <span><a href='https://github.com/Shankhanil006/SSL-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shankhanil Mitra, Rajiv Soundararajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15425">Knowledge Guided Semi-Supervised Learning for Quality Assessment of User Generated Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual quality assessment of user generated content (UGC) videos is challenging due to the requirement of large scale human annotated videos for training. In this work, we address this challenge by first designing a self-supervised Spatio-Temporal Visual Quality Representation Learning (ST-VQRL) framework to generate robust quality aware features for videos. Then, we propose a dual-model based Semi Supervised Learning (SSL) method specifically designed for the Video Quality Assessment (SSL-VQA) task, through a novel knowledge transfer of quality predictions between the two models. Our SSL-VQA method uses the ST-VQRL backbone to produce robust performances across various VQA datasets including cross-database settings, despite being learned with limited human annotated videos. Our model improves the state-of-the-art performance when trained only with limited data by around 10%, and by around 15% when unlabelled data is also used in SSL. Source codes and checkpoints are available at https://github.com/Shankhanil006/SSL-VQA.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2312.14209.pdf' target='_blank'>https://arxiv.org/pdf/2312.14209.pdf</a></span>   <span><a href='https://github.com/AWCXV/TextFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyang Cheng, Tianyang Xu, Xiao-Jun Wu, Hui Li, Xi Li, Zhangyong Tang, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14209">TextFusion: Unveiling the Power of Textual Semantics for Controllable Image Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advanced image fusion methods are devoted to generating the fusion results by aggregating the complementary information conveyed by the source images. However, the difference in the source-specific manifestation of the imaged scene content makes it difficult to design a robust and controllable fusion process. We argue that this issue can be alleviated with the help of higher-level semantics, conveyed by the text modality, which should enable us to generate fused images for different purposes, such as visualisation and downstream tasks, in a controllable way. This is achieved by exploiting a vision-and-language model to build a coarse-to-fine association mechanism between the text and image signals. With the guidance of the association maps, an affine fusion unit is embedded in the transformer network to fuse the text and vision modalities at the feature level. As another ingredient of this work, we propose the use of textual attention to adapt image quality assessment to the fusion task. To facilitate the implementation of the proposed text-guided fusion paradigm, and its adoption by the wider research community, we release a text-annotated image fusion dataset IVT. Extensive experiments demonstrate that our approach (TextFusion) consistently outperforms traditional appearance-based fusion methods. Our code and dataset will be publicly available at https://github.com/AWCXV/TextFusion.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2312.08864.pdf' target='_blank'>https://arxiv.org/pdf/2312.08864.pdf</a></span>   <span><a href='https://chenfeng-bristol.github.io/RankDVQA-mini/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Duolikun Danier, Haoran Wang, Fan Zhang, Benoit Vallade, Alex Mackin, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08864">RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based video quality assessment (deep VQA) has demonstrated significant potential in surpassing conventional metrics, with promising improvements in terms of correlation with human perception. However, the practical deployment of such deep VQA models is often limited due to their high computational complexity and large memory requirements. To address this issue, we aim to significantly reduce the model size and runtime of one of the state-of-the-art deep VQA methods, RankDVQA, by employing a two-phase workflow that integrates pruning-driven model compression with multi-level knowledge distillation. The resulting lightweight full reference quality metric, RankDVQA-mini, requires less than 10% of the model parameters compared to its full version (14% in terms of FLOPs), while still retaining a quality prediction performance that is superior to most existing deep VQA methods. The source code of the RankDVQA-mini has been released at https://chenfeng-bristol.github.io/RankDVQA-mini/ for public evaluation.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2312.06995.pdf' target='_blank'>https://arxiv.org/pdf/2312.06995.pdf</a></span>   <span><a href='https://github.com/I2-Multimedia-Lab/SaTQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsong Shi, Pan Gao, Jie Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06995">Transformer-based No-Reference Image Quality Assessment via Supervised Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) has long been a research hotspot in the field of image processing, especially No-Reference Image Quality Assessment (NR-IQA). Due to the powerful feature extraction ability, existing Convolution Neural Network (CNN) and Transformers based NR-IQA methods have achieved considerable progress. However, they still exhibit limited capability when facing unknown authentic distortion datasets. To further improve NR-IQA performance, in this paper, a novel supervised contrastive learning (SCL) and Transformer-based NR-IQA model SaTQA is proposed. We first train a model on a large-scale synthetic dataset by SCL (no image subjective score is required) to extract degradation features of images with various distortion types and levels. To further extract distortion information from images, we propose a backbone network incorporating the Multi-Stream Block (MSB) by combining the CNN inductive bias and Transformer long-term dependence modeling capability. Finally, we propose the Patch Attention Block (PAB) to obtain the final distorted image quality score by fusing the degradation features learned from contrastive learning with the perceptual distortion information extracted by the backbone network. Experimental results on seven standard IQA datasets show that SaTQA outperforms the state-of-the-art methods for both synthetic and authentic datasets. Code is available at https://github.com/I2-Multimedia-Lab/SaTQA
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2312.05972.pdf' target='_blank'>https://arxiv.org/pdf/2312.05972.pdf</a></span>   <span><a href='https://github.com/o-messai/3D-PCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oussama Messai, Abdelouahid Bentamou, Abbass Zein-Eddine, Yann Gavet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05972">Activating Frequency and ViT for 3D Point Cloud Quality Assessment without Reference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based quality assessments have significantly enhanced perceptual multimedia quality assessment, however it is still in the early stages for 3D visual data such as 3D point clouds (PCs). Due to the high volume of 3D-PCs, such quantities are frequently compressed for transmission and viewing, which may affect perceived quality. Therefore, we propose no-reference quality metric of a given 3D-PC. Comparing to existing methods that mostly focus on geometry or color aspects, we propose integrating frequency magnitudes as indicator of spatial degradation patterns caused by the compression. To map the input attributes to quality score, we use a light-weight hybrid deep model; combined of Deformable Convolutional Network (DCN) and Vision Transformers (ViT). Experiments are carried out on ICIP20 [1], PointXR [2] dataset, and a new big dataset called BASICS [3]. The results show that our approach outperforms state-of-the-art NR-PCQA measures and even some FR-PCQA on PointXR. The implementation code can be found at: https://github.com/o-messai/3D-PCQA
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2312.05897.pdf' target='_blank'>https://arxiv.org/pdf/2312.05897.pdf</a></span>   <span><a href='https://github.com/jiquan123/PSCR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiquan Yuan, Xinyan Cao, Linjing Cao, Jinlong Lin, Xixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05897">PSCR: Patches Sampling-based Contrastive Regression for AIGC Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Artificial Intelligence Generated Content (AIGC) has gained widespread attention beyond the computer science community. Due to various issues arising from continuous creation of AI-generated images (AIGI), AIGC image quality assessment (AIGCIQA), which aims to evaluate the quality of AIGIs from human perception perspectives, has emerged as a novel topic in the field of computer vision. However, most existing AIGCIQA methods directly regress predicted scores from a single generated image, overlooking the inherent differences among AIGIs and scores. Additionally, operations like resizing and cropping may cause global geometric distortions and information loss, thus limiting the performance of models. To address these issues, we propose a patches sampling-based contrastive regression (PSCR) framework. We suggest introducing a contrastive regression framework to leverage differences among various generated images for learning a better representation space. In this space, differences and score rankings among images can be measured by their relative scores. By selecting exemplar AIGIs as references, we also overcome the limitations of previous models that could not utilize reference images on the no-reference image databases. To avoid geometric distortions and information loss in image inputs, we further propose a patches sampling strategy. To demonstrate the effectiveness of our proposed PSCR framework, we conduct extensive experiments on three mainstream AIGCIQA databases including AGIQA-1K, AGIQA-3K and AIGCIQA2023. The results show significant improvements in model performance with the introduction of our proposed PSCR framework. Code will be available at \url{https://github.com/jiquan123/PSCR}.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2312.04838.pdf' target='_blank'>https://arxiv.org/pdf/2312.04838.pdf</a></span>   <span><a href='https://github.com/suhas-srinath/GRepQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhas Srinath, Shankhanil Mitra, Shika Rao, Rajiv Soundararajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04838">Learning Generalizable Perceptual Representations for Data-Efficient No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference (NR) image quality assessment (IQA) is an important tool in enhancing the user experience in diverse visual applications. A major drawback of state-of-the-art NR-IQA techniques is their reliance on a large number of human annotations to train models for a target IQA application. To mitigate this requirement, there is a need for unsupervised learning of generalizable quality representations that capture diverse distortions. We enable the learning of low-level quality features agnostic to distortion types by introducing a novel quality-aware contrastive loss. Further, we leverage the generalizability of vision-language models by fine-tuning one such model to extract high-level image quality information through relevant text prompts. The two sets of features are combined to effectively predict quality by training a simple regressor with very few samples on a target dataset. Additionally, we design zero-shot quality predictions from both pathways in a completely blind setting. Our experiments on diverse datasets encompassing various distortions show the generalizability of the features and their superior performance in the data-efficient and zero-shot settings. Code will be made available at https://github.com/suhas-srinath/GRepQ.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2312.00856.pdf' target='_blank'>https://arxiv.org/pdf/2312.00856.pdf</a></span>   <span><a href='https://github.com/shuchaoduan/QAFE-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchao Duan, Amirhossein Dadashzadeh, Alan Whone, Majid Mirmehdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00856">QAFE-Net: Quality Assessment of Facial Expressions with Landmark Heatmaps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial expression recognition (FER) methods have made great inroads in categorising moods and feelings in humans. Beyond FER, pain estimation methods assess levels of intensity in pain expressions, however assessing the quality of all facial expressions is of critical value in health-related applications. In this work, we address the quality of five different facial expressions in patients affected by Parkinson's disease. We propose a novel landmark-guided approach, QAFE-Net, that combines temporal landmark heatmaps with RGB data to capture small facial muscle movements that are encoded and mapped to severity scores. The proposed approach is evaluated on a new Parkinson's Disease Facial Expression dataset (PFED5), as well as on the pain estimation benchmark, the UNBC-McMaster Shoulder Pain Expression Archive Database. Our comparative experiments demonstrate that the proposed method outperforms SOTA action quality assessment works on PFED5 and achieves lower mean absolute error than the SOTA pain estimation methods on UNBC-McMaster. Our code and the new PFED5 dataset are available at https://github.com/shuchaoduan/QAFE-Net.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2311.18564.pdf' target='_blank'>https://arxiv.org/pdf/2311.18564.pdf</a></span>   <span><a href='https://github.com/tlliao/LPAM_seam-cutting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianli Liao, Chenyang Zhao, Lei Li, Heling Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18564">Leveraging Local Patch Alignment to Seam-cutting for Large Parallax Image Stitching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seam cutting has shown significant effectiveness in the composition phase of image stitching, particularly for scenarios involving parallax. However, conventional implementations typically position seam-cutting as a downstream process contingent upon successful image alignment. This approach inherently assumes the existence of locally aligned regions where visually plausible seams can be established. Current alignment methods frequently fail to satisfy this prerequisite in large parallax scenarios despite considerable research efforts dedicated to improving alignment accuracy. In this paper, we propose an alignment-compensation paradigm that dissociates seam quality from initial alignment accuracy by integrating a Local Patch Alignment Module (LPAM) into the seam-cutting pipeline. Concretely, given the aligned images with an estimated initial seam, our method first identifies low-quality pixels along the seam through a seam quality assessment, then performs localized SIFT-flow alignment on the critical patches enclosing these pixels. Finally, we recomposite the aligned patches using adaptive seam-cutting and merge them into the original aligned images to generate the final mosaic. Comprehensive experiments on large parallax stitching datasets demonstrate that LPAM significantly enhances stitching quality while maintaining computational efficiency. The code is available at https://github.com/tlliao/LPAM_seam-cutting.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2311.15556.pdf' target='_blank'>https://arxiv.org/pdf/2311.15556.pdf</a></span>   <span><a href='https://github.com/jiquan123/I2IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiquan Yuan, Xinyan Cao, Changjin Li, Fanyi Yang, Jinlong Lin, Xixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15556">PKU-I2IQA: An Image-to-Image Quality Assessment Database for AI Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As image generation technology advances, AI-based image generation has been applied in various fields and Artificial Intelligence Generated Content (AIGC) has garnered widespread attention. However, the development of AI-based image generative models also brings new problems and challenges. A significant challenge is that AI-generated images (AIGI) may exhibit unique distortions compared to natural images, and not all generated images meet the requirements of the real world. Therefore, it is of great significance to evaluate AIGIs more comprehensively. Although previous work has established several human perception-based AIGC image quality assessment (AIGCIQA) databases for text-generated images, the AI image generation technology includes scenarios like text-to-image and image-to-image, and assessing only the images generated by text-to-image models is insufficient. To address this issue, we establish a human perception-based image-to-image AIGCIQA database, named PKU-I2IQA. We conduct a well-organized subjective experiment to collect quality labels for AIGIs and then conduct a comprehensive analysis of the PKU-I2IQA database. Furthermore, we have proposed two benchmark models: NR-AIGCIQA based on the no-reference image quality assessment method and FR-AIGCIQA based on the full-reference image quality assessment method. Finally, leveraging this database, we conduct benchmark experiments and compare the performance of the proposed benchmark models. The PKU-I2IQA database and benchmarks will be released to facilitate future research on \url{https://github.com/jiquan123/I2IQA}.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2311.13880.pdf' target='_blank'>https://arxiv.org/pdf/2311.13880.pdf</a></span>   <span><a href='https://github.com/cwi-dis/pointpca_suite/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuemei Zhou, Evangelos Alexiou, Irene Viola, Pablo Cesar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13880">PointPCA+: Extending PointPCA objective quality assessment metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A computationally-simplified and descriptor-richer Point Cloud Quality Assessment (PCQA) metric, namely PointPCA+, is proposed in this paper, which is an extension of PointPCA. PointPCA proposed a set of perceptually-relevant descriptors based on PCA decomposition that were applied to both the geometry and texture data of point clouds for full reference PCQA. PointPCA+ employs PCA only on the geometry data while enriching existing geometry and texture descriptors, that are computed more efficiently. Similarly to PointPCA, a total quality score is obtained through a learning-based fusion of individual predictions from geometry and texture descriptors that capture local shape and appearance properties, respectively. Before feature fusion, a feature selection module is introduced to choose the most effective features from a proposed super-set. Experimental results show that PointPCA+ achieves high predictive performance against subjective ground truth scores obtained from publicly available datasets. The code is available at \url{https://github.com/cwi-dis/pointpca_suite/}.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2311.11059.pdf' target='_blank'>https://arxiv.org/pdf/2311.11059.pdf</a></span>   <span><a href='https://github.com/avinabsaha/HIDRO-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreshth Saini, Avinab Saha, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11059">HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HIDRO-VQA, a no-reference (NR) video quality assessment model designed to provide precise quality evaluations of High Dynamic Range (HDR) videos. HDR videos exhibit a broader spectrum of luminance, detail, and color than Standard Dynamic Range (SDR) videos. As HDR content becomes increasingly popular, there is a growing demand for video quality assessment (VQA) algorithms that effectively address distortions unique to HDR content. To address this challenge, we propose a self-supervised contrastive fine-tuning approach to transfer quality-aware features from the SDR to the HDR domain, utilizing unlabeled HDR videos. Our findings demonstrate that self-supervised pre-trained neural networks on SDR content can be further fine-tuned in a self-supervised setting using limited unlabeled HDR videos to achieve state-of-the-art performance on the only publicly available VQA database for HDR content, the LIVE-HDR VQA database. Moreover, our algorithm can be extended to the Full Reference VQA setting, also achieving state-of-the-art performance. Our code is available publicly at https://github.com/avinabsaha/HIDRO-VQA.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2311.08024.pdf' target='_blank'>https://arxiv.org/pdf/2311.08024.pdf</a></span>   <span><a href='https://github.com/zunzhumu/MD-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Song, Ruizhi Hou, Lisong Dai, Lei Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08024">MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with Semi Supervised Learning for Low Dose CT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) plays a critical role in optimizing radiation dose and developing novel medical imaging techniques in computed tomography (CT). Traditional IQA methods relying on hand-crafted features have limitations in summarizing the subjective perceptual experience of image quality. Recent deep learning-based approaches have demonstrated strong modeling capabilities and potential for medical IQA, but challenges remain regarding model generalization and perceptual accuracy. In this work, we propose a multi-scale distributions regression approach to predict quality scores by constraining the output distribution, thereby improving model generalization. Furthermore, we design a dual-branch alignment network to enhance feature extraction capabilities. Additionally, semi-supervised learning is introduced by utilizing pseudo-labels for unlabeled data to guide model training. Extensive qualitative experiments demonstrate the effectiveness of our proposed method for advancing the state-of-the-art in deep learning-based medical IQA. Code is available at: https://github.com/zunzhumu/MD-IQA.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2311.07603.pdf' target='_blank'>https://arxiv.org/pdf/2311.07603.pdf</a></span>   <span><a href='https://github.com/Plrbear/PECoP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Dadashzadeh, Shuchao Duan, Alan Whone, Majid Mirmehdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07603">PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules' parameters are updated. We demonstrate PECoP's ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS ($\uparrow6.0\%$), MTL-AQA ($\uparrow0.99\%$), and FineDiving ($\uparrow2.54\%$). We also present a new Parkinson's Disease dataset, PD4T, of real patients performing four various actions, where we surpass ($\uparrow3.56\%$) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2310.16319.pdf' target='_blank'>https://arxiv.org/pdf/2310.16319.pdf</a></span>   <span><a href='https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Zhao, Lingyong Yan, Weiwei Sun, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16319">DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the assessment criteria based on the dimensions conforming to human judgements on dialogue qualities, and (2) annotate large-scale dialogues that conversed between real users based on these annotation criteria, which contains around 100,000 dialogues. We conduct several experiments and report the performances of the baselines as the benchmark on DiQAD. The dataset is openly accessible at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2310.14918.pdf' target='_blank'>https://arxiv.org/pdf/2310.14918.pdf</a></span>   <span><a href='https://github.com/miccunifi/ARNIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14918">ARNIQA: Learning Distortion Manifold for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Therefore, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2310.10123.pdf' target='_blank'>https://arxiv.org/pdf/2310.10123.pdf</a></span>   <span><a href='https://jiangyitong.github.io/AutoDIR_webpage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, Jinwei Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10123">AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present AutoDIR, an innovative all-in-one image restoration system incorporating latent diffusion. AutoDIR excels in its ability to automatically identify and restore images suffering from a range of unknown degradations. AutoDIR offers intuitive open-vocabulary image editing, empowering users to customize and enhance images according to their preferences. Specifically, AutoDIR consists of two key stages: a Blind Image Quality Assessment (BIQA) stage based on a semantic-agnostic vision-language model which automatically detects unknown image degradations for input images, an All-in-One Image Restoration (AIR) stage utilizes structural-corrected latent diffusion which handles multiple types of image degradations. Extensive experimental evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches for a wider range of image restoration tasks. The design of AutoDIR also enables flexible user control (via text prompt) and generalization to new tasks as a foundation model of image restoration. Project is available at: \url{https://jiangyitong.github.io/AutoDIR_webpage/}.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2310.09560.pdf' target='_blank'>https://arxiv.org/pdf/2310.09560.pdf</a></span>   <span><a href='https://github.com/BarCodeReader/YOTO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Ke Yun, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09560">You Only Train Once: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent efforts in image quality assessment (IQA) have achieved promising performance, there still exists a considerable gap compared to the human visual system (HVS). One significant disparity lies in humans' seamless transition between full reference (FR) and no reference (NR) tasks, whereas existing models are constrained to either FR or NR tasks. This disparity implies the necessity of designing two distinct systems, thereby greatly diminishing the model's versatility. Therefore, our focus lies in unifying FR and NR IQA under a single framework. Specifically, we first employ an encoder to extract multi-level features from input images. Then a Hierarchical Attention (HA) module is proposed as a universal adapter for both FR and NR inputs to model the spatial distortion at each encoder stage. Furthermore, considering that different distortions contaminate encoder stages and damage image semantic meaning differently, a Semantic Distortion Aware (SDA) module is proposed to examine feature correlations between shallow and deep layers of the encoder. By adopting HA and SDA, the proposed network can effectively perform both FR and NR IQA. When our proposed model is independently trained on NR or FR IQA tasks, it outperforms existing models and achieves state-of-the-art performance. Moreover, when trained jointly on NR and FR IQA tasks, it further enhances the performance of NR IQA while achieving on-par performance in the state-of-the-art FR IQA. You only train once to perform both IQA tasks. Code will be released at: https://github.com/BarCodeReader/YOTO.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2309.17105.pdf' target='_blank'>https://arxiv.org/pdf/2309.17105.pdf</a></span>   <span><a href='https://github.com/iSEE-Laboratory/Continual-AQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan-Ming Li, Ling-An Zeng, Jing-Ke Meng, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17105">Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) is a task that tries to answer how well an action is carried out. While remarkable progress has been achieved, existing works on AQA assume that all the training data are visible for training at one time, but do not enable continual learning on assessing new technical actions. In this work, we address such a Continual Learning problem in AQA (Continual-AQA), which urges a unified model to learn AQA tasks sequentially without forgetting. Our idea for modeling Continual-AQA is to sequentially learn a task-consistent score-discriminative feature distribution, in which the latent features express a strong correlation with the score labels regardless of the task or action types.From this perspective, we aim to mitigate the forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of new and previous data into a score-discriminative distribution, a novel Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data from previous tasks with limited memory size. Secondly, an Action General-Specific Graph is developed to learn and decouple the action-general and action-specific knowledge so that the task-consistent score-discriminative features can be better extracted across various tasks. Extensive experiments are conducted to evaluate the contributions of proposed components. The comparisons with the existing continual learning methods additionally verify the effectiveness and versatility of our approach. Data and code are available at https://github.com/iSEE-Laboratory/Continual-AQA.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2309.14181.pdf' target='_blank'>https://arxiv.org/pdf/2309.14181.pdf</a></span>   <span><a href='https://q-future.github.io/Q-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14181">Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://q-future.github.io/Q-Bench.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2309.13609.pdf' target='_blank'>https://arxiv.org/pdf/2309.13609.pdf</a></span>   <span><a href='https://github.com/GZHU-DVL/AttackVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13609">Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Video Quality Assessment (NR-VQA) plays an essential role in improving the viewing experience of end-users. Driven by deep learning, recent NR-VQA models based on Convolutional Neural Networks (CNNs) and Transformers have achieved outstanding performance. To build a reliable and practical assessment system, it is of great necessity to evaluate their robustness. However, such issue has received little attention in the academic community. In this paper, we make the first attempt to evaluate the robustness of NR-VQA models against adversarial attacks, and propose a patch-based random search method for black-box attack. Specifically, considering both the attack effect on quality score and the visual quality of adversarial video, the attack problem is formulated as misleading the estimated quality score under the constraint of just-noticeable difference (JND). Built upon such formulation, a novel loss function called Score-Reversed Boundary Loss is designed to push the adversarial video's estimated quality score far away from its ground-truth score towards a specific boundary, and the JND constraint is modeled as a strict $L_2$ and $L_\infty$ norm restriction. By this means, both white-box and black-box attacks can be launched in an effective and imperceptible manner. The source code is available at https://github.com/GZHU-DVL/AttackVQA.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2309.07385.pdf' target='_blank'>https://arxiv.org/pdf/2309.07385.pdf</a></span>   <span><a href='https://github.com/microsoft/P.808' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Babak Naderi, Ross Cutler, Nicolae-Catalin Ristea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07385">Multi-dimensional Speech Quality Assessment in Crowdsourcing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective speech quality assessment is the gold standard for evaluating speech enhancement processing and telecommunication systems. The commonly used standard ITU-T Rec. P.800 defines how to measure speech quality in lab environments, and ITU-T Rec.~P.808 extended it for crowdsourcing. ITU-T Rec. P.835 extends P.800 to measure the quality of speech in the presence of noise. ITU-T Rec. P.804 targets the conversation test and introduces perceptual speech quality dimensions which are measured during the listening phase of the conversation. The perceptual dimensions are noisiness, coloration, discontinuity, and loudness. We create a crowdsourcing implementation of a multi-dimensional subjective test following the scales from P.804 and extend it to include reverberation, the speech signal, and overall quality. We show the tool is both accurate and reproducible. The tool has been used in the ICASSP 2023 Speech Signal Improvement challenge and we show the utility of these speech quality dimensions in this challenge. The tool will be publicly available as open-source at https://github.com/microsoft/P.808.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2309.03472.pdf' target='_blank'>https://arxiv.org/pdf/2309.03472.pdf</a></span>   <span><a href='https://github.com/xiangjieSui/GSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangjie Sui, Hanwei Zhu, Xuelin Liu, Yuming Fang, Shiqi Wang, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03472">Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial efforts dedicated to the design of heuristic models for omnidirectional (i.e., 360$^\circ$) image quality assessment (OIQA), a conspicuous gap remains due to the lack of consideration for the diversity of viewing behaviors that leads to the varying perceptual quality of 360$^\circ$ images. Two critical aspects underline this oversight: the neglect of viewing conditions that significantly sway user gaze patterns and the overreliance on a single viewport sequence from the 360$^\circ$ image for quality inference. To address these issues, we introduce a unique generative scanpath representation (GSR) for effective quality inference of 360$^\circ$ images, which aggregates varied perceptual experiences of multi-hypothesis users under a predefined viewing condition. More specifically, given a viewing condition characterized by the starting point of viewing and exploration time, a set of scanpaths consisting of dynamic visual fixations can be produced using an apt scanpath generator. Following this vein, we use the scanpaths to convert the 360$^\circ$ image into the unique GSR, which provides a global overview of gazed-focused contents derived from scanpaths. As such, the quality inference of the 360$^\circ$ image is swiftly transformed to that of GSR. We then propose an efficient OIQA computational framework by learning the quality maps of GSR. Comprehensive experimental results validate that the predictions of the proposed framework are highly consistent with human perception in the spatiotemporal domain, especially in the challenging context of locally distorted 360$^\circ$ images under varied viewing conditions. The code will be released at https://github.com/xiangjieSui/GSR
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2308.06853.pdf' target='_blank'>https://arxiv.org/pdf/2308.06853.pdf</a></span>   <span><a href='https://github.com/xinyiW915/SPIE-2023-Supplementary' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Angeliki Katsenou, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06853">UGC Quality Assessment: Exploring the Impact of Saliency in Deep Feature-Based Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The volume of User Generated Content (UGC) has increased in recent years. The challenge with this type of content is assessing its quality. So far, the state-of-the-art metrics are not exhibiting a very high correlation with perceptual quality. In this paper, we explore state-of-the-art metrics that extract/combine natural scene statistics and deep neural network features. We experiment with these by introducing saliency maps to improve perceptibility. We train and test our models using public datasets, namely, YouTube-UGC and KoNViD-1k. Preliminary results indicate that high correlations are achieved by using only deep features while adding saliency is not always boosting the performance. Our results and code will be made publicly available to serve as a benchmark for the research community and can be found on our project page: https://github.com/xinyiW915/SPIE-2023-Supplementary.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2308.04904.pdf' target='_blank'>https://arxiv.org/pdf/2308.04904.pdf</a></span>   <span><a href='https://github.com/QMME/StableVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, Xiongkuo Min, Guangtao Zhai, Ning Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04904">StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present. In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S). To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score. Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models. The database and codes are available at https://github.com/QMME/StableVQA.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2308.04156.pdf' target='_blank'>https://arxiv.org/pdf/2308.04156.pdf</a></span>   <span><a href='https://github.com/Fanning-Zhang/SATNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilin Zhang, Sumei Li, Haoxiang Chang, Peiming Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04156">Towards Top-Down Stereo Image Quality Assessment via Stereo Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stereo image quality assessment (SIQA) plays a crucial role in evaluating and improving the visual experience of 3D content. Existing visual properties-based methods for SIQA have achieved promising performance. However, these approaches ignore the top-down philosophy, leading to a lack of a comprehensive grasp of the human visual system (HVS) and SIQA. This paper presents a novel Stereo AttenTion Network (SATNet), which employs a top-down perspective to guide the quality assessment process. Specifically, our generalized Stereo AttenTion (SAT) structure adapts components and input/output for stereo scenarios. It leverages the fusion-generated attention map as a higher-level binocular modulator to influence two lower-level monocular features, allowing progressive recalibration of both throughout the pipeline. Additionally, we introduce an Energy Coefficient (EC) to flexibly tune the magnitude of binocular response, accounting for the fact that binocular responses in the primate primary visual cortex are less than the sum of monocular responses. To extract the most discriminative quality information from the summation and subtraction of the two branches of monocular features, we utilize a dual-pooling strategy that applies min-pooling and max-pooling operations to the respective branches. Experimental results highlight the superiority of our top-down method in advancing the state-of-the-art in the SIQA field. The code is available at https://github.com/Fanning-Zhang/SATNet.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2308.03060.pdf' target='_blank'>https://arxiv.org/pdf/2308.03060.pdf</a></span>   <span><a href='https://github.com/chaofengc/IQA-PyTorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03060">TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) is a fundamental task in computer vision that has witnessed remarkable progress with deep neural networks. Inspired by the characteristics of the human visual system, existing methods typically use a combination of global and local representations (\ie, multi-scale features) to achieve superior performance. However, most of them adopt simple linear fusion of multi-scale features, and neglect their possibly complex relationship and interaction. In contrast, humans typically first form a global impression to locate important regions and then focus on local details in those regions. We therefore propose a top-down approach that uses high-level semantics to guide the IQA network to focus on semantically important local distortion regions, named as \emph{TOPIQ}. Our approach to IQA involves the design of a heuristic coarse-to-fine network (CFANet) that leverages multi-scale features and progressively propagates multi-level semantic information to low-level representations in a top-down manner. A key component of our approach is the proposed cross-scale attention mechanism, which calculates attention maps for lower level features guided by higher level features. This mechanism emphasizes active semantic regions for low-level distortions, thereby improving performance. CFANet can be used for both Full-Reference (FR) and No-Reference (NR) IQA. We use ResNet50 as its backbone and demonstrate that CFANet achieves better or competitive performance on most public FR and NR benchmarks compared with state-of-the-art methods based on vision transformers, while being much more efficient (with only ${\sim}13\%$ FLOPS of the current best FR method). Codes are released at \url{https://github.com/chaofengc/IQA-PyTorch}.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2307.15353.pdf' target='_blank'>https://arxiv.org/pdf/2307.15353.pdf</a></span>   <span><a href='https://github.com/JianghaiSCU/RealSH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Jiang, Haipeng Li, Songchen Han, Haoqiang Fan, Bing Zeng, Shuaicheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15353">Supervised Homography Learning with Realistic Dataset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose an iterative framework, which consists of two phases: a generation phase and a training phase, to generate realistic training data and yield a supervised homography network. In the generation phase, given an unlabeled image pair, we utilize the pre-estimated dominant plane masks and homography of the pair, along with another sampled homography that serves as ground truth to generate a new labeled training pair with realistic motion. In the training phase, the generated data is used to train the supervised homography network, in which the training data is refined via a content consistency module and a quality assessment module. Once an iteration is finished, the trained network is used in the next data generation phase to update the pre-estimated homography. Through such an iterative strategy, the quality of the dataset and the performance of the network can be gradually and simultaneously improved. Experimental results show that our method achieves state-of-the-art performance and existing supervised methods can be also improved based on the generated dataset. Code and dataset are available at https://github.com/JianghaiSCU/RealSH.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2307.12027.pdf' target='_blank'>https://arxiv.org/pdf/2307.12027.pdf</a></span>   <span><a href='https://github.com/Luciennnnnnn/DualFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Luo, Yunan Zhu, Shunxin Xu, Dong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12027">On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several recent studies advocate the use of spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well interpreted yet. We tackle this issue by examining the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is susceptible to spectral changes. Our analyses reveal that the spectral discriminator indeed performs better than the ordinary (a.k.a. spatial) discriminator in identifying the differences in the high-frequency range; however, the spatial discriminator holds an advantage in the low-frequency range. Thus, we suggest that the spectral and spatial discriminators shall be used simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra by Transformer. We verify the effectiveness of the proposed method twofold. On the one hand, thanks to the additional spectral discriminator, our obtained SR images have their spectra better aligned to those of the real images, which leads to a better PD tradeoff. On the other hand, our ensembled discriminator predicts the perceptual quality more accurately, as evidenced in the no-reference image quality assessment task.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2307.10813.pdf' target='_blank'>https://arxiv.org/pdf/2307.10813.pdf</a></span>   <span><a href='https://github.com/iamazxl/OAVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilei Zhu, Huiyu Duan, Yuqin Cao, Yuxin Zhu, Yucheng Zhu, Jing Liu, Li Chen, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10813">Perceptual Quality Assessment of Omnidirectional Audio-visual Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc. Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE). However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals. In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores. Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies. We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation. Our dataset is available at https://github.com/iamazxl/OAVQA.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2307.04455.pdf' target='_blank'>https://arxiv.org/pdf/2307.04455.pdf</a></span>   <span><a href='https://github.com/Hedlen/SAM-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04455">SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2307.02808.pdf' target='_blank'>https://arxiv.org/pdf/2307.02808.pdf</a></span>   <span><a href='https://github.com/zzc-1998/SJTU-H3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu, Chunyi Li, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02808">Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zero-shot performance. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2306.09426.pdf' target='_blank'>https://arxiv.org/pdf/2306.09426.pdf</a></span>   <span><a href='https://github.com/vsantjr/DL_BlindSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valdivino Alexandre de Santiago JÃºnior
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09426">Deep learning techniques for blind image super-resolution: A high-scale multi-domain perspective evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite several solutions and experiments have been conducted recently addressing image super-resolution (SR), boosted by deep learning (DL) techniques, they do not usually design evaluations with high scaling factors, capping it at 2x or 4x. Moreover, the datasets are generally benchmarks which do not truly encompass significant diversity of domains to proper evaluate the techniques. It is also interesting to remark that blind SR is attractive for real-world scenarios since it is based on the idea that the degradation process is unknown, and hence techniques in this context rely basically on low-resolution (LR) images. In this article, we present a high-scale (8x) controlled experiment which evaluates five recent DL techniques tailored for blind image SR: Adaptive Pseudo Augmentation (APA), Blind Image SR with Spatially Variant Degradations (BlindSR), Deep Alternating Network (DAN), FastGAN, and Mixture of Experts Super-Resolution (MoESR). We consider 14 small datasets from five different broader domains which are: aerial, fauna, flora, medical, and satellite. Another distinctive characteristic of our evaluation is that some of the DL approaches were designed for single-image SR but others not. Two no-reference metrics were selected, being the classical natural image quality evaluator (NIQE) and the recent transformer-based multi-dimension attention network for no-reference image quality assessment (MANIQA) score, to assess the techniques. Overall, MoESR can be regarded as the best solution although the perceptual quality of the created HR images of all the techniques still needs to improve. Supporting code: https://github.com/vsantjr/DL_BlindSR. Datasets: https://www.kaggle.com/datasets/valdivinosantiago/dl-blindsr-datasets.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2306.08243.pdf' target='_blank'>https://arxiv.org/pdf/2306.08243.pdf</a></span>   <span><a href='https://github.com/Li-Jicheng/MMASD-A-Multimodal-Dataset-for-Autism-Intervention-Analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jicheng Li, Vuthea Chheang, Pinar Kullu, Eli Brignac, Zhang Guo, Kenneth E. Barner, Anjana Bhat, Roghayeh Leila Barmaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08243">MMASD: A Multimodal Dataset for Autism Intervention Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data; some of which are derived from original videos: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evaluation scores of children, e.g., ADOS scores. MMASD aims to assist researchers and therapists in understanding children's cognitive status, monitoring their progress during therapy, and customizing the treatment plan accordingly. It also has inspiration for downstream tasks such as action quality assessment and interpersonal synchrony estimation. MMASD dataset can be easily accessed at https://github.com/Li-Jicheng/MMASD-A-Multimodal-Dataset-for-Autism-Intervention-Analysis.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2306.05658.pdf' target='_blank'>https://arxiv.org/pdf/2306.05658.pdf</a></span>   <span><a href='https://github.com/zzc-1998/GMS-3DQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Houning Wu, Yingjie Zhou, Chunyi Li, Xiongkuo Min, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05658">GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, most 3D model quality assessment (3DQA) methods have been aimed at improving performance. However, little attention has been paid to the computational cost and inference time required for practical applications. Model-based 3DQA methods extract features directly from the 3D models, which are characterized by their high degree of complexity. As a result, many researchers are inclined towards utilizing projection-based 3DQA methods. Nevertheless, previous projection-based 3DQA methods directly extract features from multi-projections to ensure quality prediction accuracy, which calls for more resource consumption and inevitably leads to inefficiency. Thus in this paper, we address this challenge by proposing a no-reference (NR) projection-based \textit{\underline{G}rid \underline{M}ini-patch \underline{S}ampling \underline{3D} Model \underline{Q}uality \underline{A}ssessment (GMS-3DQA)} method. The projection images are rendered from six perpendicular viewpoints of the 3D model to cover sufficient quality information. To reduce redundancy and inference resources, we propose a multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid mini-patches from the multi-projections and forms the sampled grid mini-patches into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is then used to extract quality-aware features from the QMMs. The experimental results show that the proposed GMS-3DQA outperforms existing state-of-the-art NR-3DQA methods on the point cloud quality assessment databases. The efficiency analysis reveals that the proposed GMS-3DQA requires far less computational resources and inference time than other 3DQA competitors. The code will be available at https://github.com/zzc-1998/GMS-3DQA.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2306.04717.pdf' target='_blank'>https://arxiv.org/pdf/2306.04717.pdf</a></span>   <span><a href='https://github.com/lcysyzxdxc/AGIQA-3k-Database' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04717">AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K will inspire subsequent AGI quality models to fit human subjective perception mechanisms at both perception and alignment levels and to optimize the generation result of future AGI models. The database is released on https://github.com/lcysyzxdxc/AGIQA-3k-Database.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2306.02398.pdf' target='_blank'>https://arxiv.org/pdf/2306.02398.pdf</a></span>   <span><a href='https://github.com/JunFu1995/SGH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02398">Scale Guided Hypernetwork for Blind Super-Resolution Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the emergence of image super-resolution (SR) algorithm, how to blindly evaluate the quality of super-resolution images has become an urgent task. However, existing blind SR image quality assessment (IQA) metrics merely focus on visual characteristics of super-resolution images, ignoring the available scale information. In this paper, we reveal that the scale factor has a statistically significant impact on subjective quality scores of SR images, indicating that the scale information can be used to guide the task of blind SR IQA. Motivated by this, we propose a scale guided hypernetwork framework that evaluates SR image quality in a scale-adaptive manner. Specifically, the blind SR IQA procedure is divided into three stages, i.e., content perception, evaluation rule generation, and quality prediction. After content perception, a hypernetwork generates the evaluation rule used in quality prediction based on the scale factor of the SR image. We apply the proposed scale guided hypernetwork framework to existing representative blind IQA metrics, and experimental results show that the proposed framework not only boosts the performance of these IQA metrics but also enhances their generalization abilities. Source code will be available at https://github.com/JunFu1995/SGH.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2305.14691.pdf' target='_blank'>https://arxiv.org/pdf/2305.14691.pdf</a></span>   <span><a href='https://github.com/DongChen06/Label-efficient-in-Agriculture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajia Li, Dong Chen, Xinda Qi, Zhaojian Li, Yanbo Huang, Daniel Morris, Xiaobo Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14691">Label-Efficient Learning in Agriculture: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The past decade has witnessed many great successes of machine learning (ML) and deep learning (DL) applications in agricultural systems, including weed control, plant disease diagnosis, agricultural robotics, and precision livestock management. Despite tremendous progresses, one downside of such ML/DL models is that they generally rely on large-scale labeled datasets for training, and the performance of such models is strongly influenced by the size and quality of available labeled data samples. In addition, collecting, processing, and labeling such large-scale datasets is extremely costly and time-consuming, partially due to the rising cost in human labor. Therefore, developing label-efficient ML/DL methods for agricultural applications has received significant interests among researchers and practitioners. In fact, there are more than 50 papers on developing and applying deep-learning-based label-efficient techniques to address various agricultural problems since 2016, which motivates the authors to provide a timely and comprehensive review of recent label-efficient ML/DL methods in agricultural applications. To this end, we first develop a principled taxonomy to organize these methods according to the degree of supervision, including weak supervision (i.e., active learning and semi-/weakly- supervised learning), and no supervision (i.e., un-/self- supervised learning), supplemented by representative state-of-the-art label-efficient ML/DL methods. In addition, a systematic review of various agricultural applications exploiting these label-efficient algorithms, such as precision agriculture, plant phenotyping, and postharvest quality assessment, is presented. Finally, we discuss the current problems and challenges, as well as future research directions. A well-classified paper list can be accessed at https://github.com/DongChen06/Label-efficient-in-Agriculture.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2305.14684.pdf' target='_blank'>https://arxiv.org/pdf/2305.14684.pdf</a></span>   <span><a href='https://github.com/Macro-Zhou/NRIQA-VISOR/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehong Zhou, Fei Zhou, Guoping Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14684">Collaborative Auto-encoding for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) is a challenging problem with important real-world applications. Recent efforts attempting to exploit powerful representations by deep neural networks (DNN) are hindered by the lack of subjectively annotated data. This paper presents a novel BIQA method which overcomes this fundamental obstacle. Specifically, we design a pair of collaborative autoencoders (COAE) consisting of a content autoencoder (CAE) and a distortion autoencoder (DAE) that work together to extract content and distortion representations, which are shown to be highly descriptive of image quality. While the CAE follows a standard codec procedure, we introduce the CAE-encoded feature as an extra input to the DAE's decoder for reconstructing distorted images, thus effectively forcing DAE's encoder to extract distortion representations. The self-supervised learning framework allows the COAE including two feature extractors to be trained by almost unlimited amount of data, thus leaving limited samples with annotations to finetune a BIQA model. We will show that the proposed BIQA method achieves state-of-the-art performance and has superior generalization capability over other learning based models. The codes are available at: https://github.com/Macro-Zhou/NRIQA-VISOR/.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2305.12726.pdf' target='_blank'>https://arxiv.org/pdf/2305.12726.pdf</a></span>   <span><a href='https://github.com/VQAssessment/MaxVQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12726">Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of in-the-wild videos has greatly expanded the Video Quality Assessment (VQA) problem. Unlike early definitions that usually focus on limited distortion types, VQA on in-the-wild videos is especially challenging as it could be affected by complicated factors, including various distortions and diverse contents. Though subjective studies have collected overall quality scores for these videos, how the abstract quality scores relate with specific factors is still obscure, hindering VQA methods from more concrete quality evaluations (e.g. sharpness of a video). To solve this problem, we collect over two million opinions on 4,543 in-the-wild videos on 13 dimensions of quality-related factors, including in-capture authentic distortions (e.g. motion blur, noise, flicker), errors introduced by compression and transmission, and higher-level experiences on semantic contents and aesthetic issues (e.g. composition, camera trajectory), to establish the multi-dimensional Maxwell database. Specifically, we ask the subjects to label among a positive, a negative, and a neutral choice for each dimension. These explanation-level opinions allow us to measure the relationships between specific quality factors and abstract subjective quality ratings, and to benchmark different categories of VQA algorithms on each dimension, so as to more comprehensively analyze their strengths and weaknesses. Furthermore, we propose the MaxVQA, a language-prompted VQA approach that modifies vision-language foundation model CLIP to better capture important quality issues as observed in our analyses. The MaxVQA can jointly evaluate various specific quality factors and final quality scores with state-of-the-art accuracy on all dimensions, and superb generalization ability on existing datasets. Code and data available at https://github.com/VQAssessment/MaxVQA.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2305.10983.pdf' target='_blank'>https://arxiv.org/pdf/2305.10983.pdf</a></span>   <span><a href='https://github.com/TianheWu/Assessor360' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Wu, Shuwei Shi, Haoming Cai, Mingdeng Cao, Jing Xiao, Yinqiang Zheng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10983">Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively assess the human perceptual quality of omnidirectional images (ODIs) without relying on pristine-quality image information. It is becoming more significant with the increasing advancement of virtual reality (VR) technology. However, the quality assessment of ODIs is severely hampered by the fact that the existing BOIQA pipeline lacks the modeling of the observer's browsing process. To tackle this issue, we propose a novel multi-sequence network for BOIQA called Assessor360, which is derived from the realistic multi-assessor ODI quality assessment procedure. Specifically, we propose a generalized Recursive Probability Sampling (RPS) method for the BOIQA task, combining content and details information to generate multiple pseudo-viewport sequences from a given starting point. Additionally, we design a Multi-scale Feature Aggregation (MFA) module with a Distortion-aware Block (DAB) to fuse distorted and semantic features of each viewport. We also devise Temporal Modeling Module (TMM) to learn the viewport transition in the temporal domain. Extensive experimental results demonstrate that Assessor360 outperforms state-of-the-art methods on multiple OIQA datasets. The code and models are available at https://github.com/TianheWu/Assessor360.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2305.09512.pdf' target='_blank'>https://arxiv.org/pdf/2305.09512.pdf</a></span>   <span><a href='https://github.com/wenzhouyidu/Light-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunlong Dong, Xiaohong Liu, Yixuan Gao, Xunchu Zhou, Tao Tan, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09512">Light-VQA: A Multi-Dimensional Quality Assessment Model for Low-Light Video Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Users Generated Content (UGC) videos becomes ubiquitous in our daily lives. However, due to the limitations of photographic equipments and techniques, UGC videos often contain various degradations, in which one of the most visually unfavorable effects is the underexposure. Therefore, corresponding video enhancement algorithms such as Low-Light Video Enhancement (LLVE) have been proposed to deal with the specific degradation. However, different from video enhancement algorithms, almost all existing Video Quality Assessment (VQA) models are built generally rather than specifically, which measure the quality of a video from a comprehensive perspective. To the best of our knowledge, there is no VQA model specially designed for videos enhanced by LLVE algorithms. To this end, we first construct a Low-Light Video Enhancement Quality Assessment (LLVE-QA) dataset in which 254 original low-light videos are collected and then enhanced by leveraging 8 LLVE algorithms to obtain 2,060 videos in total. Moreover, we propose a quality assessment model specialized in LLVE, named Light-VQA. More concretely, since the brightness and noise have the most impact on low-light enhanced VQA, we handcraft corresponding features and integrate them with deep-learning-based semantic features as the overall spatial information. As for temporal information, in addition to deep-learning-based motion features, we also investigate the handcrafted brightness consistency among video frames, and the overall temporal information is their concatenation. Subsequently, spatial and temporal information is fused to obtain the quality-aware representation of a video. Extensive experimental results show that our Light-VQA achieves the best performance against the current State-Of-The-Art (SOTA) on LLVE-QA and public dataset. Dataset and Codes can be found at https://github.com/wenzhouyidu/Light-VQA.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2305.07829.pdf' target='_blank'>https://arxiv.org/pdf/2305.07829.pdf</a></span>   <span><a href='https://github.com/philox12358/COPP-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Cheng, Honglei Su, Jari Korhonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07829">No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of 3D vision applications based on point clouds, point cloud quality assessment(PCQA) is becoming an important research topic. However, the prior PCQA methods ignore the effect of local quality variance across different areas of the point cloud. To take an advantage of the quality distribution imbalance, we propose a no-reference point cloud quality assessment (NR-PCQA) method with local area correlation analysis capability, denoted as COPP-Net. More specifically, we split a point cloud into patches, generate texture and structure features for each patch, and fuse them into patch features to predict patch quality. Then, we gather the features of all the patches of a point cloud for correlation analysis, to obtain the correlation weights. Finally, the predicted qualities and correlation weights for all the patches are used to derive the final quality score. Experimental results show that our method outperforms the state-of-the-art benchmark NR-PCQA methods. The source code for the proposed COPP-Net can be found at https://github.com/philox12358/COPP-Net.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2305.02422.pdf' target='_blank'>https://arxiv.org/pdf/2305.02422.pdf</a></span>   <span><a href='https://github.com/lskdream/GAMIVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Chih Chen, Avinab Saha, Chase Davis, Bo Qiu, Xiaoming Wang, Rahul Gowda, Ioannis Katsavounidis, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02422">GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, and deep semantic features. Using a support vector regression (SVR) as a regressor, GAMIVAL achieves superior performance on the new LIVE-Meta Mobile Cloud Gaming (LIVE-Meta MCG) video quality database.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2305.01899.pdf' target='_blank'>https://arxiv.org/pdf/2305.01899.pdf</a></span>   <span><a href='https://github.com/Frenkie14/Agrifood-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Chen, Liang Lv, Di Wang, Jing Zhang, Yue Yang, Zeyang Zhao, Chen Wang, Xiaowei Guo, Hao Chen, Qingye Wang, Yufei Xu, Qiming Zhang, Bo Du, Liangpei Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01899">Empowering Agrifood System with Artificial Intelligence: A Survey of the Progress, Challenges and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the world population rapidly increasing, transforming our agrifood systems to be more productive, efficient, safe, and sustainable is crucial to mitigate potential food shortages. Recently, artificial intelligence (AI) techniques such as deep learning (DL) have demonstrated their strong abilities in various areas, including language, vision, remote sensing (RS), and agrifood systems applications. However, the overall impact of AI on agrifood systems remains unclear. In this paper, we thoroughly review how AI techniques can transform agrifood systems and contribute to the modern agrifood industry. Firstly, we summarize the data acquisition methods in agrifood systems, including acquisition, storage, and processing techniques. Secondly, we present a progress review of AI methods in agrifood systems, specifically in agriculture, animal husbandry, and fishery, covering topics such as agrifood classification, growth monitoring, yield prediction, and quality assessment. Furthermore, we highlight potential challenges and promising research opportunities for transforming modern agrifood systems with AI. We hope this survey could offer an overall picture to newcomers in the field and serve as a starting point for their further research. The project website is https://github.com/Frenkie14/Agrifood-Survey.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2304.07056.pdf' target='_blank'>https://arxiv.org/pdf/2304.07056.pdf</a></span>   <span><a href='https://github.com/Yixuan423/Compressed-Face-Videos-Quality-Assessment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Li, Bolin Chen, Baoliang Chen, Meng Wang, Shiqi Wang, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07056">Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversified content using six representative video codecs, including two traditional methods based on hybrid coding frameworks, two end-to-end methods, and two generative methods. In addition, a FAce VideO IntegeRity (FAVOR) index for face video compression was developed to measure the perceptual quality, considering the distinct content characteristics and temporal priors of the face videos. Experimental results exhibit its superior performance on the proposed CFVQA dataset. The benchmark is now made publicly available at: https://github.com/Yixuan423/Compressed-Face-Videos-Quality-Assessment.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2304.06440.pdf' target='_blank'>https://arxiv.org/pdf/2304.06440.pdf</a></span>   <span><a href='https://github.com/k-zha14/Zoom-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Kun Yuan, Ming Sun, Xing Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06440">Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) aims to simulate the human perception of video quality, which is influenced by factors ranging from low-level color and texture details to high-level semantic content. To effectively model these complicated quality-related factors, in this paper, we decompose video into three levels (\ie, patch level, frame level, and clip level), and propose a novel Zoom-VQA architecture to perceive spatio-temporal features at different levels. It integrates three components: patch attention module, frame pyramid alignment, and clip ensemble strategy, respectively for capturing region-of-interest in the spatial dimension, multi-level information at different feature levels, and distortions distributed over the temporal dimension. Owing to the comprehensive design, Zoom-VQA obtains state-of-the-art results on four VQA benchmarks and achieves 2nd place in the NTIRE 2023 VQA challenge. Notably, Zoom-VQA has outperformed the previous best results on two subsets of LSVQ, achieving 0.8860 (+1.0%) and 0.7985 (+1.9%) of SRCC on the respective subsets. Adequate ablation studies further verify the effectiveness of each component. Codes and models are released in https://github.com/k-zha14/Zoom-VQA.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2304.05879.pdf' target='_blank'>https://arxiv.org/pdf/2304.05879.pdf</a></span>   <span><a href='https://github.com/Medical-Image-Analysis-Laboratory/fetal_brain_qc/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Sanchez, Oscar Esteban, Yvan Gomez, Elisenda Eixarch, Meritxell Bach Cuadra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05879">FetMRQC: Automated Quality Control for fetal brain MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where large and unpredictable fetal motion can lead to substantial artifacts in the acquired images. Existing methods for fetal brain quality assessment operate at the \textit{slice} level, and fail to get a comprehensive picture of the quality of an image, that can only be achieved by looking at the \textit{entire} brain volume. In this work, we propose FetMRQC, a machine learning framework for automated image quality assessment tailored to fetal brain MRI, which extracts an ensemble of quality metrics that are then used to predict experts' ratings. Based on the manual ratings of more than 1000 low-resolution stacks acquired across two different institutions, we show that, compared with existing quality metrics, FetMRQC is able to generalize out-of-domain, while being interpretable and data efficient. We also release a novel manual quality rating tool designed to facilitate and optimize quality rating of fetal brain images.
  Our tool, along with all the code to generate, train and evaluate the model is available at https://github.com/Medical-Image-Analysis-Laboratory/fetal_brain_qc/ .
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2304.05772.pdf' target='_blank'>https://arxiv.org/pdf/2304.05772.pdf</a></span>   <span><a href='https://github.com/DXOMARK-Research/PIQ2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Chahine, Ana-Stefania Calarasanu, Davide Garcia-Civiero, Theo Cayla, Sira Ferradans, Jean Ponce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05772">An Image Quality Assessment Dataset for Portraits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Year after year, the demand for ever-better smartphone photos continues to grow, in particular in the domain of portrait photography. Manufacturers thus use perceptual quality criteria throughout the development of smartphone cameras. This costly procedure can be partially replaced by automated learning-based methods for image quality assessment (IQA). Due to its subjective nature, it is necessary to estimate and guarantee the consistency of the IQA process, a characteristic lacking in the mean opinion scores (MOS) widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA) datasets pay little attention to the difficulty of cross-content assessment, which may degrade the quality of annotations. This paper introduces PIQ23, a portrait-specific IQA dataset of 5116 images of 50 predefined scenarios acquired by 100 smartphones, covering a high variety of brands, models, and use cases. The dataset includes individuals of various genders and ethnicities who have given explicit and informed consent for their photographs to be used in public research. It is annotated by pairwise comparisons (PWC) collected from over 30 image quality experts for three image attributes: face detail preservation, face target exposure, and overall image quality. An in-depth statistical analysis of these annotations allows us to evaluate their consistency over PIQ23. Finally, we show through an extensive comparison with existing baselines that semantic information (image context) can be used to improve IQA predictions. The dataset along with the proposed statistical analysis and BIQA algorithms are available: https://github.com/DXOMARK-Research/PIQ2023
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2304.04203.pdf' target='_blank'>https://arxiv.org/pdf/2304.04203.pdf</a></span>   <span><a href='https://github.com/bdne/OpenDriver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Delong Liu, Shichao Li, Tianyi Shi, Zhu Meng, Guanyu Chen, Yadong Huang, Jin Dong, Zhicheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04203">OpenDriver: An Open-Road Driver State Detection Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Among numerous studies for driver state detection, wearable physiological measurements offer a practical method for real-time monitoring. However, there are few driver physiological datasets in open-road scenarios, and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset, OpenDriver, for driver state detection is developed. The OpenDriver encompasses a total of 3,278 driving trips, with a signal collection duration spanning approximately 4,600 hours. Two modalities of driving signals are enrolled in OpenDriver: electrocardiogram (ECG) signals and six-axis motion data of the steering wheel from a motion measurement unit (IMU), which were recorded from 81 drivers and their vehicles. Furthermore, three challenging tasks are involved in our work, namely ECG signal quality assessment, individual biometric identification based on ECG signals, and physiological signal analysis in complex driving environments. To facilitate research in these tasks, corresponding benchmarks have also been introduced. First, a noisy augmentation strategy is applied to generate a larger-scale ECG signal dataset with realistic noise simulation for quality assessment. Second, an end-to-end contrastive learning framework is employed for individual biometric identification. Finally, a comprehensive analysis of drivers' HRV features under different driving conditions is conducted. Each benchmark provides evaluation metrics and reference results. The OpenDriver dataset will be publicly available at https://github.com/bdne/OpenDriver.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2303.14968.pdf' target='_blank'>https://arxiv.org/pdf/2303.14968.pdf</a></span>   <span><a href='https://github.com/zwx8981/LIQE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14968">Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) using a textual template, and compute the joint probability from the cosine similarities of the visual-textual embeddings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learning three tasks - BIQA, scene classification, and distortion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and distortion type identification tasks and outperforms the state-of-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) realigns the quality annotations from different IQA datasets more effectively. The source code is available at https://github.com/zwx8981/LIQE.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2303.08050.pdf' target='_blank'>https://arxiv.org/pdf/2303.08050.pdf</a></span>   <span><a href='https://github.com/zzc-1998/CGIQA6K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Yingjie Zhou, Jun Jia, Zhichao Zhang, Jing Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08050">Subjective and Objective Quality Assessment for in-the-Wild Computer Graphics Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer graphics images (CGIs) are artificially generated by means of computer programs and are widely perceived under various scenarios, such as games, streaming media, etc. In practice, the quality of CGIs consistently suffers from poor rendering during production, inevitable compression artifacts during the transmission of multimedia applications, and low aesthetic quality resulting from poor composition and design. However, few works have been dedicated to dealing with the challenge of computer graphics image quality assessment (CGIQA). Most image quality assessment (IQA) metrics are developed for natural scene images (NSIs) and validated on databases consisting of NSIs with synthetic distortions, which are not suitable for in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled laboratory environment to obtain the accurate perceptual ratings of the CGIs. Then, we propose an effective deep learning-based no-reference (NR) IQA model by utilizing both distortion and aesthetic quality representation. Experimental results show that the proposed method outperforms all other state-of-the-art NR IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. The database is released at https://github.com/zzc-1998/CGIQA6K.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2303.06907.pdf' target='_blank'>https://arxiv.org/pdf/2303.06907.pdf</a></span>   <span><a href='https://github.com/Nafiseh-Tofighi/ST360IQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nafiseh Jabbari Tofighi, Mohamed Hedi Elfkir, Nevrez Imamoglu, Cagri Ozcinar, Erkut Erdem, Aykut Erdem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06907">ST360IQ: No-Reference Omnidirectional Image Quality Assessment with Spherical Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional images, aka 360 images, can deliver immersive and interactive visual experiences. As their popularity has increased dramatically in recent years, evaluating the quality of 360 images has become a problem of interest since it provides insights for capturing, transmitting, and consuming this new media. However, directly adapting quality assessment methods proposed for standard natural images for omnidirectional data poses certain challenges. These models need to deal with very high-resolution data and implicit distortions due to the spherical form of the images. In this study, we present a method for no-reference 360 image quality assessment. Our proposed ST360IQ model extracts tangent viewports from the salient parts of the input omnidirectional image and employs a vision-transformers based module processing saliency selective patches/tokens that estimates a quality score from each viewport. Then, it aggregates these scores to give a final quality score. Our experiments on two benchmark datasets, namely OIQA and CVIQ datasets, demonstrate that as compared to the state-of-the-art, our approach predicts the quality of an omnidirectional image correlated with the human-perceived image quality. The code has been available on https://github.com/Nafiseh-Tofighi/ST360IQ
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2303.01223.pdf' target='_blank'>https://arxiv.org/pdf/2303.01223.pdf</a></span>   <span><a href='https://github.com/anerv/BikeDNA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ane Rahbek VierÃ¸, Anastassia Vybornova, Michael Szell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01223">BikeDNA: A Tool for Bicycle Infrastructure Data & Network Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality data on existing bicycle infrastructure are a requirement for evidence-based bicycle network planning, which supports a green transition of human mobility. However, this requirement is rarely met: Data from governmental agencies or crowdsourced projects like OpenStreetMap often suffer from unknown, heterogeneous, or low quality. Currently available tools for road network data quality assessment often fail to account for network topology, spatial heterogeneity, and bicycle-specific data characteristics. To fill these gaps, we introduce BikeDNA, an open-source tool for reproducible quality assessment tailored to bicycle infrastructure data with a focus on network structure and connectivity. BikeDNA performs either a standalone analysis of one data set or a comparative analysis between OpenStreetMap and a reference data set, including feature matching. Data quality metrics are considered both globally for the entire study area and locally on grid cell level, thus exposing spatial variation in data quality. Interactive maps and HTML/PDF reports are generated to facilitate the visual exploration and communication of results. BikeDNA supports quality assessments of bicycle infrastructure data for a wide range of applications -- from urban planning to OpenStreetMap data improvement or network research for sustainable mobility.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2302.11464.pdf' target='_blank'>https://arxiv.org/pdf/2302.11464.pdf</a></span>   <span><a href='https://github.com/Baoliang93/IACA_For_Lowlight_IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoliang Chen, Lingyu Zhu, Hanwei Zhu, Wenhan Yang, Linqi Song, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11464">Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing consensus in the research community that the optimization of low-light image enhancement approaches should be guided by the visual quality perceived by end users. Despite the substantial efforts invested in the design of low-light enhancement algorithms, there has been comparatively limited focus on assessing subjective and objective quality systematically. To mitigate this gap and provide a clear path towards optimizing low-light image enhancement for better visual quality, we propose a gap-closing framework. In particular, our gap-closing framework starts with the creation of a large-scale dataset for Subjective QUality Assessment of REconstructed LOw-Light Images (SQUARE-LOL). This database serves as the foundation for studying the quality of enhanced images and conducting a comprehensive subjective user study. Subsequently, we propose an objective quality assessment measure that plays a critical role in bridging the gap between visual quality and enhancement. Finally, we demonstrate that our proposed objective quality measure can be incorporated into the process of optimizing the learning of the enhancement model toward perceptual optimality. We validate the effectiveness of our proposed framework through both the accuracy of quality prediction and the perceptual quality of image enhancement. Our database and codes are publicly available at https://github.com/Baoliang93/IACA_For_Lowlight_IQA.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2302.09598.pdf' target='_blank'>https://arxiv.org/pdf/2302.09598.pdf</a></span>   <span><a href='https://github.com/zhwzhong/Guided-Depth-Map-Super-resolution-A-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Zhong, Xianming Liu, Junjun Jiang, Debin Zhao, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09598">Guided Depth Map Super-resolution: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guided depth map super-resolution (GDSR), which aims to reconstruct a high-resolution (HR) depth map from a low-resolution (LR) observation with the help of a paired HR color image, is a longstanding and fundamental problem, it has attracted considerable attention from computer vision and image processing communities. A myriad of novel and effective approaches have been proposed recently, especially with powerful deep learning techniques. This survey is an effort to present a comprehensive survey of recent progress in GDSR. We start by summarizing the problem of GDSR and explaining why it is challenging. Next, we introduce some commonly used datasets and image quality assessment methods. In addition, we roughly classify existing GDSR methods into three categories, i.e., filtering-based methods, prior-based methods, and learning-based methods. In each category, we introduce the general description of the published algorithms and design principles, summarize the representative methods, and discuss their highlights and limitations. Moreover, the depth related applications are introduced. Furthermore, we conduct experiments to evaluate the performance of some representative methods based on unified experimental configurations, so as to offer a systematic and fair performance evaluation to readers. Finally, we conclude this survey with possible directions and open problems for further research. All the related materials can be found at \url{https://github.com/zhwzhong/Guided-Depth-Map-Super-resolution-A-Survey}.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2302.00918.pdf' target='_blank'>https://arxiv.org/pdf/2302.00918.pdf</a></span>   <span><a href='https://github.com/XianyunSun/VRA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianyun Sun, Beibei Dong, Caiyong Wang, Bo Peng, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00918">Visual Realism Assessment for Face-swap Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep-learning based face-swap videos, also known as deep fakes, are becoming more and more realistic and deceiving. The malicious usage of these face-swap videos has caused wide concerns. The research community has been focusing on the automatic detection of these fake videos, but the assessment of their visual realism, as perceived by human eyes, is still an unexplored dimension. Visual realism assessment, or VRA, is essential for assessing the potential impact that may be brought by a specific face-swap video, and it is also important as a quality assessment metric to compare different face-swap methods. In this paper, we make a small step towards this new VRA direction by building a benchmark for evaluating the effectiveness of different automatic VRA models, which range from using traditional hand-crafted features to different kinds of deep-learning features. The evaluations are based on a recent competition dataset named DFGC 2022, which contains 1400 diverse face-swap videos that are annotated with Mean Opinion Scores (MOS) on visual realism. Comprehensive experiment results using 11 models and 3 protocols are shown and discussed. We demonstrate the feasibility of devising effective VRA models for assessing face-swap videos and methods. The particular usefulness of existing deepfake detection features for VRA is also noted. The code can be found at https://github.com/XianyunSun/VRA.git.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2211.04927.pdf' target='_blank'>https://arxiv.org/pdf/2211.04927.pdf</a></span>   <span><a href='https://github.com/h4nwei/DeepDC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwei Zhu, Baoliang Chen, Lingyu Zhu, Shiqi Wang, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.04927">DeepDC: Deep Distance Correlation as a Perceptual Image Quality Evaluator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ImageNet pre-trained deep neural networks (DNNs) show notable transferability for building effective image quality assessment (IQA) models. Such a remarkable byproduct has often been identified as an emergent property in previous studies. In this work, we attribute such capability to the intrinsic texture-sensitive characteristic that classifies images using texture features. We fully exploit this characteristic to develop a novel full-reference IQA (FR-IQA) model based exclusively on pre-trained DNN features. Specifically, we compute the distance correlation, a highly promising yet relatively under-investigated statistic, between reference and distorted images in the deep feature domain. In addition, the distance correlation quantifies both linear and nonlinear feature relationships, which is far beyond the widely used first-order and second-order statistics in the feature space. We conduct comprehensive experiments to demonstrate the superiority of the proposed quality model on five standard IQA datasets, one perceptual similarity dataset, two texture similarity datasets, and one geometric transformation dataset. Moreover, we optimize the proposed model to generate a broad spectrum of texture patterns, by treating the model as the style loss function for neural style transfer (NST). Extensive experiments demonstrate that the proposed texture synthesis and NST methods achieve the best quantitative and qualitative results. We release our code at https://github.com/h4nwei/DeepDC.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2211.04894.pdf' target='_blank'>https://arxiv.org/pdf/2211.04894.pdf</a></span>   <span><a href='https://github.com/VQAssessment/DOVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.04894">Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid increase in user-generated-content (UGC) videos calls for the development of effective video quality assessment (VQA) algorithms. However, the objective of the UGC-VQA problem is still ambiguous and can be viewed from two perspectives: the technical perspective, measuring the perception of distortions; and the aesthetic perspective, which relates to preference and recommendation on contents. To understand how these two perspectives affect overall subjective opinions in UGC-VQA, we conduct a large-scale subjective study to collect human quality opinions on overall quality of videos as well as perceptions from aesthetic and technical perspectives. The collected Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality opinions on UGC videos are universally and inevitably affected by both aesthetic and technical perspectives. In light of this, we propose the Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of UGC videos based on the two perspectives. The DOVER proves state-of-the-art performance in UGC-VQA under very high efficiency. With perspective opinions in DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable clear-cut quality evaluations from a single aesthetic or technical perspective. Code at https://github.com/VQAssessment/DOVER.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2210.10249.pdf' target='_blank'>https://arxiv.org/pdf/2210.10249.pdf</a></span>   <span><a href='https://github.com/caperock/imagequality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Dai, Daniel Berleant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.10249">Discovering Limitations of Image Quality Assessments with Noised Deep Learning Image Sets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality is important, and can affect overall performance in image processing and computer vision as well as for numerous other reasons. Image quality assessment (IQA) is consequently a vital task in different applications from aerial photography interpretation to object detection to medical image analysis. In previous research, the BRISQUE algorithm and the PSNR algorithm were evaluated with high resolution (atleast 512x384 pixels), but relatively small image sets (no more than 4,744 images). However, scientists have not evaluated IQA algorithms on low resolution (no more than 32x32 pixels), multi-perturbation, big image sets (for example, tleast 60,000 different images not counting their perturbations). This study explores these two IQA algorithms through experimental investigation. We first chose two deep learning image sets, CIFAR-10 and MNIST. Then, we added 68 perturbations that add noise to the images in specific sequences and noise intensities. In addition, we tracked the performance outputs of the two IQA algorithms with singly and multiply noised images. After quantitatively analyzing experimental results, we report the limitations of the two IQAs with these noised CIFAR-10 and MNIST image sets. We also explain three potential root causes for performance degradation. These findings point out weaknesses of the two IQA algorithms. The research results provide guidance to scientists and engineers developing accurate, robust IQA algorithms. All source codes, related image sets, and figures are shared on the website (https://github.com/caperock/imagequality) to support future scientific and industrial projects.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2210.04671.pdf' target='_blank'>https://arxiv.org/pdf/2210.04671.pdf</a></span>   <span><a href='https://github.com/zyj1318053/TCDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhang, Qi Yang, Yifei Zhou, Xiaozhong Xu, Le Yang, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04671">TCDM: Transformational Complexity Based Distortion Metric for Perceptual Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of objective point cloud quality assessment (PCQA) research is to develop quantitative metrics that measure point cloud quality in a perceptually consistent manner. Merging the research of cognitive science and intuition of the human visual system (HVS), in this paper, we evaluate the point cloud quality by measuring the complexity of transforming the distorted point cloud back to its reference, which in practice can be approximated by the code length of one point cloud when the other is given. For this purpose, we first make space segmentation for the reference and distorted point clouds based on a 3D Voronoi diagram to obtain a series of local patch pairs. Next, inspired by the predictive coding theory, we utilize a space-aware vector autoregressive (SA-VAR) model to encode the geometry and color channels of each reference patch with and without the distorted patch, respectively. Assuming that the residual errors follow the multi-variate Gaussian distributions, the self-complexity of the reference and transformational complexity between the reference and distorted samples are computed using covariance matrices. Additionally, the prediction terms generated by SA-VAR are introduced as one auxiliary feature to promote the final quality prediction. The effectiveness of the proposed transformational complexity based distortion metric (TCDM) is evaluated through extensive experiments conducted on five public point cloud quality assessment databases. The results demonstrate that TCDM achieves state-of-the-art (SOTA) performance, and further analysis confirms its robustness in various scenarios. The code is publicly available at https://github.com/zyj1318053/TCDM.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2210.00823.pdf' target='_blank'>https://arxiv.org/pdf/2210.00823.pdf</a></span>   <span><a href='https://github.com/danier97/BVI-VFI-database' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duolikun Danier, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00823">BVI-VFI: A Video Quality Database for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video frame interpolation (VFI) is a fundamental research topic in video processing, which is currently attracting increased attention across the research community. While the development of more advanced VFI algorithms has been extensively researched, there remains little understanding of how humans perceive the quality of interpolated content and how well existing objective quality assessment methods perform when measuring the perceived quality. In order to narrow this research gap, we have developed a new video quality database named BVI-VFI, which contains 540 distorted sequences generated by applying five commonly used VFI algorithms to 36 diverse source videos with various spatial resolutions and frame rates. We collected more than 10,800 quality ratings for these videos through a large scale subjective study involving 189 human subjects. Based on the collected subjective scores, we further analysed the influence of VFI algorithms and frame rates on the perceptual quality of interpolated videos. Moreover, we benchmarked the performance of 33 classic and state-of-the-art objective image/video quality metrics on the new database, and demonstrated the urgent requirement for more accurate bespoke quality assessment methods for VFI. To facilitate further research in this area, we have made BVI-VFI publicly available at https://github.com/danier97/BVI-VFI-database.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2209.09489.pdf' target='_blank'>https://arxiv.org/pdf/2209.09489.pdf</a></span>   <span><a href='https://github.com/zzc-1998/DHHQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Yuzhe Wu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09489">Perceptual Quality Assessment for Digital Human Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humans are attracting more and more research interest during the last decade, the generation, representation, rendering, and animation of which have been put into large amounts of effort. However, the quality assessment of digital humans has fallen behind. Therefore, to tackle the challenge of digital human quality assessment issues, we propose the first large-scale quality assessment database for three-dimensional (3D) scanned digital human heads (DHHs). The constructed database consists of 55 reference DHHs and 1,540 distorted DHHs along with the subjective perceptual ratings. Then, a simple yet effective full-reference (FR) projection-based method is proposed to evaluate the visual quality of DHHs. The pretrained Swin Transformer tiny is employed for hierarchical feature extraction and the multi-head attention module is utilized for feature fusion. The experimental results reveal that the proposed method exhibits state-of-the-art performance among the mainstream FR metrics. The database is released at https://github.com/zzc-1998/DHHQA.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2209.07126.pdf' target='_blank'>https://arxiv.org/pdf/2209.07126.pdf</a></span>   <span><a href='https://github.com/maruiperfect/SILF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Ma, Qingbo Wu, King Ngi Ngan, Hongliang Li, Fanman Meng, Linfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07126">Forgetting to Remember: A Scalable Incremental Learning Framework for Cross-Task Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed the great success of blind image quality assessment (BIQA) in various task-specific scenarios, which present invariable distortion types and evaluation criteria. However, due to the rigid structure and learning framework, they cannot apply to the cross-task BIQA scenario, where the distortion types and evaluation criteria keep changing in practical applications. This paper proposes a scalable incremental learning framework (SILF) that could sequentially conduct BIQA across multiple evaluation tasks with limited memory capacity. More specifically, we develop a dynamic parameter isolation strategy to sequentially update the task-specific parameter subsets, which are non-overlapped with each other. Each parameter subset is temporarily settled to Remember one evaluation preference toward its corresponding task, and the previously settled parameter subsets can be adaptively reused in the following BIQA to achieve better performance based on the task relevance. To suppress the unrestrained expansion of memory capacity in sequential tasks learning, we develop a scalable memory unit by gradually and selectively pruning unimportant neurons from previously settled parameter subsets, which enable us to Forget part of previous experiences and free the limited memory capacity for adapting to the emerging new tasks. Extensive experiments on eleven IQA datasets demonstrate that our proposed method significantly outperforms the other state-of-the-art methods in cross-task BIQA. The source code of the proposed method is available at https://github.com/maruiperfect/SILF.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2209.06950.pdf' target='_blank'>https://arxiv.org/pdf/2209.06950.pdf</a></span>   <span><a href='https://github.com/buggyyang/CDC_compression' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Yang, Stephan Mandt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06950">Lossy Image Compression with Conditional Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional ``content'' latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining ``texture'' variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based model, while also yielding competitive performance with VAE-based models in several distortion metrics. Furthermore, training the diffusion with $\mathcal{X}$-parameterization enables high-quality reconstructions in only a handful of decoding steps, greatly affecting the model's practicality. Our code is available at: \url{https://github.com/buggyyang/CDC_compression}
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2209.05321.pdf' target='_blank'>https://arxiv.org/pdf/2209.05321.pdf</a></span>   <span><a href='https://github.com/Baoliang93/DFSS-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05321">Deep Feature Statistics Mapping for Generalized Screen Content Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The statistical regularities of natural images, referred to as natural scene statistics, play an important role in no-reference image quality assessment. However, it has been widely acknowledged that screen content images (SCIs), which are typically computer generated, do not hold such statistics. Here we make the first attempt to learn the statistics of SCIs, based upon which the quality of SCIs can be effectively determined. The underlying mechanism of the proposed approach is based upon the mild assumption that the SCIs, which are not physically acquired, still obey certain statistics that could be understood in a learning fashion. We empirically show that the statistics deviation could be effectively leveraged in quality assessment, and the proposed method is superior when evaluated in different settings. Extensive experimental results demonstrate the Deep Feature Statistics based SCI Quality Assessment (DFSS-IQA) model delivers promising performance compared with existing NR-IQA models and shows a high generalization capability in the cross-dataset settings. The implementation of our method is publicly available at https://github.com/Baoliang93/DFSS-IQA.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2209.04634.pdf' target='_blank'>https://arxiv.org/pdf/2209.04634.pdf</a></span>   <span><a href='https://cogsys-tuebingen.github.io/realtime_event_simulator/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Ziegler, Daniel Teigland, Jonas Tebbe, Thomas Gossard, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.04634">Real-time event simulation with frame-based cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras are becoming increasingly popular in robotics and computer vision due to their beneficial properties, e.g., high temporal resolution, high bandwidth, almost no motion blur, and low power consumption. However, these cameras remain expensive and scarce in the market, making them inaccessible to the majority. Using event simulators minimizes the need for real event cameras to develop novel algorithms. However, due to the computational complexity of the simulation, the event streams of existing simulators cannot be generated in real-time but rather have to be pre-calculated from existing video sequences or pre-rendered and then simulated from a virtual 3D scene. Although these offline generated event streams can be used as training data for learning tasks, all response time dependent applications cannot benefit from these simulators yet, as they still require an actual event camera. This work proposes simulation methods that improve the performance of event simulation by two orders of magnitude (making them real-time capable) while remaining competitive in the quality assessment.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2209.00244.pdf' target='_blank'>https://arxiv.org/pdf/2209.00244.pdf</a></span>   <span><a href='https://github.com/zzc-1998/MM-PCQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Xiongkuo Min, Quan Zhou, Jun He, Qiyuan Wang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00244">MM-PCQA: Multi-Modal Learning for No-reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The visual quality of point clouds has been greatly emphasized since the ever-increasing 3D vision applications are expected to provide cost-effective and high-quality experiences for users. Looking back on the development of point cloud quality assessment (PCQA) methods, the visual quality is usually evaluated by utilizing single-modal information, i.e., either extracted from the 2D projections or 3D point cloud. The 2D projections contain rich texture and semantic information but are highly dependent on viewpoints, while the 3D point clouds are more sensitive to geometry distortions and invariant to viewpoints. Therefore, to leverage the advantages of both point cloud and projected image modalities, we propose a novel no-reference point cloud quality assessment (NR-PCQA) metric in a multi-modal fashion. In specific, we split the point clouds into sub-models to represent local geometry distortions such as point shift and down-sampling. Then we render the point clouds into 2D image projections for texture feature extraction. To achieve the goals, the sub-models and projected images are encoded with point-based and image-based neural networks. Finally, symmetric cross-modal attention is employed to fuse multi-modal quality-aware information. Experimental results show that our approach outperforms all compared state-of-the-art methods and is far ahead of previous NR-PCQA methods, which highlights the effectiveness of the proposed method. The code is available at https://github.com/zzc-1998/MM-PCQA.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2208.14085.pdf' target='_blank'>https://arxiv.org/pdf/2208.14085.pdf</a></span>   <span><a href='https://github.com/zzc-1998/VQA_PC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Yucheng Zhu, Xiongkuo Min, Wei Wu, Ying Chen, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.14085">Evaluating Point Cloud from Moving Camera Videos: A No-Reference Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud is one of the most widely used digital representation formats for three-dimensional (3D) contents, the visual quality of which may suffer from noise and geometric shift distortions during the production procedure as well as compression and downsampling distortions during the transmission process. To tackle the challenge of point cloud quality assessment (PCQA), many PCQA methods have been proposed to evaluate the visual quality levels of point clouds by assessing the rendered static 2D projections. Although such projection-based PCQA methods achieve competitive performance with the assistance of mature image quality assessment (IQA) methods, they neglect that the 3D model is also perceived in a dynamic viewing manner, where the viewpoint is continually changed according to the feedback of the rendering device. Therefore, in this paper, we evaluate the point clouds from moving camera videos and explore the way of dealing with PCQA tasks via using video quality assessment (VQA) methods. First, we generate the captured videos by rotating the camera around the point clouds through several circular pathways. Then we extract both spatial and temporal quality-aware features from the selected key frames and the video clips through using trainable 2D-CNN and pre-trained 3D-CNN models respectively. Finally, the visual quality of point clouds is represented by the video quality values. The experimental results reveal that the proposed method is effective for predicting the visual quality levels of the point clouds and even competitive with full-reference (FR) PCQA methods. The ablation studies further verify the rationality of the proposed framework and confirm the contributions made by the quality-aware features extracted via the dynamic viewing manner. The code is available at https://github.com/zzc-1998/VQA_PC.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2208.00623.pdf' target='_blank'>https://arxiv.org/pdf/2208.00623.pdf</a></span>   <span><a href='https://github.com/Hangwei-Chen/AST-IQAD-SRQE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangwei Chen, Feng Shao, Xiongli Chai, Yuese Gu, Qiuping Jiang, Xiangchao Meng, Yo-Sung Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00623">Quality Evaluation of Arbitrary Style Transfer: Subjective Study and Objective Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary neural style transfer is a vital topic with great research value and wide industrial application, which strives to render the structure of one image using the style of another. Recent researches have devoted great efforts on the task of arbitrary style transfer (AST) for improving the stylization quality. However, there are very few explorations about the quality evaluation of AST images, even it can potentially guide the design of different algorithms. In this paper, we first construct a new AST images quality assessment database (AST-IQAD), which consists 150 content-style image pairs and the corresponding 1200 stylized images produced by eight typical AST algorithms. Then, a subjective study is conducted on our AST-IQAD database, which obtains the subjective rating scores of all stylized images on the three subjective evaluations, i.e., content preservation (CP), style resemblance (SR), and overall vision (OV). To quantitatively measure the quality of AST image, we propose a new sparse representation-based method, which computes the quality according to the sparse feature similarity. Experimental results on our AST-IQAD have demonstrated the superiority of the proposed method. The dataset and source code will be released at https://github.com/Hangwei-Chen/AST-IQAD-SRQE
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2207.08119.pdf' target='_blank'>https://arxiv.org/pdf/2207.08119.pdf</a></span>   <span><a href='https://danier97.github.io/FloLPIPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duolikun Danier, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.08119">FloLPIPS: A Bespoke Video Quality Metric for Frame Interpoation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video frame interpolation (VFI) serves as a useful tool for many video processing applications. Recently, it has also been applied in the video compression domain for enhancing both conventional video codecs and learning-based compression architectures. While there has been an increased focus on the development of enhanced frame interpolation algorithms in recent years, the perceptual quality assessment of interpolated content remains an open field of research. In this paper, we present a bespoke full reference video quality metric for VFI, FloLPIPS, that builds on the popular perceptual image quality metric, LPIPS, which captures the perceptual degradation in extracted image feature space. In order to enhance the performance of LPIPS for evaluating interpolated content, we re-designed its spatial feature aggregation step by using the temporal distortion (through comparing optical flows) to weight the feature difference maps. Evaluated on the BVI-VFI database, which contains 180 test sequences with various frame interpolation artefacts, FloLPIPS shows superior correlation performance (with statistical significance) with subjective ground truth over 12 popular quality assessors. To facilitate further research in VFI quality assessment, our code is publicly available at https://danier97.github.io/FloLPIPS.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2206.09885.pdf' target='_blank'>https://arxiv.org/pdf/2206.09885.pdf</a></span>   <span><a href='https://github.com/MaritimeDataset/KOLOMVERSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhilasha Nanda, Sung Won Cho, Hyeopwoo Lee, Jin Hyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.09885">KOLOMVERSE: Korea open large-scale image dataset for object detection in the maritime universe</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the years, datasets have been developed for various object detection tasks. Object detection in the maritime domain is essential for the safety and navigation of ships. However, there is still a lack of publicly available large-scale datasets in the maritime domain. To overcome this challenge, we present KOLOMVERSE, an open large-scale image dataset for object detection in the maritime domain by KRISO (Korea Research Institute of Ships and Ocean Engineering). We collected 5,845 hours of video data captured from 21 territorial waters of South Korea. Through an elaborate data quality assessment process, we gathered around 2,151,470 4K resolution images from the video data. This dataset considers various environments: weather, time, illumination, occlusion, viewpoint, background, wind speed, and visibility. The KOLOMVERSE consists of five classes (ship, buoy, fishnet buoy, lighthouse and wind farm) for maritime object detection. The dataset has images of 3840$\times$2160 pixels and to our knowledge, it is by far the largest publicly available dataset for object detection in the maritime domain. We performed object detection experiments and evaluated our dataset on several pre-trained state-of-the-art architectures to show the effectiveness and usefulness of our dataset. The dataset is available at: \url{https://github.com/MaritimeDataset/KOLOMVERSE}.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2206.08751.pdf' target='_blank'>https://arxiv.org/pdf/2206.08751.pdf</a></span>   <span><a href='https://github.com/limuhit/VR-Video-Quality-in-the-Wild' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wen, Mu Li, Yiru Yao, Xiangjie Sui, Yabin Zhang, Long Lan, Yuming Fang, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.08751">Perceptual Quality Assessment of Virtual Reality Videos in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Investigating how people perceive virtual reality (VR) videos in the wild (i.e., those captured by everyday users) is a crucial and challenging task in VR-related applications due to complex authentic distortions localized in space and time. Existing panoramic video databases only consider synthetic distortions, assume fixed viewing conditions, and are limited in size. To overcome these shortcomings, we construct the VR Video Quality in the Wild (VRVQW) database, containing $502$ user-generated videos with diverse content and distortion characteristics. Based on VRVQW, we conduct a formal psychophysical experiment to record the scanpaths and perceived quality scores from $139$ participants under two different viewing conditions. We provide a thorough statistical analysis of the recorded data, observing significant impact of viewing conditions on both human scanpaths and perceived quality. Moreover, we develop an objective quality assessment model for VR videos based on pseudocylindrical representation and convolution. Results on the proposed VRVQW show that our method is superior to existing video quality assessment models. We have made the database and code available at https://github.com/limuhit/VR-Video-Quality-in-the-Wild.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2205.13847.pdf' target='_blank'>https://arxiv.org/pdf/2205.13847.pdf</a></span>   <span><a href='http://github.com/yuqing-liu-dut/NRIQA_SR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Liu, Qi Jia, Shanshe Wang, Siwei Ma, Wen Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.13847">Textural-Perceptual Joint Learning for No-Reference Super-Resolution Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image super-resolution (SR) has been widely investigated in recent years. However, it is challenging to fairly estimate the performance of various SR methods, as the lack of reliable and accurate criteria for the perceptual quality. Existing metrics concentrate on the specific kind of degradation without distinguishing the visual sensitive areas, which have no ability to describe the diverse SR degeneration situations in both low-level textural and high-level perceptual information. In this paper, we focus on the textural and perceptual degradation of SR images, and design a dual stream network to jointly explore the textural and perceptual information for quality assessment, dubbed TPNet. By mimicking the human vision system (HVS) that pays more attention to the significant image areas, we develop the spatial attention to make the visual sensitive information more distinguishable and utilize feature normalization (F-Norm) to boost the network representation. Experimental results show the TPNet predicts the visual quality score more accurate than other methods and demonstrates better consistency with the human's perspective. The source code will be available at \url{http://github.com/yuqing-liu-dut/NRIQA_SR}
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2202.07727.pdf' target='_blank'>https://arxiv.org/pdf/2202.07727.pdf</a></span>   <span><a href='https://danier97.github.io/BVI-VFI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duolikun Danier, Fan Zhang, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.07727">A Subjective Quality Study for Video Frame Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video frame interpolation (VFI) is one of the fundamental research areas in video processing and there has been extensive research on novel and enhanced interpolation algorithms. The same is not true for quality assessment of the interpolated content. In this paper, we describe a subjective quality study for VFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36 reference sequences at three different frame rates and 180 distorted videos generated using five conventional and learning based VFI algorithms. Subjective opinion scores have been collected from 60 human participants, and then employed to evaluate eight popular quality metrics, including PSNR, SSIM and LPIPS which are all commonly used for assessing VFI methods. The results indicate that none of these metrics provide acceptable correlation with the perceived quality on interpolated content, with the best-performing metric, LPIPS, offering a SROCC value below 0.6. Our findings show that there is an urgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFI dataset is publicly available and can be accessed at https://danier97.github.io/BVI-VFI/.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2111.12663.pdf' target='_blank'>https://arxiv.org/pdf/2111.12663.pdf</a></span>   <span><a href='https://github.com/cwi-dis/pointpca' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Evangelos Alexiou, Xuemei Zhou, Irene Viola, Pablo Cesar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.12663">PointPCA: Point Cloud Objective Quality Assessment Using PCA-Based Descriptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point clouds denote a prominent solution for the representation of 3D photo-realistic content in immersive applications. Similarly to other imaging modalities, quality predictions for point cloud contents are vital for a wide range of applications, enabling trade-off optimizations between data quality and data size in every processing step from acquisition to rendering. In this work, we focus on use cases that consider human end-users consuming point cloud contents and, hence, we concentrate on visual quality metrics. In particular, we propose a set of perceptually relevant descriptors based on Principal Component Analysis (PCA) decomposition, which is applied to both geometry and texture data for full-reference point cloud quality assessment. Statistical features are derived from these descriptors to characterize local shape and appearance properties for both a reference and a distorted point cloud. The extracted statistical features are subsequently compared to provide corresponding predictions of visual quality for the distorted point cloud. As part of our method, a learning-based approach is proposed to fuse these individual predictors to a unified perceptual score. We validate the accuracy of the individual predictors, as well as the unified quality scores obtained after regression against subjectively annotated datasets, showing that our metric outperforms state-of-the-art solutions. Insights regarding design decisions are provided through exploratory studies, evaluating the performance of our metric under different parameter configurations, attribute domains, color spaces, and regression models. A software implementation of the proposed metric is made available at the following link: https://github.com/cwi-dis/pointpca.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2110.09992.pdf' target='_blank'>https://arxiv.org/pdf/2110.09992.pdf</a></span>   <span><a href='https://github.com/msu-video-group/ERQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasia Kirillova, Eugene Lyapustin, Anastasia Antsiferova, Dmitry Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.09992">ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the growing popularity of video super-resolution (VSR), there is still no good way to assess the quality of the restored details in upscaled frames. Some SR methods may produce the wrong digit or an entirely different face. Whether a method's results are trustworthy depends on how well it restores truthful details. Image super-resolution can use natural distributions to produce a high-resolution image that is only somewhat similar to the real one. VSR enables exploration of additional information in neighboring frames to restore details from the original scene. The ERQA metric, which we propose in this paper, aims to estimate a model's ability to restore real details using VSR. On the assumption that edges are significant for detail and character recognition, we chose edge fidelity as the foundation for this metric. Experimental validation of our work is based on the MSU Video Super-Resolution Benchmark, which includes the most difficult patterns for detail restoration and verifies the fidelity of details from the original frame. Code for the proposed metric is publicly available at https://github.com/msu-video-group/ERQA.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2109.14335.pdf' target='_blank'>https://arxiv.org/pdf/2109.14335.pdf</a></span>   <span><a href='https://github.com/CV-JunchengLi/SISR-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Li, Zehua Pei, Wenjie Li, Guangwei Gao, Longguang Wang, Yingqian Wang, Tieyong Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.14335">A Systematic Survey of Deep Learning-based Single-Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-image super-resolution (SISR) is an important task in image processing, which aims to enhance the resolution of imaging systems. Recently, SISR has made a huge leap and has achieved promising results with the help of deep learning (DL). In this survey, we give an overview of DL-based SISR methods and group them according to their design targets. Specifically, we first introduce the problem definition, research background, and the significance of SISR. Secondly, we introduce some related works, including benchmark datasets, upsampling methods, optimization objectives, and image quality assessment methods. Thirdly, we provide a detailed investigation of SISR and give some domain-specific applications of it. Fourthly, we present the reconstruction results of some classic SISR methods to intuitively know their performance. Finally, we discuss some issues that still exist in SISR and summarize some new trends and future directions. This is an exhaustive survey of SISR, which can help researchers better understand SISR and inspire more exciting research in this field. An investigation project for SISR is provided at https://github.com/CV-JunchengLi/SISR-Survey.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2105.14550.pdf' target='_blank'>https://arxiv.org/pdf/2105.14550.pdf</a></span>   <span><a href='https://github.com/sunwei925/StairIQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Xiongkuo Min, Danyang Tu, Guangtao Zhai, Siwei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.14550">Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature Fusion and Iterative Mixed Database Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) is very important for both end-users and service providers since a high-quality image can significantly improve the user's quality of experience (QoE) and also benefit lots of computer vision algorithms. Most existing blind image quality assessment (BIQA) models were developed for synthetically distorted images, however, they perform poorly on in-the-wild images, which are widely existed in various practical applications. In this paper, we propose a novel BIQA model for in-the-wild images by addressing two critical problems in this field: how to learn better quality-aware feature representation, and how to solve the problem of insufficient training samples in terms of their content and distortion diversity. Considering that perceptual visual quality is affected by both low-level visual features (e.g. distortions) and high-level semantic information (e.g. content), we first propose a staircase structure to hierarchically integrate the features from intermediate layers into the final feature representation, which enables the model to make full use of visual information from low-level to high-level. Then an iterative mixed database training (IMDT) strategy is proposed to train the BIQA model on multiple databases simultaneously, so the model can benefit from the increase in both training samples and image content and distortion diversity and can learn a more general feature representation. Experimental results show that the proposed model outperforms other state-of-the-art BIQA models on six in-the-wild IQA databases by a large margin. Moreover, the proposed model shows an excellent performance in the cross-database evaluation experiments, which further demonstrates that the learned feature representation is robust to images with diverse distortions and content. The code is available at https://github.com/sunwei925/StairIQA.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2005.00356.pdf' target='_blank'>https://arxiv.org/pdf/2005.00356.pdf</a></span>   <span><a href='https://nagabhushansn95.github.io/publications/2020/pvqa.html' target='_blank'>  GitHub</a></span> <span><a href='https://nagabhushansn95.github.io/publications/2020/pvqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nagabhushan Somraj, Manoj Surya Kashi, S. P. Arun, Rajiv Soundararajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2005.00356">Understanding the Perceived Quality of Video Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of video prediction models is believed to be a fundamental approach to representation learning for videos. While a plethora of generative models for predicting the future frame pixel values given the past few frames exist, the quantitative evaluation of the predicted frames has been found to be extremely challenging. In this context, we study the problem of quality assessment of predicted videos. We create the Indian Institute of Science Predicted Videos Quality Assessment (IISc PVQA) Database consisting of 300 videos, obtained by applying different prediction models on different datasets, and accompanying human opinion scores. We collected subjective ratings of quality from 50 human participants for these videos. Our subjective study reveals that human observers were highly consistent in their judgments of quality of predicted videos. We benchmark several popularly used measures for evaluating video prediction and show that they do not adequately correlate with these subjective scores. We introduce two new features to effectively capture the quality of predicted videos, motion-compensated cosine similarities of deep features of predicted frames with past frames, and deep features extracted from rescaled frame differences. We show that our feature design leads to state of the art quality prediction in accordance with human judgments on our IISc PVQA Database. The database and code are publicly available on our project website: https://nagabhushansn95.github.io/publications/2020/pvqa
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2510.00743.pdf' target='_blank'>https://arxiv.org/pdf/2510.00743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Cao, Changhao Jiang, Jiabao Zhuang, Jiajun Sun, Ming Zhang, Zhiheng Xi, Hui Li, Shihan Dou, Yuran Wang, Yunke Zhang, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00743">From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the perceptual quality of synthetic speech is crucial for guiding the development and refinement of speech generation models. However, it has traditionally relied on human subjective ratings such as the Mean Opinion Score (MOS), which depend on manual annotations and often suffer from inconsistent rating standards and poor reproducibility. To address these limitations, we introduce MOS-RMBench, a unified benchmark that reformulates diverse MOS datasets into a preference-comparison setting, enabling rigorous evaluation across different datasets. Building on MOS-RMBench, we systematically construct and evaluate three paradigms for reward modeling: scalar reward models, semi-scalar reward models, and generative reward models (GRMs). Our experiments reveal three key findings: (1) scalar models achieve the strongest overall performance, consistently exceeding 74% accuracy; (2) most models perform considerably worse on synthetic speech than on human speech; and (3) all models struggle on pairs with very small MOS differences. To improve performance on these challenging pairs, we propose a MOS-aware GRM that incorporates an MOS-difference-based reward function, enabling the model to adaptively scale rewards according to the difficulty of each sample pair. Experimental results show that the MOS-aware GRM significantly improves fine-grained quality discrimination and narrows the gap with scalar models on the most challenging cases. We hope this work will establish both a benchmark and a methodological framework to foster more rigorous and scalable research in automatic speech quality assessment.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2509.17012.pdf' target='_blank'>https://arxiv.org/pdf/2509.17012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Ma, Fan Huang, Lu Zhao, Fengjun Guo, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17012">DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document image quality assessment (DIQA) is an important component for various applications, including optical character recognition (OCR), document restoration, and the evaluation of document image processing systems. In this paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset comprises 5,000 document images, generated by applying multiple document enhancement techniques to 500 real-world images with diverse distortions. Each enhanced image was rated by 15 subjects across three rating dimensions: overall quality, sharpness, and color fidelity. Furthermore, we propose a specialized no-reference DIQA model that exploits document layout features to maintain quality perception at reduced resolutions to lower computational cost. Recognizing that image quality is influenced by both low-level and high-level visual features, we designed a feature fusion module to extract and integrate multi-level features from document images. To generate multi-dimensional scores, our model employs independent quality heads for each dimension to predict score distributions, allowing it to learn distinct aspects of document image quality. Experimental results demonstrate that our method outperforms current state-of-the-art general-purpose IQA models on both DIQA-5000 and an additional document image dataset focused on OCR accuracy.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2509.09190.pdf' target='_blank'>https://arxiv.org/pdf/2509.09190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwei Zhu, Haoning Wu, Zicheng Zhang, Lingyu Zhu, Yixuan Li, Peilin Chen, Shiqi Wang, Chris Wei Zhou, Linhan Cao, Wei Sun, Xiangyang Zhu, Weixia Zhang, Yucheng Zhu, Jing Liu, Dandan Zhu, Guangtao Zhai, Xiongkuo Min, Zhichao Zhang, Xinyue Li, Shubo Xu, Anh Dao, Yifan Li, Hongyuan Yu, Jiaojiao Yi, Yiding Tian, Yupeng Wu, Feiran Sun, Lijuan Liao, Song Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09190">VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a summary of the VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025 Workshop on Visual Quality Assessment. The challenge aims to evaluate and enhance the ability of state-of-the-art LMMs to perform open-ended and detailed reasoning about visual quality differences across multiple images. To this end, the competition introduces a novel benchmark comprising thousands of coarse-to-fine grained visual quality comparison tasks, spanning single images, pairs, and multi-image groups. Each task requires models to provide accurate quality judgments. The competition emphasizes holistic evaluation protocols, including 2AFC-based binary preference and multi-choice questions (MCQs). Around 100 participants submitted entries, with five models demonstrating the emerging capabilities of instruction-tuned LMMs on quality assessment. This challenge marks a significant step toward open-domain visual quality reasoning and comparison and serves as a catalyst for future research on interpretable and human-aligned quality evaluation systems.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2508.18445.pdf' target='_blank'>https://arxiv.org/pdf/2508.18445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhuo Ma, Wei-Ting Chen, Qiang Gao, Jian Wang, Chris Wei Zhou, Wei Sun, Weixia Zhang, Linhan Cao, Jun Jia, Xiangyang Zhu, Dandan Zhu, Xiongkuo Min, Guangtao Zhai, Baoying Chen, Xiongwei Xiao, Jishen Zeng, Wei Wu, Tiexuan Lou, Yuchen Tan, Chunyi Song, Zhiwei Xu, MohammadAli Hamidi, Hadi Amirpour, Mingyin Bai, Jiawang Du, Zhenyu Jiang, Zilong Lu, Ziguan Cui, Zongliang Gan, Xinpeng Li, Shiqi Jiang, Chenhui Li, Changbo Wang, Weijun Yuan, Zhan Li, Yihang Chen, Yifan Deng, Ruting Deng, Zhanglu Chen, Boyang Yao, Shuling Zheng, Feng Zhang, Zhiheng Fu, Abhishek Joshi, Aman Agarwal, Rakhil Immidisetti, Ajay Narasimha Mopidevi, Vishwajeet Shukla, Hao Yang, Ruikun Zhang, Liyuan Pan, Kaixin Deng, Hang Ouyang, Fan yang, Zhizun Luo, Zhuohang Shi, Songning Lai, Weilin Ruan, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18445">VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face images play a crucial role in numerous applications; however, real-world conditions frequently introduce degradations such as noise, blur, and compression artifacts, affecting overall image quality and hindering subsequent tasks. To address this challenge, we organized the VQualA 2025 Challenge on Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops. Participants created lightweight and efficient models (limited to 0.5 GFLOPs and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on face images with arbitrary resolutions and realistic degradations. Submissions underwent comprehensive evaluations through correlation metrics on a dataset of in-the-wild face images. This challenge attracted 127 participants, with 1519 final submissions. This report summarizes the methodologies and findings for advancing the development of practical FIQA approaches.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2508.06051.pdf' target='_blank'>https://arxiv.org/pdf/2508.06051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Jun Jia, Kaiwei Zhang, Dandan Zhu, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06051">VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) aims to objectively quantify perceptual quality degradation in alignment with human visual perception. Despite recent advances, existing VQA models still suffer from two critical limitations: \textit{poor generalization to out-of-distribution (OOD) videos} and \textit{limited explainability}, which restrict their applicability in real-world scenarios. To address these challenges, we propose \textbf{VQAThinker}, a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a \textbf{bell-shaped regression reward} that increases rapidly as the prediction error decreases and becomes progressively less sensitive near the ground truth; (2) a \textbf{pairwise ranking reward} that guides the model to correctly determine the relative quality between video pairs; and (3) a \textbf{temporal consistency reward} that encourages the model to prefer temporally coherent videos over their perturbed counterparts. Extensive experiments demonstrate that VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2506.22790.pdf' target='_blank'>https://arxiv.org/pdf/2506.22790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixu Chen, Bowen Chen, Hai Wei, Alan C. Bovik, Baojun Li, Wei Sun, Linhan Cao, Kang Fu, Dandan Zhu, Jun Jia, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Dounia Hammou, Fei Yin, Rafal Mantiuk, Amritha Premkumar, Prajit T Rajendran, Vignesh V Menon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22790">ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports IEEE International Conference on Multimedia \& Expo (ICME) 2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement. With the rapid development of video technology, especially High Dynamic Range (HDR) and Standard Dynamic Range (SDR) contents, the need for robust and generalizable Video Quality Assessment (VQA) methods has become increasingly demanded. Existing VQA models often struggle to deliver consistent performance across varying dynamic ranges, distortion types, and diverse content. This challenge was established to benchmark and promote VQA approaches capable of jointly handling HDR and SDR content. In the final evaluation phase, five teams submitted seven models along with technical reports to the Full Reference (FR) and No Reference (NR) tracks. Among them, four methods outperformed VMAF baseline, while the top-performing model achieved state-of-the-art performance, setting a new benchmark for generalizable video quality assessment.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2506.02875.pdf' target='_blank'>https://arxiv.org/pdf/2506.02875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohong Liu, Xiongkuo Min, Qiang Hu, Xiaoyun Zhang, Jie Guo, Guangtao Zhai, Shushi Wang, Yingjie Zhou, Lu Liu, Jingxin Li, Liu Yang, Farong Wen, Li Xu, Yanwei Jiang, Xilei Zhu, Chunyi Li, Zicheng Zhang, Huiyu Duan, Xiele Wu, Yixuan Gao, Yuqin Cao, Jun Jia, Wei Sun, Jiezhang Cao, Radu Timofte, Baojun Li, Jiamian Huang, Dan Luo, Tao Liu, Weixia Zhang, Bingkun Zheng, Junlin Chen, Ruikai Zhou, Meiya Chen, Yu Wang, Hao Jiang, Xiantao Li, Yuxiang Jiang, Jun Tang, Yimeng Zhao, Bo Hu, Zelu Qi, Chaoyang Zhang, Fei Zhao, Ping Shi, Lingzhi Fu, Heng Cong, Shuai He, Rongyu Zhang, Jiarong He, Zongyao Hu, Wei Luo, Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen, Mengjing Su, Yi Wang, Tuo Chen, Chunxiao Li, Shuaiyu Zhao, Jiaxin Wen, Chuyi Lin, Sitong Liu, Ningxin Chu, Jing Wan, Yu Zhou, Baoying Chen, Jishen Zeng, Jiarui Liu, Xianjin Liu, Xin Chen, Lanzhi Zhou, Hangyu Li, You Han, Bibo Xiang, Zhenjie Liu, Jianzhang Lu, Jialin Gui, Renjie Lu, Shangfei Wang, Donghao Zhou, Jingyu Lin, Quanjian Song, Jiancheng Huang, Yufeng Yang, Changwei Wang, Shupeng Zhong, Yang Yang, Lihuo He, Jia Liu, Yuting Xing, Tida Fang, Yuchun Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02875">NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major challenge in the field of video and talking head processing. The challenge is divided into three tracks, including user generated video, AI generated video and talking head. The user-generated video track uses the FineVD-GC, which contains 6,284 user generated videos. The user-generated video track has a total of 125 registered participants. A total of 242 submissions are received in the development phase, and 136 submissions are received in the test phase. Finally, 5 participating teams submitted their models and fact sheets. The AI generated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated Videos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of 133 participants have registered in this track. A total of 396 submissions are received in the development phase, and 226 submissions are received in the test phase. Finally, 6 participating teams submitted their models and fact sheets. The talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D talking heads. A total of 89 participants have registered in this track. A total of 225 submissions are received in the development phase, and 118 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Each participating team in every track has proposed a method that outperforms the baseline, which has contributed to the development of fields in three tracks.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2505.22543.pdf' target='_blank'>https://arxiv.org/pdf/2505.22543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Yingji Liang, Xiaorong Zhu, Chunyi Li, Jinliang Han, Haoning Wu, Bin Wang, Haoran Zhang, Guanyu Zhu, Qiyong Zhao, Xiaohong Liu, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22543">Scaling-up Perceptual Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The data scaling law has been shown to significantly enhance the performance of large multi-modal models (LMMs) across various downstream tasks. However, in the domain of perceptual video quality assessment (VQA), the potential of scaling law remains unprecedented due to the scarcity of labeled resources and the insufficient scale of datasets. To address this, we propose \textbf{OmniVQA}, an efficient framework designed to efficiently build high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs). We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the VQA field concurrently. Our focus is on the technical and aesthetic quality dimensions, with abundant in-context instruction data to provide fine-grained VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset to enhance the model's quantitative quality rating capabilities. We then introduce a \textbf{complementary} training strategy that effectively leverages the knowledge from datasets for quality understanding and quality rating tasks. Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to evaluate the fine-grained performance of the models. Our results demonstrate that our models achieve state-of-the-art performance in both quality understanding and rating tasks.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2505.19535.pdf' target='_blank'>https://arxiv.org/pdf/2505.19535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juntong Wang, Jiarui Wang, Huiyu Duan, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19535">TDVE-Assessor: Benchmarking and Evaluating the Quality of Text-Driven Video Editing with LMMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-driven video editing is rapidly advancing, yet its rigorous evaluation remains challenging due to the absence of dedicated video quality assessment (VQA) models capable of discerning the nuances of editing quality. To address this critical gap, we introduce TDVE-DB, a large-scale benchmark dataset for text-driven video editing. TDVE-DB consists of 3,857 edited videos generated from 12 diverse models across 8 editing categories, and is annotated with 173,565 human subjective ratings along three crucial dimensions, i.e., edited video quality, editing alignment, and structural consistency. Based on TDVE-DB, we first conduct a comprehensive evaluation for the 12 state-of-the-art editing models revealing the strengths and weaknesses of current video techniques, and then benchmark existing VQA methods in the context of text-driven video editing evaluation. Building on these insights, we propose TDVE-Assessor, a novel VQA model specifically designed for text-driven video editing assessment. TDVE-Assessor integrates both spatial and temporal video features into a large language model (LLM) for rich contextual understanding to provide comprehensive quality assessment. Extensive experiments demonstrate that TDVE-Assessor substantially outperforms existing VQA models on TDVE-DB across all three evaluation dimensions, setting a new state-of-the-art. Both TDVE-DB and TDVE-Assessor will be released upon the publication.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2505.16314.pdf' target='_blank'>https://arxiv.org/pdf/2505.16314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Han, Haotian Fan, Fangyuan Kong, Wenjie Liao, Chunle Guo, Chongyi Li, Radu Timofte, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Jianhui Sun, Xinli Yue, Tianyi Wang, Huan Hou, Junda Lu, Xinyang Huang, Zitang Zhou, Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao, Trong-Hieu Nguyen-Mau, Minh-Hoang Le, Minh-Khoa Le-Phan, Duy-Nam Ly, Hai-Dang Nguyen, Minh-Triet Tran, Yukang Lin, Yan Hong, Chuanbiao Song, Siyuan Li, Jun Lan, Zhichao Zhang, Xinyue Li, Wei Sun, Zicheng Zhang, Yunhao Li, Xiaohong Liu, Guangtao Zhai, Zitong Xu, Huiyu Duan, Jiarui Wang, Guangji Ma, Liu Yang, Lu Liu, Qiang Hu, Xiongkuo Min, Zichuan Wang, Zhenchen Tang, Bo Peng, Jing Dong, Fengbin Guan, Zihao Yu, Yiting Lu, Wei Luo, Xin Li, Minhao Lin, Haofeng Chen, Xuanxuan He, Kele Xu, Qisheng Xu, Zijian Gao, Tianjiao Wan, Bo-Cheng Qiu, Chih-Chung Hsu, Chia-ming Lee, Yu-Fan Lin, Bo Yu, Zehao Wang, Da Mu, Mingxiu Chen, Junkang Fang, Huamei Sun, Wending Zhao, Zhiyu Wang, Wang Liu, Weikang Yu, Puhong Duan, Bin Sun, Xudong Kang, Shutao Li, Shuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, Jiarong He, Zhishan Qiao, Yongqing Huang, Zewen Chen, Zhe Pang, Juan Wang, Jian Guo, Zhizhuo Shao, Ziyu Feng, Bing Li, Weiming Hu, Hesong Li, Dehua Liu, Zeming Liu, Qingsong Xie, Ruichen Wang, Zhihao Li, Yuqi Liang, Jianqi Bi, Jun Luo, Junfeng Yang, Can Li, Jing Fu, Hongwei Xu, Mingrui Long, Lulin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16314">NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on the NTIRE 2025 challenge on Text to Image (T2I) generation model quality assessment, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. The aim of this challenge is to address the fine-grained quality assessment of text-to-image generation models. This challenge evaluates text-to-image models from two aspects: image-text alignment and image structural distortion detection, and is divided into the alignment track and the structural track. The alignment track uses the EvalMuse-40K, which contains around 40K AI-Generated Images (AIGIs) generated by 20 popular generative models. The alignment track has a total of 371 registered participants. A total of 1,883 submissions are received in the development phase, and 507 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. The structure track uses the EvalMuse-Structure, which contains 10,000 AI-Generated Images (AIGIs) with corresponding structural distortion mask. A total of 211 participants have registered in the structure track. A total of 1155 submissions are received in the development phase, and 487 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Almost all methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on T2I model quality assessment.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2505.13841.pdf' target='_blank'>https://arxiv.org/pdf/2505.13841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Gao, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13841">Exploring Image Quality Assessment from a New Perspective: Pupil Size</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores how the image quality assessment (IQA) task affects the cognitive processes of people from the perspective of pupil size and studies the relationship between pupil size and image quality. Specifically, we first invited subjects to participate in a subjective experiment, which includes two tasks: free observation and IQA. In the free observation task, subjects did not need to perform any action, and they only needed to observe images as they usually do with an album. In the IQA task, subjects were required to score images according to their overall impression of image quality. Then, by analyzing the difference in pupil size between the two tasks, we find that people may activate the visual attention mechanism when evaluating image quality. Meanwhile, we also find that the change in pupil size is closely related to image quality in the IQA task. For future research on IQA, this research can not only provide a theoretical basis for the objective IQA method and promote the development of more effective objective IQA methods, but also provide a new subjective IQA method for collecting the authentic subjective impression of image quality.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2505.03631.pdf' target='_blank'>https://arxiv.org/pdf/2505.03631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhan Cao, Wei Sun, Kaiwei Zhang, Yicong Peng, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03631">Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. While recent supervised VQA models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. To bridge this gap, we introduce a self-supervised learning framework for VQA to learn quality assessment capabilities from large-scale, unlabeled web videos. Our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (LMM) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing VQA models and relative quality ranking based on synthetic distortion simulations. Furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. By training on a dataset $10\times$ larger than the existing VQA benchmarks, our model: (1) achieves zero-shot performance on in-domain VQA benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (OOD) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. Extensive experimental results validate the effectiveness of our self-supervised approach in training generalized VQA models. The datasets and code will be publicly released to facilitate future research.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2504.21308.pdf' target='_blank'>https://arxiv.org/pdf/2504.21308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Sijing Wu, Wei Sun, Zhichao Zhang, Yucheng Zhu, Zicheng Zhang, Huiyu Duan, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21308">AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of text-to-image (T2I) generation approaches has attracted extensive interest in evaluating the quality of generated images, leading to the development of various quality assessment methods for general-purpose T2I outputs. However, existing image quality assessment (IQA) methods are limited to providing global quality scores, failing to deliver fine-grained perceptual evaluations for structurally complex subjects like humans, which is a critical challenge considering the frequent anatomical and textural distortions in AI-generated human images (AGHIs). To address this gap, we introduce AGHI-QA, the first large-scale benchmark specifically designed for quality assessment of AGHIs. The dataset comprises 4,000 images generated from 400 carefully crafted text prompts using 10 state of-the-art T2I models. We conduct a systematic subjective study to collect multidimensional annotations, including perceptual quality scores, text-image correspondence scores, visible and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and weaknesses of current T2I methods in generating human images from multiple dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that integrates the large multimodal model (LMM) with domain-specific human features for precise quality prediction and identification of visible and distorted body parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor showcases state-of-the-art performance, significantly outperforming existing IQA methods in multidimensional quality assessment and surpassing leading LMMs in detecting structural distortions in AGHIs.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2504.20466.pdf' target='_blank'>https://arxiv.org/pdf/2504.20466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20466">LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2504.09291.pdf' target='_blank'>https://arxiv.org/pdf/2504.09291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaying Qian, Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09291">Towards Explainable Partial-AIGC Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of AI-driven visual generation technologies has catalyzed significant breakthroughs in image manipulation, particularly in achieving photorealistic localized editing effects on natural scene images (NSIs). Despite extensive research on image quality assessment (IQA) for AI-generated images (AGIs), most studies focus on fully AI-generated outputs (e.g., text-to-image generation), leaving the quality assessment of partial-AIGC images (PAIs)-images with localized AI-driven edits an almost unprecedented field. Motivated by this gap, we construct the first large-scale PAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA), the EPAIQA-15K, which includes 15K images with localized AI manipulation in different regions and over 300K multi-dimensional human ratings. Based on this, we leverage large multi-modal models (LMMs) and propose a three-stage model training paradigm. This paradigm progressively trains the LMM for editing region grounding, quantitative quality scoring, and quality explanation. Finally, we develop the EPAIQA series models, which possess explainable quality feedback capabilities. Our work represents a pioneering effort in the perceptual IQA field for comprehensive PAI quality assessment.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2503.20673.pdf' target='_blank'>https://arxiv.org/pdf/2503.20673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20673">Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of multimodal large language models has resulted in remarkable advancements in visual perception and understanding, consolidating several tasks into a single visual question-answering framework. However, these models are prone to hallucinations, which limit their reliability as artificial intelligence systems. While this issue is extensively researched in natural language processing and image captioning, there remains a lack of investigation of hallucinations in Low-level Visual Perception and Understanding (HLPU), especially in the context of image quality assessment tasks. We consider that these hallucinations arise from an absence of clear self-awareness within the models. To address this issue, we first introduce the HLPU instruction database, the first instruction database specifically focused on hallucinations in low-level vision tasks. This database contains approximately 200K question-answer pairs and comprises four subsets, each covering different types of instructions. Subsequently, we propose the Self-Awareness Failure Elimination (SAFEQA) model, which utilizes image features, salient region features and quality features to improve the perception and comprehension abilities of the model in low-level vision tasks. Furthermore, we propose the Enhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase the model's awareness of knowledge boundaries, thereby mitigating the incidence of hallucination. Finally, we conduct comprehensive experiments on low-level vision tasks, with the results demonstrating that our proposed method significantly enhances self-awareness of the model in these tasks and reduces hallucinations. Notably, our proposed method improves both accuracy and self-awareness of the proposed model and outperforms close-source models in terms of various evaluation metrics.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2502.16915.pdf' target='_blank'>https://arxiv.org/pdf/2502.16915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang Fu, Huiyu Duan, Zicheng Zhang, Xiaohong Liu, Xiongkuo Min, Jia Wang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16915">Multi-Dimensional Quality Assessment for Text-to-3D Assets: Dataset and Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-image (T2I) generation have spurred the development of text-to-3D asset (T23DA) generation, leveraging pretrained 2D text-to-image diffusion models for text-to-3D asset synthesis. Despite the growing popularity of text-to-3D asset generation, its evaluation has not been well considered and studied. However, given the significant quality discrepancies among various text-to-3D assets, there is a pressing need for quality assessment models aligned with human subjective judgments. To tackle this challenge, we conduct a comprehensive study to explore the T23DA quality assessment (T23DAQA) problem in this work from both subjective and objective perspectives. Given the absence of corresponding databases, we first establish the largest text-to-3D asset quality assessment database to date, termed the AIGC-T23DAQA database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings for these assets from the perspectives of quality, authenticity, and text-asset correspondence, respectively. Subsequently, we establish a comprehensive benchmark based on the AIGC-T23DAQA database, and devise an effective T23DAQA model to evaluate the generated 3D assets from the aforementioned three perspectives, respectively.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2501.01116.pdf' target='_blank'>https://arxiv.org/pdf/2501.01116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitong Xu, Huiyu Duan, Guangji Ma, Liu Yang, Jiarui Wang, Qingbo Wu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01116">HarmonyIQA: Pioneering Benchmark and Model for Image Harmonization Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image composition involves extracting a foreground object from one image and pasting it into another image through Image harmonization algorithms (IHAs), which aim to adjust the appearance of the foreground object to better match the background. Existing image quality assessment (IQA) methods may fail to align with human visual preference on image harmonization due to the insensitivity to minor color or light inconsistency. To address the issue and facilitate the advancement of IHAs, we introduce the first Image Quality Assessment Database for image Harmony evaluation (HarmonyIQAD), which consists of 1,350 harmonized images generated by 9 different IHAs, and the corresponding human visual preference scores. Based on this database, we propose a Harmony Image Quality Assessment (HarmonyIQA), to predict human visual preference for harmonized images. Extensive experiments show that HarmonyIQA achieves state-of-the-art performance on human visual preference evaluation for harmonized images, and also achieves competing results on traditional IQA tasks. Furthermore, cross-dataset evaluation also shows that HarmonyIQA exhibits better generalization ability than self-supervised learning-based IQA methods. Both HarmonyIQAD and HarmonyIQA will be made publicly available upon paper publication.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2412.19238.pdf' target='_blank'>https://arxiv.org/pdf/2412.19238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19238">FineVQ: Fine-Grained User Generated Content Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of user-generated content (UGC) videos has produced an urgent need for effective video quality assessment (VQA) algorithms to monitor video quality and guide optimization and recommendation procedures. However, current VQA models generally only give an overall rating for a UGC video, which lacks fine-grained labels for serving video processing and recommendation applications. To address the challenges and promote the development of UGC videos, we establish the first large-scale Fine-grained Video quality assessment Database, termed FineVD, which comprises 6104 UGC videos with fine-grained quality scores and descriptions across multiple dimensions. Based on this database, we propose a Fine-grained Video Quality assessment (FineVQ) model to learn the fine-grained quality of UGC videos, with the capabilities of quality rating, quality scoring, and quality attribution. Extensive experimental results demonstrate that our proposed FineVQ can produce fine-grained video-quality results and achieve state-of-the-art performance on FineVD and other commonly used UGC-VQA datasets.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2411.17221.pdf' target='_blank'>https://arxiv.org/pdf/2411.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Wang, Huiyu Duan, Guangtao Zhai, Juntong Wang, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17221">AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large multimodal models (LMMs) has led to the rapid expansion of artificial intelligence generated videos (AIGVs), which highlights the pressing need for effective video quality assessment (VQA) models designed specifically for AIGVs. Current VQA models generally fall short in accurately assessing the perceptual quality of AIGVs due to the presence of unique distortions, such as unrealistic objects, unnatural movements, or inconsistent visual elements. To address this challenge, we first present AIGVQA-DB, a large-scale dataset comprising 36,576 AIGVs generated by 15 advanced text-to-video models using 1,048 diverse prompts. With these AIGVs, a systematic annotation pipeline including scoring and ranking processes is devised, which collects 370k expert ratings to date. Based on AIGVQA-DB, we further introduce AIGV-Assessor, a novel VQA model that leverages spatiotemporal features and LMM frameworks to capture the intricate quality attributes of AIGVs, thereby accurately predicting precise video quality scores and video pair preferences. Through comprehensive experiments on both AIGVQA-DB and existing AIGV databases, AIGV-Assessor demonstrates state-of-the-art performance, significantly surpassing existing scoring or evaluation methods in terms of multiple perceptual quality dimensions.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2411.03795.pdf' target='_blank'>https://arxiv.org/pdf/2411.03795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03795">VQA$^2$: Visual Question Answering for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent and proliferation of large multi-modal models (LMMs) have introduced new paradigms to computer vision, transforming various tasks into a unified visual question answering framework. Video Quality Assessment (VQA), a classic field in low-level visual perception, focused initially on quantitative video quality scoring. However, driven by advances in LMMs, it is now progressing toward more holistic visual quality understanding tasks. Recent studies in the image domain have demonstrated that Visual Question Answering (VQA) can markedly enhance low-level visual quality evaluation. Nevertheless, related work has not been explored in the video domain, leaving substantial room for improvement. To address this gap, we introduce the VQA2 Instruction Dataset - the first visual question answering instruction dataset that focuses on video quality assessment. This dataset consists of 3 subsets and covers various video types, containing 157,755 instruction question-answer pairs. Then, leveraging this foundation, we present the VQA2 series models. The VQA2 series models interleave visual and motion tokens to enhance the perception of spatial-temporal quality details in videos. We conduct extensive experiments on video quality scoring and understanding tasks, and results demonstrate that the VQA2series models achieve excellent performance in both tasks. Notably, our final model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality understanding tasks while maintaining strong competitiveness in quality scoring tasks. Our work provides a foundation and feasible approach for integrating low-level video quality assessment and understanding with LMMs.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2410.04225.pdf' target='_blank'>https://arxiv.org/pdf/2410.04225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Molodetskikh, Artem Borisov, Dmitriy Vatolin, Radu Timofte, Jianzhao Liu, Tianwu Zhi, Yabin Zhang, Yang Li, Jingwen Xu, Yiting Liao, Qing Luo, Ao-Xiang Zhang, Peng Zhang, Haibo Lei, Linyan Jiang, Yaqing Li, Yuqin Cao, Wei Sun, Weixia Zhang, Yinan Sun, Ziheng Jia, Yuxin Zhu, Xiongkuo Min, Guangtao Zhai, Weihua Luo, Yupeng Z., Hong Y
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04225">AIM 2024 Challenge on Video Super-Resolution Quality Assessment: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Video Super-Resolution (SR) Quality Assessment (QA) Challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2024. The task of this challenge was to develop an objective QA method for videos upscaled 2x and 4x by modern image- and video-SR algorithms. QA methods were evaluated by comparing their output with aggregate subjective scores collected from >150,000 pairwise votes obtained through crowd-sourced comparisons across 52 SR methods and 1124 upscaled videos. The goal was to advance the state-of-the-art in SR QA, which had proven to be a challenging problem with limited applicability of traditional QA methods. The challenge had 29 registered participants, and 5 teams had submitted their final results, all outperforming the current state-of-the-art. All data, including the private test subset, has been made publicly available on the challenge homepage at https://challenges.videoprocessing.ai/challenges/super-resolution-metrics-challenge.html
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2409.14827.pdf' target='_blank'>https://arxiv.org/pdf/2409.14827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrey Moskalenko, Alexey Bryncev, Dmitry Vatolin, Radu Timofte, Gen Zhan, Li Yang, Yunlong Tang, Yiting Liao, Jiongzhi Lin, Baitao Huang, Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo, Yuxin Zhu, Yinan Sun, Huiyu Duan, Yuqin Cao, Ziheng Jia, Qiang Hu, Xiongkuo Min, Guangtao Zhai, Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Wei Zhang, Chunyu Zhao, Wentao Mu, Tao Deng, Hamed R. Tavakoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14827">AIM 2024 Challenge on Video Saliency Prediction: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reviews the Challenge on Video Saliency Prediction at AIM 2024. The goal of the participants was to develop a method for predicting accurate saliency maps for the provided set of video sequences. Saliency maps are widely exploited in various applications, including video compression, quality assessment, visual perception studies, the advertising industry, etc. For this competition, a previously unused large-scale audio-visual mouse saliency (AViMoS) dataset of 1500 videos with more than 70 observers per video was collected using crowdsourced mouse tracking. The dataset collection methodology has been validated using conventional eye-tracking data and has shown high consistency. Over 30 teams registered in the challenge, and there are 7 teams that submitted the results in the final phase. The final phase solutions were tested and ranked by commonly used quality metrics on a private test subset. The results of this evaluation and the descriptions of the solutions are presented in this report. All data, including the private test subset, is made publicly available on the challenge homepage - https://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2409.05540.pdf' target='_blank'>https://arxiv.org/pdf/2409.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiongkuo Min, Yixuan Gao, Yuqin Cao, Guangtao Zhai, Wenjun Zhang, Huifang Sun, Chang Wen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05540">Exploring Rich Subjective Quality Information for Image Quality Assessment in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional in the wild image quality assessment (IQA) models are generally trained with the quality labels of mean opinion score (MOS), while missing the rich subjective quality information contained in the quality ratings, for example, the standard deviation of opinion scores (SOS) or even distribution of opinion scores (DOS). In this paper, we propose a novel IQA method named RichIQA to explore the rich subjective rating information beyond MOS to predict image quality in the wild. RichIQA is characterized by two key novel designs: (1) a three-stage image quality prediction network which exploits the powerful feature representation capability of the Convolutional vision Transformer (CvT) and mimics the short-term and long-term memory mechanisms of human brain; (2) a multi-label training strategy in which rich subjective quality information like MOS, SOS and DOS are concurrently used to train the quality prediction network. Powered by these two novel designs, RichIQA is able to predict the image quality in terms of a distribution, from which the mean image quality can be subsequently obtained. Extensive experimental results verify that the three-stage network is tailored to predict rich quality information, while the multi-label training strategy can fully exploit the potentials within subjective quality rating and enhance the prediction performance and generalizability of the network. RichIQA outperforms state-of-the-art competitors on multiple large-scale in the wild IQA databases with rich subjective rating labels. The code of RichIQA will be made publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2408.11982.pdf' target='_blank'>https://arxiv.org/pdf/2408.11982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksim Smirnov, Aleksandr Gushchin, Anastasia Antsiferova, Dmitry Vatolin, Radu Timofte, Ziheng Jia, Zicheng Zhang, Wei Sun, Jiaying Qian, Yuqin Cao, Yinan Sun, Yuxin Zhu, Xiongkuo Min, Guangtao Zhai, Kanjar De, Qing Luo, Ao-Xiang Zhang, Peng Zhang, Haibo Lei, Linyan Jiang, Yaqing Li, Wenhui Meng, Zhenzhong Chen, Zhengxue Cheng, Jiahao Xiao, Jun Xu, Chenlong He, Qi Zheng, Ruoxi Zhu, Min Li, Yibo Fan, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11982">AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is a crucial task in the development of video compression standards, as it directly impacts the viewer experience. This paper presents the results of the Compressed Video Quality Assessment challenge, held in conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV 2024. The challenge aimed to evaluate the performance of VQA methods on a diverse dataset of 459 videos, encoded with 14 codecs of various compression standards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a comprehensive collection of compression artifacts. To measure the methods performance, we employed traditional correlation coefficients between their predictions and subjective scores, which were collected via large-scale crowdsourced pairwise human comparisons. For training purposes, participants were provided with the Compressed Video Quality Assessment Dataset (CVQAD), a previously developed dataset of 1022 videos. Up to 30 participating teams registered for the challenge, while we report the results of 6 teams, which submitted valid final solutions and code for reproducing the results. Moreover, we calculated and present the performance of state-of-the-art VQA methods on the developed dataset, providing a comprehensive benchmark for future research. The dataset, results, and online leaderboard are publicly available at https://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2407.19704.pdf' target='_blank'>https://arxiv.org/pdf/2407.19704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19704">UNQA: Unified No-Reference Quality Assessment for Audio, Image, Video, and Audio-Visual Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multimedia data flourishes on the Internet, quality assessment (QA) of multimedia data becomes paramount for digital media applications. Since multimedia data includes multiple modalities including audio, image, video, and audio-visual (A/V) content, researchers have developed a range of QA methods to evaluate the quality of different modality data. While they exclusively focus on addressing the single modality QA issues, a unified QA model that can handle diverse media across multiple modalities is still missing, whereas the latter can better resemble human perception behaviour and also have a wider range of applications. In this paper, we propose the Unified No-reference Quality Assessment model (UNQA) for audio, image, video, and A/V content, which tries to train a single QA model across different media modalities. To tackle the issue of inconsistent quality scales among different QA databases, we develop a multi-modality strategy to jointly train UNQA on multiple QA databases. Based on the input modality, UNQA selectively extracts the spatial features, motion features, and audio features, and calculates a final quality score via the four corresponding modality regression modules. Compared with existing QA methods, UNQA has two advantages: 1) the multi-modality training strategy makes the QA model learn more general and robust quality-aware feature representation as evidenced by the superior performance of UNQA compared to state-of-the-art QA methods. 2) UNQA reduces the number of models required to assess multimedia data across different modalities. and is friendly to deploy to practical applications.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2406.06087.pdf' target='_blank'>https://arxiv.org/pdf/2406.06087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Chen, Wei Sun, Yuan Tian, Jun Jia, Zicheng Zhang, Jiarui Wang, Ru Huang, Xiongkuo Min, Guangtao Zhai, Wenjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06087">GAIA: Rethinking Action Quality Assessment for AI-Generated Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2404.18162.pdf' target='_blank'>https://arxiv.org/pdf/2404.18162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhang, Ying Hu, Xiongkuo Min, Yan Zhou, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18162">fMRI Exploration of Visual Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant strides in visual quality assessment, the neural mechanisms underlying visual quality perception remain insufficiently explored. This study employed fMRI to examine brain activity during image quality assessment and identify differences in human processing of images with varying quality. Fourteen healthy participants underwent tasks assessing both image quality and content classification while undergoing functional MRI scans. The collected behavioral data was statistically analyzed, and univariate and functional connectivity analyses were conducted on the imaging data. The findings revealed that quality assessment is a more complex task than content classification, involving enhanced activation in high-level cognitive brain regions for fine-grained visual analysis. Moreover, the research showed the brain's adaptability to different visual inputs, adopting different strategies depending on the input's quality. In response to high-quality images, the brain primarily uses specialized visual areas for precise analysis, whereas with low-quality images, it recruits additional resources including higher-order visual cortices and related cognitive and attentional networks to decode and recognize complex, ambiguous signals effectively. This study pioneers the intersection of neuroscience and image quality research, providing empirical evidence through fMRI linking image quality to neural processing. It contributes novel insights into the human visual system's response to diverse image qualities, thereby paving the way for advancements in objective image quality assessment algorithms.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2404.16687.pdf' target='_blank'>https://arxiv.org/pdf/2404.16687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohong Liu, Xiongkuo Min, Guangtao Zhai, Chunyi Li, Tengchuan Kou, Wei Sun, Haoning Wu, Yixuan Gao, Yuqin Cao, Zicheng Zhang, Xiele Wu, Radu Timofte, Fei Peng, Huiyuan Fu, Anlong Ming, Chuanming Wang, Huadong Ma, Shuai He, Zifei Dou, Shu Chen, Huacong Zhang, Haiyi Xie, Chengwei Wang, Baoying Chen, Jishen Zeng, Jianquan Yang, Weigang Wang, Xi Fang, Xiaoxin Lv, Jun Yan, Tianwu Zhi, Yabin Zhang, Yaohui Li, Yang Li, Jingwen Xu, Jianzhao Liu, Yiting Liao, Junlin Li, Zihao Yu, Yiting Lu, Xin Li, Hossein Motamednia, S. Farhad Hosseini-Benvidi, Fengbin Guan, Ahmad Mahmoudi-Aznaveh, Azadeh Mansouri, Ganzorig Gankhuyag, Kihwan Yoon, Yifang Xu, Haotian Fan, Fangyuan Kong, Shiling Zhao, Weifeng Dong, Haibing Yin, Li Zhu, Zhiling Wang, Bingchen Huang, Avinab Saha, Sandeep Mishra, Shashank Gupta, Rajesh Sureddi, Oindrila Saha, Luigi Celona, Simone Bianco, Paolo Napoletano, Raimondo Schettini, Junfeng Yang, Jing Fu, Wei Zhang, Wenzhi Cao, Limei Liu, Han Peng, Weijun Yuan, Zhan Li, Yihang Cheng, Yifan Deng, Haohui Li, Bowen Qu, Yao Li, Shuqing Luo, Shunzhou Wang, Wei Gao, Zihao Lu, Marcos V. Conde, Xinrui Wang, Zhibo Chen, Ruling Liao, Yan Ye, Qiulin Wang, Bing Li, Zhaokun Zhou, Miao Geng, Rui Chen, Xin Tao, Xiaoyu Liang, Shangkun Sun, Xingyuan Ma, Jiaze Li, Mengduo Yang, Haoran Xu, Jie Zhou, Shiding Zhu, Bohan Yu, Pengfei Chen, Xinrui Xu, Jiabin Shen, Zhichao Duan, Erfan Asadi, Jiahe Liu, Qi Yan, Youran Qu, Xiaohui Zeng, Lele Wang, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16687">NTIRE 2024 Quality Assessment of AI-Generated Content Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated Content Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challenge is to address a major challenge in the field of image and video processing, namely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for AI-Generated Content (AIGC). The challenge is divided into the image track and the video track. The image track uses the AIGIQA-20K, which contains 20,000 AI-Generated Images (AIGIs) generated by 15 popular generative models. The image track has a total of 318 registered participants. A total of 1,646 submissions are received in the development phase, and 221 submissions are received in the test phase. Finally, 16 participating teams submitted their models and fact sheets. The video track uses the T2VQA-DB, which contains 10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V) models. A total of 196 participants have registered in the video track. A total of 991 submissions are received in the development phase, and 185 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. Some methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on AIGC.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2404.16205.pdf' target='_blank'>https://arxiv.org/pdf/2404.16205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcos V. Conde, Saman Zadtootaghaj, Nabajeet Barman, Radu Timofte, Chenlong He, Qi Zheng, Ruoxi Zhu, Zhengzhong Tu, Haiqiang Wang, Xiangguang Chen, Wenhui Meng, Xiang Pan, Huiying Shi, Han Zhu, Xiaozhong Xu, Lei Sun, Zhenzhong Chen, Shan Liu, Zicheng Zhang, Haoning Wu, Yingjie Zhou, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Wei Sun, Yuqin Cao, Yanwei Jiang, Jun Jia, Zhichao Zhang, Zijian Chen, Weixia Zhang, Xiongkuo Min, Steve GÃ¶ring, Zihao Qi, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16205">AIS 2024 Challenge on Video Quality Assessment of User-Generated Content: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reviews the AIS 2024 Video Quality Assessment (VQA) Challenge, focused on User-Generated Content (UGC). The aim of this challenge is to gather deep learning-based methods capable of estimating the perceptual quality of UGC videos. The user-generated videos from the YouTube UGC Dataset include diverse content (sports, games, lyrics, anime, etc.), quality and resolutions. The proposed methods must process 30 FHD frames under 1 second. In the challenge, a total of 102 participants registered, and 15 submitted code and models. The performance of the top-5 submissions is reviewed and provided here as a survey of diverse deep models for efficient video quality assessment of user-generated content.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2404.03407.pdf' target='_blank'>https://arxiv.org/pdf/2404.03407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Tengchuan Kou, Yixuan Gao, Yuqin Cao, Wei Sun, Zicheng Zhang, Yingjie Zhou, Zhichao Zhang, Weixia Zhang, Haoning Wu, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03407">AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media. However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings. To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and text-to-image alignment altogether comprehensively involving 21 subjects. This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K. Furthermore, we conduct benchmark experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception. We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision. The database is released on https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2404.01024.pdf' target='_blank'>https://arxiv.org/pdf/2404.01024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Yang, Huiyu Duan, Long Teng, Yucheng Zhu, Xiaohong Liu, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01024">AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the rapid advancement of Artificial Intelligence Generated Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated omnidirectional images hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have also been widely studied. AI-generated omnidirectional images exhibit unique distortions compared to natural omnidirectional images, however, there is no dedicated Image Quality Assessment (IQA) criteria for assessing them. This study addresses this gap by establishing a large-scale AI generated omnidirectional image IQA database named AIGCOIQA2024 and constructing a comprehensive benchmark. We first generate 300 omnidirectional images based on 5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is conducted subsequently to assess human visual preferences from three perspectives including quality, comfortability, and correspondence. Finally, we conduct a benchmark experiment to evaluate the performance of state-of-the-art IQA models on our database. The database will be released to facilitate future research.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2402.03413.pdf' target='_blank'>https://arxiv.org/pdf/2402.03413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiongkuo Min, Huiyu Duan, Wei Sun, Yucheng Zhu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03413">Perceptual Video Quality Assessment: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual video quality assessment plays a vital role in the field of video processing due to the existence of quality degradations introduced in various stages of video signal acquisition, compression, transmission and display. With the advancement of internet communication and cloud service technology, video content and traffic are growing exponentially, which further emphasizes the requirement for accurate and rapid assessment of video quality. Therefore, numerous subjective and objective video quality assessment studies have been conducted over the past two decades for both generic videos and specific videos such as streaming, user-generated content (UGC), 3D, virtual and augmented reality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey provides an up-to-date and comprehensive review of these video quality assessment studies. Specifically, we first review the subjective video quality assessment methodologies and databases, which are necessary for validating the performance of video quality metrics. Second, the objective video quality assessment algorithms for general purposes are surveyed and concluded according to the methodologies utilized in the quality measures. Third, we overview the objective video quality assessment measures for specific applications and emerging topics. Finally, the performances of the state-of-the-art video quality assessment measures are compared and analyzed. This survey provides a systematic overview of both classical works and recent progresses in the realm of video quality assessment, which can help other researchers quickly access the field and conduct relevant research.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2401.01117.pdf' target='_blank'>https://arxiv.org/pdf/2401.01117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunyi Li, Haoning Wu, Zicheng Zhang, Hongkun Hao, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01117">Q-Refine: A Perceptual Quality Refiner for AI-Generated Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid evolution of the Text-to-Image (T2I) model in recent years, their unsatisfactory generation result has become a challenge. However, uniformly refining AI-Generated Images (AIGIs) of different qualities not only limited optimization capabilities for low-quality AIGIs but also brought negative optimization to high-quality AIGIs. To address this issue, a quality-award refiner named Q-Refine is proposed. Based on the preference of the Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA) metric to guide the refining process for the first time, and modify images of different qualities through three adaptive pipelines. Experimental shows that for mainstream T2I models, Q-Refine can perform effective optimization to AIGIs of different qualities. It can be a general refiner to optimize AIGIs from both fidelity and aesthetic quality levels, thus expanding the application of the T2I generation models.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2312.15300.pdf' target='_blank'>https://arxiv.org/pdf/2312.15300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Haoning Wu, Zhongpeng Ji, Chunyi Li, Erli Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Fengyu Sun, Shangling Jui, Weisi Lin, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15300">Q-Boost: On Visual Quality Assessment Ability of Low-level Multi-Modality Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Multi-modality Large Language Models (MLLMs) have demonstrated remarkable capabilities in complex high-level vision tasks. However, the exploration of MLLM potential in visual quality assessment, a vital aspect of low-level vision, remains limited. To address this gap, we introduce Q-Boost, a novel strategy designed to enhance low-level MLLMs in image quality assessment (IQA) and video quality assessment (VQA) tasks, which is structured around two pivotal components: 1) Triadic-Tone Integration: Ordinary prompt design simply oscillates between the binary extremes of $positive$ and $negative$. Q-Boost innovates by incorporating a `middle ground' approach through $neutral$ prompts, allowing for a more balanced and detailed assessment. 2) Multi-Prompt Ensemble: Multiple quality-centric prompts are used to mitigate bias and acquire more accurate evaluation. The experimental results show that the low-level MLLMs exhibit outstanding zeros-shot performance on the IQA/VQA tasks equipped with the Q-Boost strategy.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2311.18216.pdf' target='_blank'>https://arxiv.org/pdf/2311.18216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Chen, Wei Sun, Zicheng Zhang, Ru Huang, Fangfang Lu, Xiongkuo Min, Guangtao Zhai, Wenjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18216">FS-BAND: A Frequency-Sensitive Banding Detector</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Banding artifact, as known as staircase-like contour, is a common quality annoyance that happens in compression, transmission, etc. scenarios, which largely affects the user's quality of experience (QoE). The banding distortion typically appears as relatively small pixel-wise variations in smooth backgrounds, which is difficult to analyze in the spatial domain but easily reflected in the frequency domain. In this paper, we thereby study the banding artifact from the frequency aspect and propose a no-reference banding detection model to capture and evaluate banding artifacts, called the Frequency-Sensitive BANding Detector (FS-BAND). The proposed detector is able to generate a pixel-wise banding map with a perception correlated quality score. Experimental results show that the proposed FS-BAND method outperforms state-of-the-art image quality assessment (IQA) approaches with higher accuracy in banding classification task.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2311.17752.pdf' target='_blank'>https://arxiv.org/pdf/2311.17752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Chen, Wei Sun, Jun Jia, Fangfang Lu, Zicheng Zhang, Jing Liu, Ru Huang, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17752">BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms. As undesirable artifacts, banding destroys the original image structure, thus degrading users' quality of experience (QoE). In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e. mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality ratings. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts. A dual convolutional neural network is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters. Experiments demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2310.17147.pdf' target='_blank'>https://arxiv.org/pdf/2310.17147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17147">Simple Baselines for Projection-based Full-reference and No-reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point clouds are widely used in 3D content representation and have various applications in multimedia. However, compression and simplification processes inevitably result in the loss of quality-aware information under storage and bandwidth constraints. Therefore, there is an increasing need for effective methods to quantify the degree of distortion in point clouds. In this paper, we propose simple baselines for projection-based point cloud quality assessment (PCQA) to tackle this challenge. We use multi-projections obtained via a common cube-like projection process from the point clouds for both full-reference (FR) and no-reference (NR) PCQA tasks. Quality-aware features are extracted with popular vision backbones. The FR quality representation is computed as the similarity between the feature maps of reference and distorted projections while the NR quality representation is obtained by simply squeezing the feature maps of distorted projections with average pooling The corresponding quality representations are regressed into visual quality scores by fully-connected layers. Taking part in the ICIP 2023 PCVQA Challenge, we succeeded in achieving the top spot in four out of the five competition tracks.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2310.16732.pdf' target='_blank'>https://arxiv.org/pdf/2310.16732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiongkuo Min, Xianghe Ma, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16732">A No-Reference Quality Assessment Method for Digital Human Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, digital humans have been widely applied in augmented/virtual reality (A/VR), where viewers are allowed to freely observe and interact with the volumetric content. However, the digital humans may be degraded with various distortions during the procedure of generation and transmission. Moreover, little effort has been put into the perceptual quality assessment of digital humans. Therefore, it is urgent to carry out objective quality assessment methods to tackle the challenge of digital human quality assessment (DHQA). In this paper, we develop a novel no-reference (NR) method based on Transformer to deal with DHQA in a multi-task manner. Specifically, the front 2D projections of the digital humans are rendered as inputs and the vision transformer (ViT) is employed for the feature extraction. Then we design a multi-task module to jointly classify the distortion types and predict the perceptual quality levels of digital humans. The experimental results show that the proposed method well correlates with the subjective ratings and outperforms the state-of-the-art quality assessment methods.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2310.15984.pdf' target='_blank'>https://arxiv.org/pdf/2310.15984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15984">Geometry-Aware Video Quality Assessment for Dynamic Digital Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Digital Humans (DDHs) are 3D digital models that are animated using predefined motions and are inevitably bothered by noise/shift during the generation process and compression distortion during the transmission process, which needs to be perceptually evaluated. Usually, DDHs are displayed as 2D rendered animation videos and it is natural to adapt video quality assessment (VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQA methods are highly dependent on viewpoints and less sensitive to geometry-based distortions. Therefore, in this paper, we propose a novel no-reference (NR) geometry-aware video quality assessment method for DDH-QA challenge. Geometry characteristics are described by the statistical parameters estimated from the DDHs' geometry attribute distributions. Spatial and temporal features are acquired from the rendered videos. Finally, all kinds of features are integrated and regressed into quality values. Experimental results show that the proposed method achieves state-of-the-art performance on the DDH-QA database.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2307.13981.pdf' target='_blank'>https://arxiv.org/pdf/2307.13981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13981">Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind video quality assessment (BVQA) plays an indispensable role in monitoring and improving the end-users' viewing experience in various real-world video-enabled media applications. As an experimental field, the improvements of BVQA models have been measured primarily on a few human-rated VQA datasets. Thus, it is crucial to gain a better understanding of existing VQA datasets in order to properly evaluate the current progress in BVQA. Towards this goal, we conduct a first-of-its-kind computational analysis of VQA datasets via designing minimalistic BVQA models. By minimalistic, we restrict our family of BVQA models to build only upon basic blocks: a video preprocessor (for aggressive spatiotemporal downsampling), a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor, all with the simplest possible instantiations. By comparing the quality prediction performance of different model variants on eight VQA datasets with realistic distortions, we find that nearly all datasets suffer from the easy dataset problem of varying severity, some of which even admit blind image quality assessment (BIQA) solutions. We additionally justify our claims by contrasting our model generalizability on these VQA datasets, and by ablating a dizzying set of BVQA design choices related to the basic building blocks. Our results cast doubt on the current progress in BVQA, and meanwhile shed light on good practices of constructing next-generation VQA datasets and models.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2307.09729.pdf' target='_blank'>https://arxiv.org/pdf/2307.09729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang, Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia, Yilin Li, Wei Wu, Shuming Hu, Sibin Deng, Pengxiang Xiao, Ying Chen, Kai Li, Kai Zhao, Kun Yuan, Ming Sun, Heng Cong, Hao Wang, Lingzhi Fu, Yusheng Zhang, Rongyu Zhang, Hang Shi, Qihang Xu, Longan Xiao, Zhiliang Ma, Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini, Zhiwei Huang, Yanan Li, Xiaotao Wang, Lei Lei, Hongye Liu, Wei Hong, Ironhead Chuang, Allen Lin, Drake Guan, Iris Chen, Kae Lou, Willy Huang, Yachun Tasi, Yvonne Kao, Haotian Fan, Fangyuan Kong, Shiqi Zhou, Hao Liu, Yu Lai, Shanshan Chen, Wenqi Wang, Haoning Wu, Chaofeng Chen, Chunzheng Zhu, Zekun Guo, Shiling Zhao, Haibing Yin, Hongkui Wang, Hanene Brachemi Meftah, Sid Ahmed Fezza, Wassim Hamidouche, Olivier DÃ©forges, Tengfei Shi, Azadeh Mansouri, Hossein Motamednia, Amir Hossein Bakhtiari, Ahmad Mahmoudi Aznaveh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09729">NTIRE 2023 Quality Assessment of Video Enhancement Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2307.00211.pdf' target='_blank'>https://arxiv.org/pdf/2307.00211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Wang, Huiyu Duan, Jing Liu, Shi Chen, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00211">AIGCIQA2023: A Large-scale Image Quality Assessment Database for AI Generated Images: from the Perspectives of Quality, Authenticity and Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, in order to get a better understanding of the human visual preferences for AIGIs, a large-scale IQA database for AIGC is established, which is named as AIGCIQA2023. We first generate over 2000 images based on 6 state-of-the-art text-to-image generation models using 100 prompts. Based on these images, a well-organized subjective experiment is conducted to assess the human visual preferences for each image from three perspectives including quality, authenticity and correspondence. Finally, based on this large-scale database, we conduct a benchmark experiment to evaluate the performance of several state-of-the-art IQA metrics on our constructed database.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2303.14933.pdf' target='_blank'>https://arxiv.org/pdf/2303.14933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Wu, Wei Sun, Dangyang Tu, Wei Lu, Xiongkuo Min, Ying Chen, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14933">MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User-generated content (UGC) live videos are often bothered by various distortions during capture procedures and thus exhibit diverse visual qualities. Such source videos are further compressed and transcoded by media server providers before being distributed to end-users. Because of the flourishing of UGC live videos, effective video quality assessment (VQA) tools are needed to monitor and perceptually optimize live streaming videos in the distributing process. In this paper, we address \textbf{UGC Live VQA} problems by constructing a first-of-a-kind subjective UGC Live VQA database and developing an effective evaluation tool. Concretely, 418 source UGC videos are collected in real live streaming scenarios and 3,762 compressed ones at different bit rates are generated for the subsequent subjective VQA experiments. Based on the built database, we develop a \underline{M}ulti-\underline{D}imensional \underline{VQA} (\textbf{MD-VQA}) evaluator to measure the visual quality of UGC live videos from semantic, distortion, and motion aspects respectively. Extensive experimental results show that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA database and existing compressed UGC VQA databases.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2303.12618.pdf' target='_blank'>https://arxiv.org/pdf/2303.12618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12618">A Perceptual Quality Assessment Exploration for AIGC Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\underline{AI} \underline{G}enerated \underline{C}ontent (\textbf{AIGC}) has gained widespread attention with the increasing efficiency of deep learning in content creation. AIGC, created with the assistance of artificial intelligence technology, includes various forms of content, among which the AI-generated images (AGIs) have brought significant impact to society and have been applied to various fields such as entertainment, education, social media, etc. However, due to hardware limitations and technical proficiency, the quality of AIGC images (AGIs) varies, necessitating refinement and filtering before practical use. Consequently, there is an urgent need for developing objective models to assess the quality of AGIs. Unfortunately, no research has been carried out to investigate the perceptual quality assessment for AGIs specifically. Therefore, in this paper, we first discuss the major evaluation aspects such as technical issues, AI artifacts, unnaturalness, discrepancy, and aesthetics for AGI quality assessment. Then we present the first perceptual AGI quality assessment database, AGIQA-1K, which consists of 1,080 AGIs generated from diffusion models. A well-organized subjective experiment is followed to collect the quality labels of the AGIs. Finally, we conduct a benchmark experiment to evaluate the performance of current image quality assessment (IQA) models.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2303.02392.pdf' target='_blank'>https://arxiv.org/pdf/2303.02392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqin Cao, Xiongkuo Min, Wei Sun, Xiaoping Zhang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02392">Audio-Visual Quality Assessment for User Generated Content: Database and Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the explosive increase of User Generated Content (UGC), UGC video quality assessment (VQA) becomes more and more important for improving users' Quality of Experience (QoE). However, most existing UGC VQA studies only focus on the visual distortions of videos, ignoring that the user's QoE also depends on the accompanying audio signals. In this paper, we conduct the first study to address the problem of UGC audio and video quality assessment (AVQA). Specifically, we construct the first UGC AVQA database named the SJTU-UAV database, which includes 520 in-the-wild UGC audio and video (A/V) sequences, and conduct a user study to obtain the mean opinion scores of the A/V sequences. The content of the SJTU-UAV database is then analyzed from both the audio and video aspects to show the database characteristics. We also design a family of AVQA models, which fuse the popular VQA methods and audio features via support vector regressor (SVR). We validate the effectiveness of the proposed models on the three databases. The experimental results show that with the help of audio signals, the VQA models can evaluate the perceptual quality more accurately. The database will be released to facilitate further research.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2302.08715.pdf' target='_blank'>https://arxiv.org/pdf/2302.08715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Wei Sun, Yingjie Zhou, Wei Lu, Yucheng Zhu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08715">EEP-3DQA: Efficient and Effective Projection-based 3D Model Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, great numbers of efforts have been put into improving the effectiveness of 3D model quality assessment (3DQA) methods. However, little attention has been paid to the computational costs and inference time, which is also important for practical applications. Unlike 2D media, 3D models are represented by more complicated and irregular digital formats, such as point cloud and mesh. Thus it is normally difficult to perform an efficient module to extract quality-aware features of 3D models. In this paper, we address this problem from the aspect of projection-based 3DQA and develop a no-reference (NR) \underline{E}fficient and \underline{E}ffective \underline{P}rojection-based \underline{3D} Model \underline{Q}uality \underline{A}ssessment (\textbf{EEP-3DQA}) method. The input projection images of EEP-3DQA are randomly sampled from the six perpendicular viewpoints of the 3D model and are further spatially downsampled by the grid-mini patch sampling strategy. Further, the lightweight Swin-Transformer tiny is utilized as the backbone to extract the quality-aware features. Finally, the proposed EEP-3DQA and EEP-3DQA-t (tiny version) achieve the best performance than the existing state-of-the-art NR-3DQA methods and even outperforms most full-reference (FR) 3DQA methods on the point cloud and mesh quality assessment databases while consuming less inference time than the compared 3DQA methods.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2212.12734.pdf' target='_blank'>https://arxiv.org/pdf/2212.12734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Wei Sun, Wei Lu, Xiongkuo Min, Yu Wang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.12734">DDH-QA: A Dynamic Digital Humans Quality Assessment Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large amounts of effort have been put into pushing forward the real-world application of dynamic digital human (DDH). However, most current quality assessment research focuses on evaluating static 3D models and usually ignores motion distortions. Therefore, in this paper, we construct a large-scale dynamic digital human quality assessment (DDH-QA) database with diverse motion content as well as multiple distortions to comprehensively study the perceptual quality of DDHs. Both model-based distortion (noise, compression) and motion-based distortion (binding error, motion unnaturalness) are taken into consideration. Ten types of common motion are employed to drive the DDHs and a total of 800 DDHs are generated in the end. Afterward, we render the video sequences of the distorted DDHs as the evaluation media and carry out a well-controlled subjective experiment. Then a benchmark experiment is conducted with the state-of-the-art video quality assessment (VQA) methods and the experimental results show that existing VQA methods are limited in assessing the perceptual loss of DDHs.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2503.03149.pdf' target='_blank'>https://arxiv.org/pdf/2503.03149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>YiQiu Guo, Yuchen Yang, Zhe Chen, Pingjie Wang, Yusheng Liao, Ya Zhang, Yanfeng Wang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03149">DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reliability of large language models remains a critical challenge, particularly due to their susceptibility to hallucinations and factual inaccuracies during text generation. Existing solutions either underutilize models' self-correction with preemptive strategies or use costly post-hoc verification. To further explore the potential of real-time self-verification and correction, we present Dynamic Self-Verify Decoding (DSVD), a novel decoding framework that enhances generation reliability through real-time hallucination detection and efficient error correction. DSVD integrates two key components: (1) parallel self-verification architecture for continuous quality assessment, (2) dynamic rollback mechanism for targeted error recovery. Extensive experiments across five benchmarks demonstrate DSVD's effectiveness, achieving significant improvement in truthfulness (Quesetion-Answering) and factual accuracy (FActScore). Results show the DSVD can be further incorporated with existing faithful decoding methods to achieve stronger performance. Our work establishes that real-time self-verification during generation offers a viable path toward more trustworthy language models without sacrificing practical deployability.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2510.03874.pdf' target='_blank'>https://arxiv.org/pdf/2510.03874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Sijing Wu, Yucheng Zhu, Huiyu Duan, Zicheng Zhang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03874">DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of 3D scanning and reconstruction technologies, dynamic digital human avatars based on 4D meshes have become increasingly popular. A high-precision dynamic digital human avatar can be applied to various fields such as game production, animation generation, and remote immersive communication. However, these 4D human avatar meshes are prone to being degraded by various types of noise during the processes of collection, compression, and transmission, thereby affecting the viewing experience of users. In light of this fact, quality assessment of dynamic 4D digital humans becomes increasingly important. In this paper, we first propose a large-scale dynamic digital human quality assessment dataset, DHQA-4D, which contains 32 high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D human meshes degraded by 11 textured distortions, as well as their corresponding textured and non-textured mean opinion scores (MOSs). Equipped with DHQA-4D dataset, we analyze the influence of different types of distortion on human perception for textured dynamic 4D meshes and non-textured dynamic 4D meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model (LMM) based approach that is able to assess both textured 4D meshes and non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts multi-dimensional features, including visual features from a projected 2D video, motion features from cropped video clips, and geometry features from the 4D human mesh to provide comprehensive quality-related information. Then we utilize a LMM model to integrate the multi-dimensional features and conduct a LoRA-based instruction tuning technique to teach the LMM model to predict the quality scores. Extensive experimental results on the DHQA-4D dataset demonstrate the superiority of our DynaMesh-Rater method over previous quality assessment methods.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2509.24297.pdf' target='_blank'>https://arxiv.org/pdf/2509.24297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Ye Shen, Yalun Wu, Yingji Liang, Yijin Guo, Farong Wen, Wenzhe Li, Xuezhi Zhao, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24297">Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality, multi-modal benchmarks are crucial for advancing scientific reasoning in large models yet their manual creation is costly and unscalable. To address this bottleneck, we explore the potential for transforming Text-Only QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA framework and establish a comprehensive, multi-dimensional MMQA quality rubric that provides principles for the transformation. 2) Benchmark Construction: Then we construct two extensive benchmarks to rigorously evaluate state-of-the-art generation \& understanding models on the distinct tasks of MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop an agentic system (Q-Mirror), which operationalizes our framework by integrating MMQA generation and evaluation into a closed loop for iterative refinement. Our experiments show that while state-of-the-art models can generate MMQAs, their outputs still leave substantial gaps, underscoring the need for reliable evaluation. We further demonstrate that top-tier understanding models align closely with human judgment in MMQA quality assessment. Leveraging both insights, the Q-Mirror agent raises average scores from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path to large-scale scientific benchmarks.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2409.00031.pdf' target='_blank'>https://arxiv.org/pdf/2409.00031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Zhang, Yingjie Zhou, Chunyi Li, Baixuan Zhao, Xiaohong Liu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00031">Quality Assessment in the Era of Large Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment, which evaluates the visual quality level of multimedia experiences, has garnered significant attention from researchers and has evolved substantially through dedicated efforts. Before the advent of large models, quality assessment typically relied on small expert models tailored for specific tasks. While these smaller models are effective at handling their designated tasks and predicting quality levels, they often lack explainability and robustness. With the advancement of large models, which align more closely with human cognitive and perceptual processes, many researchers are now leveraging the prior knowledge embedded in these large models for quality assessment tasks. This emergence of quality assessment within the context of large models motivates us to provide a comprehensive review focusing on two key aspects: 1) the assessment of large models, and 2) the role of large models in assessment tasks. We begin by reflecting on the historical development of quality assessment. Subsequently, we move to detailed discussions of related works concerning quality assessment in the era of large models. Finally, we offer insights into the future progression and potential pathways for quality assessment in this new era. We hope this survey will enable a rapid understanding of the development of quality assessment in the era of large models and inspire further advancements in the field.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2405.19298.pdf' target='_blank'>https://arxiv.org/pdf/2405.19298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwei Zhu, Haoning Wu, Yixuan Li, Zicheng Zhang, Baoliang Chen, Lingyu Zhu, Yuming Fang, Guangtao Zhai, Weisi Lin, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19298">Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score. Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2405.03333.pdf' target='_blank'>https://arxiv.org/pdf/2405.03333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunchu Zhou, Xiaohong Liu, Yunlong Dong, Tengchuan Kou, Yixuan Gao, Zicheng Zhang, Chunyi Li, Haoning Wu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03333">Light-VQA+: A Video Quality Assessment Model for Exposure Correction with Vision-Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, User-Generated Content (UGC) videos have gained popularity in our daily lives. However, UGC videos often suffer from poor exposure due to the limitations of photographic equipment and techniques. Therefore, Video Exposure Correction (VEC) algorithms have been proposed, Low-Light Video Enhancement (LLVE) and Over-Exposed Video Recovery (OEVR) included. Equally important to the VEC is the Video Quality Assessment (VQA). Unfortunately, almost all existing VQA models are built generally, measuring the quality of a video from a comprehensive perspective. As a result, Light-VQA, trained on LLVE-QA, is proposed for assessing LLVE. We extend the work of Light-VQA by expanding the LLVE-QA dataset into Video Exposure Correction Quality Assessment (VEC-QA) dataset with over-exposed videos and their corresponding corrected versions. In addition, we propose Light-VQA+, a VQA model specialized in assessing VEC. Light-VQA+ differs from Light-VQA mainly from the usage of the CLIP model and the vision-language guidance during the feature extraction, followed by a new module referring to the Human Visual System (HVS) for more accurate assessment. Extensive experimental results show that our model achieves the best performance against the current State-Of-The-Art (SOTA) VQA models on the VEC-QA dataset and other public datasets.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2402.16641.pdf' target='_blank'>https://arxiv.org/pdf/2402.16641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16641">Towards Open-ended Visual Quality Comparison</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comparative settings (e.g. pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to open-range questions on quality comparison; 2) can provide detailed reasonings beyond direct answers. To this end, we propose the Co-Instruct. To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image quality description, (b) GPT-4V "teacher" responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the MICBench, the first benchmark on multi-image comparison for LMMs. We demonstrate that Co-Instruct not only achieves in average 30% higher accuracy than state-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher), on both existing related benchmarks and the proposed MICBench. Our model is published at https://huggingface.co/q-future/co-instruct.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2510.07842.pdf' target='_blank'>https://arxiv.org/pdf/2510.07842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Peng, Maolin Wang, Hengyi Cai, Yuchen Li, Kai Zhang, Shuaiqiang Wang, Dawei Yin, Xiangyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07842">AdaSwitch: Adaptive Switching Generation for Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2301.12152.pdf' target='_blank'>https://arxiv.org/pdf/2301.12152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anfeng Cheng, Yiding Liu, Weibin Li, Qian Dong, Shuaiqiang Wang, Zhengjie Huang, Shikun Feng, Zhicong Cheng, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12152">Layout-aware Webpage Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying high-quality webpages is fundamental for real-world search engines, which can fulfil users' information need with the less cognitive burden. Early studies of \emph{webpage quality assessment} usually design hand-crafted features that may only work on particular categories of webpages (e.g., shopping websites, medical websites). They can hardly be applied to real-world search engines that serve trillions of webpages with various types and purposes. In this paper, we propose a novel layout-aware webpage quality assessment model currently deployed in our search engine. Intuitively, layout is a universal and critical dimension for the quality assessment of different categories of webpages. Based on this, we directly employ the meta-data that describes a webpage, i.e., Document Object Model (DOM) tree, as the input of our model. The DOM tree data unifies the representation of webpages with different categories and purposes and indicates the layout of webpages. To assess webpage quality from complex DOM tree data, we propose a graph neural network (GNN) based method that extracts rich layout-aware information that implies webpage quality in an end-to-end manner. Moreover, we improve the GNN method with an attentive readout function, external web categories and a category-aware sampling method. We conduct rigorous offline and online experiments to show that our proposed solution is effective in real search engines, improving the overall usability and user experience.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2406.13375.pdf' target='_blank'>https://arxiv.org/pdf/2406.13375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Xu, Jinhua Gao, Xiaoming Yu, Baolong Bi, Huawei Shen, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13375">ALiiCE: Evaluating Positional Fine-grained Citation Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) can enhance the credibility and verifiability by generating text with citations. However, existing tasks and evaluation methods are predominantly limited to sentence-level statement, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our framework first parses the sentence claim into atomic claims via dependency analysis and then calculates citation quality at the atomic claim level. ALiiCE introduces three novel metrics for positional fined-grained citation quality assessment, including positional fine-grained citation recall and precision, and coefficient of variation of citation positions. We evaluate the positional fine-grained citation generation performance of several LLMs on two long-form QA datasets. Our experiments and analyses demonstrate the effectiveness and reasonableness of ALiiCE. The results also indicate that existing LLMs still struggle to provide positional fine-grained citations.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2406.01884.pdf' target='_blank'>https://arxiv.org/pdf/2406.01884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinghui Zhou, Wenbo Zhou, Tianyi Wei, Shen Chen, Taiping Yao, Shouhong Ding, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01884">Rank-based No-reference Quality Assessment for Face Swapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face swapping has become a prominent research area in computer vision and image processing due to rapid technological advancements. The metric of measuring the quality in most face swapping methods relies on several distances between the manipulated images and the source image, or the target image, i.e., there are suitable known reference face images. Therefore, there is still a gap in accurately assessing the quality of face interchange in reference-free scenarios. In this study, we present a novel no-reference image quality assessment (NR-IQA) method specifically designed for face swapping, addressing this issue by constructing a comprehensive large-scale dataset, implementing a method for ranking image quality based on multiple facial attributes, and incorporating a Siamese network based on interpretable qualitative comparisons. Our model demonstrates the state-of-the-art performance in the quality assessment of swapped faces, providing coarse- and fine-grained. Enhanced by this metric, an improved face-swapping model achieved a more advanced level with respect to expressions and poses. Extensive experiments confirm the superiority of our method over existing general no-reference image quality assessment metrics and the latest metric of facial image quality assessment, making it well suited for evaluating face swapping images in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2504.09255.pdf' target='_blank'>https://arxiv.org/pdf/2504.09255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijing Wu, Yunhao Li, Ziwen Xu, Yixuan Gao, Huiyu Duan, Wei Sun, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09255">FVQ: A Large-Scale Dataset and A LMM-based Method for Face Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face video quality assessment (FVQA) deserves to be explored in addition to general video quality assessment (VQA), as face videos are the primary content on social media platforms and human visual system (HVS) is particularly sensitive to human faces. However, FVQA is rarely explored due to the lack of large-scale FVQA datasets. To fill this gap, we present the first large-scale in-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face videos together with corresponding mean opinion score (MOS) annotations. Along with the FVQ-20K dataset, we further propose a specialized FVQA method named FVQ-Rater to achieve human-like rating and scoring for face video, which is the first attempt to explore the potential of large multimodal models (LMMs) for the FVQA task. Concretely, we elaborately extract multi-dimensional features including spatial features, temporal features, and face-specific features (i.e., portrait features and face embeddings) to provide comprehensive visual information, and take advantage of the LoRA-based instruction tuning technique to achieve quality-specific fine-tuning, which shows superior performance on both FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive analysis demonstrate the significant potential of the FVQ-20K dataset and FVQ-Rater method in promoting the development of FVQA.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2311.13847.pdf' target='_blank'>https://arxiv.org/pdf/2311.13847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Qin, Bin Chen, Yujun Huang, Baoyi An, Tao Dai, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13847">Perceptual Image Compression with Cooperative Cross-Modal Side Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosion of data has resulted in more and more associated text being transmitted along with images. Inspired by from distributed source coding, many works utilize image side information to enhance image compression. However, existing methods generally do not consider using text as side information to enhance perceptual compression of images, even though the benefits of multimodal synergy have been widely demonstrated in research. This begs the following question: How can we effectively transfer text-level semantic dependencies to help image compression, which is only available to the decoder? In this work, we propose a novel deep image compression method with text-guided side information to achieve a better rate-perception-distortion tradeoff. Specifically, we employ the CLIP text encoder and an effective Semantic-Spatial Aware block to fuse the text and image features. This is done by predicting a semantic mask to guide the learned text-adaptive affine transformation at the pixel level. Furthermore, we design a text-conditional generative adversarial networks to improve the perceptual quality of reconstructed images. Extensive experiments involving four datasets and ten image quality assessment metrics demonstrate that the proposed approach achieves superior results in terms of rate-perception trade-off and semantic distortion.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2508.03763.pdf' target='_blank'>https://arxiv.org/pdf/2508.03763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Jia, Jiaying Qian, Zicheng Zhang, Zijian Chen, Xiongkuo Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03763">Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training. Analogous to high-level reasoning tasks, RFT is similarly applicable to low-level vision domains, including image quality assessment (IQA). Existing RFT-based IQA methods typically use rule-based output rewards to verify the model's rollouts but provide no reward supervision for the "think" process, leaving its correctness and efficacy uncontrolled. Furthermore, these methods typically fine-tune directly on downstream IQA tasks without explicitly enhancing the model's native low-level visual quality perception, which may constrain its performance upper bound. In response to these gaps, we propose the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the Refine-Perception-20K dataset (with 12 main distortions, 20,907 locally-distorted images, and over 55K RFT samples) and design multi-task reward functions to strengthen the model's visual quality perception. In Stage-2, targeting the quality scoring task, we introduce a probability difference reward involved strategy for "think" process supervision. The resulting Refine-IQA Series Models achieve outstanding performance on both perception and scoring tasks-and, notably, our paradigm activates a robust "think" (quality interpreting) capability that also attains exceptional results on the corresponding quality interpreting benchmark.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2409.14335.pdf' target='_blank'>https://arxiv.org/pdf/2409.14335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyu Lu, Liang Ding, Kanjian Zhang, Jinxia Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14335">MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have shown significant potential as judges for Machine Translation (MT) quality assessment, providing both scores and fine-grained feedback. Although approaches such as GEMBA-MQM have shown state-of-the-art performance on reference-free evaluation, the predicted errors do not align well with those annotated by human, limiting their interpretability as feedback signals. To enhance the quality of error annotations predicted by LLM evaluators, we introduce a universal and training-free framework, $\textbf{MQM-APE}$, based on the idea of filtering out non-impactful errors by Automatically Post-Editing (APE) the original translation based on each error, leaving only those errors that contribute to quality improvement. Specifically, we prompt the LLM to act as 1) $\textit{evaluator}$ to provide error annotations, 2) $\textit{post-editor}$ to determine whether errors impact quality improvement and 3) $\textit{pairwise quality verifier}$ as the error filter. Experiments show that our approach consistently improves both the reliability and quality of error spans against GEMBA-MQM, across eight LLMs in both high- and low-resource languages. Orthogonal to trained approaches, MQM-APE complements translation-specific evaluators such as Tower, highlighting its broad applicability. Further analysis confirms the effectiveness of each module and offers valuable insights into evaluator design and LLMs selection.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2303.13809.pdf' target='_blank'>https://arxiv.org/pdf/2303.13809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13809">Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2510.08508.pdf' target='_blank'>https://arxiv.org/pdf/2510.08508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08508">MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo \underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \underline{Res}tored \underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2510.03880.pdf' target='_blank'>https://arxiv.org/pdf/2510.03880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Sijing Wu, Huiyu Duan, Yucheng Zhu, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03880">Exploring Instruction Data Quality for Explainable Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, with the rapid development of powerful multimodal large language models (MLLMs), explainable image quality assessment (IQA) has gradually become popular, aiming at providing quality-related descriptions and answers of images. To achieve this goal, recent methods seek to construct a large-scale instruction tuning dataset to empower the MLLM with quality perception ability following the well-known scaling law. However, a large amount of instruction tuning data may cause substantial computational costs and redundant data, which in turn will cause harm to the performance of the model. To cope with this problem, in this paper, we challenge the scaling law and systematically investigate the role of data quality of the instruction tuning dataset for explainable IQA. Using a powerful pre-trained MLLM, we first investigate the changes in model performance after fine-tuning with different sizes of instruction tuning data. We find that selecting a subset of the data set randomly using an appropriate ratio can even lead to better results than training with the entire instruction tuning dataset, demonstrating the redundancy of current explainable IQA instruction tuning data. Beyond randomly sampling a subset, we propose a clustering-based data selection framework with three stages: clustering feature extraction, cluster quota allocation, and cluster sampling strategy. Then we systematically analyze the choices of each stage and propose a simple but efficient data selection method IQA-Select for explainable IQA. The experimental results demonstrate that IQA-Select can achieve 102.1% and 103.7% performance of full fine-tuning using only 10% selected data in Q-Bench and AesBench respectively, significantly reducing computational costs while achieving better performance.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2508.12020.pdf' target='_blank'>https://arxiv.org/pdf/2508.12020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilin Gao, Yunhao Li, Sijing Wu, Yuqin Cao, Huiyu Duan, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12020">Ges-QA: A Multidimensional Quality Assessment Dataset for Audio-to-3D Gesture Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Audio-to-3D-Gesture (A2G) task has enormous potential for various applications in virtual reality and computer graphics, etc. However, current evaluation metrics, such as FrÃ©chet Gesture Distance or Beat Constancy, fail at reflecting the human preference of the generated 3D gestures. To cope with this problem, exploring human preference and an objective quality assessment metric for AI-generated 3D human gestures is becoming increasingly significant. In this paper, we introduce the Ges-QA dataset, which includes 1,400 samples with multidimensional scores for gesture quality and audio-gesture consistency. Moreover, we collect binary classification labels to determine whether the generated gestures match the emotions of the audio. Equipped with our Ges-QA dataset, we propose a multi-modal transformer-based neural network with 3 branches for video, audio and 3D skeleton modalities, which can score A2G contents in multiple dimensions. Comparative experimental results and ablation studies demonstrate that Ges-QAer yields state-of-the-art performance on our dataset.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2412.17574.pdf' target='_blank'>https://arxiv.org/pdf/2412.17574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17574">HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech-visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 16 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 22 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and emotion perception, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2412.13155.pdf' target='_blank'>https://arxiv.org/pdf/2412.13155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Liu, Huiyu Duan, Qiang Hu, Liu Yang, Chunlei Cai, Tianxiao Ye, Huayu Liu, Xiaoyun Zhang, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13155">F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence generative models exhibit remarkable capabilities in content creation, particularly in face image generation, customization, and restoration. However, current AI-generated faces (AIGFs) often fall short of human preferences due to unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation framework for AIGFs. To address this need, we introduce FaceQ, a large-scale, comprehensive database of AI-generated Face images with fine-grained Quality annotations reflecting human preferences. The FaceQ database comprises 12,255 images generated by 29 models across three tasks: (1) face generation, (2) face customization, and (3) face restoration. It includes 32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple dimensions: quality, authenticity, identity (ID) fidelity, and text-image correspondence. Using the FaceQ database, we establish F-Bench, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA), face quality assessment (FQA), AI-generated content image quality assessment (AIGCIQA), and preference evaluation metrics, manifesting that these standard metrics are relatively ineffective in evaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ database will be publicly available upon publication.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2504.08411.pdf' target='_blank'>https://arxiv.org/pdf/2504.08411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08411">A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious applications of visual manipulation have raised serious threats to the security and reputation of users in many fields. To alleviate these issues, adversarial noise-based defenses have been enthusiastically studied in recent years. However, ``data-only" methods tend to distort fake samples in the low-level feature space rather than the high-level semantic space, leading to limitations in resisting malicious manipulation. Frontier research has shown that integrating knowledge in deep learning can produce reliable and generalizable solutions. Inspired by these, we propose a knowledge-guided adversarial defense (KGAD) to actively force malicious manipulation models to output semantically confusing samples. Specifically, in the process of generating adversarial noise, we focus on constructing significant semantic confusions at the domain-specific knowledge level, and exploit a metric closely related to visual perception to replace the general pixel-wise metrics. The generated adversarial noise can actively interfere with the malicious manipulation model by triggering knowledge-guided and perception-related disruptions in the fake samples. To validate the effectiveness of the proposed method, we conduct qualitative and quantitative experiments on human perception and visual quality assessment. The results on two different tasks both show that our defense provides better protection compared to state-of-the-art methods and achieves great generalizability.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2405.21075.pdf' target='_blank'>https://arxiv.org/pdf/2405.21075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, Xing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: https://video-mme.github.io
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2407.02762.pdf' target='_blank'>https://arxiv.org/pdf/2407.02762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushan Zhu, Wen Zhang, Yajing Xu, Zhen Yao, Mingyang Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02762">SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Network (GNN), with the main idea of encoding graph structure information of graphs by propagation and aggregation, has developed rapidly. It achieved excellent performance in representation learning of multiple types of graphs such as homogeneous graphs, heterogeneous graphs, and more complex graphs like knowledge graphs. However, merely stacking GNN layers may not improve the model's performance and can even be detrimental. For the phenomenon of performance degradation in deep GNNs, we propose a new perspective. Unlike the popular explanations of over-smoothing or over-squashing, we think the issue arises from the interference of low-quality node representations during message propagation. We introduce a simple and general method, SF-GNN, to address this problem. In SF-GNN, we define two representations for each node, one is the node representation that represents the feature of the node itself, and the other is the message representation specifically for propagating messages to neighbor nodes. A self-filter module evaluates the quality of the node representation and decides whether to integrate it into the message propagation based on this quality assessment. Experiments on node classification tasks for both homogeneous and heterogeneous graphs, as well as link prediction tasks on knowledge graphs, demonstrate that our method can be applied to various GNN models and outperforms state-of-the-art baseline methods in addressing deep GNN degradation.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2509.17100.pdf' target='_blank'>https://arxiv.org/pdf/2509.17100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Alapatt, Jennifer Eckhoff, Zhiliang Lyu, Yutong Ban, Jean-Paul Mazellier, Sarah Choksi, Kunyi Yang, 2024 CVS Challenge Consortium, Quanzheng Li, Filippo Filicori, Xiang Li, Pietro Mascagni, Daniel A. Hashimoto, Guy Rosman, Ozanan Meireles, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17100">The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence (AI) for surgical quality assessment promise to democratize access to expertise, with applications in training, guidance, and accreditation. This study presents the SAGES Critical View of Safety (CVS) Challenge, the first AI competition organized by a surgical society, using the CVS in laparoscopic cholecystectomy, a universally recommended yet inconsistently performed safety step, as an exemplar of surgical quality assessment. A global collaboration across 54 institutions in 24 countries engaged hundreds of clinicians and engineers to curate 1,000 videos annotated by 20 surgical experts according to a consensus-validated protocol. The challenge addressed key barriers to real-world deployment in surgery, including achieving high performance, capturing uncertainty in subjective assessment, and ensuring robustness to clinical variability. To enable this scale of effort, we developed EndoGlacier, a framework for managing large, heterogeneous surgical video and multi-annotator workflows. Thirteen international teams participated, achieving up to a 17\% relative gain in assessment performance, over 80\% reduction in calibration error, and a 17\% relative improvement in robustness over the state-of-the-art. Analysis of results highlighted methodological trends linked to model performance, providing guidance for future research toward robust, clinically deployable AI for surgical quality assessment.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2510.01691.pdf' target='_blank'>https://arxiv.org/pdf/2510.01691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Liu, Jinjie Wei, Wanying Qu, Chenglong Ma, Junzhi Ning, Yunheng Li, Ying Chen, Xinzhe Luo, Pengcheng Chen, Xin Gao, Ming Hu, Huihui Xu, Xin Wang, Shujian Gao, Dingkang Yang, Zhongying Deng, Jin Ye, Lihao Liu, Junjun He, Ningsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01691">MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2410.04239.pdf' target='_blank'>https://arxiv.org/pdf/2410.04239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunkit Chan, Cheng Jiayang, Xin Liu, Yauwai Yim, Yuxin Jiang, Zheye Deng, Haoran Li, Yangqiu Song, Ginny Y. Wong, Simon See
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04239">Persona Knowledge-Aligned Prompt Tuning Method for Online Debate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Debate is the process of exchanging viewpoints or convincing others on a particular issue. Recent research has provided empirical evidence that the persuasiveness of an argument is determined not only by language usage but also by communicator characteristics. Researchers have paid much attention to aspects of languages, such as linguistic features and discourse structures, but combining argument persuasiveness and impact with the social personae of the audience has not been explored due to the difficulty and complexity. We have observed the impressive simulation and personification capability of ChatGPT, indicating a giant pre-trained language model may function as an individual to provide personae and exert unique influences based on diverse background knowledge. Therefore, we propose a persona knowledge-aligned framework for argument quality assessment tasks from the audience side. This is the first work that leverages the emergence of ChatGPT and injects such audience personae knowledge into smaller language models via prompt tuning. The performance of our pipeline demonstrates significant and consistent improvement compared to competitive architectures.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2303.13859.pdf' target='_blank'>https://arxiv.org/pdf/2303.13859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Huang, Chunyi Li, Abdelhak Bentaleb, Roger Zimmermann, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13859">XGC-VQA: A unified video quality assessment model for User, Professionally, and Occupationally-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of Internet video data amounts and types, a unified Video Quality Assessment (VQA) is needed to inspire video communication with perceptual quality. To meet the real-time and universal requirements in providing such inspiration, this study proposes a VQA model from a classification of User Generated Content (UGC), Professionally Generated Content (PGC), and Occupationally Generated Content (OGC). In the time domain, this study utilizes non-uniform sampling, as each content type has varying temporal importance based on its perceptual quality. In the spatial domain, centralized downsampling is performed before the VQA process by utilizing a patch splicing/sampling mechanism to lower complexity for real-time assessment. The experimental results demonstrate that the proposed method achieves a median correlation of $0.7$ while limiting the computation time below 5s for three content types, which ensures that the communication experience of UGC, PGC, and OGC can be optimized altogether.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2505.20741.pdf' target='_blank'>https://arxiv.org/pdf/2505.20741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiatong Shi, Hye-Jin Shim, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20741">Uni-VERSA: Versatile Speech Assessment with a Unified Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2510.14664.pdf' target='_blank'>https://arxiv.org/pdf/2510.14664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Wang, Jinghua Zhao, Yifan Yang, Shujie Liu, Junyang Chen, Yanzhe Zhang, Shiwan Zhao, Jinyu Li, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14664">SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative speech technologies are progressing rapidly, but evaluating the perceptual quality of synthetic speech remains a core challenge. Existing methods typically rely on scalar scores or binary decisions, which lack interpretability and generalization across tasks and languages. We present SpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs) to conduct structured and explanation-based speech quality evaluation. To support this direction, we introduce SpeechEval, a large-scale dataset containing 32,207 multilingual speech clips and 128,754 annotations spanning four tasks: quality assessment, pairwise comparison, improvement suggestion, and deepfake detection. Based on this resource, we develop SQ-LLM, a speech-quality-aware LLM trained with chain-of-thought reasoning and reward optimization to improve capability. Experimental results show that SQ-LLM delivers strong performance across tasks and languages, revealing the potential of this paradigm for advancing speech quality evaluation. Relevant resources will be open-sourced.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2510.08081.pdf' target='_blank'>https://arxiv.org/pdf/2510.08081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaochong Lan, Jie Feng, Yinxing Liu, Xinlei Shi, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08081">AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2402.00282.pdf' target='_blank'>https://arxiv.org/pdf/2402.00282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00282">PAM: Prompting Audio-Language Models for Audio Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other "reference-free" metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2307.09416.pdf' target='_blank'>https://arxiv.org/pdf/2307.09416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Betti, Jacopo Staiano, Lorenzo Baraldi, Lorenzo Baraldi, Rita Cucchiara, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09416">Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods. We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour. ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment. This method outlines visual concepts, formulates image-specific verification questions, utilizes the Q&A system to investigate the image, and scores the combined outcome. Although this brave new hypothesis of mimicking humans in the image evaluation process is in its preliminary assessment stage, results are promising and open the door to a new form of automatic evaluation which could have significant impact as the image generation or the image target editing tasks become more and more sophisticated.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2508.18636.pdf' target='_blank'>https://arxiv.org/pdf/2508.18636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Wang, Xinyi Hou, Yanjie Zhao, Weiguo Lin, Haoyu Wang, Junjun Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18636">LaQual: A Novel Framework for Automated Evaluation of LLM App Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM app stores are quickly emerging as platforms that gather a wide range of intelligent applications based on LLMs, giving users many choices for content creation, coding support, education, and more. However, the current methods for ranking and recommending apps in these stores mostly rely on static metrics like user activity and favorites, which makes it hard for users to efficiently find high-quality apps. To address these challenges, we propose LaQual, an automated framework for evaluating the quality of LLM apps. LaQual consists of three main stages: first, it labels and classifies LLM apps in a hierarchical way to accurately match them to different scenarios; second, it uses static indicators, such as time-weighted user engagement and functional capability metrics, to filter out low-quality apps; and third, it conducts a dynamic, scenario-adaptive evaluation, where the LLM itself generates scenario-specific evaluation metrics, scoring rules, and tasks for a thorough quality assessment. Experiments on a popular LLM app store show that LaQual is effective. Its automated scores are highly consistent with human judgments (with Spearman's rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in travel planning). By effectively screening, LaQual can reduce the pool of candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual significantly outperforms baseline systems in decision confidence, comparison efficiency (with average scores of 5.45 compared to 3.30), and the perceived value of its evaluation reports (4.75 versus 2.25). Overall, these results demonstrate that LaQual offers a scalable, objective, and user-centered solution for finding and recommending high-quality LLM apps in real-world use cases.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2510.10609.pdf' target='_blank'>https://arxiv.org/pdf/2510.10609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiting Lu, Fengbin Guan, Yixin Gao, Yan Zhong, Xinge Peng, Jiakang Yuan, Yihao Liu, Bo Zhang, Xin Li, Zhibo Chen, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10609">OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current visual evaluation approaches are typically constrained to a single task. To address this, we propose OmniQuality-R, a unified reward modeling framework that transforms multi-task quality reasoning into continuous and interpretable reward signals for policy optimization. Inspired by subjective experiments, where participants are given task-specific instructions outlining distinct assessment principles prior to evaluation, we propose OmniQuality-R, a structured reward modeling framework that transforms multi-dimensional reasoning into continuous and interpretable reward signals. To enable this, we construct a reasoning-enhanced reward modeling dataset by sampling informative plan-reason trajectories via rejection sampling, forming a reliable chain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on this, we apply Group Relative Policy Optimization (GRPO) for post-training, using a Gaussian-based reward to support continuous score prediction. To further stabilize the training and improve downstream generalization, we incorporate standard deviation (STD) filtering and entropy gating mechanisms during reinforcement learning. These techniques suppress unstable updates and reduce variance in policy optimization. We evaluate OmniQuality-R on three key IQA tasks: aesthetic quality assessment, technical quality evaluation, and text-image alignment.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2502.19026.pdf' target='_blank'>https://arxiv.org/pdf/2502.19026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengbin Guan, Zihao Yu, Yiting Lu, Xin Li, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19026">InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment tasks rely heavily on the rich features required for video understanding, such as semantic information, texture, and temporal motion. The existing video foundational model, InternVideo2, has demonstrated strong potential in video understanding tasks due to its large parameter size and large-scale multimodal data pertaining. Building on this, we explored the transferability of InternVideo2 to video quality assessment under compression scenarios. To design a lightweight model suitable for this task, we proposed a distillation method to equip the smaller model with rich compression quality priors. Additionally, we examined the performance of different backbones during the distillation process. The results showed that, compared to other methods, our lightweight model distilled from InternVideo2 achieved excellent performance in compression video quality assessment.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2409.05381.pdf' target='_blank'>https://arxiv.org/pdf/2409.05381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Li, Zihao Huang, Yan Zhang, Yunhang Shen, Ke Li, Xiawu Zheng, Liujuan Cao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05381">Few-Shot Image Quality Assessment via Adaptation of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) remains an unresolved challenge in computer vision due to complex distortions, diverse image content, and limited data availability. Existing Blind IQA (BIQA) methods largely rely on extensive human annotations, which are labor-intensive and costly due to the demanding nature of creating IQA datasets. To reduce this dependency, we propose the Gradient-Regulated Meta-Prompt IQA Framework (GRMP-IQA), designed to efficiently adapt the visual-language pre-trained model, CLIP, to IQA tasks, achieving high accuracy even with limited data. GRMP-IQA consists of two core modules: (i) Meta-Prompt Pre-training Module and (ii) Quality-Aware Gradient Regularization. The Meta Prompt Pre-training Module leverages a meta-learning paradigm to pre-train soft prompts with shared meta-knowledge across different distortions, enabling rapid adaptation to various IQA tasks. On the other hand, the Quality-Aware Gradient Regularization is designed to adjust the update gradients during fine-tuning, focusing the model's attention on quality-relevant features and preventing overfitting to semantic information. Extensive experiments on standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods under limited data setting. Notably, utilizing just 20% of the training data, GRMP-IQA is competitive with most existing fully supervised BIQA approaches.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2406.19247.pdf' target='_blank'>https://arxiv.org/pdf/2406.19247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timin Gao, Wensheng Pan, Yan Zhang, Sicheng Zhao, Shengchuan Zhang, Xiawu Zheng, Ke Li, Liujuan Cao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19247">Local Manifold Learning for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning has considerably advanced the field of Image Quality Assessment (IQA), emerging as a widely adopted technique. The core mechanism of contrastive learning involves minimizing the distance between quality-similar (positive) examples while maximizing the distance between quality-dissimilar (negative) examples. Despite its successes, current contrastive learning methods often neglect the importance of preserving the local manifold structure. This oversight can result in a high degree of similarity among hard examples within the feature space, thereby impeding effective differentiation and assessment. To address this issue, we propose an innovative framework that integrates local manifold learning with contrastive learning for No-Reference Image Quality Assessment (NR-IQA). Our method begins by sampling multiple crops from a given image, identifying the most visually salient crop. This crop is then used to cluster other crops from the same image as the positive class, while crops from different images are treated as negative classes to increase inter-class distance. Uniquely, our approach also considers non-saliency crops from the same image as intra-class negative classes to preserve their distinctiveness. Additionally, we employ a mutual learning framework, which further enhances the model's ability to adaptively learn and identify visual saliency regions. Our approach demonstrates a better performance compared to state-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942 (compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2406.16297.pdf' target='_blank'>https://arxiv.org/pdf/2406.16297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajing Pei, Shiyu Huang, Yiting Lu, Xin Li, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16297">Priorformer: A UGC-VQA Method with content and distortion priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User Generated Content (UGC) videos are susceptible to complicated and variant degradations and contents, which prevents the existing blind video quality assessment (BVQA) models from good performance since the lack of the adapability of distortions and contents. To mitigate this, we propose a novel prior-augmented perceptual vision transformer (PriorFormer) for the BVQA of UGC, which boots its adaptability and representation capability for divergent contents and distortions. Concretely, we introduce two powerful priors, i.e., the content and distortion priors, by extracting the content and distortion embeddings from two pre-trained feature extractors. Then we adopt these two powerful embeddings as the adaptive prior tokens, which are transferred to the vision transformer backbone jointly with implicit quality features. Based on the above strategy, the proposed PriorFormer achieves state-of-the-art performance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and YouTube-UGC.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2404.14949.pdf' target='_blank'>https://arxiv.org/pdf/2404.14949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wensheng Pan, Timin Gao, Yan Zhang, Runze Hu, Xiawu Zheng, Enwei Zhang, Yuting Gao, Yutao Liu, Yunhang Shen, Ke Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14949">Multi-Modal Prompt Learning on Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) models benefit significantly from semantic information, which allows them to treat different types of objects distinctly. Currently, leveraging semantic information to enhance IQA is a crucial research direction. Traditional methods, hindered by a lack of sufficiently annotated data, have employed the CLIP image-text pretraining model as their backbone to gain semantic awareness. However, the generalist nature of these pre-trained Vision-Language (VL) models often renders them suboptimal for IQA-specific tasks. Recent approaches have attempted to address this mismatch using prompt technology, but these solutions have shortcomings. Existing prompt-based VL models overly focus on incremental semantic information from text, neglecting the rich insights available from visual data analysis. This imbalance limits their performance improvements in IQA tasks. This paper introduces an innovative multi-modal prompt-based methodology for IQA. Our approach employs carefully crafted prompts that synergistically mine incremental semantic information from both visual and linguistic data. Specifically, in the visual branch, we introduce a multi-layer prompt structure to enhance the VL model's adaptability. In the text branch, we deploy a dual-prompt scheme that steers the model to recognize and differentiate between scene category and distortion type, thereby refining the model's capacity to assess image quality. Our experimental findings underscore the effectiveness of our method over existing Blind Image Quality Assessment (BIQA) approaches. Notably, it demonstrates competitive performance across various datasets. Our method achieves Spearman Rank Correlation Coefficient (SRCC) values of 0.961(surpassing 0.946 in CSIQ) and 0.941 (exceeding 0.930 in KADID), illustrating its robustness and accuracy in diverse contexts.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2402.07220.pdf' target='_blank'>https://arxiv.org/pdf/2402.07220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07220">KVQ: Kwai Video Quality Assessment for Short-form Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short-form UGC video platforms, like Kwai and TikTok, have been an emerging and irreplaceable mainstream media form, thriving on user-friendly engagement, and kaleidoscope creation, etc. However, the advancing content-generation modes, e.g., special effects, and sophisticated processing workflows, e.g., de-artifacts, have introduced significant challenges to recent UGC video quality assessment: (i) the ambiguous contents hinder the identification of quality-determined regions. (ii) the diverse and complicated hybrid distortions are hard to distinguish. To tackle the above challenges and assist in the development of short-form videos, we establish the first large-scale Kaleidoscope short Video database for Quality assessment, termed KVQ, which comprises 600 user-uploaded short videos and 3600 processed videos through the diverse practical processing workflows, including pre-processing, transcoding, and enhancement. Among them, the absolute quality score of each video and partial ranking score among indistinguishable samples are provided by a team of professional researchers specializing in image processing. Based on this database, we propose the first short-form video quality evaluator, i.e., KSVQE, which enables the quality evaluator to identify the quality-determined semantics with the content understanding of large vision language models (i.e., CLIP) and distinguish the distortions with the distortion understanding module. Experimental results have shown the effectiveness of KSVQE on our KVQ database and popular VQA databases.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2401.11949.pdf' target='_blank'>https://arxiv.org/pdf/2401.11949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11949">Feature Denoising Diffusion Model for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC).
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2401.08522.pdf' target='_blank'>https://arxiv.org/pdf/2401.08522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08522">Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of non-reference video quality assessment is to evaluate the quality of distorted video without access to reference high-definition references. In this study, we introduce an enhanced spatial perception module, pre-trained on multiple image quality assessment datasets, and a lightweight temporal fusion module to address the no-reference visual quality assessment (NR-VQA) task. This model implements Swin Transformer V2 as a local-level spatial feature extractor and fuses these multi-stage representations through a series of transformer layers. Furthermore, a temporal transformer is utilized for spatiotemporal feature fusion across the video. To accommodate compressed videos of varying bitrates, we incorporate a coarse-to-fine contrastive strategy to enrich the model's capability to discriminate features from videos of different bitrates. This is an expanded version of the one-page abstract.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2312.06158.pdf' target='_blank'>https://arxiv.org/pdf/2312.06158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Li, Timin Gao, Runze Hu, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Jingyuan Zheng, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06158">Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The current state-of-the-art No-Reference Image Quality Assessment (NR-IQA) methods typically rely on feature extraction from upstream semantic backbone networks, assuming that all extracted features are relevant. However, we make a key observation that not all features are beneficial, and some may even be harmful, necessitating careful selection. Empirically, we find that many image pairs with small feature spatial distances can have vastly different quality scores, indicating that the extracted features may contain a significant amount of quality-irrelevant noise. To address this issue, we propose a Quality-Aware Feature Matching IQA Metric (QFM-IQM) that employs an adversarial perspective to remove harmful semantic noise features from the upstream task. Specifically, QFM-IQM enhances the semantic noise distinguish capabilities by matching image pairs with similar quality scores but varying semantic features as adversarial semantic noise and adaptively adjusting the upstream task's features by reducing sensitivity to adversarial noise perturbation. Furthermore, we utilize a distillation framework to expand the dataset and improve the model's generalization ability. Our approach achieves superior performance to the state-of-the-art NR-IQA methods on eight standard IQA datasets.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2312.00591.pdf' target='_blank'>https://arxiv.org/pdf/2312.00591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Li, Jingyuan Zheng, Xiawu Zheng, Runze Hu, Enwei Zhang, Yuting Gao, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Yan Zhang, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00591">Less is More: Learning Reference Knowledge Using No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) with reference images have achieved great success by imitating the human vision system, in which the image quality is effectively assessed by comparing the query image with its pristine reference image. However, for the images in the wild, it is quite difficult to access accurate reference images. We argue that it is possible to learn reference knowledge under the No-Reference Image Quality Assessment (NR-IQA) setting, which is effective and efficient empirically. Concretely, by innovatively introducing a novel feature distillation method in IQA, we propose a new framework to learn comparative knowledge from non-aligned reference images. And then, to achieve fast convergence and avoid overfitting, we further propose an inductive bias regularization. Such a framework not only solves the congenital defects of NR-IQA but also improves the feature extraction framework, enabling it to express more abundant quality information. Surprisingly, our method utilizes less input while obtaining a more significant improvement compared to the teacher models. Extensive experiments on eight standard NR-IQA datasets demonstrate the superior performance to the state-of-the-art NR-IQA methods, i.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs. 0.661 in LIVEFB).
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2509.03292.pdf' target='_blank'>https://arxiv.org/pdf/2509.03292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dyah A. M. G. Wisnu, Ryandhimas E. Zezario, Stefano Rini, Hsin-Min Wang, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03292">Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system for automatic multi-axis perceptual quality prediction of generative audio, developed for Track 2 of the AudioMOS Challenge 2025. The task is to predict four Audio Aesthetic Scores--Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness--for audio generated by text-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. A main challenge is the domain shift between natural training data and synthetic evaluation data. To address this, we combine BEATs, a pretrained transformer-based audio representation model, with a multi-branch long short-term memory (LSTM) predictor and use a triplet loss with buffer-based sampling to structure the embedding space by perceptual similarity. Our results show that this improves embedding discriminability and generalization, enabling domain-robust audio quality assessment without synthetic training data.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2509.01336.pdf' target='_blank'>https://arxiv.org/pdf/2509.01336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Chin Huang, Hui Wang, Cheng Liu, Yi-Chiao Wu, Andros Tjandra, Wei-Ning Hsu, Erica Cooper, Yong Qin, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01336">The AudioMOS Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This is the summary paper for the AudioMOS Challenge 2025, the very first challenge for automatic subjective quality prediction for synthetic audio. The challenge consists of three tracks. The first track aims to assess text-to-music samples in terms of overall quality and textual alignment. The second track is based on the four evaluation dimensions of Meta Audiobox Aesthetics, and the test set consists of text-to-speech, text-to-audio, and text-to-music samples. The third track focuses on synthetic speech quality assessment in different sampling rates. The challenge attracted 24 unique teams from both academia and industry, and improvements over the baselines were confirmed. The outcome of this challenge is expected to facilitate development and progress in the field of automatic evaluation for audio generation systems.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2505.21356.pdf' target='_blank'>https://arxiv.org/pdf/2505.21356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21356">Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders by providing standardized evaluations of vocal function. Traditionally, expert raters use standard scales such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. This study proposes Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to extract high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we also introduce VOQANet+, which integrates low-level speech descriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings into a hybrid representation. Unlike prior studies focused only on vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level speech (PVQD-S subset) to improve generalizability. Results show that sentence-based input outperforms vowel-based input, especially at the patient level, underscoring the value of longer utterances for capturing perceptual voice attributes. VOQANet consistently surpasses baseline methods in root mean squared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even better performance. Additional experiments under noisy conditions show that VOQANet+ maintains high prediction accuracy and robustness, supporting its potential for real-world and telehealth deployment.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2401.01145.pdf' target='_blank'>https://arxiv.org/pdf/2401.01145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dyah A. M. G. Wisnu, Stefano Rini, Ryandhimas E. Zezario, Hsin-Min Wang, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01145">HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model for Hearing Aids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces HAAQI-Net, a non-intrusive deep learning-based music audio quality assessment model for hearing aid users. Unlike traditional methods like the Hearing Aid Audio Quality Index (HAAQI) that require intrusive reference signal comparisons, HAAQI-Net offers a more accessible and computationally efficient alternative. By utilizing a Bidirectional Long Short-Term Memory (BLSTM) architecture with attention mechanisms and features extracted from the pre-trained BEATs model, it can predict HAAQI scores directly from music audio clips and hearing loss patterns. Experimental results demonstrate HAAQI-Net's effectiveness, achieving a Linear Correlation Coefficient (LCC) of 0.9368 , a Spearman's Rank Correlation Coefficient (SRCC) of 0.9486 , and a Mean Squared Error (MSE) of 0.0064 and inference time significantly reduces from 62.52 to 2.54 seconds. To address computational overhead, a knowledge distillation strategy was applied, reducing parameters by 75.85% and inference time by 96.46%, while maintaining strong performance (LCC: 0.9071 , SRCC: 0.9307 , MSE: 0.0091 ). To expand its capabilities, HAAQI-Net was adapted to predict subjective human scores like the Mean Opinion Score (MOS) through fine-tuning. This adaptation significantly improved prediction accuracy, validated through statistical analysis. Furthermore, the robustness of HAAQI-Net was evaluated under varying Sound Pressure Level (SPL) conditions, revealing optimal performance at a reference SPL of 65 dB, with accuracy gradually decreasing as SPL deviated from this point. The advancements in subjective score prediction, SPL robustness, and computational efficiency position HAAQI-Net as a scalable solution for music audio quality assessment in hearing aid applications, contributing to efficient and accurate models in audio signal processing and hearing aid technology.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2308.09262.pdf' target='_blank'>https://arxiv.org/pdf/2308.09262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09262">Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a multi-task pseudo-label learning (MPL)-based non-intrusive speech quality assessment model called MTQ-Net. MPL consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The pretrained MOSA-Net model is utilized to estimate three pseudo labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning is then employed to train MTQ-Net by combining a supervised loss (derived from the difference between the estimated score and the ground-truth label) and a semi-supervised loss (derived from the difference between the estimated score and the pseudo label), where the Huber loss is employed as the loss function. Experimental results first demonstrate the advantages of MPL compared to training a model from scratch and using a direct knowledge transfer mechanism. Second, the benefit of the Huber loss for improving the predictive ability of MTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher overall predictive power compared to other SSL-based speech assessment models.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2501.12382.pdf' target='_blank'>https://arxiv.org/pdf/2501.12382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12382">DiffDoctor: Diagnosing Image Diffusion Models Before Treating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In spite of recent progress, image diffusion models still produce artifacts. A common solution is to leverage the feedback provided by quality assessment systems or human annotators to optimize the model, where images are generally rated in their entirety. In this work, we believe problem-solving starts with identification, yielding the request that the model should be aware of not just the presence of defects in an image, but their specific locations. Motivated by this, we propose DiffDoctor, a two-stage pipeline to assist image diffusion models in generating fewer artifacts. Concretely, the first stage targets developing a robust artifact detector, for which we collect a dataset of over 1M flawed synthesized images and set up an efficient human-in-the-loop annotation process, incorporating a carefully designed class-balance strategy. The learned artifact detector is then involved in the second stage to optimize the diffusion model by providing pixel-level feedback. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness of our artifact detector as well as the soundness of our diagnose-then-treat design.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2412.04871.pdf' target='_blank'>https://arxiv.org/pdf/2412.04871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04871">Building a Family of Data Augmentation Models for Low-cost LLM Fine-tuning on the Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Specializing LLMs in various domain-specific tasks has emerged as a critical step towards achieving high performance. However, the construction and annotation of datasets in specific domains are always very costly. Apart from using superior and expensive closed-source LLM APIs to construct datasets, some open-source models have become strong enough to handle dataset construction in many scenarios. Thus, we present a family of data augmentation models designed to significantly improve the efficiency for model fine-tuning. These models, trained based on sufficiently small LLMs, support key functionalities with low inference costs: instruction expansion, instruction refinement, and instruction-response pair expansion. To fulfill this goal, we first construct an automatic data collection system with seed datasets generated from both public repositories and our in-house datasets. This system leverages powerful LLMs to expand, refine and re-write the instructions and responses, incorporating quality assessment techniques. Following this, we introduce the training process of our models, which effectively distills task-solving and text synthesis abilities from teacher LLMs. Finally, we demonstrate how we integrate these functionalities into a machine learning platform to support low-cost LLM fine-tuning from both dataset preparation and training perspectives for users. Experiments and an application study prove the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2506.01455.pdf' target='_blank'>https://arxiv.org/pdf/2506.01455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Fei Shi, Yang Ai, Zhen-Hua Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01455">Universal Preference-Score-based Pairwise Speech Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To compare the performance of two speech generation systems, one of the most effective approaches is estimating the preference score between their generated speech. This paper proposes a novel universal preference-score-based pairwise speech quality assessment (UPPSQA) model, aimed at predicting the preference score between paired speech samples to determine which one has better quality. The model first predicts the absolute mean opinion score (MOS) for the two speech samples separately, and then aggregates them into a relative preference score using a preference function. To address the scarcity of preference data, we also construct a new pairwise speech dataset based on a MOS dataset for experiments. Experimental results confirm that, whether in training scenarios with different data types and label conditions, or in both in-domain and out-of-domain test scenarios, the prediction accuracy of UPP-SQA outperforms that of the baseline models, demonstrating its universality.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2411.11123.pdf' target='_blank'>https://arxiv.org/pdf/2411.11123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Fei Shi, Yang Ai, Ye-Xin Lu, Hui-Peng Du, Zhen-Hua Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11123">Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2408.07171.pdf' target='_blank'>https://arxiv.org/pdf/2408.07171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Qi, Chen Feng, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07171">BVI-UGC: A Video Quality Database for User-Generated Content Transcoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, user-generated content (UGC) has become one of the major video types consumed via streaming networks. Numerous research contributions have focused on assessing its visual quality through subjective tests and objective modeling. In most cases, objective assessments are based on a no-reference scenario, where the corresponding reference content is assumed not to be available. However, full-reference video quality assessment is also important for UGC in the delivery pipeline, particularly associated with the video transcoding process. In this context, we present a new UGC video quality database, BVI-UGC, for user-generated content transcoding, which contains 60 (non-pristine) reference videos and 1,080 test sequences. In this work, we simulated the creation of non-pristine reference sequences (with a wide range of compression distortions), typical of content uploaded to UGC platforms for transcoding. A comprehensive crowdsourced subjective study was then conducted involving more than 3,500 human participants. Based on this collected subjective data, we benchmarked the performance of 10 full-reference and 11 no-reference quality metrics. Our results demonstrate the poor performance (SROCC values are lower than 0.6) of these metrics in predicting the perceptual quality of UGC in two different scenarios (with or without a reference).
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2405.08621.pdf' target='_blank'>https://arxiv.org/pdf/2405.08621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Peng, Chen Feng, Duolikun Danier, Fan Zhang, Benoit Vallade, Alex Mackin, David Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08621">RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artifacts, and improve perceptual quality. However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications. In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content. It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimized through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content. The extracted quality representations are then combined through linear regression to generate video-level quality indices. The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation. The results show its superior correlation performance when compared to ten existing no-reference quality metrics.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2304.07036.pdf' target='_blank'>https://arxiv.org/pdf/2304.07036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijing Liu, Qilong Ying, Shuangchi He, Xin Yang, Dong Ni, Ruobing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07036">Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound is the primary modality to examine fetal growth during pregnancy, while the image quality could be affected by various factors. Quality assessment is essential for controlling the quality of ultrasound images to guarantee both the perceptual and diagnostic values. Existing automated approaches often require heavy structural annotations and the predictions may not necessarily be consistent with the assessment results by human experts. Furthermore, the overall quality of a scan and the correlation between the quality of frames should not be overlooked. In this work, we propose a reinforcement learning framework powered by two hierarchical agents that collaboratively learn to perform both frame-level and video-level quality assessments. It is equipped with a specially-designed reward mechanism that considers temporal dependency among frame quality and only requires sparse binary annotations to train. Experimental results on a challenging fetal brain dataset verify that the proposed framework could perform dual-level quality assessment and its predictions correlate well with the subjective assessment results.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2507.04094.pdf' target='_blank'>https://arxiv.org/pdf/2507.04094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04094">MMMOS: Multi-domain Multi-axis Audio Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendall's Ï versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2401.16578.pdf' target='_blank'>https://arxiv.org/pdf/2401.16578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan Mathai, Pritam Mukherjee, Xin Gao, Ronald M Summers, Zhiyong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16578">Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our "Detailed GPT-4 (5-shot)" model achieves a 0.48 score, outperforming the METEOR metric by 0.19, while our "Regressed GPT-4" model shows even greater alignment with expert evaluations, exceeding the best existing metric by a 0.35 margin. Moreover, the robustness of our explanations has been validated through a thorough iterative strategy. We plan to publicly release annotations from radiology experts, setting a new standard for accuracy in future assessments. This underscores the potential of our approach in enhancing the quality assessment of AI-driven medical reports.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2506.05384.pdf' target='_blank'>https://arxiv.org/pdf/2506.05384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Peng-Tao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05384">Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2505.20655.pdf' target='_blank'>https://arxiv.org/pdf/2505.20655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lujian Yao, Siming Zheng, Xinbin Yuan, Zhuoxuan Cai, Pu Wu, Jinwei Chen, Bo Li, Peng-Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20655">Photography Perspective Composition: Towards Aesthetic Perspective Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional photography composition approaches are dominated by 2D cropping-based methods. However, these methods fall short when scenes contain poorly arranged subjects. Professional photographers often employ perspective adjustment as a form of 3D recomposition, modifying the projected 2D relationships between subjects while maintaining their actual spatial positions to achieve better compositional balance. Inspired by this artistic practice, we propose photography perspective composition (PPC), extending beyond traditional cropping-based methods. However, implementing the PPC faces significant challenges: the scarcity of perspective transformation datasets and undefined assessment criteria for perspective quality. To address these challenges, we present three key contributions: (1) An automated framework for building PPC datasets through expert photographs. (2) A video generation approach that demonstrates the transformation process from less favorable to aesthetically enhanced perspectives. (3) A perspective quality assessment (PQA) model constructed based on human performance. Our approach is concise and requires no additional prompt instructions or camera trajectories, helping and guiding ordinary users to enhance their composition skills.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2411.09007.pdf' target='_blank'>https://arxiv.org/pdf/2411.09007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Huang, Xudong Li, Bohan Fu, Xiaohui Chu, Ke Li, Yunhang Shen, Yan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09007">Scale Contrastive Learning with Selective Attentions for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) serves as a fundamental task in computer vision, yet it often fails to consistently align with human subjective perception. Recent advances show that multi-scale evaluation strategies are promising due to their ability to replicate the hierarchical structure of human vision. However, the effectiveness of these strategies is limited by a lack of understanding of how different image scales influence perceived quality. This paper addresses two primary challenges: the significant redundancy of information across different scales, and the confusion caused by combining features from these scales, which may vary widely in quality. To this end, a new multi-scale BIQA framework is proposed, namely Contrast-Constrained Scale-Focused IQA Framework (CSFIQA). CSFIQA features a selective focus attention mechanism to minimize information redundancy and highlight critical quality-related information. Additionally, CSFIQA includes a scale-level contrastive learning module equipped with a noise sample matching mechanism to identify quality discrepancies across the same image content at different scales. By exploring the intrinsic relationship between image scales and the perceived quality, the proposed CSFIQA achieves leading performance on eight benchmark datasets, e.g., achieving SRCC values of 0.967 (versus 0.947 in CSIQ) and 0.905 (versus 0.876 in LIVEC).
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2410.11540.pdf' target='_blank'>https://arxiv.org/pdf/2410.11540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaxin Du, Rui Ye, Fengting Yuchi, Wanru Zhao, Jingjing Qu, Yanfeng Wang, Siheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11540">Data Quality Control in Federated Instruction-tuning of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) enables privacy-preserving collaborative instruction tuning of large language models (LLMs) by leveraging massively distributed data. However, the decentralized nature of FL exacerbates data quality challenges, as local clients lack global visibility to filter noisy or low-quality samples before training. To resolve this issue, we propose FedDQC, a novel federated instruction tuning framework with dynamic data quality control. Our approach introduces two key innovations. First, we propose instruction-response alignment (IRA), an efficient client-side metric for quality evaluation requiring only low-cost inference. We validate that higher-IRA data corresponds to more relevant and easier-to-learn question-answer pairs. Second, mirroring the human easy-to-hard knowledge acquisition process, we design a quality-aware hierarchical FL training framework, where the LLM is progressively fine-tuned from high- to low-IRA data in a collaborative manner. The framework also supports adaptive data quality assessment at each hierarchy, enabling dynamic adjustments throughout the training process. Extensive experiments on synthetic and real-world datasets show that our method significantly improves LLM performance on mixed-quality data in FL.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2409.16271.pdf' target='_blank'>https://arxiv.org/pdf/2409.16271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vlad Hosu, Marcos V. Conde, Lorenzo Agnolucci, Nabajeet Barman, Saman Zadtootaghaj, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16271">AIM 2024 Challenge on UHD Blind Photo Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the No-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution photos. The challenge is based on the recently released UHD-IQA Benchmark Database, which comprises 6,073 UHD-1 (4K) images annotated with perceptual quality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA focuses on highly aesthetic photos of superior technical quality, reflecting the ever-increasing standards of digital photography. This challenge aims to develop efficient and effective NR-IQA models. Participants are tasked with creating novel architectures and training strategies to achieve high predictive performance on UHD-1 images within a computational budget of 50G MACs. This enables model deployment on edge devices and scalable processing of extensive image collections. Winners are determined based on a combination of performance metrics, including correlation measures (SRCC, PLCC, KRCC), absolute error metrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this challenge, participants leverage techniques like knowledge distillation, low-precision inference, and multi-scale training. By pushing the boundaries of NR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the development of practical models that can keep pace with the rapidly evolving landscape of digital photography. The innovative solutions emerging from this competition will have implications for various applications, from photo curation and enhancement to image compression.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2404.12970.pdf' target='_blank'>https://arxiv.org/pdf/2404.12970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Dronova, Vladislav Cheremnykh, Alexey Kotcov, Aleksey Fedoseev, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12970">FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for 3D reconstruction and environmental mapping frequently face challenges in achieving high precision, highlighting the need for practical and effective solutions. In response to this issue, our study introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with drone-based data acquisition for high-quality 3D reconstruction. Utilizing unmanned aerial vehicle (UAV) for capturing images and corresponding spatial coordinates, the obtained data is subsequently used for the initial NeRF-based 3D reconstruction of the environment. Further evaluation of the reconstruction render quality is accomplished by the image evaluation neural network developed within the scope of our system. According to the results of the image evaluation module, an autonomous algorithm determines the position for additional image capture, thereby improving the reconstruction quality. The neural network introduced for render quality assessment demonstrates an accuracy of 97%. Furthermore, our adaptive methodology enhances the overall reconstruction quality, resulting in an average improvement of 2.5 dB in Peak Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates promising results, offering advancements in such fields as environmental monitoring, surveillance, and digital twins, where high-fidelity 3D reconstructions are crucial.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2404.11159.pdf' target='_blank'>https://arxiv.org/pdf/2404.11159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Chahine, Marcos V. Conde, Daniela Carfora, Gabriel Pacianotto, Benoit Pochon, Sira Ferradans, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11159">Deep Portrait Quality Assessment. A NTIRE 2024 Challenge Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reviews the NTIRE 2024 Portrait Quality Assessment Challenge, highlighting the proposed solutions and results. This challenge aims to obtain an efficient deep neural network capable of estimating the perceptual quality of real portrait photos. The methods must generalize to diverse scenes and diverse lighting conditions (indoor, outdoor, low-light), movement, blur, and other challenging conditions. In the challenge, 140 participants registered, and 35 submitted results during the challenge period. The performance of the top 5 submissions is reviewed and provided here as a gauge for the current state-of-the-art in Portrait Quality Assessment.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2505.18988.pdf' target='_blank'>https://arxiv.org/pdf/2505.18988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Varun Jain, Zongwei Wu, Quan Zou, Louis Florentin, Henrik Turbell, Sandeep Siddhartha, Radu Timofte, others
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18988">NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive review of the 1st Challenge on Video Quality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR 2025, and highlights the problem statement, datasets, proposed solutions, and results. The aim of this challenge was to design a Video Quality Enhancement (VQE) model to enhance video quality in video conferencing scenarios by (a) improving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing sharpness - giving a professional studio-like effect. Participants were given a differentiable Video Quality Assessment (VQA) model, training, and test videos. A total of 91 participants registered for the challenge. We received 10 valid submissions that were evaluated in a crowdsourced framework.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2510.08789.pdf' target='_blank'>https://arxiv.org/pdf/2510.08789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Xing, Soumik Dey, Mingyang Wu, Ashirbad Mishra, Naveen Ravipati, Binbin Li, Hansi Wu, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08789">Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is a fundamental computer vision task that aims to predict the perceptual quality of a given video in alignment with human judgments. Existing performant VQA models trained with direct score supervision suffer from (1) poor generalization across diverse content and tasks, ranging from user-generated content (UGC), short-form videos, to AI-generated content (AIGC), (2) limited interpretability, and (3) lack of extensibility to novel use cases or content types. We propose Q-Router, an agentic framework for universal VQA with a multi-tier model routing system. Q-Router integrates a diverse set of expert models and employs vision--language models (VLMs) as real-time routers that dynamically reason and then ensemble the most appropriate experts conditioned on the input video semantics. We build a multi-tiered routing system based on the computing budget, with the heaviest tier involving a specific spatiotemporal artifacts localization for interpretability. This agentic design enables Q-Router to combine the complementary strengths of specialized experts, achieving both flexibility and robustness in delivering consistent performance across heterogeneous video sources and tasks. Extensive experiments demonstrate that Q-Router matches or surpasses state-of-the-art VQA models on a variety of benchmarks, while substantially improving generalization and interpretability. Moreover, Q-Router excels on the quality-based question answering benchmark, Q-Bench-Video, highlighting its promise as a foundation for next-generation VQA systems. Finally, we show that Q-Router capably localizes spatiotemporal artifacts, showing potential as a reward function for post-training video generation models.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2509.26006.pdf' target='_blank'>https://arxiv.org/pdf/2509.26006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwei Zhu, Yu Tian, Keyan Ding, Baoliang Chen, Bolin Chen, Shiqi Wang, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26006">AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) is inherently complex, as it reflects both the quantification and interpretation of perceptual quality rooted in the human visual system. Conventional approaches typically rely on fixed models to output scalar scores, limiting their adaptability to diverse distortions, user-specific queries, and interpretability needs. Furthermore, scoring and interpretation are often treated as independent processes, despite their interdependence: interpretation identifies perceptual degradations, while scoring abstracts them into a compact metric. To address these limitations, we propose AgenticIQA, a modular agentic framework that integrates vision-language models (VLMs) with traditional IQA tools in a dynamic, query-aware manner. AgenticIQA decomposes IQA into four subtasks -- distortion detection, distortion analysis, tool selection, and tool execution -- coordinated by a planner, executor, and summarizer. The planner formulates task-specific strategies, the executor collects perceptual evidence via tool invocation, and the summarizer integrates this evidence to produce accurate scores with human-aligned explanations. To support training and evaluation, we introduce AgenticIQA-200K, a large-scale instruction dataset tailored for IQA agents, and AgenticIQA-Eval, the first benchmark for assessing the planning, execution, and summarization capabilities of VLM-based IQA agents. Extensive experiments across diverse IQA datasets demonstrate that AgenticIQA consistently surpasses strong baselines in both scoring accuracy and explanatory alignment.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2508.09158.pdf' target='_blank'>https://arxiv.org/pdf/2508.09158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwen Jiao, Kangan Qian, Hao Ye, Yang Zhong, Ziang Luo, Sicong Jiang, Zilin Huang, Yangyi Fang, Jinyu Miao, Zheng Fu, Yunlong Wang, Kun Jiang, Diange Yang, Rui Fan, Baoyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09158">EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving faces significant challenges in achieving human-like iterative decision-making, which continuously generates, evaluates, and refines trajectory proposals. Current generation-evaluation frameworks isolate trajectory generation from quality assessment, preventing iterative refinement essential for planning, while reinforcement learning methods collapse multi-dimensional preferences into scalar rewards, obscuring critical trade-offs and yielding scalarization bias.To overcome these issues, we present EvaDrive, a novel multi-objective reinforcement learning framework that establishes genuine closed-loop co-evolution between trajectory generation and evaluation via adversarial optimization. EvaDrive frames trajectory planning as a multi-round adversarial game. In this game, a hierarchical generator continuously proposes candidate paths by combining autoregressive intent modeling for temporal causality with diffusion-based refinement for spatial flexibility. These proposals are then rigorously assessed by a trainable multi-objective critic that explicitly preserves diverse preference structures without collapsing them into a single scalarization bias.This adversarial interplay, guided by a Pareto frontier selection mechanism, enables iterative multi-round refinement, effectively escaping local optima while preserving trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic weighting without external preference data, introducing a closed-loop adversarial framework for human-like iterative decision-making, offering a novel scalarization-free trajectory optimization approach.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2409.09726.pdf' target='_blank'>https://arxiv.org/pdf/2409.09726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benny Wijaya, Kun Jiang, Mengmeng Yang, Tuopu Wen, Yunlong Wang, Xuewei Tang, Zheng Fu, Taohua Zhou, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09726">High Definition Map Mapping and Update: A General Overview and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Along with the rapid growth of autonomous vehicles (AVs), more and more demands are required for environment perception technology. Among others, HD mapping has become one of the more prominent roles in helping the vehicle realize essential tasks such as localization and path planning. While increasing research efforts have been directed toward HD Map development. However, a comprehensive overview of the overall HD map mapping and update framework is still lacking. This article introduces the development and current state of the algorithm involved in creating HD map mapping and its maintenance. As part of this study, the primary data preprocessing approach of processing raw data to information ready to feed for mapping and update purposes, semantic segmentation, and localization are also briefly reviewed. Moreover, the map taxonomy, ontology, and quality assessment are extensively discussed, the map data's general representation method is presented, and the mapping algorithm ranging from SLAM to transformers learning-based approaches are also discussed. The development of the HD map update algorithm, from change detection to the update methods, is also presented. Finally, the authors discuss possible future developments and the remaining challenges in HD map mapping and update technology. This paper simultaneously serves as a position paper and tutorial to those new to HD map mapping and update domains.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2406.15252.pdf' target='_blank'>https://arxiv.org/pdf/2406.15252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15252">VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis) based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman correlation between VideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with human judges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2406.07280.pdf' target='_blank'>https://arxiv.org/pdf/2406.07280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuto Igarashi, Yuki Saito, Kentaro Seki, Shinnosuke Takamichi, Ryuichi Yamamoto, Kentaro Tachibana, Hiroshi Saruwatari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07280">Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose noise-robust voice conversion (VC) which takes into account the recording quality and environment of noisy source speech. Conventional denoising training improves the noise robustness of a VC model by learning noisy-to-clean VC process. However, the naturalness of the converted speech is limited when the noise of the source speech is unseen during the training. To this end, our proposed training conditions a VC model on two latent variables representing the recording quality and environment of the source speech. These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner. As a result, the trained VC model can explicitly learn information about speech degradation during the training. Objective and subjective evaluations show that our training improves the quality of the converted speech compared to the conventional training.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2311.15846.pdf' target='_blank'>https://arxiv.org/pdf/2311.15846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Qingbo Wu, Desen Yuan, King Ngi Ngan, Hongliang Li, Fanman Meng, Linfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15846">Learning with Noisy Low-Cost MOS for Image Quality Assessment via Dual-Bias Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning based image quality assessment (IQA) models have obtained impressive performance with the help of reliable subjective quality labels, where mean opinion score (MOS) is the most popular choice. However, in view of the subjective bias of individual annotators, the labor-abundant MOS (LA-MOS) typically requires a large collection of opinion scores from multiple annotators for each image, which significantly increases the learning cost. In this paper, we aim to learn robust IQA models from low-cost MOS (LC-MOS), which only requires very few opinion scores or even a single opinion score for each image. More specifically, we consider the LC-MOS as the noisy observation of LA-MOS and enforce the IQA model learned from LC-MOS to approach the unbiased estimation of LA-MOS. In this way, we represent the subjective bias between LC-MOS and LA-MOS, and the model bias between IQA predictions learned from LC-MOS and LA-MOS (i.e., dual-bias) as two latent variables with unknown parameters. By means of the expectation-maximization based alternating optimization, we can jointly estimate the parameters of the dual-bias, which suppresses the misleading of LC-MOS via a gated dual-bias calibration (GDBC) module. To the best of our knowledge, this is the first exploration of robust IQA model learning from noisy low-cost labels. Theoretical analysis and extensive experiments on four popular IQA datasets show that the proposed method is robust toward different bias rates and annotation numbers and significantly outperforms the other learning based IQA models when only LC-MOS is available. Furthermore, we also achieve comparable performance with respect to the other models learned with LA-MOS.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2410.20309.pdf' target='_blank'>https://arxiv.org/pdf/2410.20309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Lei, Yih-Chung Tham, Jocelyn Hui Lin Goh, Yangqin Feng, Yang Bai, Zhi Da Soh, Rick Siow Mong Goh, Xinxing Xu, Yong Liu, Ching-Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20309">Enhancing Community Vision Screening -- AI Driven Retinal Photography for Early Disease Detection and Patient Trust</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Community vision screening plays a crucial role in identifying individuals with vision loss and preventing avoidable blindness, particularly in rural communities where access to eye care services is limited. Currently, there is a pressing need for a simple and efficient process to screen and refer individuals with significant eye disease-related vision loss to tertiary eye care centers for further care. An ideal solution should seamlessly and readily integrate with existing workflows, providing comprehensive initial screening results to service providers, thereby enabling precise patient referrals for timely treatment. This paper introduces the Enhancing Community Vision Screening (ECVS) solution, which addresses the aforementioned concerns with a novel and feasible solution based on simple, non-invasive retinal photography for the detection of pathology-based visual impairment. Our study employs four distinct deep learning models: RETinal photo Quality Assessment (RETQA), Pathology Visual Impairment detection (PVI), Eye Disease Diagnosis (EDD) and Visualization of Lesion Regions of the eye (VLR). We conducted experiments on over 10 datasets, totaling more than 80,000 fundus photos collected from various sources. The models integrated into ECVS achieved impressive AUC scores of 0.98 for RETQA, 0.95 for PVI, and 0.90 for EDD, along with a DICE coefficient of 0.48 for VLR. These results underscore the promising capabilities of ECVS as a straightforward and scalable method for community-based vision screening.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2401.15042.pdf' target='_blank'>https://arxiv.org/pdf/2401.15042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15042">PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA's demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at \url{https://proxy-qa.com}.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2308.12001.pdf' target='_blank'>https://arxiv.org/pdf/2308.12001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12001">Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) constitutes a fundamental task within the field of computer vision, yet it remains an unresolved challenge, owing to the intricate distortion conditions, diverse image contents, and limited availability of data. Recently, the community has witnessed the emergence of numerous large-scale pretrained foundation models, which greatly benefit from dramatically increased data and parameter capacities. However, it remains an open problem whether the scaling law in high-level tasks is also applicable to IQA task which is closely related to low-level clues. In this paper, we demonstrate that with proper injection of local distortion features, a larger pretrained and fixed foundation model performs better in IQA tasks. Specifically, for the lack of local distortion structure and inductive bias of vision transformer (ViT), alongside the large-scale pretrained ViT, we use another pretrained convolution neural network (CNN), which is well known for capturing the local structure, to extract multi-scale image features. Further, we propose a local distortion extractor to obtain local distortion features from the pretrained CNN and a local distortion injector to inject the local distortion features into ViT. By only training the extractor and injector, our method can benefit from the rich knowledge in the powerful foundation models and achieve state-of-the-art performance on popular IQA datasets, indicating that IQA is not only a low-level problem but also benefits from stronger high-level features drawn from large-scale pretrained models.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2304.14672.pdf' target='_blank'>https://arxiv.org/pdf/2304.14672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Liang Liao, Annan Wang, Chaofeng Chen, Jingwen Hou, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14672">Towards Robust Text-Prompted Semantic Criterion for In-the-Wild Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of videos collected during in-the-wild natural settings has pushed the development of effective Video Quality Assessment (VQA) methodologies. Contemporary supervised opinion-driven VQA strategies predominantly hinge on training from expensive human annotations for quality scores, which limited the scale and distribution of VQA datasets and consequently led to unsatisfactory generalization capacity of methods driven by these data. On the other hand, although several handcrafted zero-shot quality indices do not require training from human opinions, they are unable to account for the semantics of videos, rendering them ineffective in comprehending complex authentic distortions (e.g., white balance, exposure) and assessing the quality of semantic content within videos. To address these challenges, we introduce the text-prompted Semantic Affinity Quality Index (SAQI) and its localized version (SAQI-Local) using Contrastive Language-Image Pre-training (CLIP) to ascertain the affinity between textual prompts and visual features, facilitating a comprehensive examination of semantic quality concerns without the reliance on human quality annotations. By amalgamating SAQI with existing low-level metrics, we propose the unified Blind Video Quality Index (BVQI) and its improved version, BVQI-Local, which demonstrates unprecedented performance, surpassing existing zero-shot indices by at least 24\% on all datasets. Moreover, we devise an efficient fine-tuning scheme for BVQI-Local that jointly optimizes text prompts and final fusion weights, resulting in state-of-the-art performance and superior generalization ability in comparison to prevalent opinion-driven VQA methods. We conduct comprehensive analyses to investigate different quality concerns of distinct indices, demonstrating the effectiveness and rationality of our design.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2302.13269.pdf' target='_blank'>https://arxiv.org/pdf/2302.13269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Liang Liao, Jingwen Hou, Chaofeng Chen, Erli Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13269">Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent learning-based video quality assessment (VQA) algorithms are expensive to implement due to the cost of data collection of human quality opinions, and are less robust across various scenarios due to the biases of these opinions. This motivates our exploration on opinion-unaware (a.k.a zero-shot) VQA approaches. Existing approaches only considers low-level naturalness in spatial or temporal domain, without considering impacts from high-level semantics. In this work, we introduce an explicit semantic affinity index for opinion-unaware VQA using text-prompts in the contrastive language-image pre-training (CLIP) model. We also aggregate it with different traditional low-level naturalness indexes through gaussian normalization and sigmoid rescaling strategies. Composed of aggregated semantic and technical metrics, the proposed Blind Unified Opinion-Unaware Video Quality Index via Semantic and Technical Metric Aggregation (BUONA-VISTA) outperforms existing opinion-unaware VQA methods by at least 20% improvements, and is more robust than opinion-aware approaches.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2212.03447.pdf' target='_blank'>https://arxiv.org/pdf/2212.03447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.03447">Integration of Pre-trained Protein Language Models into Geometric Deep Learning Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several previous studies consider combining these different protein modalities to promote the representation power of geometric neural networks, but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines. Strong evidence indicates that the incorporation of protein language models' knowledge enhances geometric networks' capacity by a significant margin and can be generalized to complex tasks.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2511.07017.pdf' target='_blank'>https://arxiv.org/pdf/2511.07017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruida Hu, Xinchen Wang, Xin-Cheng Wen, Zhao Zhang, Bo Jiang, Pengfei Gao, Chao Peng, Cuiyun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07017">Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review. We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries. ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2510.17130.pdf' target='_blank'>https://arxiv.org/pdf/2510.17130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17130">SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code generation, the task of creating executable programs from natural language requirements, has recently seen tremendous advances through Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to develop high-level reasoning plans before writing code. Recent research has proposed various methods to enhance models' CoT reasoning for code generation such as prompt engineering and supervised fine-tuning. However, existing approaches still face three critical limitations: (1) limited exploration of diverse reasoning paths, which constrains generalization across various programming scenarios, (2) lack of quality assessment for intermediate reasoning steps, which hampers the reliability of the generated plans and code, and (3) the potential negative impact of "overthinking", potentially leading to unnecessarily complex and incorrect solutions. To address these limitations, we frame CoT code generation as a decision making problem and present SEER, a SElf-Exploring deep Reasoning framework that enables accurate and adaptive reasoning for code generation. SEER introduces three key components: (1) Diverse reasoning path exploration, which aims at exploring diverse reasoning paths and annotating intermediate steps without relying on manual experts or closed-source proprietary models; (2) Reasoning quality-aware model training, which trains a policy model for generating candidate reasoning steps and a value model for assessing their quality; and (3) Adaptive CoT reasoning, which dynamically switches between direct generation and step-by-step reasoning for different problems.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2508.13957.pdf' target='_blank'>https://arxiv.org/pdf/2508.13957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Atzori, Fadi Boutros, Naser Damer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13957">ViT-FIQA: Assessing Face Image Quality using Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Image Quality Assessment (FIQA) aims to predict the utility of a face image for face recognition (FR) systems. State-of-the-art FIQA methods mainly rely on convolutional neural networks (CNNs), leaving the potential of Vision Transformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a novel approach that extends standard ViT backbones, originally optimized for FR, through a learnable quality token designed to predict a scalar utility score for any given face image. The learnable quality token is concatenated with the standard image patch tokens, and the whole sequence is processed via global self-attention by the ViT encoders to aggregate contextual information across all patches. At the output of the backbone, ViT-FIQA branches into two heads: (1) the patch tokens are passed through a fully connected layer to learn discriminative face representations via a margin-penalty softmax loss, and (2) the quality token is fed into a regression head to learn to predict the face sample's utility. Extensive experiments on challenging benchmarks and several FR models, including both CNN- and ViT-based architectures, demonstrate that ViT-FIQA consistently achieves top-tier performance. These results underscore the effectiveness of transformer-based architectures in modeling face image utility and highlight the potential of ViTs as a scalable foundation for future FIQA research https://cutt.ly/irHlzXUC.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2503.07032.pdf' target='_blank'>https://arxiv.org/pdf/2503.07032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Qin, Qianhui Gui, Mouxiao Bian, Rui Wang, Hong Ge, Dandan Yao, Ziying Sun, Yuan Zhao, Yu Zhang, Hui Shi, Dongdong Wang, Chenxin Song, Shenghong Ju, Lihao Liu, Junjun He, Jie Xu, Yuan-Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07032">Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical imaging quality control (QC) is essential for accurate diagnosis, yet traditional QC methods remain labor-intensive and subjective. To address this challenge, in this study, we establish a standardized dataset and evaluation framework for medical imaging QC, systematically assessing large language models (LLMs) in image quality assessment and report standardization. Specifically, we first constructed and anonymized a dataset of 161 chest X-ray (CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs, including Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on recall, precision, and F1 score to detect technical errors and inconsistencies. Experimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90 in CXR tasks, demonstrating strong generalization but limited fine-grained performance. DeepSeek-R1 excelled in CT report auditing with a 62.23\% recall rate, outperforming other models. However, its distilled variants performed poorly, while InternLM2.5-7B-chat exhibited the highest additional discovery rate, indicating broader but less precise error detection. These findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash demonstrating superior performance.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2412.01249.pdf' target='_blank'>https://arxiv.org/pdf/2412.01249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Zhenyu Zhang, Yanyan Zhao, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01249">Data Uncertainty-Aware Learning for Multimodal Aspect-based Sentiment Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a fine-grained task, multimodal aspect-based sentiment analysis (MABSA) mainly focuses on identifying aspect-level sentiment information in the text-image pair. However, we observe that it is difficult to recognize the sentiment of aspects in low-quality samples, such as those with low-resolution images that tend to contain noise. And in the real world, the quality of data usually varies for different samples, such noise is called data uncertainty. But previous works for the MABSA task treat different quality samples with the same importance and ignored the influence of data uncertainty. In this paper, we propose a novel data uncertainty-aware multimodal aspect-based sentiment analysis approach, UA-MABSA, which weighted the loss of different samples by the data quality and difficulty. UA-MABSA adopts a novel quality assessment strategy that takes into account both the image quality and the aspect-based cross-modal relevance, thus enabling the model to pay more attention to high-quality and challenging samples. Extensive experiments show that our method achieves state-of-the-art (SOTA) performance on the Twitter-2015 dataset. Further analysis demonstrates the effectiveness of the quality assessment strategy.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2404.12203.pdf' target='_blank'>https://arxiv.org/pdf/2404.12203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Niklas Kolf, Naser Damer, Fadi Boutros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12203">GraFIQs: Face Image Quality Assessment Using Gradient Magnitudes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Image Quality Assessment (FIQA) estimates the utility of face images for automated face recognition (FR) systems. We propose in this work a novel approach to assess the quality of face images based on inspecting the required changes in the pre-trained FR model weights to minimize differences between testing samples and the distribution of the FR training dataset. To achieve that, we propose quantifying the discrepancy in Batch Normalization statistics (BNS), including mean and variance, between those recorded during FR training and those obtained by processing testing samples through the pretrained FR model. We then generate gradient magnitudes of pretrained FR weights by backpropagating the BNS through the pretrained model. The cumulative absolute sum of these gradient magnitudes serves as the FIQ for our approach. Through comprehensive experimentation, we demonstrate the effectiveness of our training-free and quality labeling-free approach, achieving competitive performance to recent state-of-theart FIQA approaches without relying on quality labeling, the need to train regression networks, specialized architectures, or designing and optimizing specific loss functions.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2112.06592.pdf' target='_blank'>https://arxiv.org/pdf/2112.06592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Boutros, Meiling Fang, Marcel Klemt, Biying Fu, Naser Damer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.06592">CR-FIQA: Face Image Quality Assessment by Learning Sample Relative Classifiability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quality of face images significantly influences the performance of underlying face recognition algorithms. Face image quality assessment (FIQA) estimates the utility of the captured image in achieving reliable and accurate recognition performance. In this work, we propose a novel learning paradigm that learns internal network observations during the training process. Based on that, our proposed CR-FIQA uses this paradigm to estimate the face image quality of a sample by predicting its relative classifiability. This classifiability is measured based on the allocation of the training sample feature representation in angular space with respect to its class center and the nearest negative class center. We experimentally illustrate the correlation between the face image quality and the sample relative classifiability. As such property is only observable for the training dataset, we propose to learn this property from the training dataset and utilize it to predict the quality measure on unseen samples. This training is performed simultaneously while optimizing the class centers by an angular margin penalty-based softmax loss used for face recognition model training. Through extensive evaluation experiments on eight benchmarks and four face recognition models, we demonstrate the superiority of our proposed CR-FIQA over state-of-the-art (SOTA) FIQA algorithms.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2410.17834.pdf' target='_blank'>https://arxiv.org/pdf/2410.17834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danilo de Oliveira, Julius Richter, Jean-Marie Lemercier, Simon Welker, Timo Gerkmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17834">Non-intrusive Speech Quality Assessment with Diffusion Models Trained on Clean Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have found great success in generating high quality, natural samples of speech, but their potential for density estimation for speech has so far remained largely unexplored. In this work, we leverage an unconditional diffusion model trained only on clean speech for the assessment of speech quality. We show that the quality of a speech utterance can be assessed by estimating the likelihood of a corresponding sample in the terminating Gaussian distribution, obtained via a deterministic noising process. The resulting method is purely unsupervised, trained only on clean speech, and therefore does not rely on annotations. Our diffusion-based approach leverages clean speech priors to assess quality based on how the input relates to the learned distribution of clean data. Our proposed log-likelihoods show promising results, correlating well with intrusive speech quality metrics and showing the best correlation with human scores in a listening experiment.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2306.07486.pdf' target='_blank'>https://arxiv.org/pdf/2306.07486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Min Zhang, Shimin Tao, Minghan Wang, Daimeng Wei, Yanfei Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07486">Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-lingual Machine Translation (MT) quality estimation plays a crucial role in evaluating translation performance. GEMBA, the first MT quality assessment metric based on Large Language Models (LLMs), employs one-step prompting to achieve state-of-the-art (SOTA) in system-level MT quality estimation; however, it lacks segment-level analysis. In contrast, Chain-of-Thought (CoT) prompting outperforms one-step prompting by offering improved reasoning and explainability. In this paper, we introduce Knowledge-Prompted Estimator (KPE), a CoT prompting method that combines three one-step prompting techniques, including perplexity, token-level similarity, and sentence-level similarity. This method attains enhanced performance for segment-level estimation compared with previous deep learning models and one-step prompting approaches. Furthermore, supplementary experiments on word-level visualized alignment demonstrate that our KPE method significantly improves token alignment compared with earlier models and provides better interpretability for MT quality estimation. Code will be released upon publication.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2302.05776.pdf' target='_blank'>https://arxiv.org/pdf/2302.05776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohit Prabhushankar, Ghassan AlRegib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05776">Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper conjectures and validates a framework that allows for action during inference in supervised neural networks. Supervised neural networks are constructed with the objective to maximize their performance metric in any given task. This is done by reducing free energy and its associated surprisal during training. However, the bottom-up inference nature of supervised networks is a passive process that renders them fallible to noise. In this paper, we provide a thorough background of supervised neural networks, both generative and discriminative, and discuss their functionality from the perspective of free energy principle. We then provide a framework for introducing action during inference. We introduce a new measurement called stochastic surprisal that is a function of the network, the input, and any possible action. This action can be any one of the outputs that the neural network has learnt, thereby lending stochasticity to the measurement. Stochastic surprisal is validated on two applications: Image Quality Assessment and Recognition under noisy conditions. We show that, while noise characteristics are ignored to make robust recognition, they are analyzed to estimate image quality scores. We apply stochastic surprisal on two applications, three datasets, and as a plug-in on twelve networks. In all, it provides a statistically significant increase among all measures. We conclude by discussing the implications of the proposed stochastic surprisal in other areas of cognitive psychology including expectancy-mismatch and abductive reasoning.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2506.11549.pdf' target='_blank'>https://arxiv.org/pdf/2506.11549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Wang, Wen Lu, Jie Li, Lihuo He, Maoguo Gong, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11549">EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Free-energy-guided self-repair mechanisms have shown promising results in image quality assessment (IQA), but remain under-explored in video quality assessment (VQA), where temporal dynamics and model constraints pose unique challenges. Unlike static images, video content exhibits richer spatiotemporal complexity, making perceptual restoration more difficult. Moreover, VQA systems often rely on pre-trained backbones, which limits the direct integration of enhancement modules without affecting model stability. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs-resized full-frame images and patch-based fragments-to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2402.14401.pdf' target='_blank'>https://arxiv.org/pdf/2402.14401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14401">Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.07525.pdf' target='_blank'>https://arxiv.org/pdf/2509.07525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Amprimo, Alberto Ancilotto, Alessandro Savino, Fabio Quazzolo, Claudia Ferraris, Gabriella Olmo, Elisabetta Farella, Stefano Di Carlo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07525">EHWGesture -- A dataset for multimodal understanding of clinical gestures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2508.05512.pdf' target='_blank'>https://arxiv.org/pdf/2508.05512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Abdallah, Mahmoud Abdalla, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05512">RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the quality of retrieval-augmented generation (RAG) and document reranking systems remains challenging due to the lack of scalable, user-centric, and multi-perspective evaluation tools. We introduce RankArena, a unified platform for comparing and analysing the performance of retrieval pipelines, rerankers, and RAG systems using structured human and LLM-based feedback as well as for collecting such feedback. RankArena supports multiple evaluation modes: direct reranking visualisation, blind pairwise comparisons with human or LLM voting, supervised manual document annotation, and end-to-end RAG answer quality assessment. It captures fine-grained relevance feedback through both pairwise preferences and full-list annotations, along with auxiliary metadata such as movement metrics, annotation time, and quality ratings. The platform also integrates LLM-as-a-judge evaluation, enabling comparison between model-generated rankings and human ground truth annotations. All interactions are stored as structured evaluation datasets that can be used to train rerankers, reward models, judgment agents, or retrieval strategy selectors. Our platform is publicly available at https://rankarena.ngrok.io/, and the Demo video is provided https://youtu.be/jIYAP4PaSSI.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2501.02751.pdf' target='_blank'>https://arxiv.org/pdf/2501.02751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Miao, Jun Jia, Yankun Cao, Yingjie Zhou, Yanwei Jiang, Zhi Liu, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02751">Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound Imaging?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the dramatic upsurge in the volume of ultrasound examinations, low-quality ultrasound imaging has gradually increased due to variations in operator proficiency and imaging circumstances, imposing a severe burden on diagnosis accuracy and even entailing the risk of restarting the diagnosis in critical cases. To assist clinicians in selecting high-quality ultrasound images and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a comprehensive benchmark that systematically evaluates multimodal large language models (MLLMs) on quality assessment tasks of ultrasound images. Ultrasound-QBench establishes two datasets collected from diverse sources: IVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863 images. These images encompassing common ultrasound imaging artifacts are annotated by professional ultrasound experts and classified into three quality levels: high, medium, and low. To better evaluate MLLMs, we decompose the quality assessment task into three dimensionalities: qualitative classification, quantitative scoring, and comparative assessment. The evaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates that MLLMs possess preliminary capabilities for low-level visual tasks in ultrasound image quality classification. We hope this benchmark will inspire the research community to delve deeper into uncovering and enhancing the untapped potential of MLLMs for medical imaging tasks.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2407.19675.pdf' target='_blank'>https://arxiv.org/pdf/2407.19675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wulian Yun, Mengshi Qi, Fei Peng, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19675">Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing action quality assessment (AQA) methods often require a large number of label annotations for fully supervised learning, which are laborious and expensive. In practice, the labeled data are difficult to obtain because the AQA annotation process requires domain-specific expertise. In this paper, we propose a novel semi-supervised method, which can be utilized for better assessment of the AQA task by exploiting a large amount of unlabeled data and a small portion of labeled data. Differing from the traditional teacher-student network, we propose a teacher-reference-student architecture to learn both unlabeled and labeled data, where the teacher network and the reference network are used to generate pseudo-labels for unlabeled data to supervise the student network. Specifically, the teacher predicts pseudo-labels by capturing high-level features of unlabeled data. The reference network provides adequate supervision of the student network by referring to additional action information. Moreover, we introduce confidence memory to improve the reliability of pseudo-labels by storing the most accurate ever output of the teacher network and reference network. To validate our method, we conduct extensive experiments on three AQA benchmark datasets. Experimental results show that our method achieves significant improvements and outperforms existing semi-supervised AQA methods.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2401.02841.pdf' target='_blank'>https://arxiv.org/pdf/2401.02841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi An, Mengshi Qi, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02841">Multi-Stage Contrastive Regression for Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been growing interest in the video-based action quality assessment (AQA). Most existing methods typically solve AQA problem by considering the entire video yet overlooking the inherent stage-level characteristics of actions. To address this issue, we design a novel Multi-stage Contrastive Regression (MCoRe) framework for the AQA task. This approach allows us to efficiently extract spatial-temporal information, while simultaneously reducing computational costs by segmenting the input video into multiple stages or procedures. Inspired by the graph contrastive learning, we propose a new stage-wise contrastive learning loss function to enhance performance. As a result, MCoRe demonstrates the state-of-the-art result so far on the widely-adopted fine-grained AQA dataset.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2201.09407.pdf' target='_blank'>https://arxiv.org/pdf/2201.09407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjiao Wu, Luwei Xiao, Xiangcheng Du, Yingbin Zheng, Xin Li, Tianlong Ma, Cheng Jin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.09407">Cross-Domain Document Layout Analysis Using Document Style Guide</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The document layout analysis (DLA) aims to decompose document images into high-level semantic areas (i.e., figures, tables, texts, and background). Creating a DLA framework with strong generalization capabilities is a challenge due to document objects are diversity in layout, size, aspect ratio, texture, etc. Many researchers devoted this challenge by synthesizing data to build large training sets. However, the synthetic training data has different styles and erratic quality. Besides, there is a large gap between the source data and the target data. In this paper, we propose an unsupervised cross-domain DLA framework based on document style guidance. We integrated the document quality assessment and the document cross-domain analysis into a unified framework. Our framework is composed of three components, Document Layout Generator (GLD), Document Elements Decorator(GED), and Document Style Discriminator(DSD). The GLD is used to document layout generates, the GED is used to document layout elements fill, and the DSD is used to document quality assessment and cross-domain guidance. First, we apply GLD to predict the positions of the generated document. Then, we design a novel algorithm based on aesthetic guidance to fill the document positions. Finally, we use contrastive learning to evaluate the quality assessment of the document. Besides, we design a new strategy to change the document quality assessment component into a document cross-domain style guide component. Our framework is an unsupervised document layout analysis framework. We have proved through numerous experiments that our proposed method has achieved remarkable performance.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2508.06125.pdf' target='_blank'>https://arxiv.org/pdf/2508.06125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Zhang, Xianfang Zeng, Kangcong Li, Gang Yu, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06125">SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose SC-Captioner, a reinforcement learning framework that enables the self-correcting capability of image caption models. Our crucial technique lies in the design of the reward function to incentivize accurate caption corrections. Specifically, the predicted and reference captions are decomposed into object, attribute, and relation sets using scene-graph parsing algorithms. We calculate the set difference between sets of initial and self-corrected captions to identify added and removed elements. These elements are matched against the reference sets to calculate correctness bonuses for accurate refinements and mistake punishments for wrong additions and removals, thereby forming the final reward. For image caption quality assessment, we propose a set of metrics refined from CAPTURE that alleviate its incomplete precision evaluation and inefficient relation matching problems. Furthermore, we collect a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K diverse images from COCO dataset. Experiments show that applying SC-Captioner on large visual-language models can generate better image captions across various scenarios, significantly outperforming the direct preference optimization training strategy.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2504.15281.pdf' target='_blank'>https://arxiv.org/pdf/2504.15281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng, Jiacheng Bao, Shengqi Liu, Yiying Yang, Xianfang Zeng, Gang Yu, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15281">StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2503.03422.pdf' target='_blank'>https://arxiv.org/pdf/2503.03422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariusz Trzeciakiewicz, Aleixo Cambeiro Barreiro, Niklas Gard, Anna Hilsmann, Peter Eisert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03422">Automatic Drywall Analysis for Progress Tracking and Quality Control in Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digitalization in the construction industry has become essential, enabling centralized, easy access to all relevant information of a building. Automated systems can facilitate the timely and resource-efficient documentation of changes, which is crucial for key processes such as progress tracking and quality control. This paper presents a method for image-based automated drywall analysis enabling construction progress and quality assessment through on-site camera systems. Our proposed solution integrates a deep learning-based instance segmentation model to detect and classify various drywall elements with an analysis module to cluster individual wall segments, estimate camera perspective distortions, and apply the corresponding corrections. This system extracts valuable information from images, enabling more accurate progress tracking and quality assessment on construction sites. Our main contributions include a fully automated pipeline for drywall analysis, improving instance segmentation accuracy through architecture modifications and targeted data augmentation, and a novel algorithm to extract important information from the segmentation results. Our modified model, enhanced with data augmentation, achieves significantly higher accuracy compared to other architectures, offering more detailed and precise information than existing approaches. Combined with the proposed drywall analysis steps, it enables the reliable automation of construction progress and quality assessment.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2103.06205.pdf' target='_blank'>https://arxiv.org/pdf/2103.06205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Kofler, Ivan Ezhov, Fabian Isensee, Fabian Balsiger, Christoph Berger, Maximilian Koerner, Beatrice Demiray, Julia Rackerseder, Johannes Paetzold, Hongwei Li, Suprosanna Shit, Richard McKinley, Marie Piraud, Spyridon Bakas, Claus Zimmer, Nassir Navab, Jan Kirschke, Benedikt Wiestler, Bjoern Menze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.06205">Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metrics optimized in complex machine learning tasks are often selected in an ad-hoc manner. It is unknown how they align with human expert perception. We explore the correlations between established quantitative segmentation quality metrics and qualitative evaluations by professionally trained human raters. Therefore, we conduct psychophysical experiments for two complex biomedical semantic segmentation problems. We discover that current standard metrics and loss functions correlate only moderately with the segmentation quality assessment of experts. Importantly, this effect is particularly pronounced for clinically relevant structures, such as the enhancing tumor compartment of glioma in brain magnetic resonance and grey matter in ultrasound imaging. It is often unclear how to optimize abstract metrics, such as human expert perception, in convolutional neural network (CNN) training. To cope with this challenge, we propose a novel strategy employing techniques of classical statistics to create complementary compound loss functions to better approximate human expert perception. Across all rating experiments, human experts consistently scored computer-generated segmentations better than the human-curated reference labels. Our results, therefore, strongly question many current practices in medical image segmentation and provide meaningful cues for future research.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2510.11369.pdf' target='_blank'>https://arxiv.org/pdf/2510.11369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11369">Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning-based image quality assessment (IQA) models trained through reinforcement learning (RL) exhibit exceptional generalization, yet the underlying mechanisms and critical factors driving this capability remain underexplored in current research. Moreover, despite their superior performance, these models incur inference energy usage and latency orders of magnitude higher than their earlier counterparts, restricting their deployment in specific scenarios. Through extensive experiments, this paper verifies and elaborates that through RL training, MLLMs leverage their reasoning capability to convert redundant visual representations into compact, cross-domain aligned text representations. This conversion is precisely the source of the generalization exhibited by these reasoning-based IQA models. Building on this fundamental insight, we propose a novel algorithm, RALI, which employs contrastive learning to directly align images with these generalizable text representations learned by RL. This approach eliminates the reliance on reasoning processes and even obviates the need to load an LLM. For the quality scoring task, this framework achieves generalization performance comparable to reasoning-based models while requiring less than 5% of their model parameters and inference time.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2509.10127.pdf' target='_blank'>https://arxiv.org/pdf/2509.10127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Hu, Jianxun Lian, Zheyuan Xiao, Max Xiong, Yuxuan Lei, Tianfu Wang, Kaize Ding, Ziang Xiao, Nicholas Jing Yuan, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10127">Population-Aligned Persona Generation for LLM-based Social Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2508.00454.pdf' target='_blank'>https://arxiv.org/pdf/2508.00454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00454">Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the "LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2506.18564.pdf' target='_blank'>https://arxiv.org/pdf/2506.18564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Zhang, Weiqi Li, Shijie Zhao, Junlin Li, Li Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18564">VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in AI-generated content (AIGC) have led to the emergence of powerful text-to-video generation models. Despite these successes, evaluating the quality of AIGC-generated videos remains challenging due to limited generalization, lack of temporal awareness, heavy reliance on large-scale annotated datasets, and the lack of effective interaction with generation models. Most current approaches rely on supervised finetuning of vision-language models (VLMs), which often require large-scale annotated datasets and tend to decouple understanding and generation. To address these shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for AIGC video quality assessment. Our approach features: (1) a progressive video quality learning scheme that combines image quality warm-up, general task-specific temporal learning, and joint optimization with the video generation model; (2) the design of multi-dimension scoring rewards, preference comparison rewards, and temporal modeling rewards to enhance both generalization and specialization in video quality evaluation. Extensive experiments demonstrate that VQ-Insight consistently outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring, bringing significant improvements for video generation tasks.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2505.17619.pdf' target='_blank'>https://arxiv.org/pdf/2505.17619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Wang, De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Nu-Fang Xiao, Jian-Long Hao, Ming-Yuan Liu, Zeng-Guang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17619">CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic X-ray angiographies generated by modern generative models hold great potential to reduce the use of contrast agents in vascular interventional procedures. However, low-quality synthetic angiographies can significantly increase procedural risk, underscoring the need for reliable image quality assessment (IQA) methods. Existing IQA models, however, fail to leverage auxiliary images as references during evaluation and lack fine-grained, task-specific metrics necessary for clinical relevance. To address these limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based framework that predicts fine-grained quality scores by effectively incorporating auxiliary information from related images. In the absence of angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic angiographies along with score annotations. To ensure clinically meaningful assessment, three task-specific evaluation metrics are defined. Furthermore, a Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance image representations by adaptively fusing and routing visual tokens to metric-specific branches. Extensive experiments on the CAS-3K dataset demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods by a considerable margin.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2410.09507.pdf' target='_blank'>https://arxiv.org/pdf/2410.09507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Artem Bobrov, Runcong Zhao, Cesare Aloisi, Yulan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09507">AERA Chat: An Interactive Platform for Automated Explainable Student Answer Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainability in automated student answer scoring systems is critical for building trust and enhancing usability among educators. Yet, generating high-quality assessment rationales remains challenging due to the scarcity of annotated data and the prohibitive cost of manual verification, prompting heavy reliance on rationales produced by large language models (LLMs), which are often noisy and unreliable. To address these limitations, we present AERA Chat, an interactive visualization platform designed for automated explainable student answer assessment. AERA Chat leverages multiple LLMs to concurrently score student answers and generate explanatory rationales, offering innovative visualization features that highlight critical answer components and rationale justifications. The platform also incorporates intuitive annotation and evaluation tools, supporting educators in marking tasks and researchers in evaluating rationale quality from different models. We demonstrate the effectiveness of our platform through evaluations of multiple rationale-generation methods on several datasets, showcasing its capability for facilitating robust rationale evaluation and comparative analysis.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2307.04296.pdf' target='_blank'>https://arxiv.org/pdf/2307.04296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoyang Xie, Jinbao Wang, Yawen Huang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04296">K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of how to assess cross-modality medical image synthesis has been largely unexplored. The most used measures like PSNR and SSIM focus on analyzing the structural features but neglect the crucial lesion location and fundamental k-space speciality of medical images. To overcome this problem, we propose a new metric K-CROSS to spur progress on this challenging problem. Specifically, K-CROSS uses a pre-trained multi-modality segmentation network to predict the lesion location, together with a tumor encoder for representing features, such as texture details and brightness intensities. To further reflect the frequency-specific information from the magnetic resonance imaging principles, both k-space features and vision features are obtained and employed in our comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities. As a consequence, the features learned from lesion regions, k-space, and anatomical structures are all captured, which serve as our quality evaluators. We evaluate the performance by constructing a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments. Extensive experiments demonstrate that the proposed method outperforms other metrics, especially in comparison with the radiologists on NIRPS.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2511.07812.pdf' target='_blank'>https://arxiv.org/pdf/2511.07812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenchen Tang, Songlin Yang, Bo Peng, Zichuan Wang, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07812">Revisiting MLLM Based Image Quality Assessment: Errors and Remedy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of multi-modal large language models (MLLMs) has boosted the task of image quality assessment (IQA). However, a key challenge arises from the inherent mismatch between the discrete token outputs of MLLMs and the continuous nature of quality scores required by IQA tasks. This discrepancy significantly hinders the performance of MLLM-based IQA methods. Previous approaches that convert discrete token predictions into continuous scores often suffer from conversion errors. Moreover, the semantic confusion introduced by level tokens (e.g., ``good'') further constrains the performance of MLLMs on IQA tasks and degrades their original capabilities for related tasks. To tackle these problems, we provide a theoretical analysis of the errors inherent in previous approaches and, motivated by this analysis, propose a simple yet effective framework, Q-Scorer. This framework incorporates a lightweight regression module and IQA-specific score tokens into the MLLM pipeline. Extensive experiments demonstrate that Q-Scorer achieves state-of-the-art performance across multiple IQA benchmarks, generalizes well to mixed datasets, and further improves when combined with other methods.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2510.08978.pdf' target='_blank'>https://arxiv.org/pdf/2510.08978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichuan Wang, Bo Peng, Songlin Yang, Zhenchen Tang, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08978">HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent text-to-image (T2I) models have significantly improved the overall visual quality of generated images, they still struggle in the generation of accurate details in complex local regions, especially human hands. Generated hands often exhibit structural distortions and unrealistic textures, which can be very noticeable even when the rest of the body is well-generated. However, the quality assessment of hand regions remains largely neglected, limiting downstream task performance like human-centric generation quality optimization and AIGC detection. To address this, we propose the first quality assessment task targeting generated hand regions and showcase its abundant downstream applications. We first introduce the HandPair dataset for training hand quality assessment models. It consists of 48k images formed by high- and low-quality hand pairs, enabling low-cost, efficient supervision without manual annotation. Based on it, we develop HandEval, a carefully designed hand-specific quality assessment model. It leverages the powerful visual understanding capability of Multimodal Large Language Model (MLLM) and incorporates prior knowledge of hand keypoints, gaining strong perception of hand quality. We further construct a human-annotated test set with hand images from various state-of-the-art (SOTA) T2I models to validate its quality evaluation capability. Results show that HandEval aligns better with human judgments than existing SOTA methods. Furthermore, we integrate HandEval into image generation and AIGC detection pipelines, prominently enhancing generated hand realism and detection accuracy, respectively, confirming its universal effectiveness in downstream applications. Code and dataset will be available.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2506.23874.pdf' target='_blank'>https://arxiv.org/pdf/2506.23874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Wang, Chenda Li, Wei Wang, Wangyou Zhang, Samuele Cornell, Marvin Sach, Robin Scheibler, Kohei Saijo, Yihui Fu, Zhaoheng Ni, Anurag Kumar, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23874">URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Mean Opinion Score (MOS) is fundamental to speech quality assessment. However, its acquisition requires significant human annotation. Although deep neural network approaches, such as DNSMOS and UTMOS, have been developed to predict MOS to avoid this issue, they often suffer from insufficient training data. Recognizing that the comparison of speech enhancement (SE) systems prioritizes a reliable system comparison over absolute scores, we propose URGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK takes homologous enhanced speech pairs as input to predict relative quality rankings. This pairwise paradigm efficiently utilizes limited training data, as all pairwise permutations of multiple systems constitute a training instance. Experiments across multiple open test sets demonstrate URGENT-PK's superior system-level ranking performance over state-of-the-art baselines, despite its simple network architecture and limited training data.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2408.15098.pdf' target='_blank'>https://arxiv.org/pdf/2408.15098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenchen Tang, Zichuan Wang, Bo Peng, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15098">CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality Assessment with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of generative technologies, AI-Generated Images (AIGIs) have been widely applied in various aspects of daily life. However, due to the immaturity of the technology, the quality of the generated images varies, so it is important to develop quality assessment techniques for the generated images. Although some models have been proposed to assess the quality of generated images, they are inadequate when faced with the ever-increasing and diverse categories of generated images. Consequently, the development of more advanced and effective models for evaluating the quality of generated images is urgently needed. Recent research has explored the significant potential of the visual language model CLIP in image quality assessment, finding that it performs well in evaluating the quality of natural images. However, its application to generated images has not been thoroughly investigated. In this paper, we build on this idea and further explore the potential of CLIP in evaluating the quality of generated images. We design CLIP-AGIQA, a CLIP-based regression model for quality assessment of generated images, leveraging rich visual and textual knowledge encapsulated in CLIP. Particularly, we implement multi-category learnable prompts to fully utilize the textual knowledge in CLIP for quality assessment. Extensive experiments on several generated image quality assessment benchmarks, including AGIQA-3K and AIGCIQA2023, demonstrate that CLIP-AGIQA outperforms existing IQA models, achieving excellent results in evaluating the quality of generated images.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2506.22902.pdf' target='_blank'>https://arxiv.org/pdf/2506.22902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiling Xu, Yujie Zhang, Shuting Xia, Kaifa Yang, He Huang, Ziyu Shan, Wenjie Huang, Qi Yang, Le Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22902">Point Cloud Compression and Objective Quality Assessment: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of 3D point cloud data, driven by applications in autonomous driving, robotics, and immersive environments, has led to criticals demand for efficient compression and quality assessment techniques. Unlike traditional 2D media, point clouds present unique challenges due to their irregular structure, high data volume, and complex attributes. This paper provides a comprehensive survey of recent advances in point cloud compression (PCC) and point cloud quality assessment (PCQA), emphasizing their significance for real-time and perceptually relevant applications. We analyze a wide range of handcrafted and learning-based PCC algorithms, along with objective PCQA metrics. By benchmarking representative methods on emerging datasets, we offer detailed comparisons and practical insights into their strengths and limitations. Despite notable progress, challenges such as enhancing visual fidelity, reducing latency, and supporting multimodal data remain. This survey outlines future directions, including hybrid compression frameworks and advanced feature extraction strategies, to enable more efficient, immersive, and intelligent 3D applications.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2505.12431.pdf' target='_blank'>https://arxiv.org/pdf/2505.12431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Liu, Yujie Zhang, Qi Yang, Yiling Xu, Zhu Li, Ye-Kui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12431">DPCD: A Quality Assessment Database for Dynamic Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven the demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are capable of capturing temporal changes within objects or scenes, offering a more accurate simulation of the real world. While significant progress has been made in the quality assessment research of static point cloud, little study has been done on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the development of quality-oriented applications, such as interframe compression and transmission in practical scenarios. In this paper, we introduce a large-scale DPCQA database, named DPCD, which includes 15 reference DPCs and 525 distorted DPCs from seven types of lossy compression and noise distortion. By rendering these samples to Processed Video Sequences (PVS), a comprehensive subjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21 viewers for analysis. The characteristic of contents, impact of various distortions, and accuracy of MOSs are presented to validate the heterogeneity and reliability of the proposed database. Furthermore, we evaluate the performance of several objective metrics on DPCD. The experiment results show that DPCQA is more challenge than that of static point cloud. The DPCD, which serves as a catalyst for new research endeavors on DPCQA, is publicly available at https://huggingface.co/datasets/Olivialyt/DPCD.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2501.15485.pdf' target='_blank'>https://arxiv.org/pdf/2501.15485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yipeng Liu, Qi Yang, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15485">Differentiable Low-computation Global Correlation Loss for Monotonicity Evaluation in Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a global monotonicity consistency training strategy for quality assessment, which includes a differentiable, low-computation monotonicity evaluation loss function and a global perception training mechanism. Specifically, unlike conventional ranking loss and linear programming approaches that indirectly implement the Spearman rank-order correlation coefficient (SROCC) function, our method directly converts SROCC into a loss function by making the sorting operation within SROCC differentiable and functional. Furthermore, to mitigate the discrepancies between batch optimization during network training and global evaluation of SROCC, we introduce a memory bank mechanism. This mechanism stores gradient-free predicted results from previous batches and uses them in the current batch's training to prevent abrupt gradient changes. We evaluate the performance of the proposed method on both images and point clouds quality assessment tasks, demonstrating performance gains in both cases.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2501.13387.pdf' target='_blank'>https://arxiv.org/pdf/2501.13387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yipeng Liu, Qi Yang, Yujie Zhang, Yiling Xu, Le Yang, Zhu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13387">From Images to Point Clouds: An Efficient Solution for Cross-media Blind Quality Assessment without Annotated Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel quality assessment method which can predict the perceptual quality of point clouds from new scenes without available annotations by leveraging the rich prior knowledge in images, called the Distribution-Weighted Image-Transferred Point Cloud Quality Assessment (DWIT-PCQA). Recognizing the human visual system (HVS) as the decision-maker in quality assessment regardless of media types, we can emulate the evaluation criteria for human perception via neural networks and further transfer the capability of quality prediction from images to point clouds by leveraging the prior knowledge in the images. Specifically, domain adaptation (DA) can be leveraged to bridge the images and point clouds by aligning feature distributions of the two media in the same feature space. However, the different manifestations of distortions in images and point clouds make feature alignment a difficult task. To reduce the alignment difficulty and consider the different distortion distribution during alignment, we have derived formulas to decompose the optimization objective of the conventional DA into two suboptimization functions with distortion as a transition. Specifically, through network implementation, we propose the distortion-guided biased feature alignment which integrates existing/estimated distortion distribution into the adversarial DA framework, emphasizing common distortion patterns during feature alignment. Besides, we propose the quality-aware feature disentanglement to mitigate the destruction of the mapping from features to quality during alignment with biased distortions. Experimental results demonstrate that our proposed method exhibits reliable performance compared to general blind PCQA methods without needing point cloud annotations.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2403.10066.pdf' target='_blank'>https://arxiv.org/pdf/2403.10066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10066">Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference point cloud quality assessment (NR-PCQA) aims to automatically evaluate the perceptual quality of distorted point clouds without available reference, which have achieved tremendous improvements due to the utilization of deep neural networks. However, learning-based NR-PCQA methods suffer from the scarcity of labeled data and usually perform suboptimally in terms of generalization. To solve the problem, we propose a novel contrastive pre-training framework tailored for PCQA (CoPA), which enables the pre-trained model to learn quality-aware representations from unlabeled data. To obtain anchors in the representation space, we project point clouds with different distortions into images and randomly mix their local patches to form mixed images with multiple distortions. Utilizing the generated anchors, we constrain the pre-training process via a quality-aware contrastive loss following the philosophy that perceptual quality is closely related to both content and distortion. Furthermore, in the model fine-tuning stage, we propose a semantic-guided multi-view fusion module to effectively integrate the features of projected images from multiple perspectives. Extensive experiments show that our method outperforms the state-of-the-art PCQA methods on popular benchmarks. Further investigations demonstrate that CoPA can also benefit existing learning-based PCQA models.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2403.10061.pdf' target='_blank'>https://arxiv.org/pdf/2403.10061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10061">PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference point cloud quality assessment (NR-PCQA) aims to automatically predict the perceptual quality of point clouds without reference, which has achieved remarkable performance due to the utilization of deep learning-based models. However, these data-driven models suffer from the scarcity of labeled data and perform unsatisfactorily in cross-dataset evaluations. To address this problem, we propose a self-supervised pre-training framework using masked autoencoders (PAME) to help the model learn useful representations without labels. Specifically, after projecting point clouds into images, our PAME employs dual-branch autoencoders, reconstructing masked patches from distorted images into the original patches within reference and distorted images. In this manner, the two branches can separately learn content-aware features and distortion-aware features from the projected images. Furthermore, in the model fine-tuning stage, the learned content-aware features serve as a guide to fuse the point cloud quality features extracted from different perspectives. Extensive experiments show that our method outperforms the state-of-the-art NR-PCQA methods on popular benchmarks in terms of prediction accuracy and generalizability.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2312.15663.pdf' target='_blank'>https://arxiv.org/pdf/2312.15663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Chen, Bin Hu, Chuang Niu, Tao Chen, Yuxin Li, Hongming Shan, Ge Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15663">IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in various tasks and attracted an increasing interest as a natural language interface across many domains. Recently, large vision-language models (VLMs) like BLIP-2 and GPT-4 have been intensively investigated, which learn rich vision-language correlation from image-text pairs. However, despite these developments, the application of LLMs and VLMs in image quality assessment (IQA), particularly in medical imaging, remains to be explored, which is valuable for objective performance evaluation and potential supplement or even replacement of radiologists' opinions. To this end, this paper introduces IQAGPT, an innovative image quality assessment system integrating an image quality captioning VLM with ChatGPT for generating quality scores and textual reports. First, we build a CT-IQA dataset for training and evaluation, comprising 1,000 CT slices with diverse quality levels professionally annotated. To better leverage the capabilities of LLMs, we convert annotated quality scores into semantically rich text descriptions using a prompt template. Second, we fine-tune the image quality captioning VLM on the CT-IQA dataset to generate quality descriptions. The captioning model fuses the image and text features through cross-modal attention. Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report. Our preliminary results demonstrate the feasibility of assessing image quality with large models. Remarkably, our IQAGPT outperforms GPT-4 and CLIP-IQA, as well as the multi-task classification and regression models that solely rely on images.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2309.15675.pdf' target='_blank'>https://arxiv.org/pdf/2309.15675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyang Cui, Qi Yang, Kaifa Yang, Yiling Xu, Xiaozhong Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15675">SJTU-TMQA: A quality assessment database for static mesh with texture map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, static meshes with texture maps have become one of the most prevalent digital representations of 3D shapes in various applications, such as animation, gaming, medical imaging, and cultural heritage applications. However, little research has been done on the quality assessment of textured meshes, which hinders the development of quality-oriented applications, such as mesh compression and enhancement. In this paper, we create a large-scale textured mesh quality assessment database, namely SJTU-TMQA, which includes 21 reference meshes and 945 distorted samples. The meshes are rendered into processed video sequences and then conduct subjective experiments to obtain mean opinion scores (MOS). The diversity of content and accuracy of MOS has been shown to validate its heterogeneity and reliability. The impact of various types of distortion on human perception is demonstrated. 13 state-of-the-art objective metrics are evaluated on SJTU-TMQA. The results report the highest correlation of around 0.6, indicating the need for more effective objective metrics. The SJTU-TMQA is available at https://ccccby.github.io
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2304.00996.pdf' target='_blank'>https://arxiv.org/pdf/2304.00996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Huang, Pedro F. Ferreira, Lichao Wang, Yinzhe Wu, Angelica I. Aviles-Rivero, Carola-Bibiane Schonlieb, Andrew D. Scott, Zohya Khalique, Maria Dwornik, Ramyah Rajakulasingam, Ranil De Silva, Dudley J. Pennell, Sonia Nielles-Vallespin, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00996">Deep Learning-based Diffusion Tensor Cardiac Magnetic Resonance Reconstruction: A Comparison Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In vivo cardiac diffusion tensor imaging (cDTI) is a promising Magnetic Resonance Imaging (MRI) technique for evaluating the micro-structure of myocardial tissue in the living heart, providing insights into cardiac function and enabling the development of innovative therapeutic strategies. However, the integration of cDTI into routine clinical practice is challenging due to the technical obstacles involved in the acquisition, such as low signal-to-noise ratio and long scanning times. In this paper, we investigate and implement three different types of deep learning-based MRI reconstruction models for cDTI reconstruction. We evaluate the performance of these models based on reconstruction quality assessment and diffusion tensor parameter assessment. Our results indicate that the models we discussed in this study can be applied for clinical use at an acceleration factor (AF) of $\times 2$ and $\times 4$, with the D5C5 model showing superior fidelity for reconstruction and the SwinMR model providing higher perceptual scores. There is no statistical difference with the reference for all diffusion tensor parameters at AF $\times 2$ or most DT parameters at AF $\times 4$, and the quality of most diffusion tensor parameter maps are visually acceptable. SwinMR is recommended as the optimal approach for reconstruction at AF $\times 2$ and AF $\times 4$. However, we believed the models discussed in this studies are not prepared for clinical use at a higher AF. At AF $\times 8$, the performance of all models discussed remains limited, with only half of the diffusion tensor parameters being recovered to a level with no statistical difference from the reference. Some diffusion tensor parameter maps even provide wrong and misleading information.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2210.16478.pdf' target='_blank'>https://arxiv.org/pdf/2210.16478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Shan, Qi Yang, Rui Ye, Yujie Zhang, Yiling Xu, Xiaozhong Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.16478">GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task Graph Convolutional Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of 3D vision, point cloud has become an increasingly popular 3D visual media content. Due to the irregular structure, point cloud has posed novel challenges to the related research, such as compression, transmission, rendering and quality assessment. In these latest researches, point cloud quality assessment (PCQA) has attracted wide attention due to its significant role in guiding practical applications, especially in many cases where the reference point cloud is unavailable. However, current no-reference metrics which based on prevalent deep neural network have apparent disadvantages. For example, to adapt to the irregular structure of point cloud, they require preprocessing such as voxelization and projection that introduce extra distortions, and the applied grid-kernel networks, such as Convolutional Neural Networks, fail to extract effective distortion-related features. Besides, they rarely consider the various distortion patterns and the philosophy that PCQA should exhibit shifting, scaling, and rotational invariance. In this paper, we propose a novel no-reference PCQA metric named the Graph convolutional PCQA network (GPA-Net). To extract effective features for PCQA, we propose a new graph convolution kernel, i.e., GPAConv, which attentively captures the perturbation of structure and texture. Then, we propose the multi-task framework consisting of one main task (quality regression) and two auxiliary tasks (distortion type and degree predictions). Finally, we propose a coordinate normalization module to stabilize the results of GPAConv under shift, scale and rotation transformations. Experimental results on two independent databases show that GPA-Net achieves the best performance compared to the state-of-the-art no-reference PCQA metrics, even better than some full-reference metrics in some cases.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2202.09798.pdf' target='_blank'>https://arxiv.org/pdf/2202.09798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaheer U. Saeed, Wen Yan, Yunguan Fu, Francesco Giganti, Qianye Yang, Zachary M. C. Baum, Mirabela Rusu, Richard E. Fan, Geoffrey A. Sonn, Mark Emberton, Dean C. Barratt, Yipeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.09798">Image quality assessment by overlapping task-specific and task-agnostic measures: application to prostate multiparametric MR images for cancer segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) in medical imaging can be used to ensure that downstream clinical tasks can be reliably performed. Quantifying the impact of an image on the specific target tasks, also named as task amenability, is needed. A task-specific IQA has recently been proposed to learn an image-amenability-predicting controller simultaneously with a target task predictor. This allows for the trained IQA controller to measure the impact an image has on the target task performance, when this task is performed using the predictor, e.g. segmentation and classification neural networks in modern clinical applications. In this work, we propose an extension to this task-specific IQA approach, by adding a task-agnostic IQA based on auto-encoding as the target task. Analysing the intersection between low-quality images, deemed by both the task-specific and task-agnostic IQA, may help to differentiate the underpinning factors that caused the poor target task performance. For example, common imaging artefacts may not adversely affect the target task, which would lead to a low task-agnostic quality and a high task-specific quality, whilst individual cases considered clinically challenging, which can not be improved by better imaging equipment or protocols, is likely to result in a high task-agnostic quality but a low task-specific quality. We first describe a flexible reward shaping strategy which allows for the adjustment of weighting between task-agnostic and task-specific quality scoring. Furthermore, we evaluate the proposed algorithm using a clinically challenging target task of prostate tumour segmentation on multiparametric magnetic resonance (mpMR) images, from 850 patients. The proposed reward shaping strategy, with appropriately weighted task-specific and task-agnostic qualities, successfully identified samples that need re-acquisition due to defected imaging process.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2511.11265.pdf' target='_blank'>https://arxiv.org/pdf/2511.11265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikel Robredo, Matteo Esposito, Davide Taibi, Rafael Peñaloza, Valentina Lenarduzzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11265">SQuaD: The Software Quality Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2511.01645.pdf' target='_blank'>https://arxiv.org/pdf/2511.01645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaogang Xu, Ruihang Chu, Jian Wang, Kun Zhou, Wenjie Shu, Harry Yang, Ser-Nam Lim, Hao Chen, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01645">Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth's distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2510.06231.pdf' target='_blank'>https://arxiv.org/pdf/2510.06231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhe Zheng, Dingjie Song, Guanyu Zhou, Jun You, Jiahao Zhan, Xuran Ma, Xinyuan Song, Ser-Nam Lim, Qifeng Chen, Harry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06231">CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable proficiency in generating highly structured texts. However, while exhibiting a high degree of structural organization, movie scripts demand an additional layer of nuanced storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs often fail to capture. To investigate this deficiency, we first curated CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup Language (CML), where 'content' consists of segments from esteemed, high-quality movie scripts and 'summary' is a concise description of the content. Through an in-depth analysis of the intrinsic multi-shot continuity and narrative structures within these authentic scripts, we identified three pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we propose the CML-Bench, featuring quantitative metrics across these dimensions. CML-Bench effectively assigns high scores to well-crafted, human-written scripts while concurrently pinpointing the weaknesses in screenplays generated by LLMs. To further validate our benchmark, we introduce CML-Instruction, a prompting strategy with detailed instructions on character dialogue and event logic, to guide LLMs to generate more structured and cinematically sound scripts. Extensive experiments validate the effectiveness of our benchmark and demonstrate that LLMs guided by CML-Instruction generate higher-quality screenplays, with results aligned with human preferences.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2505.07902.pdf' target='_blank'>https://arxiv.org/pdf/2505.07902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruikun Hou, Babette BÃ¼hler, Tim FÃ¼tterer, Efe Bozkir, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07902">Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a novel text-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role of text modality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2304.00466.pdf' target='_blank'>https://arxiv.org/pdf/2304.00466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Wang, Luyang Luo, Mingxiang Wu, Qiong Wang, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00466">Learning Robust Medical Image Segmentation from Multi-source Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting annotations from multiple independent sources could mitigate the impact of potential noises and biases from a single source, which is a common practice in medical image segmentation. Learning segmentation networks from multi-source annotations remains a challenge due to the uncertainties brought by the variance of annotations and the quality of images. In this paper, we propose an Uncertainty-guided Multi-source Annotation Network (UMA-Net), which guides the training process by uncertainty estimation at both the pixel and the image levels. First, we developed the annotation uncertainty estimation module (AUEM) to learn the pixel-wise uncertainty of each annotation, which then guided the network to learn from reliable pixels by weighted segmentation loss. Second, a quality assessment module (QAM) was proposed to assess the image-level quality of the input samples based on the former assessed annotation uncertainties. Importantly, we introduced an auxiliary predictor to learn from the low-quality samples instead of discarding them, which ensured the preservation of their representation knowledge in the backbone without directly accumulating errors within the primary predictor. Extensive experiments demonstrated the effectiveness and feasibility of our proposed UMA-Net on various datasets, including 2D chest X-ray segmentation, fundus image segmentation, and 3D breast DCE-MRI segmentation.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2508.07750.pdf' target='_blank'>https://arxiv.org/pdf/2508.07750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowen Wang, Yun Yue, Zhiling Ye, Shuowen Zhang, Lei Fan, Jiaxin Liang, Jiadi Jiang, Cheng Wei, Jingyuan Deng, Xudong Han, Ji Li, Chunxiao Guo, Peng Wei, Jian Wang, Jinjie Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07750">Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2403.04993.pdf' target='_blank'>https://arxiv.org/pdf/2403.04993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Chen, Haina Qin, Juan Wang, Chunfeng Yuan, Bing Li, Weiming Hu, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04993">PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the diversity of assessment requirements in various application scenarios for the IQA task, existing IQA methods struggle to directly adapt to these varied requirements after training. Thus, when facing new requirements, a typical approach is fine-tuning these models on datasets specifically created for those requirements. However, it is time-consuming to establish IQA datasets. In this work, we propose a Prompt-based IQA (PromptIQA) that can directly adapt to new requirements without fine-tuning after training. On one hand, it utilizes a short sequence of Image-Score Pairs (ISP) as prompts for targeted predictions, which significantly reduces the dependency on the data requirements. On the other hand, PromptIQA is trained on a mixed dataset with two proposed data augmentation strategies to learn diverse requirements, thus enabling it to effectively adapt to new requirements. Experiments indicate that the PromptIQA outperforms SOTA methods with higher performance and better generalization. The code will be available.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2401.10511.pdf' target='_blank'>https://arxiv.org/pdf/2401.10511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Chen, Juan Wang, Bing Li, Chunfeng Yuan, Weiming Hu, Junxian Liu, Peng Li, Yan Wang, Youqun Zhang, Congxuan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10511">GMC-IQA: Exploiting Global-correlation and Mean-opinion Consistency for No-reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the subjective nature of image quality assessment (IQA), assessing which image has better quality among a sequence of images is more reliable than assigning an absolute mean opinion score for an image. Thus, IQA models are evaluated by global correlation consistency (GCC) metrics like PLCC and SROCC, rather than mean opinion consistency (MOC) metrics like MAE and MSE. However, most existing methods adopt MOC metrics to define their loss functions, due to the infeasible computation of GCC metrics during training. In this work, we construct a novel loss function and network to exploit Global-correlation and Mean-opinion Consistency, forming a GMC-IQA framework. Specifically, we propose a novel GCC loss by defining a pairwise preference-based rank estimation to solve the non-differentiable problem of SROCC and introducing a queue mechanism to reserve previous data to approximate the global results of the whole data. Moreover, we propose a mean-opinion network, which integrates diverse opinion features to alleviate the randomness of weight learning and enhance the model robustness. Experiments indicate that our method outperforms SOTA methods on multiple authentic datasets with higher accuracy and generalization. We also adapt the proposed loss to various networks, which brings better performance and more stable training.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2509.00405.pdf' target='_blank'>https://arxiv.org/pdf/2509.00405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihao Yuan, Siqi Liu, Yan Chen, Hang Zhou, Chang Liu, Hanting Chen, Jie Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00405">SaD: A Scenario-Aware Discriminator for Speech Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2504.07532.pdf' target='_blank'>https://arxiv.org/pdf/2504.07532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07532">AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2412.15159.pdf' target='_blank'>https://arxiv.org/pdf/2412.15159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15159">OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the field of text-to-video (T2V) generation has made significant strides. Despite this progress, there is still a gap between theoretical advancements and practical application, amplified by issues like degraded image quality and flickering artifacts. Recent advancements in enhancing the video diffusion model (VDM) through feedback learning have shown promising results. However, these methods still exhibit notable limitations, such as misaligned feedback and inferior scalability. To tackle these issues, we introduce OnlineVPO, a more efficient preference learning approach tailored specifically for video diffusion models. Our method features two novel designs, firstly, instead of directly using image-based reward feedback, we leverage the video quality assessment (VQA) model trained on synthetic data as the reward model to provide distribution and modality-aligned feedback on the video diffusion model. Additionally, we introduce an online DPO algorithm to address the off-policy optimization and scalability issue in existing video preference learning frameworks. By employing the video reward model to offer concise video feedback on the fly, OnlineVPO offers effective and efficient preference guidance. Extensive experiments on the open-source video-diffusion model demonstrate OnlineVPO as a simple yet effective and more importantly scalable preference learning algorithm for video diffusion models, offering valuable insights for future advancements in this domain.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2408.15366.pdf' target='_blank'>https://arxiv.org/pdf/2408.15366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>VilÃ©m Zouhar, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, Barry Haddow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15366">Pitfalls and Outlooks in Using COMET</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The COMET metric has blazed a trail in the machine translation community, given its strong correlation with human judgements of translation quality. Its success stems from being a modified pre-trained multilingual model finetuned for quality assessment. However, it being a machine learning model also gives rise to a new set of pitfalls that may not be widely known. We investigate these unexpected behaviours from three aspects: 1) technical: obsolete software versions and compute precision; 2) data: empty content, language mismatch, and translationese at test time as well as distribution and domain biases in training; 3) usage and reporting: multi-reference support and model referencing in the literature. All of these problems imply that COMET scores are not comparable between papers or even technical setups and we put forward our perspective on fixing each issue. Furthermore, we release the sacreCOMET package that can generate a signature for the software and model configuration as well as an appropriate citation. The goal of this work is to help the community make more sound use of the COMET metric.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2406.12732.pdf' target='_blank'>https://arxiv.org/pdf/2406.12732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco de Arriba-PÃ©rez, Silvia GarcÃ­a-MÃ©ndez, Javier Otero-Mosquera, Francisco J. GonzÃ¡lez-CastaÃ±o, Felipe Gil-CastiÃ±eira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12732">Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>New technologies such as Machine Learning (ML) gave great potential for evaluating industry workflows and automatically generating key performance indicators (KPIs). However, despite established standards for measuring the efficiency of industrial machinery, there is no precise equivalent for workers' productivity, which would be highly desirable given the lack of a skilled workforce for the next generation of industry workflows. Therefore, an ML solution combining data from manufacturing processes and workers' performance for that goal is required. Additionally, in recent times intense effort has been devoted to explainable ML approaches that can automatically explain their decisions to a human operator, thus increasing their trustworthiness. We propose to apply explainable ML solutions to differentiate between expert and inexpert workers in industrial workflows, which we validate at a quality assessment industrial workstation. Regarding the methodology used, input data are captured by a manufacturing machine and stored in a NoSQL database. Data are processed to engineer features used in automatic classification and to compute workers' KPIs to predict their level of expertise (with all classification metrics exceeding 90 %). These KPIs, and the relevant features in the decisions are textually explained by natural language expansion on an explainability dashboard. These automatic explanations made it possible to infer knowledge from expert workers for inexpert workers. The latter illustrates the interest of research in self-explainable ML for automatically generating insights to improve productivity in industrial workflows.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2403.12806.pdf' target='_blank'>https://arxiv.org/pdf/2403.12806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, Baining Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12806">VisualCritic: Making LMMs Perceive Visual Quality Like Humans</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer "Yes!". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2309.02743.pdf' target='_blank'>https://arxiv.org/pdf/2309.02743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihang Xu, Shaofei Zhang, Xi Wang, Jiajun Zhang, Wenning Wei, Lei He, Sheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02743">MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present MuLanTTS, the Microsoft end-to-end neural text-to-speech (TTS) system designed for the Blizzard Challenge 2023. About 50 hours of audiobook corpus for French TTS as hub task and another 2 hours of speaker adaptation as spoke task are released to build synthesized voices for different test purposes including sentences, paragraphs, homographs, lists, etc. Building upon DelightfulTTS, we adopt contextual and emotion encoders to adapt the audiobook data to enrich beyond sentences for long-form prosody and dialogue expressiveness. Regarding the recording quality, we also apply denoise algorithms and long audio processing for both corpora. For the hub task, only the 50-hour single speaker data is used for building the TTS system, while for the spoke task, a multi-speaker source model is used for target speaker fine tuning. MuLanTTS achieves mean scores of quality assessment 4.3 and 4.5 in the respective tasks, statistically comparable with natural speech while keeping good similarity according to similarity assessment. The excellent and similarity in this year's new and dense statistical evaluation show the effectiveness of our proposed system in both tasks.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2207.04904.pdf' target='_blank'>https://arxiv.org/pdf/2207.04904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaolin Su, Hanhe Lin, Vlad Hosu, Oliver Wiedemann, Jinqiu Sun, Yu Zhu, Hantao Liu, Yanning Zhang, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.04904">Going the Extra Mile in Face Image Quality Assessment: A Novel Database and Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An accurate computational model for image quality assessment (IQA) benefits many vision applications, such as image filtering, image processing, and image generation. Although the study of face images is an important subfield in computer vision research, the lack of face IQA data and models limits the precision of current IQA metrics on face image processing tasks such as face superresolution, face enhancement, and face editing. To narrow this gap, in this paper, we first introduce the largest annotated IQA database developed to date, which contains 20,000 human faces -- an order of magnitude larger than all existing rated datasets of faces -- of diverse individuals in highly varied circumstances. Based on the database, we further propose a novel deep learning model to accurately predict face image quality, which, for the first time, explores the use of generative priors for IQA. By taking advantage of rich statistics encoded in well pretrained off-the-shelf generative models, we obtain generative prior information and use it as latent references to facilitate blind IQA. The experimental results demonstrate both the value of the proposed dataset for face IQA and the superior performance of the proposed model.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2504.10978.pdf' target='_blank'>https://arxiv.org/pdf/2504.10978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pu Wang, Zhihua Zhang, Dianjie Lu, Guijuan Zhang, Youshan Zhang, Zhuoran Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10978">AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since human and environmental factors interfere, captured polyp images usually suffer from issues such as dim lighting, blur, and overexposure, which pose challenges for downstream polyp segmentation tasks. To address the challenges of noise-induced degradation in polyp images, we present AgentPolyp, a novel framework integrating CLIP-based semantic guidance and dynamic image enhancement with a lightweight neural network for segmentation. The agent first evaluates image quality using CLIP-driven semantic analysis (e.g., identifying ``low-contrast polyps with vascular textures") and adapts reinforcement learning strategies to dynamically apply multi-modal enhancement operations (e.g., denoising, contrast adjustment). A quality assessment feedback loop optimizes pixel-level enhancement and segmentation focus in a collaborative manner, ensuring robust preprocessing before neural network segmentation. This modular architecture supports plug-and-play extensions for various enhancement algorithms and segmentation networks, meeting deployment requirements for endoscopic devices.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2504.05693.pdf' target='_blank'>https://arxiv.org/pdf/2504.05693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniket Deroy, Subhankar Maity
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05693">STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatically assessing question quality is crucial for educators as it saves time, ensures consistency, and provides immediate feedback for refining teaching materials. We propose a novel methodology called STRIVE (Structured Thinking and Refinement with multiLLMs for Improving Verified Question Estimation) using a series of Large Language Models (LLMs) for automatic question evaluation. This approach aims to improve the accuracy and depth of question quality assessment, ultimately supporting diverse learners and enhancing educational practices. The method estimates question quality in an automated manner by generating multiple evaluations based on the strengths and weaknesses of the provided question and then choosing the best solution generated by the LLM. Then the process is improved by iterative review and response with another LLM until the evaluation metric values converge. This sophisticated method of evaluating question quality improves the estimation of question quality by automating the task of question quality evaluation. Correlation scores show that using this proposed method helps to improve correlation with human judgments compared to the baseline method. Error analysis shows that metrics like relevance and appropriateness improve significantly relative to human judgments by using STRIVE.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2210.07617.pdf' target='_blank'>https://arxiv.org/pdf/2210.07617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Koochali, Maria Walch, Sankrutyayan Thota, Peter Schichtel, Andreas Dengel, Sheraz Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07617">Quantifying Quality of Class-Conditional Generative Models in Time-Series Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models are designed to address the data scarcity problem. Even with the exploding amount of data, due to computational advancements, some applications (e.g., health care, weather forecast, fault detection) still suffer from data insufficiency, especially in the time-series domain. Thus generative models are essential and powerful tools, but they still lack a consensual approach for quality assessment. Such deficiency hinders the confident application of modern implicit generative models on time-series data. Inspired by assessment methods on the image domain, we introduce the InceptionTime Score (ITS) and the Frechet InceptionTime Distance (FITD) to gauge the qualitative performance of class conditional generative models on the time-series domain. We conduct extensive experiments on 80 different datasets to study the discriminative capabilities of proposed metrics alongside two existing evaluation metrics: Train on Synthetic Test on Real (TSTR) and Train on Real Test on Synthetic (TRTS). Extensive evaluation reveals that the proposed assessment method, i.e., ITS and FITD in combination with TSTR, can accurately assess class-conditional generative model performance.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2510.18851.pdf' target='_blank'>https://arxiv.org/pdf/2510.18851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18851">DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2509.25787.pdf' target='_blank'>https://arxiv.org/pdf/2509.25787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wen, Tianwu Zhi, Kanglong Fan, Yang Li, Xinge Peng, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25787">Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques such as self-consistency have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM's own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model's iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM's perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM's zero-shot performance by 31.8\% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2509.15802.pdf' target='_blank'>https://arxiv.org/pdf/2509.15802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Yang, Boyang Wang, Hujun Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15802">DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable whole slide imaging (WSI) hinges on image quality,yet staining artefacts, defocus, and cellular degradations are common. We present DPC-QA Net, a no-reference dual-stream network that couples wavelet-based global difference perception with cellular quality assessment from nuclear and membrane embeddings via an Aggr-RWKV module. Cross-attention fusion and multi-term losses align perceptual and cellular cues. Across different datasets, our model detects staining, membrane, and nuclear issues with >92% accuracy and aligns well with usability scores; on LIVEC and KonIQ it outperforms state-of-the-art NR-IQA. A downstream study further shows strong positive correlations between predicted quality and cell recognition accuracy (e.g., nuclei PQ/Dice, membrane boundary F-score), enabling practical pre-screening of WSI regions for computational pathology.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2508.08849.pdf' target='_blank'>https://arxiv.org/pdf/2508.08849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingxue Pang, Shijie Zhao, Junlin Li, Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08849">Adaptive High-Frequency Preprocessing for Video Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-frequency components are crucial for maintaining video clarity and realism, but they also significantly impact coding bitrate, resulting in increased bandwidth and storage costs. This paper presents an end-to-end learning-based framework for adaptive high-frequency preprocessing to enhance subjective quality and save bitrate in video coding. The framework employs the Frequency-attentive Feature pyramid Prediction Network (FFPN) to predict the optimal high-frequency preprocessing strategy, guiding subsequent filtering operators to achieve the optimal tradeoff between bitrate and quality after compression. For training FFPN, we pseudo-label each training video with the optimal strategy, determined by comparing the rate-distortion (RD) performance across different preprocessing types and strengths. Distortion is measured using the latest quality assessment metric. Comprehensive evaluations on multiple datasets demonstrate the visually appealing enhancement capabilities and bitrate savings achieved by our framework.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2506.07047.pdf' target='_blank'>https://arxiv.org/pdf/2506.07047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xuejun, Jianyuan Zhong, Zijin Feng, Pengyi Zhai, Roozbeh Yousefzadeh, Wei Chong Ng, Haoxiong Liu, Ziyi Shou, Jing Xiong, Yudong Zhou, Claudia Beth Ong, Austen Jeremy Sugiarto, Yaoxi Zhang, Wai Ming Tai, Huan Cao, Dongcai Lu, Jiacheng Sun, Qiang Xu, Shen Xin, Zhenguo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07047">Mathesis: Towards Formal Theorem Proving from Natural Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2505.03261.pdf' target='_blank'>https://arxiv.org/pdf/2505.03261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Ting Chen, Yu-Jiet Vong, Yi-Tsung Lee, Sy-Yen Kuo, Qiang Gao, Sizhuo Ma, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03261">DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2503.11008.pdf' target='_blank'>https://arxiv.org/pdf/2503.11008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gian Antariksa, Rohit Chakraborty, Shriyank Somvanshi, Subasish Das, Mohammad Jalayer, Deep Rameshkumar Patel, David Mills
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11008">Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2501.08545.pdf' target='_blank'>https://arxiv.org/pdf/2501.08545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelu Qi, Ping Shi, Shuqi Wang, Chaoyang Zhang, Fei Zhao, Zefeng Ying, Da Pan, Xi Yang, Zheqi He, Teng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08545">T2VEval: Benchmark Dataset and Objective Evaluation Method for T2V-generated Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-to-video (T2V) technology, as demonstrated by models such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the applicability and popularity of the technology. This progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of T2V-generated videos and optimize video generation models. However, assessing the quality of text-to-video outputs remain challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. To address these challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset for text-to-video quality evaluation, which contains 148 textual prompts and 1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation, we scored each video on four dimensions in the subjective experiment, which are overall impression, text-video consistency, realness, and technical quality. Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for T2V quality evaluation. T2VEval assesses videos across three branches: text-video consistency, realness, and technical quality. Using an attention-based fusion module, T2VEval effectively integrates features from each branch and predicts scores with the aid of a large language model. Additionally, we implemented a divide-and-conquer training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. Experimental results demonstrate that T2VEval achieves state-of-the-art performance across multiple metrics.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2406.09622.pdf' target='_blank'>https://arxiv.org/pdf/2406.09622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Ting Chen, Gurunandan Krishnan, Qiang Gao, Sy-Yen Kuo, Sizhuo Ma, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09622">DSL-FIQA: Assessing Facial Image Quality via Dual-Set Degradation Learning and Landmark-Guided Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generic Face Image Quality Assessment (GFIQA) evaluates the perceptual quality of facial images, which is crucial in improving image restoration algorithms and selecting high-quality face images for downstream tasks. We present a novel transformer-based method for GFIQA, which is aided by two unique mechanisms. First, a Dual-Set Degradation Representation Learning (DSL) mechanism uses facial images with both synthetic and real degradations to decouple degradation from content, ensuring generalizability to real-world scenarios. This self-supervised method learns degradation features on a global scale, providing a robust alternative to conventional methods that use local patch information in degradation learning. Second, our transformer leverages facial landmarks to emphasize visually salient parts of a face image in evaluating its perceptual quality. We also introduce a balanced and diverse Comprehensive Generic Face IQA (CGFIQA-40k) dataset of 40K images carefully designed to overcome the biases, in particular the imbalances in skin tone and gender representation, in existing datasets. Extensive analysis and evaluation demonstrate the robustness of our method, marking a significant improvement over prior methods.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2403.06406.pdf' target='_blank'>https://arxiv.org/pdf/2403.06406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixia Zhang, Dingquan Li, Guangtao Zhai, Xiaokang Yang, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06406">When No-Reference Image Quality Models Meet MAP Estimation in Diffusion Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary no-reference image quality assessment (NR-IQA) models can effectively quantify perceived image quality, often achieving strong correlations with human perceptual scores on standard IQA benchmarks. Yet, limited efforts have been devoted to treating NR-IQA models as natural image priors for real-world image enhancement, and consequently comparing them from a perceptual optimization standpoint. In this work, we show -- for the first time -- that NR-IQA models can be plugged into the maximum a posteriori (MAP) estimation framework for image enhancement. This is achieved by performing gradient ascent in the diffusion latent space rather than in the raw pixel domain, leveraging a pretrained differentiable and bijective diffusion process. Likely, different NR-IQA models lead to different enhanced outputs, which in turn provides a new computational means of comparing them. Unlike conventional correlation-based measures, our comparison method offers complementary insights into the respective strengths and weaknesses of the competing NR-IQA models in perceptual optimization scenarios. Additionally, we aim to improve the best-performing NR-IQA model in diffusion latent MAP estimation by incorporating the advantages of other top-performing methods. The resulting model delivers noticeably better results in enhancing real-world images afflicted by unknown and complex distortions, all preserving a high degree of image fidelity.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2402.19276.pdf' target='_blank'>https://arxiv.org/pdf/2402.19276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19276">Modular Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze video content in its aggressively subsampled format, while being blind to the impact of the actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model and a method of training it to improve its modularity. Our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities to render the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user-generated content video databases show that our quality model achieves superior or comparable performance to current methods. Additionally, the modularity of our model offers an opportunity to analyze existing video quality databases in terms of their spatial and temporal complexity.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2304.02389.pdf' target='_blank'>https://arxiv.org/pdf/2304.02389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qian, Hao Chen, Xiangning Wang, Haoxuan Che, Gitaek Kwon, Jaeyoung Kim, Sungjin Choi, Seoyoung Shin, Felix Krause, Markus Unterdechler, Junlin Hou, Rui Feng, Yihao Li, Mostafa El Habib Daho, Qiang Wu, Ping Zhang, Xiaokang Yang, Yiyu Cai, Weiping Jia, Huating Li, Bin Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02389">DRAC: Diabetic Retinopathy Analysis Challenge with Ultra-Wide Optical Coherence Tomography Angiography Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-assisted automatic analysis of diabetic retinopathy (DR) is of great importance in reducing the risks of vision loss and even blindness. Ultra-wide optical coherence tomography angiography (UW-OCTA) is a non-invasive and safe imaging modality in DR diagnosis system, but there is a lack of publicly available benchmarks for model development and evaluation. To promote further research and scientific benchmarking for diabetic retinopathy analysis using UW-OCTA images, we organized a challenge named "DRAC - Diabetic Retinopathy Analysis Challenge" in conjunction with the 25th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022). The challenge consists of three tasks: segmentation of DR lesions, image quality assessment and DR grading. The scientific community responded positively to the challenge, with 11, 12, and 13 teams from geographically diverse institutes submitting different solutions in these three tasks, respectively. This paper presents a summary and analysis of the top-performing solutions and results for each task of the challenge. The obtained results from top algorithms indicate the importance of data augmentation, model architecture and ensemble of networks in improving the performance of deep learning models. These findings have the potential to enable new developments in diabetic retinopathy analysis. The challenge remains open for post-challenge registrations and submissions for benchmarking future methodology developments.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2107.13429.pdf' target='_blank'>https://arxiv.org/pdf/2107.13429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixia Zhang, Kede Ma, Guangtao Zhai, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.13429">Task-Specific Normalization for Continual Learning of Blind Image Quality Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a simple yet effective continual learning method for blind image quality assessment (BIQA) with improved quality prediction accuracy, plasticity-stability trade-off, and task-order/-length robustness. The key step in our approach is to freeze all convolution filters of a pre-trained deep neural network (DNN) for an explicit promise of stability, and learn task-specific normalization parameters for plasticity. We assign each new IQA dataset (i.e., task) a prediction head, and load the corresponding normalization parameters to produce a quality score. The final quality estimate is computed by black a weighted summation of predictions from all heads with a lightweight $K$-means gating mechanism. Extensive experiments on six IQA datasets demonstrate the advantages of the proposed method in comparison to previous training techniques for BIQA.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2506.00327.pdf' target='_blank'>https://arxiv.org/pdf/2506.00327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreshth Saini, Ru-Ling Liao, Yan Ye, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00327">Latent Guidance in Diffusion Models for Perceptual Evaluations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in latent diffusion models that generate high-dimensional image data and perform various downstream tasks, there has been little exploration into perceptual consistency within these models on the task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within the data manifold. We leverage this insight to guide on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality features to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work on guiding diffusion model with perceptual features for NR-IQA. Extensive experiments on IQA datasets show that our method, LGDM, achieves state-of-the-art performance, underscoring the superior generalization capabilities of diffusion models for NR-IQA tasks.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2411.12791.pdf' target='_blank'>https://arxiv.org/pdf/2411.12791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Pan, Baoliang Chen, Danni Huang, Hanwei Zhu, Lingyu Zhu, Xiangjie Sui, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12791">Mitigating Perception Bias: A Training-Free Approach to Enhance LMM for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the impressive performance of large multimodal models (LMMs) in high-level visual tasks, their capacity for image quality assessment (IQA) remains limited. One main reason is that LMMs are primarily trained for high-level tasks (e.g., image captioning), emphasizing unified image semantics extraction under varied quality. Such semantic-aware yet quality-insensitive perception bias inevitably leads to a heavy reliance on image semantics when those LMMs are forced for quality rating. In this paper, instead of retraining or tuning an LMM costly, we propose a training-free debiasing framework, in which the image quality prediction is rectified by mitigating the bias caused by image semantics. Specifically, we first explore several semantic-preserving distortions that can significantly degrade image quality while maintaining identifiable semantics. By applying these specific distortions to the query or test images, we ensure that the degraded images are recognized as poor quality while their semantics remain. During quality inference, both a query image and its corresponding degraded version are fed to the LMM along with a prompt indicating that the query image quality should be inferred under the condition that the degraded one is deemed poor quality.This prior condition effectively aligns the LMM's quality perception, as all degraded images are consistently rated as poor quality, regardless of their semantic difference.Finally, the quality scores of the query image inferred under different prior conditions (degraded versions) are aggregated using a conditional probability model. Extensive experiments on various IQA datasets show that our debiasing framework could consistently enhance the LMM performance and the code will be publicly available.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2411.03715.pdf' target='_blank'>https://arxiv.org/pdf/2411.03715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Chin Huang, Erica Cooper, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03715">MOS-Bench: Benchmarking Generalization Abilities of Subjective Speech Quality Assessment Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective speech quality assessment (SSQA) is critical for evaluating speech samples as perceived by human listeners. While model-based SSQA has enjoyed great success thanks to the development of deep neural networks (DNNs), generalization remains a key challenge, especially for unseen, out-of-domain data. To benchmark the generalization abilities of SSQA models, we present MOS-Bench, a diverse collection of datasets. In addition, we also introduce SHEET, an open-source toolkit containing complete recipes to conduct SSQA experiments. We provided benchmark results for MOS-Bench, and we also explored multi-dataset training to enhance generalization. Additionally, we proposed a new performance metric, best score difference/ratio, and used latent space visualizations to explain model behavior, offering valuable insights for future research.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2401.08107.pdf' target='_blank'>https://arxiv.org/pdf/2401.08107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Li, Peilin Chen, Hanwei Zhu, Keyan Ding, Leida Li, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08107">Deep Shape-Texture Statistics for Completely Blind Image Quality Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Opinion-Unaware Blind Image Quality Assessment (OU-BIQA) models aim to predict image quality without training on reference images and subjective quality scores. Thereinto, image statistical comparison is a classic paradigm, while the performance is limited by the representation ability of visual descriptors. Deep features as visual descriptors have advanced IQA in recent research, but they are discovered to be highly texture-biased and lack of shape-bias. On this basis, we find out that image shape and texture cues respond differently towards distortions, and the absence of either one results in an incomplete image representation. Therefore, to formulate a well-round statistical description for images, we utilize the shapebiased and texture-biased deep features produced by Deep Neural Networks (DNNs) simultaneously. More specifically, we design a Shape-Texture Adaptive Fusion (STAF) module to merge shape and texture information, based on which we formulate qualityrelevant image statistics. The perceptual quality is quantified by the variant Mahalanobis Distance between the inner and outer Shape-Texture Statistics (DSTS), wherein the inner and outer statistics respectively describe the quality fingerprints of the distorted image and natural images. The proposed DSTS delicately utilizes shape-texture statistical relations between different data scales in the deep domain, and achieves state-of-the-art (SOTA) quality prediction performance on images with artificial and authentic distortions.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2202.09738.pdf' target='_blank'>https://arxiv.org/pdf/2202.09738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danni Huang, Lingyu Zhu, Zihao Lin, Hanwei Zhu, Shiqi Wang, Baoliang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.09738">The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. With numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. In this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. In particular, we create a large-scale database for QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as the foundation in studying and developing objective quality assessment measures. The objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. Finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. The superiority of the proposed scheme is validated based on various low-light scenes.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2507.07105.pdf' target='_blank'>https://arxiv.org/pdf/2507.07105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushen Zuo, Qi Zheng, Mingyang Wu, Xinrui Jiang, Renjie Li, Jian Wang, Yide Zhang, Gengchen Mai, Lihong V. Wang, James Zou, Xiaoyu Wang, Ming-Hsuan Yang, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07105">4KAgent: Agentic Any Image to 4K Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2502.20790.pdf' target='_blank'>https://arxiv.org/pdf/2502.20790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Zhu, Xiyu Wei, Guangxiang Zhao, Wenhao Wu, Haosheng Zou, Junfeng Ran, Xun Wang, Lin Sun, Xiangzheng Zhang, Sujian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20790">Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have highlighted the challenge of handling long-context tasks, where models need to reason over extensive input contexts to aggregate target information. While Chain-of-Thought (CoT) prompting has shown promise for multi-step reasoning, its effectiveness for long-context scenarios remains underexplored. Through systematic investigation across diverse tasks, we demonstrate that CoT's benefits generalize across most long-context scenarios and amplify with increasing context length. Motivated by this critical observation, we propose LongRePS, a process-supervised framework that teaches models to generate high-quality reasoning paths for enhanced long-context performance. Our framework incorporates a self-sampling mechanism to bootstrap reasoning paths and a novel quality assessment protocol specifically designed for long-context scenarios. Experimental results on various long-context benchmarks demonstrate the effectiveness of our approach, achieving significant improvements over outcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for LLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on average across diverse QA tasks). Our code, data and trained models are made public to facilitate future research.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2502.16610.pdf' target='_blank'>https://arxiv.org/pdf/2502.16610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, Fons van der Sommen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16610">AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the quality and integrity of medical images is crucial for maintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis and Computer-Aided Detection (CAD) systems. Covariate shifts are subtle variations in the data distribution caused by different imaging devices or settings and can severely degrade model performance, similar to the effects of adversarial attacks. Therefore, it is vital to have a lightweight and fast method to assess the quality of these images prior to using CAD models. AdverX-Ray addresses this need by serving as an image-quality assessment layer, designed to detect covariate shifts effectively. This Adversarial Variational Autoencoder prioritizes the discriminator's role, using the suboptimal outputs of the generator as negative samples to fine-tune the discriminator's ability to identify high-frequency artifacts. Images generated by adversarial networks often exhibit severe high-frequency artifacts, guiding the discriminator to focus excessively on these components. This makes the discriminator ideal for this approach. Trained on patches from X-ray images of specific machine models, AdverX-Ray can evaluate whether a scan matches the training distribution, or if a scan from the same machine is captured under different settings. Extensive comparisons with various OOD detection methods show that AdverX-Ray significantly outperforms existing techniques, achieving a 96.2% average AUROC using only 64 random patches from an X-ray. Its lightweight and fast architecture makes it suitable for real-time applications, enhancing the reliability of medical imaging systems. The code and pretrained models are publicly available.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2409.10898.pdf' target='_blank'>https://arxiv.org/pdf/2409.10898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biplov Paneru, Bishwash Paneru
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10898">AI for Water Sustainability: Global Water Quality Assessment and Prediction with Explainable AI with LLM Chatbot for Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe water supplies requires effective water quality monitoring, especially in developing countries like Nepal, where contamination risks are high. This paper introduces various hybrid deep learning models to predict on the CCME dataset with multiple water quality parameters from Canada, China, the UK, the USA, and Ireland, with 2.82 million data records feature-engineered and evaluated using them. Models such as CatBoost, XGBoost, and Extra Trees, along with neural networks combining CNN and LSTM layers, are used to capture temporal and spatial patterns in the data. The model demonstrated notable accuracy improvements, aiding proactive water quality control. CatBoost, XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values with an average RMSE of 1.2 and an R squared score of 0.99. Additionally, classifiers achieved 99% accuracy, cross-validated across models. SHAP analysis showed the importance of indicators like F.R.C. and orthophosphate levels in hybrid architectures' classification decisions. The practical application is demonstrated along with a chatbot application for water quality insights.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2208.06222.pdf' target='_blank'>https://arxiv.org/pdf/2208.06222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangbo Gao, Cheng Luo, Qinliang Lin, Weicheng Xie, Minmin Liu, Linlin Shen, Keerthy Kusumam, Siyang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.06222">Scale-free and Task-agnostic Attack: Generating Photo-realistic Adversarial Patterns with Patch Quilting Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\noindent Traditional L_p norm-restricted image attack algorithms suffer from poor transferability to black box scenarios and poor robustness to defense algorithms. Recent CNN generator-based attack approaches can synthesize unrestricted and semantically meaningful entities to the image, which is shown to be transferable and robust. However, such methods attack images by either synthesizing local adversarial entities, which are only suitable for attacking specific contents or performing global attacks, which are only applicable to a specific image scale. In this paper, we propose a novel Patch Quilting Generative Adversarial Networks (PQ-GAN) to learn the first scale-free CNN generator that can be applied to attack images with arbitrary scales for various computer vision tasks. The principal investigation on transferability of the generated adversarial examples, robustness to defense frameworks, and visual quality assessment show that the proposed PQG-based attack framework outperforms the other nine state-of-the-art adversarial attack approaches when attacking the neural networks trained on two standard evaluation datasets (i.e., ImageNet and CityScapes).
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2508.12605.pdf' target='_blank'>https://arxiv.org/pdf/2508.12605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Liao, Jieyu Yuan, Yifang Xu, Chunle Guo, Zilong Zhang, Jihong Li, Jiachen Fu, Haotian Fan, Tao Li, Junhui Cui, Chongyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12605">ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multimodal Large Language Models (MLLMs) have introduced a paradigm shift for Image Quality Assessment (IQA) from unexplainable image quality scoring to explainable IQA, demonstrating practical applications like quality control and optimization guidance. However, current explainable IQA methods not only inadequately use the same distortion criteria to evaluate both User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also lack detailed quality analysis for monitoring image quality and guiding image restoration. In this study, we establish the first large-scale Visual Distortion Assessment Instruction Tuning Dataset for UGC images, termed ViDA-UGC, which comprises 11K images with fine-grained quality grounding, detailed quality perception, and reasoning quality description data. This dataset is constructed through a distortion-oriented pipeline, which involves human subject annotation and a Chain-of-Thought (CoT) assessment framework. This framework guides GPT-4o to generate quality descriptions by identifying and analyzing UGC distortions, which helps capturing rich low-level visual features that inherently correlate with distortion patterns. Moreover, we carefully select 476 images with corresponding 6,149 question answer pairs from ViDA-UGC and invite a professional team to ensure the accuracy and quality of GPT-generated information. The selected and revised data further contribute to the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench. Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT framework for consistently enhancing various image quality analysis abilities across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing GPT-4o.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2508.01405.pdf' target='_blank'>https://arxiv.org/pdf/2508.01405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengzhao Wang, Boyu Tan, Yunjun Gao, Hai Jin, Yingfeng Zhang, Xiangyu Ke, Xiaoliang Xu, Yifan Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01405">Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid search, the integration of lexical and semantic retrieval, has become a cornerstone of modern information retrieval systems, driven by demanding applications like Retrieval-Augmented Generation (RAG). The architectural design space for these systems is vast and complex, yet a systematic, empirical understanding of the trade-offs among their core components--retrieval paradigms, combination schemes, and re-ranking methods--is critically lacking. To address this, and informed by our experience building the Infinity open-source database, we present the first systematic benchmark of advanced hybrid search architectures. Our framework evaluates four retrieval paradigms--Full-Text Search (FTS), Sparse Vector Search (SVS), Dense Vector Search (DVS), and Tensor Search (TenS)--benchmarking their combinations and re-ranking strategies across 11 real-world datasets. Our results reveal three key findings for practitioners and researchers: (1) A "weakest link" phenomenon, where a single underperforming retrieval path can disproportionately degrade overall accuracy, highlighting the need for path-wise quality assessment before fusion. (2) A data-driven map of the performance trade-offs, demonstrating that optimal configurations depend heavily on resource constraints and data characteristics, moving beyond a one-size-fits-all approach. (3) The identification of Tensor-based Re-ranking Fusion (TRF) as a high-efficacy alternative to mainstream fusion methods, offering the semantic power of tensor search at a fraction of the computational and memory cost. Our findings offer concrete guidelines for designing the next generation of adaptive, scalable hybrid search systems while also identifying key directions for future research.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2507.21434.pdf' target='_blank'>https://arxiv.org/pdf/2507.21434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Agnideep Aich, Ashit Baran Aich, Bruce Wade
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21434">Measuring Sample Quality with Copula Discrepancies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern Bayesian machine learning, such as Stochastic Gradient Langevin Dynamics (SGLD), sacrifice asymptotic exactness for computational speed, creating a critical diagnostic gap: traditional sample quality measures fail catastrophically when applied to biased samplers. While powerful Stein-based diagnostics can detect distributional mismatches, they provide no direct assessment of dependence structure, often the primary inferential target in multivariate problems. We introduce the Copula Discrepancy (CD), a principled and computationally efficient diagnostic that leverages Sklar's theorem to isolate and quantify the fidelity of a sample's dependence structure independent of its marginals. Our theoretical framework provides the first structure-aware diagnostic specifically designed for the era of approximate inference. Empirically, we demonstrate that a moment-based CD dramatically outperforms standard diagnostics like effective sample size for hyperparameter selection in biased MCMC, correctly identifying optimal configurations where traditional methods fail. Furthermore, our robust MLE-based variant can detect subtle but critical mismatches in tail dependence that remain invisible to rank correlation-based approaches, distinguishing between samples with identical Kendall's tau but fundamentally different extreme-event behavior. With computational overhead orders of magnitude lower than existing Stein discrepancies, the CD provides both immediate practical value for MCMC practitioners and a theoretical foundation for the next generation of structure-aware sample quality assessment.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2311.05843.pdf' target='_blank'>https://arxiv.org/pdf/2311.05843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxin Du, Wenqiang Xu, Jieji Ren, Zhenjun Yu, Cewu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05843">TacIPC: Intersection- and Inversion-free FEM-based Elastomer Simulation For Optical Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile perception stands as a critical sensory modality for human interaction with the environment. Among various tactile sensor techniques, optical sensor-based approaches have gained traction, notably for producing high-resolution tactile images. This work explores gel elastomer deformation simulation through a physics-based approach. While previous works in this direction usually adopt the explicit material point method (MPM), which has certain limitations in force simulation and rendering, we adopt the finite element method (FEM) and address the challenges in penetration and mesh distortion with incremental potential contact (IPC) method. As a result, we present a simulator named TacIPC, which can ensure numerically stable simulations while accommodating direct rendering and friction modeling. To evaluate TacIPC, we conduct three tasks: pseudo-image quality assessment, deformed geometry estimation, and marker displacement prediction. These tasks show its superior efficacy in reducing the sim-to-real gap. Our method can also seamlessly integrate with existing simulators. More experiments and videos can be found in the supplementary materials and on the website: https://sites.google.com/view/tac-ipc.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2303.09429.pdf' target='_blank'>https://arxiv.org/pdf/2303.09429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Levy, Rami Ben-Ari, Nir Darshan, Dani Lischinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09429">Data Roaming and Quality Assessment for Composed Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of Composed Image Retrieval (CoIR) involves queries that combine image and text modalities, allowing users to express their intent more effectively. However, current CoIR datasets are orders of magnitude smaller compared to other vision and language (V&L) datasets. Additionally, some of these datasets have noticeable issues, such as queries containing redundant modalities. To address these shortcomings, we introduce the Large Scale Composed Image Retrieval (LaSCo) dataset, a new CoIR dataset which is ten times larger than existing ones. Pre-training on our LaSCo, shows a noteworthy improvement in performance, even in zero-shot. Furthermore, we propose a new approach for analyzing CoIR datasets and methods, which detects modality redundancy or necessity, in queries. We also introduce a new CoIR baseline, the Cross-Attention driven Shift Encoder (CASE). This baseline allows for early fusion of modalities using a cross-attention module and employs an additional auxiliary task during training. Our experiments demonstrate that this new baseline outperforms the current state-of-the-art methods on established benchmarks like FashionIQ and CIRR.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2303.04418.pdf' target='_blank'>https://arxiv.org/pdf/2303.04418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sevim Cengiz, Ibrahim Almakky, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04418">FUSQA: Fetal Ultrasound Segmentation Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have been effective for various fetal ultrasound segmentation tasks. However, generalization to new unseen data has raised questions about their effectiveness for clinical adoption. Normally, a transition to new unseen data requires time-consuming and costly quality assurance processes to validate the segmentation performance post-transition. Segmentation quality assessment efforts have focused on natural images, where the problem has been typically formulated as a dice score regression task. In this paper, we propose a simplified Fetal Ultrasound Segmentation Quality Assessment (FUSQA) model to tackle the segmentation quality assessment when no masks exist to compare with. We formulate the segmentation quality assessment process as an automated classification task to distinguish between good and poor-quality segmentation masks for more accurate gestational age estimation. We validate the performance of our proposed approach on two datasets we collect from two hospitals using different ultrasound machines. We compare different architectures, with our best-performing architecture achieving over 90% classification accuracy on distinguishing between good and poor-quality segmentation masks from an unseen dataset. Additionally, there was only a 1.45-day difference between the gestational age reported by doctors and estimated based on CRL measurements using well-segmented masks. On the other hand, this difference increased and reached up to 7.73 days when we calculated CRL from the poorly segmented masks. As a result, AI-based approaches can potentially aid fetal ultrasound segmentation quality assessment and might detect poor segmentation in real-time screening in the future.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2301.06103.pdf' target='_blank'>https://arxiv.org/pdf/2301.06103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sania Zahan, Ghulam Mubashar Hassan, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06103">Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Athlete performance measurement in sports videos requires modeling long sequences since the entire spatio-temporal progression contributes dominantly to the performance. It is crucial to comprehend local discriminative spatial dependencies and global semantics for accurate evaluation. However, existing benchmark datasets mainly incorporate sports where the performance lasts only a few seconds. Consequently, state-ofthe-art sports quality assessment methods specifically focus on spatial structure. Although they achieve high performance in short-term sports, they are unable to model prolonged video sequences and fail to achieve similar performance in long-term sports. To facilitate such analysis, we introduce a new dataset, coined AGF-Olympics, that incorporates artistic gymnastic floor routines. AFG-Olympics provides highly challenging scenarios with extensive background, viewpoint, and scale variations over an extended sample duration of up to 2 minutes. In addition, we propose a discriminative attention module to map the dense feature space into a sparse representation by disentangling complex associations. Extensive experiments indicate that our proposed module provides an effective way to embed long-range spatial and temporal correlation semantics.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2509.20679.pdf' target='_blank'>https://arxiv.org/pdf/2509.20679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duc-Tuan Truong, Tianchi Liu, Ruijie Tao, Junjie Li, Kong Aik Lee, Eng Siong Chng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20679">QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work shows that one-class learning can detect unseen deepfake attacks by modeling a compact distribution of bona fide speech around a single centroid. However, the single-centroid assumption can oversimplify the bona fide speech representation and overlook useful cues, such as speech quality, which reflects the naturalness of the speech. Speech quality can be easily obtained using existing speech quality assessment models that estimate it through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends conventional one-class learning by introducing multiple quality-aware centroids. In QAMO, each centroid is optimized to represent a distinct speech quality subspaces, enabling better modeling of intra-class variability in bona fide speech. In addition, QAMO supports a multi-centroid ensemble scoring strategy, which improves decision thresholding and reduces the need for quality labels during inference. With two centroids to represent high- and low-quality speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild dataset, outperforming previous one-class and quality-aware systems.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2507.06116.pdf' target='_blank'>https://arxiv.org/pdf/2507.06116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xintong Hu, Yixuan Chen, Rui Yang, Wenxiang Guo, Changhao Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06116">Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech quality assessment plays a crucial role in the development of speech synthesis systems, but existing models exhibit significant performance variations across different granularity levels of prediction tasks. This paper proposes an enhanced MOS prediction system based on self-supervised learning speech models, incorporating a Mixture of Experts (MoE) classification head and utilizing synthetic data from multiple commercial generation models for data augmentation. Our method builds upon existing self-supervised models such as wav2vec2, designing a specialized MoE architecture to address different types of speech quality assessment tasks. We also collected a large-scale synthetic speech dataset encompassing the latest text-to-speech, speech conversion, and speech enhancement systems. However, despite the adoption of the MoE architecture and expanded dataset, the model's performance improvements in sentence-level prediction tasks remain limited. Our work reveals the limitations of current methods in handling sentence-level quality assessment, provides new technical pathways for the field of automatic speech quality assessment, and also delves into the fundamental causes of performance differences across different assessment granularities.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2503.20290.pdf' target='_blank'>https://arxiv.org/pdf/2503.20290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Yu Tsao, Junichi Yamagishi, Yuxuan Wang, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20290">QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores a novel perspective to speech quality assessment by leveraging natural language descriptions, offering richer, more nuanced insights than traditional numerical scoring methods. Natural language feedback provides instructive recommendations and detailed evaluations, yet existing datasets lack the comprehensive annotations needed for this approach. To bridge this gap, we introduce QualiSpeech, a comprehensive low-level speech quality assessment dataset encompassing 11 key aspects and detailed natural language comments that include reasoning and contextual insights. Additionally, we propose the QualiSpeech Benchmark to evaluate the low-level speech understanding capabilities of auditory large language models (LLMs). Experimental results demonstrate that finetuned auditory LLMs can reliably generate detailed descriptions of noise and distortion, effectively identifying their types and temporal characteristics. The results further highlight the potential for incorporating reasoning to enhance the accuracy and reliability of quality assessments. The dataset will be released at https://huggingface.co/datasets/tsinghua-ee/QualiSpeech.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2503.06141.pdf' target='_blank'>https://arxiv.org/pdf/2503.06141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxing Li, Rui Wang, Lei Sun, Yancheng Bai, Xiangxiang Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06141">Next Token Is Enough: Realistic Image Quality and Aesthetic Scoring with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of mobile internet has resulted in a substantial increase in user-generated content (UGC) images, thereby making the thorough assessment of UGC images both urgent and essential. Recently, multimodal large language models (MLLMs) have shown great potential in image quality assessment (IQA) and image aesthetic assessment (IAA). Despite this progress, effectively scoring the quality and aesthetics of UGC images still faces two main challenges: 1) A single score is inadequate to capture the hierarchical human perception. 2) How to use MLLMs to output numerical scores, such as mean opinion scores (MOS), remains an open question. To address these challenges, we introduce a novel dataset, named Realistic image Quality and Aesthetic (RealQA), including 14,715 UGC images, each of which is annoted with 10 fine-grained attributes. These attributes span three levels: low level (e.g., image clarity), middle level (e.g., subject integrity) and high level (e.g., composition). Besides, we conduct a series of in-depth and comprehensive investigations into how to effectively predict numerical scores using MLLMs. Surprisingly, by predicting just two extra significant digits, the next token paradigm can achieve SOTA performance. Furthermore, with the help of chain of thought (CoT) combined with the learnt fine-grained attributes, the proposed method can outperform SOTA methods on five public datasets for IQA and IAA with superior interpretability and show strong zero-shot generalization for video quality assessment (VQA). The code and dataset will be released.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2311.04780.pdf' target='_blank'>https://arxiv.org/pdf/2311.04780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Sanchez, Oscar Esteban, Yvan Gomez, Alexandre Pron, MÃ©riam Koob, Vincent Dunet, Nadine Girard, Andras Jakab, Elisenda Eixarch, Guillaume Auzias, Meritxell Bach Cuadra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04780">FetMRQC: a robust quality control system for multi-centric fetal brain MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fetal brain MRI is becoming an increasingly relevant complement to neurosonography for perinatal diagnosis, allowing fundamental insights into fetal brain development throughout gestation. However, uncontrolled fetal motion and heterogeneity in acquisition protocols lead to data of variable quality, potentially biasing the outcome of subsequent studies. We present FetMRQC, an open-source machine-learning framework for automated image quality assessment and quality control that is robust to domain shifts induced by the heterogeneity of clinical data. FetMRQC extracts an ensemble of quality metrics from unprocessed anatomical MRI and combines them to predict experts' ratings using random forests. We validate our framework on a pioneeringly large and diverse dataset of more than 1600 manually rated fetal brain T2-weighted images from four clinical centers and 13 different scanners. Our study shows that FetMRQC's predictions generalize well to unseen data while being interpretable. FetMRQC is a step towards more robust fetal brain neuroimaging, which has the potential to shed new insights on the developing human brain.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2508.20384.pdf' target='_blank'>https://arxiv.org/pdf/2508.20384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongfu Zhu, Lin Sun, Guangxiang Zhao, Weihong Lin, Xiangzheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20384">Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Entropy Area Score (EAS), a simple yet effective metric to quantify uncertainty in the answer generation process of reasoning large language models (LLMs). EAS requires neither external models nor repeated sampling, it integrates token-level predictive entropy from the model itself to capture the evolution of uncertainty during generation. Empirical results show that EAS is strongly correlated with answer entropy across models and datasets. In training data selection, EAS identifies high-potential samples and consistently outperforms Pass Rate filtering under equal sample budgets, improving student model accuracy on math benchmarks. EAS is both efficient and interpretable, offering a practical tool for uncertainty modeling and data quality assessment in LLM training.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2508.05516.pdf' target='_blank'>https://arxiv.org/pdf/2508.05516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekaterina Shumitskaya, Dmitriy Vatolin, Anastasia Antsiferova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05516">FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel certified defense method for Image Quality Assessment (IQA) models based on randomized smoothing with noise applied in the feature space rather than the input space. Unlike prior approaches that inject Gaussian noise directly into input images, often degrading visual quality, our method preserves image fidelity while providing robustness guarantees. To formally connect noise levels in the feature space with corresponding input-space perturbations, we analyze the maximum singular value of the backbone network's Jacobian. Our approach supports both full-reference (FR) and no-reference (NR) IQA models without requiring any architectural modifications, suitable for various scenarios. It is also computationally efficient, requiring a single backbone forward pass per image. Compared to previous methods, it reduces inference time by 99.5% without certification and by 20.6% when certification is applied. We validate our method with extensive experiments on two benchmark datasets, involving six widely-used FR and NR IQA models and comparisons against five state-of-the-art certified defenses. Our results demonstrate consistent improvements in correlation with subjective quality scores by up to 30.9%.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2507.19004.pdf' target='_blank'>https://arxiv.org/pdf/2507.19004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Xun, Yue Sun, Jingkun Chen, Zitong Yu, Tong Tong, Xiaohong Liu, Mingxiang Wu, Tao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19004">MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in medical imaging technology underscore the critical need for precise and automated image quality assessment (IQA) to ensure diagnostic accuracy. Existing medical IQA methods, however, struggle to generalize across diverse modalities and clinical scenarios. In response, we introduce MedIQA, the first comprehensive foundation model for medical IQA, designed to handle variability in image dimensions, modalities, anatomical regions, and types. We developed a large-scale multi-modality dataset with plentiful manually annotated quality scores to support this. Our model integrates a salient slice assessment module to focus on diagnostically relevant regions feature retrieval and employs an automatic prompt strategy that aligns upstream physical parameter pre-training with downstream expert annotation fine-tuning. Extensive experiments demonstrate that MedIQA significantly outperforms baselines in multiple downstream tasks, establishing a scalable framework for medical IQA and advancing diagnostic workflows and clinical decision-making.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2506.04951.pdf' target='_blank'>https://arxiv.org/pdf/2506.04951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Igor Meleshin, Anna Chistyakova, Anastasia Antsiferova, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04951">Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) models are increasingly relied upon to evaluate image quality in real-world systems -- from compression and enhancement to generation and streaming. Yet their adoption brings a fundamental risk: these models are inherently unstable. Adversarial manipulations can easily fool them, inflating scores and undermining trust. Traditionally, such vulnerabilities are addressed through data-driven defenses -- adversarial retraining, regularization, or input purification. But what if this is the wrong lens? What if robustness in perceptual models is not something to learn but something to design? In this work, we propose a provocative idea: robustness as an architectural prior. Rather than training models to resist perturbations, we reshape their internal structure to suppress sensitivity from the ground up. We achieve this by enforcing orthogonal information flow, constraining the network to norm-preserving operations -- and further stabilizing the system through pruning and fine-tuning. The result is a robust IQA architecture that withstands adversarial attacks without requiring adversarial training or significant changes to the original model. This approach suggests a shift in perspective: from optimizing robustness through data to engineering it through design.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2505.19972.pdf' target='_blank'>https://arxiv.org/pdf/2505.19972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglei Zhou, Hubert P. H. Shum, Frederick W. B. Li, Xingxing Zhang, Xiaohui Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19972">PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative performance of actions in long videos. However, existing methods face challenges due to domain shifts between the pre-trained large-scale action recognition backbones and the specific AQA task, thereby hindering their performance. This arises since fine-tuning resource-intensive backbones on small AQA datasets is impractical. We address this by identifying two levels of domain shift: task-level, regarding differences in task objectives, and feature-level, regarding differences in important features. For feature-level shifts, which are more detrimental, we propose Progressive Hierarchical Instruction (PHI) with two strategies. First, Gap Minimization Flow (GMF) leverages flow matching to progressively learn a fast flow path that reduces the domain gap between initial and desired features across shallow to deep layers. Additionally, a temporally-enhanced attention module captures long-range dependencies essential for AQA. Second, List-wise Contrastive Regularization (LCR) facilitates coarse-to-fine alignment by comprehensively comparing batch pairs to learn fine-grained cues while mitigating domain shift. Integrating these modules, PHI offers an effective solution. Experiments demonstrate that PHI achieves state-of-the-art performance on three representative long-term AQA datasets, proving its superiority in addressing the domain shift for long-term AQA.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2504.12605.pdf' target='_blank'>https://arxiv.org/pdf/2504.12605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Su, Chen Wu, Yu Zhang, Chen Lyu, Zhuoran Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12605">AdaQual-Diff: Diffusion-Based Image Restoration via Adaptive Quality Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Restoring images afflicted by complex real-world degradations remains challenging, as conventional methods often fail to adapt to the unique mixture and severity of artifacts present. This stems from a reliance on indirect cues which poorly capture the true perceptual quality deficit. To address this fundamental limitation, we introduce AdaQual-Diff, a diffusion-based framework that integrates perceptual quality assessment directly into the generative restoration process. Our approach establishes a mathematical relationship between regional quality scores from DeQAScore and optimal guidance complexity, implemented through an Adaptive Quality Prompting mechanism. This mechanism systematically modulates prompt structure according to measured degradation severity: regions with lower perceptual quality receive computationally intensive, structurally complex prompts with precise restoration directives, while higher quality regions receive minimal prompts focused on preservation rather than intervention. The technical core of our method lies in the dynamic allocation of computational resources proportional to degradation severity, creating a spatially-varying guidance field that directs the diffusion process with mathematical precision. By combining this quality-guided approach with content-specific conditioning, our framework achieves fine-grained control over regional restoration intensity without requiring additional parameters or inference iterations. Experimental results demonstrate that AdaQual-Diff achieves visually superior restorations across diverse synthetic and real-world datasets.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2504.04770.pdf' target='_blank'>https://arxiv.org/pdf/2504.04770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Liu, Songhao Jiang, Chih-chan Tien, Jinbo Xu, Rick Stevens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04770">Bidirectional Hierarchical Protein Multi-Modal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural context. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. This bidirectional and hierarchical (Bi-Hierarchical) fusion approach leverages the strengths of both modalities to capture richer and more comprehensive protein representations. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including enzyme EC classification, model quality assessment, protein-ligand binding affinity prediction, protein-protein binding site prediction, and B cell epitopes prediction. Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of Bi-Hierarchical Fusion in bridging sequence and structural modalities.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2503.16771.pdf' target='_blank'>https://arxiv.org/pdf/2503.16771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David N. Palacio, Dipin Khati, Daniel Rodriguez-Cardenas, Alejandro Velasco, Denys Poshyvanyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16771">On Explaining (Large) Language Models For Code Using Global Code-Based Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Language Models for Code (LLM4Code) have significantly changed the landscape of software engineering (SE) on downstream tasks, such as code generation, by making software development more efficient. Therefore, a growing interest has emerged in further evaluating these Language Models to homogenize the quality assessment of generated code. As the current evaluation process can significantly overreact on accuracy-based metrics, practitioners often seek methods to interpret LLM4Code outputs beyond canonical benchmarks. While the majority of research reports on code generation effectiveness in terms of expected ground truth, scant attention has been paid to LLMs' explanations. In essence, the decision-making process to generate code is hard to interpret. To bridge this evaluation gap, we introduce code rationales (Code$Q$), a technique with rigorous mathematical underpinning, to identify subsets of tokens that can explain individual code predictions. We conducted a thorough Exploratory Analysis to demonstrate the method's applicability and a User Study to understand the usability of code-based explanations. Our evaluation demonstrates that Code$Q$ is a powerful interpretability method to explain how (less) meaningful input concepts (i.e., natural language particle `at') highly impact output generation. Moreover, participants of this study highlighted Code$Q$'s ability to show a causal relationship between the input and output of the model with readable and informative explanations on code completion and test generation tasks. Additionally, Code$Q$ also helps to uncover model rationale, facilitating comparison with a human rationale to promote a fair level of trust and distrust in the model.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2501.08415.pdf' target='_blank'>https://arxiv.org/pdf/2501.08415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08415">Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have revealed that modern image and video quality assessment (IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can manipulate a video through preprocessing to artificially increase its quality score according to a certain metric, despite no actual improvement in visual quality. Most of the attacks studied in the literature are white-box attacks, while black-box attacks in the context of VQA have received less attention. Moreover, some research indicates a lack of transferability of adversarial examples generated for one model to another when applied to VQA. In this paper, we propose a cross-modal attack method, IC2VQA, aimed at exploring the vulnerabilities of modern VQA models. This approach is motivated by the observation that the low-level feature spaces of images and videos are similar. We investigate the transferability of adversarial perturbations across different modalities; specifically, we analyze how adversarial perturbations generated on a white-box IQA model with an additional CLIP module can effectively target a VQA model. The addition of the CLIP module serves as a valuable aid in increasing transferability, as the CLIP model is known for its effective capture of low-level semantics. Extensive experiments demonstrate that IC2VQA achieves a high success rate in attacking three black-box VQA models. We compare our method with existing black-box attack strategies, highlighting its superiority in terms of attack success within the same number of iterations and levels of attack strength. We believe that the proposed method will contribute to the deeper analysis of robust VQA metrics.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2411.12575.pdf' target='_blank'>https://arxiv.org/pdf/2411.12575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekaterina Shumitskaya, Mikhail Pautov, Dmitriy Vatolin, Anastasia Antsiferova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12575">Stochastic BIQA: Median Randomized Smoothing for Certified Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most modern No-Reference Image-Quality Assessment (NR-IQA) metrics are based on neural networks vulnerable to adversarial attacks. Attacks on such metrics lead to incorrect image/video quality predictions, which poses significant risks, especially in public benchmarks. Developers of image processing algorithms may unfairly increase the score of a target IQA metric without improving the actual quality of the adversarial image. Although some empirical defenses for IQA metrics were proposed, they do not provide theoretical guarantees and may be vulnerable to adaptive attacks. This work focuses on developing a provably robust no-reference IQA metric. Our method is based on Median Smoothing (MS) combined with an additional convolution denoiser with ranking loss to improve the SROCC and PLCC scores of the defended IQA metric. Compared with two prior methods on three datasets, our method exhibited superior SROCC and PLCC scores while maintaining comparable certified guarantees.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2410.07538.pdf' target='_blank'>https://arxiv.org/pdf/2410.07538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshui Luo, Haoyu Liu, Yongliang Ding, Tao Zhou, Sheng wan, Runze Wu, Minmin Lin, Cong Zhang, Changjie Fan, Chen Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07538">Rank Aggregation in Crowdsourcing for Listwise Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rank aggregation through crowdsourcing has recently gained significant attention, particularly in the context of listwise ranking annotations. However, existing methods primarily focus on a single problem and partial ranks, while the aggregation of listwise full ranks across numerous problems remains largely unexplored. This scenario finds relevance in various applications, such as model quality assessment and reinforcement learning with human feedback. In light of practical needs, we propose LAC, a Listwise rank Aggregation method in Crowdsourcing, where the global position information is carefully measured and included. In our design, an especially proposed annotation quality indicator is employed to measure the discrepancy between the annotated rank and the true rank. We also take the difficulty of the ranking problem itself into consideration, as it directly impacts the performance of annotators and consequently influences the final results. To our knowledge, LAC is the first work to directly deal with the full rank aggregation problem in listwise crowdsourcing, and simultaneously infer the difficulty of problems, the ability of annotators, and the ground-truth ranks in an unsupervised way. To evaluate our method, we collect a real-world business-oriented dataset for paragraph ranking. Experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness of our proposed LAC method.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2408.08228.pdf' target='_blank'>https://arxiv.org/pdf/2408.08228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Pan, Jun Xia, Zheyu Yan, Guoyue Xu, Yawen Wu, Zhenge Jia, Jianxu Chen, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08228">Rethinking Medical Anomaly Detection in Brain MRI: An Image Quality Assessment Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstruction-based methods, particularly those leveraging autoencoders, have been widely adopted to perform anomaly detection in brain MRI. While most existing works try to improve detection accuracy by proposing new model structures or algorithms, we tackle the problem through image quality assessment, an underexplored perspective in the field. We propose a fusion quality loss function that combines Structural Similarity Index Measure loss with l1 loss, offering a more comprehensive evaluation of reconstruction quality. Additionally, we introduce a data pre-processing strategy that enhances the average intensity ratio (AIR) between normal and abnormal regions, further improving the distinction of anomalies. By fusing the aforementioned two methods, we devise the image quality assessment (IQA) approach. The proposed IQA approach achieves significant improvements (>10%) in terms of Dice coefficient (DICE) and Area Under the Precision-Recall Curve (AUPRC) on the BraTS21 (T2, FLAIR) and MSULB datasets when compared with state-of-the-art methods. These results highlight the importance of invoking the comprehensive image quality assessment in medical anomaly detection and provide a new perspective for future research in this field.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2408.01541.pdf' target='_blank'>https://arxiv.org/pdf/2408.01541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Gushchin, Khaled Abud, Georgii Bychkov, Ekaterina Shumitskaya, Anna Chistyakova, Sergey Lavrushkin, Bader Rasheed, Kirill Malyshev, Dmitriy Vatolin, Anastasia Antsiferova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01541">Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Image Quality Assessment (IQA), the adversarial robustness of the metrics poses a critical concern. This paper presents a comprehensive benchmarking study of various defense mechanisms in response to the rise in adversarial attacks on IQA. We systematically evaluate 25 defense strategies, including adversarial purification, adversarial training, and certified robustness methods. We applied 14 adversarial attack algorithms of various types in both non-adaptive and adaptive settings and tested these defenses against them. We analyze the differences between defenses and their applicability to IQA tasks, considering that they should preserve IQA scores and image quality. The proposed benchmark aims to guide future developments and accepts submissions of new methods, with the latest results available online: https://videoprocessing.ai/benchmarks/iqa-defenses.html.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2312.09899.pdf' target='_blank'>https://arxiv.org/pdf/2312.09899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Zhang, Shuo Wang, Tao Zhou, Qi Dou, Danny Z. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09899">SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing the Segment Anything Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentation quality assessment (SQA) plays a critical role in the deployment of a medical image based AI system. Users need to be informed/alerted whenever an AI system generates unreliable/incorrect predictions. With the introduction of the Segment Anything Model (SAM), a general foundation segmentation model, new research opportunities emerged in how one can utilize SAM for medical image segmentation. In this paper, we propose a novel SQA method, called SQA-SAM, which exploits SAM to enhance the accuracy of quality assessment for medical image segmentation. When a medical image segmentation model (MedSeg) produces predictions for a test image, we generate visual prompts based on the predictions, and SAM is utilized to generate segmentation maps corresponding to the visual prompts. How well MedSeg's segmentation aligns with SAM's segmentation indicates how well MedSeg's segmentation aligns with the general perception of objectness and image region partition. We develop a score measure for such alignment. In experiments, we find that the generated scores exhibit moderate to strong positive correlation (in Pearson correlation and Spearman correlation) with Dice coefficient scores reflecting the true segmentation quality.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2310.10482.pdf' target='_blank'>https://arxiv.org/pdf/2310.10482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, AndrÃ© F. T. Martins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10482">xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Widely used learned metrics for machine translation evaluation, such as COMET and BLEURT, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xCOMET, an open-source learned metric designed to bridge the gap between these approaches. xCOMET integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xCOMET is largely capable of identifying localized critical errors and hallucinations.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2306.00816.pdf' target='_blank'>https://arxiv.org/pdf/2306.00816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00816">Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors when exposed to specific trigger patterns, without affecting their performance on benign samples, dubbed \textit{backdoor attack}. Currently, implementing backdoor attacks in physical scenarios still faces significant challenges. Physical attacks are labor-intensive and time-consuming, and the triggers are selected in a manual and heuristic way. Moreover, expanding digital attacks to physical scenarios faces many challenges due to their sensitivity to visual distortions and the absence of counterparts in the real world. To address these challenges, we define a novel trigger called the \textbf{V}isible, \textbf{S}emantic, \textbf{S}ample-Specific, and \textbf{C}ompatible (VSSC) trigger, to achieve effective, stealthy and robust simultaneously, which can also be effectively deployed in the physical scenario using corresponding objects. To implement the VSSC trigger, we propose an automated pipeline comprising three modules: a trigger selection module that systematically identifies suitable triggers leveraging large language models, a trigger insertion module that employs generative models to seamlessly integrate triggers into images, and a quality assessment module that ensures the natural and successful insertion of triggers through vision-language models. Extensive experimental results and analysis validate the effectiveness, stealthiness, and robustness of the VSSC trigger. It can not only maintain robustness under visual distortions but also demonstrates strong practicality in the physical scenario. We hope that the proposed VSSC trigger and implementation approach could inspire future studies on designing more practical triggers in backdoor attacks.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2002.11409.pdf' target='_blank'>https://arxiv.org/pdf/2002.11409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Bianco, Luigi Celona, Paolo Napoletano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2002.11409">Disentangling Image Distortions in Deep Feature Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous literature suggests that perceptual similarity is an emergent property shared across deep visual representations. Experiments conducted on a dataset of human-judged image distortions have proven that deep features outperform classic perceptual metrics. In this work we take a further step in the direction of a broader understanding of such property by analyzing the capability of deep visual representations to intrinsically characterize different types of image distortions. To this end, we firstly generate a number of synthetically distorted images and then we analyze the features extracted by different layers of different Deep Neural Networks. We observe that a dimension-reduced representation of the features extracted from a given layer permits to efficiently separate types of distortions in the feature space. Moreover, each network layer exhibits a different ability to separate between different types of distortions, and this ability varies according to the network architecture. Finally, we evaluate the exploitation of features taken from the layer that better separates image distortions for: i) reduced-reference image quality assessment, and ii) distortion types and severity levels characterization on both single and multiple distortion databases. Results achieved on both tasks suggest that deep visual representations can be unsupervisedly employed to efficiently characterize various image distortions.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2511.09948.pdf' target='_blank'>https://arxiv.org/pdf/2511.09948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09948">Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2510.01812.pdf' target='_blank'>https://arxiv.org/pdf/2510.01812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxun Tang, Lan Liu, Wenhao Feng, Yiwen Zhao, Jionghao Han, Yifeng Yu, Jiatong Shi, Qin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01812">SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro expands annotations of the additional part to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent advances. Each clip receives at least five ratings from professional annotators, ensuring reliability and consistency. Furthermore, we explore how to effectively utilize MOS data annotated under different standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset can be accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2508.15496.pdf' target='_blank'>https://arxiv.org/pdf/2508.15496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Masserini, Diego Clerissi, Daniela Micucci, JoÃ£o R. Campos, Leonardo Mariani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15496">Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-based chatbots are increasingly being used to deliver real services, yet assessing their reliability, security, and robustness remains underexplored, also due to the lack of large-scale, high-quality datasets. The emerging automated quality assessment techniques targeting chatbots often rely on limited pools of subjects, such as custom-made toy examples, or outdated, no longer available, or scarcely popular agents, complicating the evaluation of such techniques. In this paper, we present two datasets and the tool support necessary to create and maintain these datasets. The first dataset is RASA TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa chatbots available on GitHub, representing the state of the practice in open-source chatbot development with Rasa. The second dataset is BOT RASA COLLECTION (BRASATO), a curated selection of the most relevant chatbots for dialogue complexity, functional complexity, and utility, whose goal is to ease reproducibility and facilitate research on chatbot reliability.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2508.09843.pdf' target='_blank'>https://arxiv.org/pdf/2508.09843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Xu Zhang, Jiaqi Ma, Linwei Zhu, Yun Zhang, Huan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09843">Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to evaluate locally non-uniform distortions due to inadequate modeling of spatial variations in quality and ineffective feature representation capturing both local details and global context. To address this, we propose a graph neural network-based OIQA framework that explicitly models structural relationships between viewports to enhance perception of spatial distortion non-uniformity. Our approach employs Fibonacci sphere sampling to generate viewports with well-structured topology, representing each as a graph node. Multi-stage feature extraction networks then derive high-dimensional node representation. To holistically capture spatial dependencies, we integrate a Graph Attention Network (GAT) modeling fine-grained local distortion variations among adjacent viewports, and a graph transformer capturing long-range quality interactions across distant regions. Extensive experiments on two large-scale OIQA databases with complex spatial distortions demonstrate that our method significantly outperforms existing approaches, confirming its effectiveness and strong generalization capability.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2506.21011.pdf' target='_blank'>https://arxiv.org/pdf/2506.21011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhi Xie, Kun Yuan, Yunpeng Qu, Jiachao Gong, Mingda Wu, Ming Sun, Chao Zhou, Jihong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21011">Bridging Video Quality Scoring and Justification via Large Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classical video quality assessment (VQA) methods generate a numerical score to judge a video's perceived visual fidelity and clarity. Yet, a score fails to describe the video's complex quality dimensions, restricting its applicability. Benefiting from the linguistic output, adapting video large multimodal models (LMMs) to VQA via instruction tuning has the potential to address this issue. The core of the approach lies in the video quality-centric instruction data. Previous explorations mainly focus on the image domain, and their data generation processes heavily rely on human quality annotations and proprietary systems, limiting data scalability and effectiveness. To address these challenges, we propose the Score-based Instruction Generation (SIG) pipeline. Specifically, SIG first scores multiple quality dimensions of an unlabeled video and maps scores to text-defined levels. It then explicitly incorporates a hierarchical Chain-of-Thought (CoT) to model the correlation between specific dimensions and overall quality, mimicking the human visual system's reasoning process. The automated pipeline eliminates the reliance on expert-written quality descriptions and proprietary systems, ensuring data scalability and generation efficiency. To this end, the resulting Score2Instruct (S2I) dataset contains over 320K diverse instruction-response pairs, laying the basis for instruction tuning. Moreover, to advance video LMMs' quality scoring and justification abilities simultaneously, we devise a progressive tuning strategy to fully unleash the power of S2I. Built upon SIG, we further curate a benchmark termed S2I-Bench with 400 open-ended questions to better evaluate the quality justification capacity of video LMMs. Experimental results on the S2I-Bench and existing benchmarks indicate that our method consistently improves quality scoring and justification capabilities across multiple video LMMs.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2501.10071.pdf' target='_blank'>https://arxiv.org/pdf/2501.10071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Liu, Yujie Zhang, Ziyu Shan, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10071">CLIP-PCQA: Exploring Subjective-Aligned Vision-Language Modeling for Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, No-Reference Point Cloud Quality Assessment (NR-PCQA) research has achieved significant progress. However, existing methods mostly seek a direct mapping function from visual data to the Mean Opinion Score (MOS), which is contradictory to the mechanism of practical subjective evaluation. To address this, we propose a novel language-driven PCQA method named CLIP-PCQA. Considering that human beings prefer to describe visual quality using discrete quality descriptions (e.g., "excellent" and "poor") rather than specific scores, we adopt a retrieval-based mapping strategy to simulate the process of subjective assessment. More specifically, based on the philosophy of CLIP, we calculate the cosine similarity between the visual features and multiple textual features corresponding to different quality descriptions, in which process an effective contrastive loss and learnable prompts are introduced to enhance the feature extraction. Meanwhile, given the personal limitations and bias in subjective experiments, we further covert the feature similarities into probabilities and consider the Opinion Score Distribution (OSD) rather than a single MOS as the final target. Experimental results show that our CLIP-PCQA outperforms other State-Of-The-Art (SOTA) approaches.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2411.07936.pdf' target='_blank'>https://arxiv.org/pdf/2411.07936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Shan, Yujie Zhang, Yipeng Liu, Yiling Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07936">Learning Disentangled Representations for Perceptual Point Cloud Quality Assessment via Mutual Information Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively assess the human perceptual quality of point clouds without relying on pristine-quality point clouds for reference. It is becoming increasingly significant with the rapid advancement of immersive media applications such as virtual reality (VR) and augmented reality (AR). However, current NR-PCQA models attempt to indiscriminately learn point cloud content and distortion representations within a single network, overlooking their distinct contributions to quality information. To address this issue, we propose DisPA, a novel disentangled representation learning framework for NR-PCQA. The framework trains a dual-branch disentanglement network to minimize mutual information (MI) between representations of point cloud content and distortion. Specifically, to fully disentangle representations, the two branches adopt different philosophies: the content-aware encoder is pretrained by a masked auto-encoding strategy, which can allow the encoder to capture semantic information from rendered images of distorted point clouds; the distortion-aware encoder takes a mini-patch map as input, which forces the encoder to focus on low-level distortion patterns. Furthermore, we utilize an MI estimator to estimate the tight upper bound of the actual MI and further minimize it to achieve explicit representation disentanglement. Extensive experimental results demonstrate that DisPA outperforms state-of-the-art methods on multiple PCQA datasets.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2408.11392.pdf' target='_blank'>https://arxiv.org/pdf/2408.11392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AndrÃ© DÃ¶rsch, Torsten Schlett, Peter Munch, Christian Rathgeb, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11392">Fairness measures for biometric quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment algorithms measure the quality of a captured biometric sample. Since the sample quality strongly affects the recognition performance of a biometric system, it is essential to only process samples of sufficient quality and discard samples of low-quality. Even though quality assessment algorithms are not intended to yield very different quality scores across demographic groups, quality score discrepancies are possible, resulting in different discard ratios. To ensure that quality assessment algorithms do not take demographic characteristics into account when assessing sample quality and consequently to ensure that the quality algorithms perform equally for all individuals, it is crucial to develop a fairness measure. In this work we propose and compare multiple fairness measures for evaluating quality components across demographic groups. Proposed measures, could be used as potential candidates for an upcoming standard in this important field.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2407.19436.pdf' target='_blank'>https://arxiv.org/pdf/2407.19436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongling Huang, Yihan Zhuang, Zipei Zhong, Feng Xu, Gong Cheng, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19436">X-Fake: Juggling Utility Evaluation and Explanation of Simulated SAR Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SAR image simulation has attracted much attention due to its great potential to supplement the scarce training data for deep learning algorithms. Consequently, evaluating the quality of the simulated SAR image is crucial for practical applications. The current literature primarily uses image quality assessment techniques for evaluation that rely on human observers' perceptions. However, because of the unique imaging mechanism of SAR, these techniques may produce evaluation results that are not entirely valid. The distribution inconsistency between real and simulated data is the main obstacle that influences the utility of simulated SAR images. To this end, we propose a novel trustworthy utility evaluation framework with a counterfactual explanation for simulated SAR images for the first time, denoted as X-Fake. It unifies a probabilistic evaluator and a causal explainer to achieve a trustworthy utility assessment. We construct the evaluator using a probabilistic Bayesian deep model to learn the posterior distribution, conditioned on real data. Quantitatively, the predicted uncertainty of simulated data can reflect the distribution discrepancy. We build the causal explainer with an introspective variational auto-encoder to generate high-resolution counterfactuals. The latent code of IntroVAE is finally optimized with evaluation indicators and prior information to generate the counterfactual explanation, thus revealing the inauthentic details of simulated data explicitly. The proposed framework is validated on four simulated SAR image datasets obtained from electromagnetic models and generative artificial intelligence approaches. The results demonstrate the proposed X-Fake framework outperforms other IQA methods in terms of utility. Furthermore, the results illustrate that the generated counterfactual explanations are trustworthy, and can further improve the data utility in applications.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2406.12009.pdf' target='_blank'>https://arxiv.org/pdf/2406.12009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyue Xu, Peilin Zhou, Xinyu Shi, Jiageng Wu, Yikang Jiang, Dading Chong, Bin Ke, Jie Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12009">FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and transparent financial information disclosure is essential in accounting and finance, fostering trust and enabling informed investment decisions that drive economic development. Among many information disclosure platforms, the Chinese stock exchanges' investor interactive platform provides a novel and interactive way for listed firms to disclose information of interest to investors through an online question-and-answer (Q&A) format. However, it is common for listed firms to respond to questions with limited or no substantive information, and automatically evaluating the quality of financial information disclosure on large amounts of Q&A pairs is challenging. In this study, our interdisciplinary team of AI and finance professionals proposed FinTruthQA, a benchmark designed to evaluate advanced natural language processing (NLP) techniques for the automatic quality assessment of information disclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A entries and each Q&A was manually annotated based on four key evaluation criteria. We benchmarked various NLP techniques on FinTruthQA, including large language models(LLMs). Experiments showed that existing NLP models have strong predictive ability for question identification and question relevance tasks, but are suboptimal for answer readability and answer relevance tasks. By establishing this benchmark, we provide a robust foundation for the automatic evaluation of information disclosure, demonstrating how AI can be leveraged for social good by promoting transparency, fairness, and investor protection in financial disclosure practices. FinTruthQA can be used by auditors, regulators, and financial analysts for real-time monitoring and data-driven decision-making, as well as by researchers for advanced studies in accounting and finance, ultimately fostering greater trust and efficiency in the financial markets.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2405.17765.pdf' target='_blank'>https://arxiv.org/pdf/2405.17765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Yuan, Hongbo Liu, Mading Li, Muyi Sun, Ming Sun, Jiachao Gong, Jinhua Hao, Chao Zhou, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17765">PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is a challenging problem due to the numerous factors that can affect the perceptual quality of a video, \eg, content attractiveness, distortion type, motion pattern, and level. However, annotating the Mean opinion score (MOS) for videos is expensive and time-consuming, which limits the scale of VQA datasets, and poses a significant obstacle for deep learning-based methods. In this paper, we propose a VQA method named PTM-VQA, which leverages PreTrained Models to transfer knowledge from models pretrained on various pre-tasks, enabling benefits for VQA from different aspects.
  Specifically, we extract features of videos from different pretrained models with frozen weights and integrate them to generate representation. Since these models possess various fields of knowledge and are often trained with labels irrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility (ICID) loss to impose constraints on features extracted by multiple pretrained models. The intra-consistency constraint ensures that features extracted by different pretrained models are in the same unified quality-aware latent space, while the inter-divisibility introduces pseudo clusters based on the annotation of samples and tries to separate features of samples from different clusters. Furthermore, with a constantly growing number of pretrained models, it is crucial to determine which models to use and how to use them. To address this problem, we propose an efficient scheme to select suitable candidates. Models with better clustering performance on VQA datasets are chosen to be our candidates. Extensive experiments demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2403.16084.pdf' target='_blank'>https://arxiv.org/pdf/2403.16084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16084">Argument Quality Assessment in the Age of Instruction-Following Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2401.14088.pdf' target='_blank'>https://arxiv.org/pdf/2401.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Torsten Schlett, Christian Rathgeb, Juan Tapia, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14088">Double Trouble? Impact and Detection of Duplicates in Face Image Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various face image datasets intended for facial biometrics research were created via web-scraping, i.e. the collection of images publicly available on the internet. This work presents an approach to detect both exactly and nearly identical face image duplicates, using file and image hashes. The approach is extended through the use of face image preprocessing. Additional steps based on face recognition and face image quality assessment models reduce false positives, and facilitate the deduplication of the face images both for intra- and inter-subject duplicate sets. The presented approach is applied to five datasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a cleaned MS-Celeb-1M variant). Duplicates are detected within every dataset, with hundreds to hundreds of thousands of duplicates for all except LFW. Face recognition and quality assessment experiments indicate a minor impact on the results through the duplicate removal. The final deduplication data is publicly available.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2312.15616.pdf' target='_blank'>https://arxiv.org/pdf/2312.15616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Ravuri, Erica Cooper, Junichi Yamagishi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15616">Uncertainty as a Predictor: Leveraging Self-Supervised Learning for Zero-Shot MOS Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting audio quality in voice synthesis and conversion systems is a critical yet challenging task, especially when traditional methods like Mean Opinion Scores (MOS) are cumbersome to collect at scale. This paper addresses the gap in efficient audio quality prediction, especially in low-resource settings where extensive MOS data from large-scale listening tests may be unavailable. We demonstrate that uncertainty measures derived from out-of-the-box pretrained self-supervised learning (SSL) models, such as wav2vec, correlate with MOS scores. These findings are based on data from the 2022 and 2023 VoiceMOS challenges. We explore the extent of this correlation across different models and language contexts, revealing insights into how inherent uncertainties in SSL models can serve as effective proxies for audio quality assessment. In particular, we show that the contrastive wav2vec models are the most performant in all settings.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2308.00729.pdf' target='_blank'>https://arxiv.org/pdf/2308.00729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Liu, Mingda Wu, Kun Yuan, Ming Sun, Yansong Tang, Chuanchuan Zheng, Xing Wen, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00729">Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (\ie content, distortion, motion) and employ diverse pretrained models (\eg architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2307.16813.pdf' target='_blank'>https://arxiv.org/pdf/2307.16813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Yuan, Zishang Kong, Chuanchuan Zheng, Ming Sun, Xing Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16813">Capturing Co-existing Distortions in User-Generated Content for No-reference Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA), which aims to predict the perceptual quality of a video, has attracted raising attention with the rapid development of streaming media technology, such as Facebook, TikTok, Kwai, and so on. Compared with other sequence-based visual tasks (\textit{e.g.,} action recognition), VQA faces two under-estimated challenges unresolved in User Generated Content (UGC) videos. \textit{First}, it is not rare that several frames containing serious distortions (\textit{e.g.,}blocking, blurriness), can determine the perceptual quality of the whole video, while other sequence-based tasks require more frames of equal importance for representations. \textit{Second}, the perceptual quality of a video exhibits a multi-distortion distribution, due to the differences in the duration and probability of occurrence for various distortions. In order to solve the above challenges, we propose \textit{Visual Quality Transformer (VQT)} to extract quality-related sparse features more efficiently. Methodologically, a Sparse Temporal Attention (STA) is proposed to sample keyframes by analyzing the temporal correlation between frames, which reduces the computational complexity from $O(T^2)$ to $O(T \log T)$. Structurally, a Multi-Pathway Temporal Network (MPTN) utilizes multiple STA modules with different degrees of sparsity in parallel, capturing co-existing distortions in a video. Experimentally, VQT demonstrates superior performance than many \textit{state-of-the-art} methods in three public no-reference VQA datasets. Furthermore, VQT shows better performance in four full-reference VQA datasets against widely-adopted industrial algorithms (\textit{i.e.,} VMAF and AVQT).
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2305.11373.pdf' target='_blank'>https://arxiv.org/pdf/2305.11373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Uchigasaki, Tomo Miyazaki, Shinichiro Omachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11373">Deep Image Compression Using Scene Text Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2304.14123.pdf' target='_blank'>https://arxiv.org/pdf/2304.14123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jannis Priesnitz, Axel WeiÃenfeld, Laurenz Ruzicka, Christian Rathgeb, Bernhard Strobl, Ralph Lessmann, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14123">MCLFIQ: Mobile Contactless Fingerprint Image Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MCLFIQ: Mobile Contactless Fingerprint Image Quality, the first quality assessment algorithm for mobile contactless fingerprint samples. To this end, we re-trained the NIST Fingerprint Image Quality (NFIQ) 2 method, which was originally designed for contact-based fingerprints, with a synthetic contactless fingerprint database. We evaluate the predictive performance of the resulting MCLFIQ model in terms of Error-vs.-Discard Characteristic (EDC) curves on three real-world contactless fingerprint databases using three recognition algorithms. In experiments, the MCLFIQ method is compared against the original NFIQ 2.2 method, a sharpness-based quality assessment algorithm developed for contactless fingerprint images \rev{and the general purpose image quality assessment method BRISQUE. Furthermore, benchmarks on four contact-based fingerprint datasets are also conducted.}
  Obtained results show that the fine-tuning of NFIQ 2 on synthetic contactless fingerprints is a viable alternative to training on real databases. Moreover, the evaluation shows that our MCLFIQ method works more accurate and robust compared to all baseline methods on contactless fingerprints. We suggest considering the proposed MCLFIQ method as a \rev{starting point for the development of} a new standard algorithm for contactless fingerprint quality assessment.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2303.13294.pdf' target='_blank'>https://arxiv.org/pdf/2303.13294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Torsten Schlett, Christian Rathgeb, Juan Tapia, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13294">Considerations on the Evaluation of Biometric Quality Assessment Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment algorithms can be used to estimate the utility of a biometric sample for the purpose of biometric recognition. "Error versus Discard Characteristic" (EDC) plots, and "partial Area Under Curve" (pAUC) values of curves therein, are generally used by researchers to evaluate the predictive performance of such quality assessment algorithms. An EDC curve depends on an error type such as the "False Non Match Rate" (FNMR), a quality assessment algorithm, a biometric recognition system, a set of comparisons each corresponding to a biometric sample pair, and a comparison score threshold corresponding to a starting error. To compute an EDC curve, comparisons are progressively discarded based on the associated samples' lowest quality scores, and the error is computed for the remaining comparisons. Additionally, a discard fraction limit or range must be selected to compute pAUC values, which can then be used to quantitatively rank quality assessment algorithms.
  This paper discusses and analyses various details for this kind of quality assessment algorithm evaluation, including general EDC properties, interpretability improvements for pAUC values based on a hard lower error limit and a soft upper error limit, the use of relative instead of discrete rankings, stepwise vs. linear curve interpolation, and normalisation of quality scores to a [0, 100] integer range. We also analyse the stability of quantitative quality assessment algorithm rankings based on pAUC values across varying pAUC discard fraction limits and starting errors, concluding that higher pAUC discard fraction limits should be preferred. The analyses are conducted both with synthetic data and with real face image and fingerprint data, with a focus on general modality-independent conclusions for EDC evaluations. Various EDC alternatives are discussed as well.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2303.00521.pdf' target='_blank'>https://arxiv.org/pdf/2303.00521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Kun Yuan, Ming Sun, Mading Li, Xing Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00521">Quality-aware Pre-trained Models for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) aims to automatically evaluate the perceived quality of a single image, whose performance has been improved by deep learning-based methods in recent years. However, the paucity of labeled data somewhat restrains deep learning-based BIQA methods from unleashing their full potential. In this paper, we propose to solve the problem by a pretext task customized for BIQA in a self-supervised learning manner, which enables learning representations from orders of magnitude more data. To constrain the learning process, we propose a quality-aware contrastive loss based on a simple assumption: the quality of patches from a distorted image should be similar, but vary from patches from the same image with different degradations and patches from different images. Further, we improve the existing degradation process and form a degradation space with the size of roughly $2\times10^7$. After pre-trained on ImageNet using our method, models are more sensitive to image quality and perform significantly better on downstream BIQA tasks. Experimental results show that our method obtains remarkable improvements on popular BIQA datasets.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2303.00491.pdf' target='_blank'>https://arxiv.org/pdf/2303.00491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Grimmer, Christian Rathgeb, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00491">Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the quality of facial images is essential for operating face recognition systems with sufficient accuracy. The recent advances in face quality standardisation (ISO/IEC CD3 29794-5) recommend the usage of component quality measures for breaking down face quality into its individual factors, hence providing valuable feedback for operators to re-capture low-quality images. In light of recent advances in 3D-aware generative adversarial networks, we propose a novel dataset, Syn-YawPitch, comprising 1000 identities with varying yaw-pitch angle combinations. Utilizing this dataset, we demonstrate that pitch angles beyond 30 degrees have a significant impact on the biometric performance of current face recognition systems. Furthermore, we propose a lightweight and explainable pose quality predictor that adheres to the draft international standard of ISO/IEC CD3 29794-5 and benchmark it against state-of-the-art face image quality assessment algorithms
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2302.12593.pdf' target='_blank'>https://arxiv.org/pdf/2302.12593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Torsten Schlett, Sebastian Schachner, Christian Rathgeb, Juan Tapia, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12593">Effect of Lossy Compression Algorithms on Face Image Quality and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lossy face image compression can degrade the image quality and the utility for the purpose of face recognition. This work investigates the effect of lossy image compression on a state-of-the-art face recognition model, and on multiple face image quality assessment models. The analysis is conducted over a range of specific image target sizes. Four compression types are considered, namely JPEG, JPEG 2000, downscaled PNG, and notably the new JPEG XL format. Frontal color images from the ColorFERET database were used in a Region Of Interest (ROI) variant and a portrait variant. We primarily conclude that JPEG XL allows for superior mean and worst case face recognition performance especially at lower target sizes, below approximately 5kB for the ROI variant, while there appears to be no critical advantage among the compression types at higher target sizes. Quality assessments from modern models correlate well overall with the compression effect on face recognition performance.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2509.20118.pdf' target='_blank'>https://arxiv.org/pdf/2509.20118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Babak Naderi, Ross Cutler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20118">Comparative Study of Subjective Video Quality Assessment Test Methods in Crowdsourcing for Varied Use Cases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In crowdsourced subjective video quality assessment, practitioners often face a choice between Absolute Category Rating (ACR), ACR with Hidden Reference (ACR-HR), and Comparison Category Rating (CCR). We conducted a P.910-compliant, side-by-side comparison across six studies using 15 talking-head sources of good and fair quality, processed with realistic degradations (blur, scaling, compression, freezing, and their combinations), as well as a practical bitrate-ladder task at 720p and 1080p resolutions. We evaluated statistical efficiency (standard deviations), economic efficiency, and decision agreement. Our results show that ACR-HR and ACR correlate strongly at the condition level, while CCR is more sensitive-capturing improvements beyond the reference. ACR-HR, however, exhibits compressed scale use, particularly for videos with fair source quality. ACR-HR is approximately twice as fast and cost-effective, with lower normalized variability, yet the choice of quality measurement method shifts saturation points and bitrate-ladder recommendations. Finally, we provide practical guidance on when to use each test method.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2509.20001.pdf' target='_blank'>https://arxiv.org/pdf/2509.20001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Babak Naderi, Ross Cutler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20001">Ensuring Reliable Participation in Subjective Video Quality Tests Across Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective video quality assessment (VQA) is the gold standard for measuring end-user experience across communication, streaming, and UGC pipelines. Beyond high-validity lab studies, crowdsourcing offers accurate, reliable, faster, and cheaper evaluation-but suffers from unreliable submissions by workers who ignore instructions or game rewards. Recent tests reveal sophisticated exploits of video metadata and rising use of remote-desktop (RD) connections, both of which bias results. We propose objective and subjective detectors for RD users and compare two mainstream crowdsourcing platforms on their susceptibility and mitigation under realistic test conditions and task designs.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2508.20462.pdf' target='_blank'>https://arxiv.org/pdf/2508.20462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilong Zhao, Yindi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20462">Automated Quality Assessment for LLM-Based Complex Qualitative Coding: A Confidence-Diversity Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While previous research demonstrated effective automated quality assessment for accessible LLM coding tasks, a fundamental question remains: can confidence-diversity frameworks maintain reliability for complex analytical tasks requiring specialized domain expertise and extensive text comprehension? Traditional inter-coder reliability measures become prohibitively expensive at scale, yet the lack of reliable automated quality assessment methods creates methodological barriers to AI adoption in sophisticated qualitative research. This study extends dual-signal quality assessment combining model confidence and inter-model consensus from accessible to complex analytical domains. We systematically validate this approach across three domains: legal reasoning (390 Supreme Court cases), political analysis (645 hyperpartisan articles), and medical classification (1,000 clinical transcripts). Results demonstrate that uncertainty-based indicators maintain predictive validity in complex tasks, with external entropy showing consistent negative correlations with accuracy (r = -0.179 to -0.273, p < 0.001) and confidence exhibiting positive correlations in two domains (r = 0.104 to 0.429). Systematic weight optimization achieves 6.6 to 113.7 percent improvements over single-signal approaches, with optimized weights transferring effectively across domains (100 percent success rate). An intelligent triage system reduces manual verification effort by 44.6 percent while maintaining quality standards. These findings establish that automated quality assessment can scale from accessible to complex analytical tasks, providing practical tools for expanding AI-assisted qualitative research. Future work will focus on addressing long-tail challenges in high-disagreement, low-confidence cases to further enhance screening efficiency.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2508.02029.pdf' target='_blank'>https://arxiv.org/pdf/2508.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilong Zhao, Yindi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02029">A Confidence-Diversity Framework for Calibrating AI Judgement in Accessible Qualitative Coding Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLMs enable qualitative coding at large scale, but assessing reliability remains challenging where human experts seldom agree. We investigate confidence-diversity calibration as a quality assessment framework for accessible coding tasks where LLMs already demonstrate strong performance but exhibit overconfidence. Analysing 5,680 coding decisions from eight state-of-the-art LLMs across ten categories, we find that mean self-confidence tracks inter-model agreement closely (Pearson r=0.82). Adding model diversity quantified as normalised Shannon entropy produces a dual signal explaining agreement almost completely (R-squared=0.979), though this high predictive power likely reflects task simplicity for current LLMs. The framework enables a three-tier workflow auto-accepting 35 percent of segments with less than 5 percent error, cutting manual effort by 65 percent. Cross-domain validation confirms transferability (kappa improvements of 0.20 to 0.78). While establishing a methodological foundation for AI judgement calibration, the true potential likely lies in more challenging scenarios where LLMs may demonstrate comparative advantages over human cognitive limitations.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2505.18412.pdf' target='_blank'>https://arxiv.org/pdf/2505.18412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Tang, Ali Abedi, Tracey J. F. Colella, Shehroz S. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18412">Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exercise-based rehabilitation improves quality of life and reduces morbidity, mortality, and rehospitalization, though transportation constraints and staff shortages lead to high dropout rates from rehabilitation programs. Virtual platforms enable patients to complete prescribed exercises at home, while AI algorithms analyze performance, deliver feedback, and update clinicians. Although many studies have developed machine learning and deep learning models for exercise quality assessment, few have explored the use of large language models (LLMs) for feedback and are limited by the lack of rehabilitation datasets containing textual feedback. In this paper, we propose a new method in which exercise-specific features are extracted from the skeletal joints of patients performing rehabilitation exercises and fed into pre-trained LLMs. Using a range of prompting techniques, such as zero-shot, few-shot, chain-of-thought, and role-play prompting, LLMs are leveraged to evaluate exercise quality and provide feedback in natural language to help patients improve their movements. The method was evaluated through extensive experiments on two publicly available rehabilitation exercise assessment datasets (UI-PRMD and REHAB24-6) and showed promising results in exercise assessment, reasoning, and feedback generation. This approach can be integrated into virtual rehabilitation platforms to help patients perform exercises correctly, support recovery, and improve health outcomes.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2504.20447.pdf' target='_blank'>https://arxiv.org/pdf/2504.20447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Lian, Lizhi Wang, Hua Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20447">APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2504.07556.pdf' target='_blank'>https://arxiv.org/pdf/2504.07556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07556">TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While text-to-image (T2I) generation models have achieved remarkable progress in recent years, existing evaluation methodologies for vision-language alignment still struggle with the fine-grained semantic matching. Current approaches based on global similarity metrics often overlook critical token-level correspondences between textual descriptions and visual content. To this end, we present TokenFocus-VQA, a novel evaluation framework that leverages Large Vision-Language Models (LVLMs) through visual question answering (VQA) paradigm with position-specific probability optimization. Our key innovation lies in designing a token-aware loss function that selectively focuses on probability distributions at pre-defined vocabulary positions corresponding to crucial semantic elements, enabling precise measurement of fine-grained semantical alignment. The proposed framework further integrates ensemble learning techniques to aggregate multi-perspective assessments from diverse LVLMs architectures, thereby achieving further performance enhancement. Evaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our TokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method) on public evaluation and 2nd place (0.8426) on the official private test set, demonstrating superiority in capturing nuanced text-image correspondences compared to conventional evaluation methods.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2503.09294.pdf' target='_blank'>https://arxiv.org/pdf/2503.09294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Hu, Chunming He, Lei Xu, Jingduo Tian, Sina Farsiu, Yulun Zhang, Pei Liu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09294">IQPFR: An Image Quality Prior for Blind Face Restoration and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Face Restoration (BFR) addresses the challenge of reconstructing degraded low-quality (LQ) facial images into high-quality (HQ) outputs. Conventional approaches predominantly rely on learning feature representations from ground-truth (GT) data; however, inherent imperfections in GT datasets constrain restoration performance to the mean quality level of the training data, rather than attaining maximally attainable visual quality. To overcome this limitation, we propose a novel framework that incorporates an Image Quality Prior (IQP) derived from No-Reference Image Quality Assessment (NR-IQA) models to guide the restoration process toward optimal HQ reconstructions. Our methodology synergizes this IQP with a learned codebook prior through two critical innovations: (1) During codebook learning, we devise a dual-branch codebook architecture that disentangles feature extraction into universal structural components and HQ-specific attributes, ensuring comprehensive representation of both common and high-quality facial characteristics. (2) In the codebook lookup stage, we implement a quality-conditioned Transformer-based framework. NR-IQA-derived quality scores act as dynamic conditioning signals to steer restoration toward the highest feasible quality standard. This score-conditioned paradigm enables plug-and-play enhancement of existing BFR architectures without modifying the original structure. We also formulate a discrete representation-based quality optimization strategy that circumvents over-optimization artifacts prevalent in continuous latent space approaches. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques across multiple benchmarks. Besides, our quality-conditioned framework demonstrates consistent performance improvements when integrated with prior BFR models. The code will be released.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2408.15218.pdf' target='_blank'>https://arxiv.org/pdf/2408.15218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Xu, Saarthak Kapse, Prateek Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15218">Histo-Diffusion: A Diffusion Super-Resolution Method for Digital Pathology with Comprehensive Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital pathology has advanced significantly over the last decade, with Whole Slide Images (WSIs) encompassing vast amounts of data essential for accurate disease diagnosis. High-resolution WSIs are essential for precise diagnosis but technical limitations in scanning equipment and variablity in slide preparation can hinder obtaining these images. Super-resolution techniques can enhance low-resolution images; while Generative Adversarial Networks (GANs) have been effective in natural image super-resolution tasks, they often struggle with histopathology due to overfitting and mode collapse. Traditional evaluation metrics fall short in assessing the complex characteristics of histopathology images, necessitating robust histology-specific evaluation methods.
  We introduce Histo-Diffusion, a novel diffusion-based method specially designed for generating and evaluating super-resolution images in digital pathology. It includes a restoration module for histopathology prior and a controllable diffusion module for generating high-quality images. We have curated two histopathology datasets and proposed a comprehensive evaluation strategy which incorporates both full-reference and no-reference metrics to thoroughly assess the quality of digital pathology images.
  Comparative analyses on multiple datasets with state-of-the-art methods reveal that Histo-Diffusion outperforms GANs. Our method offers a versatile solution for histopathology image super-resolution, capable of handling multi-resolution generation from varied input sizes, providing valuable support in diagnostic processes.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2406.15695.pdf' target='_blank'>https://arxiv.org/pdf/2406.15695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Feng, Mingyang Song, Jiaqi Wang, Zhuang Chen, Guanqun Bi, Minlie Huang, Liping Jing, Jian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15695">SS-GEN: A Social Story Generation Framework with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Children with Autism Spectrum Disorder (ASD) often misunderstand social situations and struggle to participate in daily routines. Social Stories are traditionally crafted by psychology experts under strict constraints to address these challenges but are costly and limited in diversity. As Large Language Models (LLMs) advance, there's an opportunity to develop more automated, affordable, and accessible methods to generate Social Stories in real-time with broad coverage. However, adapting LLMs to meet the unique and strict constraints of Social Stories is a challenging issue. To this end, we propose SS-GEN, a Social Story GENeration framework with LLMs. Firstly, we develop a constraint-driven sophisticated strategy named StarSow to hierarchically prompt LLMs to generate Social Stories at scale, followed by rigorous human filtering to build a high-quality dataset. Additionally, we introduce quality assessment criteria to evaluate the effectiveness of these generated stories. Considering that powerful closed-source large models require very complex instructions and expensive API fees, we finally fine-tune smaller language models with our curated high-quality dataset, achieving comparable results at lower costs and with simpler instruction and deployment. This work marks a significant step in leveraging AI to personalize Social Stories cost-effectively for autistic children at scale, which we hope can encourage future research on special groups.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2405.09472.pdf' target='_blank'>https://arxiv.org/pdf/2405.09472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinying Lin, Xuyang Liu, Hong Yang, Xiaohai He, Honggang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09472">Perception- and Fidelity-aware Reduced-Reference Super-Resolution Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of image super-resolution (SR) algorithms, how to evaluate the quality of generated SR images has become an urgent task. Although full-reference methods perform well in SR image quality assessment (SR-IQA), their reliance on high-resolution (HR) images limits their practical applicability. Leveraging available reconstruction information as much as possible for SR-IQA, such as low-resolution (LR) images and the scale factors, is a promising way to enhance assessment performance for SR-IQA without HR for reference. In this letter, we attempt to evaluate the perceptual quality and reconstruction fidelity of SR images considering LR images and scale factors. Specifically, we propose a novel dual-branch reduced-reference SR-IQA network, \ie, Perception- and Fidelity-aware SR-IQA (PFIQA). The perception-aware branch evaluates the perceptual quality of SR images by leveraging the merits of global modeling of Vision Transformer (ViT) and local relation of ResNet, and incorporating the scale factor to enable comprehensive visual perception. Meanwhile, the fidelity-aware branch assesses the reconstruction fidelity between LR and SR images through their visual perception. The combination of the two branches substantially aligns with the human visual system, enabling a comprehensive SR image evaluation. Experimental results indicate that our PFIQA outperforms current state-of-the-art models across three widely-used SR-IQA benchmarks. Notably, PFIQA excels in assessing the quality of real-world SR images.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2403.02772.pdf' target='_blank'>https://arxiv.org/pdf/2403.02772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark Karlov, Ali Abedi, Shehroz S. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02772">Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise type. Addressing this issue, this paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entire dataset to train a single model applicable to all exercise types. This model, with a Spatial-Temporal Graph Convolutional Network (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity. Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, UI-PRMD, IRDS, and KIMORE, our method has proven to surpass existing methods, setting a new benchmark in rehabilitation exercise quality assessment.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2402.05482.pdf' target='_blank'>https://arxiv.org/pdf/2402.05482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cho-Yuan Lee, Kuan-Chen Wang, Kai-Chun Liu, Yu-Te Wang, Xugang Lu, Ping-Cheng Yeh, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05482">A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and precision of sEMG quality assessment in practical applications.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2312.08962.pdf' target='_blank'>https://arxiv.org/pdf/2312.08962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, Chao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08962">Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a Depicted image Quality Assessment method (DepictQA), overcoming the constraints of traditional score-based methods. DepictQA allows for detailed, language-based, human-like evaluation of image quality by leveraging Multi-modal Large Language Models (MLLMs). Unlike conventional Image Quality Assessment (IQA) methods relying on scores, DepictQA interprets image content and distortions descriptively and comparatively, aligning closely with humans' reasoning process. To build the DepictQA model, we establish a hierarchical task framework, and collect a multi-modal IQA training dataset. To tackle the challenges of limited training data and multi-image processing, we propose to use multi-source training data and specialized image tags. These designs result in a better performance of DepictQA than score-based approaches on multiple benchmarks. Moreover, compared with general MLLMs, DepictQA can generate more accurate reasoning descriptive languages. We also demonstrate that our full-reference dataset can be extended to non-reference applications. These results showcase the research potential of multi-modal IQA methods. Codes and datasets are available in https://depictqa.github.io.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2309.00769.pdf' target='_blank'>https://arxiv.org/pdf/2309.00769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abrar Majeedi, Babak Naderi, Yasaman Hosseinkashi, Juhee Cho, Ruben Alvarez Martinez, Ross Cutler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00769">Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning-based video codecs have made significant progress in the past few years. A critical area in the development of ML-based video codecs is an accurate evaluation metric that does not require an expensive and slow subjective test. We show that existing evaluation metrics that were designed and trained on DSP-based video codecs are not highly correlated to subjective opinion when used with ML video codecs due to the video artifacts being quite different between ML and video codecs. We provide a new dataset of ML video codec videos that have been accurately labeled for quality. We also propose a new full reference video quality assessment (FRVQA) model that achieves a Pearson Correlation Coefficient (PCC) of 0.99 and a Spearman's Rank Correlation Coefficient (SRCC) of 0.99 at the model level. We make the dataset and FRVQA model open source to help accelerate research in ML video codecs, and so that others can further improve the FRVQA model.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2306.09546.pdf' target='_blank'>https://arxiv.org/pdf/2306.09546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Abedi, Mobin Malmirian, Shehroz S. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09546">Cross-Modal Video to Body-joints Augmentation for Rehabilitation Exercise Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exercise-based rehabilitation programs have been shown to enhance quality of life and reduce mortality and rehospitalizations. AI-driven virtual rehabilitation programs enable patients to complete exercises independently at home while AI algorithms can analyze exercise data to provide feedback to patients and report their progress to clinicians. This paper introduces a novel approach to assessing the quality of rehabilitation exercises using RGB video. Sequences of skeletal body joints are extracted from consecutive RGB video frames and analyzed by many-to-one sequential neural networks to evaluate exercise quality. Existing datasets for exercise rehabilitation lack adequate samples for training deep sequential neural networks to generalize effectively. A cross-modal data augmentation approach is proposed to resolve this problem. Visual augmentation techniques are applied to video data, and body joints extracted from the resulting augmented videos are used for training sequential neural networks. Extensive experiments conducted on the KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE) dataset, demonstrate the superiority of the proposed method over previous baseline approaches. The ablation study highlights a significant enhancement in exercise quality assessment following cross-modal augmentation.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2508.02515.pdf' target='_blank'>https://arxiv.org/pdf/2508.02515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhan Qu, Shuzhou Yuan, Michael FÃ¤rber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02515">PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across four families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a reward signal, we fine-tune three lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2506.19335.pdf' target='_blank'>https://arxiv.org/pdf/2506.19335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko, Noboru Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19335">Learning to assess subjective impressions from speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle a new task of training neural network models that can assess subjective impressions conveyed through speech and assign scores accordingly, inspired by the work on automatic speech quality assessment (SQA). Speech impressions are often described using phrases like `cute voice.' We define such phrases as subjective voice descriptors (SVDs). Focusing on the difference in usage scenarios between the proposed task and automatic SQA, we design a framework capable of accommodating SVDs personalized to each individual, such as `my favorite voice.' In this work, we compiled a dataset containing speech labels derived from both abosolute category ratings (ACR) and comparison category ratings (CCR).
  As an evaluation metric for assessment performance, we introduce ppref, the accuracy of the predicted score ordering of two samples on CCR test samples. Alongside the conventional model and learning methods based on ACR data, we also investigated RankNet learning using CCR data. We experimentally find that the ppref is moderate even with very limited training data. We also discover the CCR training is superior to the ACR training. These results support the idea that assessment models based on personalized SVDs, which typically must be trained on limited data, can be effectively learned from CCR data.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2506.18326.pdf' target='_blank'>https://arxiv.org/pdf/2506.18326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18326">Selecting N-lowest scores for training MOS prediction models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automatic speech quality assessment (SQA) has been extensively studied to predict the speech quality without time-consuming questionnaires. Recently, neural-based SQA models have been actively developed for speech samples produced by text-to-speech or voice conversion, with a primary focus on training mean opinion score (MOS) prediction models. The quality of each speech sample may not be consistent across the entire duration, and it remains unclear which segments of the speech receive the primary focus from humans when assigning subjective evaluation for MOS calculation. We hypothesize that when humans rate speech, they tend to assign more weight to low-quality speech segments, and the variance in ratings for each sample is mainly due to accidental assignment of higher scores when overlooking the poor quality speech segments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC datasets. Based on the hypothesis, we propose the more reliable representative value N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments show that LCC and SRCC improve compared to regular MOS when employing N_low-MOS to MOSNet training. This result suggests that N_low-MOS is a more intrinsic representative value of subjective speech quality and makes MOSNet a better comparator of VC models.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2506.18307.pdf' target='_blank'>https://arxiv.org/pdf/2506.18307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Kondo, Hirokazu Kameoka, Kou Tanaka, Takuhiro Kaneko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18307">Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation through Quantized Distribution Fitting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment (SQA) aims to evaluate the quality of speech samples without relying on time-consuming listener questionnaires. Recent efforts have focused on training neural-based SQA models to predict the mean opinion score (MOS) of speech samples produced by text-to-speech or voice conversion systems. This paper targets the enhancement of MOS prediction models' performance. We propose a novel score aggregation method to address the limitations of conventional annotations for MOS, which typically involve ratings on a scale from 1 to 5. Our method is based on the hypothesis that annotators internally consider continuous scores and then choose the nearest discrete rating. By modeling this process, we approximate the generative distribution of ratings by quantizing the latent continuous distribution. We then use the peak of this latent distribution, estimated through the loss between the quantized distribution and annotated ratings, as a new representative value instead of MOS. Experimental results demonstrate that substituting MOSNet's predicted target with this proposed value improves prediction performance.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2505.17492.pdf' target='_blank'>https://arxiv.org/pdf/2505.17492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dezheng Bao, Yueci Yang, Xin Chen, Zhengxuan Jiang, Zeguo Fei, Daoze Zhang, Xuanwen Huang, Junru Chen, Chutian Yu, Xiang Yuan, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17492">PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Project duplication detection is critical for project quality assessment, as it improves resource utilization efficiency by preventing investing in newly proposed project that have already been studied. It requires the ability to understand high-level semantics and generate constructive and valuable feedback. Existing detection methods rely on basic word- or sentence-level comparison or solely apply large language models, lacking valuable insights for experts and in-depth comprehension of project content and review criteria. To tackle this issue, we propose PD$^3$, a Project Duplication Detection framework via adapted multi-agent Debate. Inspired by real-world expert debates, it employs a fair competition format to guide multi-agent debate to retrieve relevant projects. For feedback, it incorporates both qualitative and quantitative analysis to improve its practicality. Over 800 real-world power project data spanning more than 20 specialized fields are used to evaluate the framework, demonstrating that our method outperforms existing approaches by 7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online platform, Review Dingdang, to assist power experts, saving 5.73 million USD in initial detection on more than 100 newly proposed projects.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2505.14460.pdf' target='_blank'>https://arxiv.org/pdf/2505.14460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14460">VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2412.03210.pdf' target='_blank'>https://arxiv.org/pdf/2412.03210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge Vila-TomÃ¡s, Pablo HernÃ¡ndez-CÃ¡mara, Valero Laparra, JesÃºs Malo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03210">Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human vision models are at the core of image processing. For instance, classical approaches to the problem of image quality are based on models that include knowledge about human vision. However, nowadays, deep learning approaches have obtained competitive results by simply approaching this problem as regression of human decisions, and training an standard network on human-rated datasets. These approaches have the advantages of being easily adaptable to a particular problem and they fit very efficiently when data is available. However, mainly due to the excess of parameters, they have the problems of lack of interpretability, and over-fitting. Here we propose a vision model that combines the best of both worlds by using a parametric neural network architecture. We parameterize the layers to have bioplausible functionality, and provide a set of bioplausible parameters. We analyzed different versions of the model and compared it with the non-parametric version. The parametric models achieve a three orders of magnitude reduction in the number of parameters without suffering in regression performance. Furthermore, we show that the parametric models behave better during training and are easier to interpret as vision models. Interestingly, we find that, even initialized with bioplausible trained for regression using human rated datasets, which we call the feature-spreading problem. This suggests that the deep learning approach is inherently flawed, and emphasizes the need to evaluate and train models beyond regression.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2405.19097.pdf' target='_blank'>https://arxiv.org/pdf/2405.19097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Breger, Ander Biguri, Malena SabatÃ© Landman, Ian Selby, Nicole Amberg, Elisabeth Brunner, Janek GrÃ¶hl, Sepideh Hatamikia, Clemens Karner, Lipeng Ning, SÃ¶ren Dittmer, Michael Roberts, AIX-COVNET Collaboration, Carola-Bibiane SchÃ¶nlieb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19097">A study of why we need to reassess full reference image quality assessment with medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) is indispensable in clinical practice to ensure high standards, as well as in the development stage of machine learning algorithms that operate on medical images. The popular full reference (FR) IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been reported in the literature, highlighting the gap between development and actual clinical application. Such inconsistencies are not surprising, as medical images have very different properties than natural images, and PSNR and SSIM have neither been targeted nor properly tested for medical images. This may cause unforeseen problems in clinical applications due to wrong judgment of novel methods. This paper provides a structured and comprehensive overview of examples where PSNR and SSIM prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore, improvement is urgently needed in particular in this era of AI to increase reliability and explainability in machine learning for medical imaging and beyond. Lastly, we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2403.10854.pdf' target='_blank'>https://arxiv.org/pdf/2403.10854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Wu, Kede Ma, Jie Liang, Yujiu Yang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10854">A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multimodal Large Language Models (MLLMs) have experienced significant advancement in visual understanding and reasoning, their potential to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. We first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one closed-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, geometric transformations, and color differences) in both full-reference and no-reference scenarios. Experimental results show that only the closed-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2312.11024.pdf' target='_blank'>https://arxiv.org/pdf/2312.11024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyao He, Huabin Liu, Yuxi Li, Xiao Ma, Cheng Zhong, Yang Zhang, Weiyao Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11024">Collaborative Weakly Supervised Video Correlation Learning for Procedure-Aware Instructional Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Correlation Learning (VCL), which aims to analyze the relationships between videos, has been widely studied and applied in various general video tasks. However, applying VCL to instructional videos is still quite challenging due to their intrinsic procedural temporal structure. Specifically, procedural knowledge is critical for accurate correlation analyses on instructional videos. Nevertheless, current procedure-learning methods heavily rely on step-level annotations, which are costly and not scalable. To address this problem, we introduce a weakly supervised framework called Collaborative Procedure Alignment (CPA) for procedure-aware correlation learning on instructional videos. Our framework comprises two core modules: collaborative step mining and frame-to-step alignment. The collaborative step mining module enables simultaneous and consistent step segmentation for paired videos, leveraging the semantic and temporal similarity between frames. Based on the identified steps, the frame-to-step alignment module performs alignment between the frames and steps across videos. The alignment result serves as a measurement of the correlation distance between two videos. We instantiate our framework in two distinct instructional video tasks: sequence verification and action quality assessment. Extensive experiments validate the effectiveness of our approach in providing accurate and interpretable correlation analyses for instructional videos.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2304.04952.pdf' target='_blank'>https://arxiv.org/pdf/2304.04952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian Liu, Xiu Li, Yan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04952">Data-Efficient Image Quality Assessment with Attention-Panel Decoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Image Quality Assessment (BIQA) is a fundamental task in computer vision, which however remains unresolved due to the complex distortion conditions and diversified image contents. To confront this challenge, we in this paper propose a novel BIQA pipeline based on the Transformer architecture, which achieves an efficient quality-aware feature representation with much fewer data. More specifically, we consider the traditional fine-tuning in BIQA as an interpretation of the pre-trained model. In this way, we further introduce a Transformer decoder to refine the perceptual information of the CLS token from different perspectives. This enables our model to establish the quality-aware feature manifold efficiently while attaining a strong generalization capability. Meanwhile, inspired by the subjective evaluation behaviors of human, we introduce a novel attention panel mechanism, which improves the model performance and reduces the prediction uncertainty simultaneously. The proposed BIQA method maintains a lightweight design with only one layer of the decoder, yet extensive experiments on eight standard BIQA datasets (both synthetic and authentic) demonstrate its superior performance to the state-of-the-art BIQA methods, i.e., achieving the SRCC values of 0.875 (vs. 0.859 in LIVEC) and 0.980 (vs. 0.969 in LIVE).
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2212.10541.pdf' target='_blank'>https://arxiv.org/pdf/2212.10541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juntao Chen, Li Lin, Pujin Cheng, Yijin Huang, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10541">UNO-QA: An Unsupervised Anomaly-Aware Framework with Test-Time Clustering for OCTA Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image quality assessment (MIQA) is a vital prerequisite in various medical image analysis applications. Most existing MIQA algorithms are fully supervised that request a large amount of annotated data. However, annotating medical images is time-consuming and labor-intensive. In this paper, we propose an unsupervised anomaly-aware framework with test-time clustering for optical coherence tomography angiography (OCTA) image quality assessment in a setting wherein only a set of high-quality samples are accessible in the training phase. Specifically, a feature-embedding-based low-quality representation module is proposed to quantify the quality of OCTA images and then to discriminate between outstanding quality and non-outstanding quality. Within the non-outstanding quality class, to further distinguish gradable images from ungradable ones, we perform dimension reduction and clustering of multi-scale image features extracted by the trained OCTA quality representation network. Extensive experiments are conducted on one publicly accessible dataset sOCTA-3*3-10k, with superiority of our proposed framework being successfully established.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2509.24888.pdf' target='_blank'>https://arxiv.org/pdf/2509.24888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fankai Jia, Daisong Gan, Zhe Zhang, Zhaochi Wen, Chenchen Dan, Dong Liang, Haifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24888">MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Magnetic resonance imaging (MRI) quality assessment is crucial for clinical decision-making, yet remains challenging due to data scarcity and protocol variability. Traditional approaches face fundamental trade-offs: signal-based methods like MRIQC provide quantitative metrics but lack semantic understanding, while deep learning approaches achieve high accuracy but sacrifice interpretability. To address these limitations, we introduce the Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration of multimodal large language models (MLLMs) with acquisition-aware signal processing. MMRQA combines three key innovations: robust metric extraction via MRQy augmented with simulated artifacts, structured transformation of metrics into question-answer pairs using Qwen, and parameter-efficient fusion through Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI, and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with strong zero-shot generalization, as validated by comprehensive ablation studies. By bridging quantitative analysis with semantic reasoning, our framework generates clinically interpretable outputs that enhance quality control in dynamic medical settings.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2508.01503.pdf' target='_blank'>https://arxiv.org/pdf/2508.01503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clayton Cohn, Surya Rayala, Namrata Srivastava, Joyce Horn Fonteles, Shruti Jain, Xinying Luo, Divya Mereddy, Naveeduddin Mohammed, Gautam Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01503">A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2506.08429.pdf' target='_blank'>https://arxiv.org/pdf/2506.08429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Xu, Andrew Estornell, Hongzheng Yang, Yuzhi Zhao, Zhaowei Zhu, Qi Xuan, Jiaheng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08429">Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of visual instruction tuning and other post-training techniques has significantly enhanced the capabilities of Large Language Models (LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with more comprehensive visual language datasets. However, the effectiveness of VLMs is highly dependent on large-scale, high-quality datasets that ensure precise recognition and accurate reasoning. Two key challenges hinder progress: (1) noisy alignments between images and the corresponding text, which leads to misinterpretation, and (2) ambiguous or misleading text, which obscures visual content. To address these challenges, we propose SCALE (Single modality data quality and Cross modality Alignment Evaluation), a novel quality-driven data selection pipeline for VLM instruction tuning datasets. Specifically, SCALE integrates a cross-modality assessment framework that first assigns each data entry to its appropriate vision-language task, generates general and task-specific captions (covering scenes, objects, style, etc.), and evaluates the alignment, clarity, task rarity, text coherence, and image clarity of each entry based on the generated captions. We reveal that: (1) current unimodal quality assessment methods evaluate one modality while overlooking the rest, which can underestimate samples essential for specific tasks and discard the lower-quality instances that help build model robustness; and (2) appropriately generated image captions provide an efficient way to transfer the image-text multimodal task into a unified text modality.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2502.06387.pdf' target='_blank'>https://arxiv.org/pdf/2502.06387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shang Liu, Hanzhao Wang, Zhongyao Ma, Xiaocheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06387">How Humans Help LLMs: Assessing and Incentivizing Human Preference Annotators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-annotated preference data play an important role in aligning large language models (LLMs). In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations. The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data. Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators. The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme. From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent. We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $Î(1/\sqrt{n \log n})$ for the binary contracts and $Î(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\exp(-Î(n))$ for the binary contracts when the action space is discrete. Throughout the paper, we use real preference annotation data to accompany our discussions.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2501.11520.pdf' target='_blank'>https://arxiv.org/pdf/2501.11520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Li, Haojin Li, Mingyang Ou, Xiangyang Yu, Xiaoqing Zhang, Ke Niu, Huazhu Fu, Jiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11520">Fundus Image Quality Assessment and Enhancement: a Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an affordable and convenient eye scan, fundus photography holds the potential for preventing vision impairment, especially in resource-limited regions. However, fundus image degradation is common under intricate imaging environments, impacting following diagnosis and treatment. Consequently, image quality assessment (IQA) and enhancement (IQE) are essential for ensuring the clinical value and reliability of fundus images. While existing reviews offer some overview of this field, a comprehensive analysis of the interplay between IQA and IQE, along with their clinical deployment challenges, is lacking. This paper addresses this gap by providing a thorough review of fundus IQA and IQE algorithms, research advancements, and practical applications. We outline the fundamentals of the fundus photography imaging system and the associated interferences, and then systematically summarize the paradigms in fundus IQA and IQE. Furthermore, we discuss the practical challenges and solutions in deploying IQA and IQE, as well as offer insights into potential future research directions.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2501.07898.pdf' target='_blank'>https://arxiv.org/pdf/2501.07898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07898">Demographic Variability in Face Image Quality Measures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face image quality assessment (FIQA) algorithms are being integrated into online identity management applications. These applications allow users to upload a face image as part of their document issuance process, where the image is then run through a quality assessment process to make sure it meets the quality and compliance requirements. Concerns about demographic bias have been raised about biometric systems, given the societal implications this may cause. It is therefore important that demographic variability in FIQA algorithms is assessed such that mitigation measures can be created. In this work, we study the demographic variability of all face image quality measures included in the ISO/IEC 29794-5 international standard across three demographic variables: age, gender, and skin tone. The results are rather promising and show no clear bias toward any specific demographic group for most measures. Only two quality measures are found to have considerable variations in their outcomes for different groups on the skin tone variable.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2501.07202.pdf' target='_blank'>https://arxiv.org/pdf/2501.07202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07202">FaceOracle: Chat with a Face Image Oracle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A face image is a mandatory part of ID and travel documents. Obtaining high-quality face images when issuing such documents is crucial for both human examiners and automated face recognition systems. In several international standards, face image quality requirements are intricate and defined in detail. Identifying and understanding non-compliance or defects in the submitted face images is crucial for both issuing authorities and applicants. In this work, we introduce FaceOracle, an LLM-powered AI assistant that helps its users analyze a face image in a natural conversational manner using standard compliant algorithms. Leveraging the power of LLMs, users can get explanations of various face image quality concepts as well as interpret the outcome of face image quality assessment (FIQA) algorithms. We implement a proof-of-concept that demonstrates how experts at an issuing authority could integrate FaceOracle into their workflow to analyze, understand, and communicate their decisions more efficiently, resulting in enhanced productivity.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2501.07179.pdf' target='_blank'>https://arxiv.org/pdf/2501.07179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Kabbani, Tristan Le Pessot, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07179">Radial Distortion in Face Images: Detection and Impact</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acquiring face images of sufficiently high quality is important for online ID and travel document issuance applications using face recognition systems (FRS). Low-quality, manipulated (intentionally or unintentionally), or distorted images degrade the FRS performance and facilitate documents' misuse. Securing quality for enrolment images, especially in the unsupervised self-enrolment scenario via a smartphone, becomes important to assure FRS performance. In this work, we focus on the less studied area of radial distortion (a.k.a., the fish-eye effect) in face images and its impact on FRS performance. We introduce an effective radial distortion detection model that can detect and flag radial distortion in the enrolment scenario. We formalize the detection model as a face image quality assessment (FIQA) algorithm and provide a careful inspection of the effect of radial distortion on FRS performance. Evaluation results show excellent detection results for the proposed models, and the study on the impact on FRS uncovers valuable insights into how to best use these models in operational systems.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2501.07158.pdf' target='_blank'>https://arxiv.org/pdf/2501.07158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07158">Eye Sclera for Fair Face Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fair operational systems are crucial in gaining and maintaining society's trust in face recognition systems (FRS). FRS start with capturing an image and assessing its quality before using it further for enrollment or verification. Fair Face Image Quality Assessment (FIQA) schemes therefore become equally important in the context of fair FRS. This work examines the sclera as a quality assessment region for obtaining a fair FIQA. The sclera region is agnostic to demographic variations and skin colour for assessing the quality of a face image. We analyze three skin tone related ISO/IEC face image quality assessment measures and assess the sclera region as an alternative area for assessing FIQ. Our analysis of the face dataset of individuals from different demographic groups representing different skin tones indicates sclera as an alternative to measure dynamic range, over- and under-exposure of face using sclera region alone. The sclera region being agnostic to skin tone, i.e., demographic factors, provides equal utility as a fair FIQA as shown by our Error-vs-Discard Characteristic (EDC) curve analysis.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2501.06488.pdf' target='_blank'>https://arxiv.org/pdf/2501.06488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06488">NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the "same instance, similar representation" assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2412.07079.pdf' target='_blank'>https://arxiv.org/pdf/2412.07079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qu, Xiaoming Chen, Vera Chung, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07079">Light Field Image Quality Assessment With Auxiliary Learning Based on Depthwise and Anglewise Separable Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multimedia broadcasting, no-reference image quality assessment (NR-IQA) is used to indicate the user-perceived quality of experience (QoE) and to support intelligent data transmission while optimizing user experience. This paper proposes an improved no-reference light field image quality assessment (NR-LFIQA) metric for future immersive media broadcasting services. First, we extend the concept of depthwise separable convolution (DSC) to the spatial domain of light field image (LFI) and introduce "light field depthwise separable convolution (LF-DSC)", which can extract the LFI's spatial features efficiently. Second, we further theoretically extend the LF-DSC to the angular space of LFI and introduce the novel concept of "light field anglewise separable convolution (LF-ASC)", which is capable of extracting both the spatial and angular features for comprehensive quality assessment with low complexity. Third, we define the spatial and angular feature estimations as auxiliary tasks in aiding the primary NR-LFIQA task by providing spatial and angular quality features as hints. To the best of our knowledge, this work is the first exploration of deep auxiliary learning with spatial-angular hints on NR-LFIQA. Experiments were conducted in mainstream LFI datasets such as Win5-LID and SMART with comparisons to the mainstream full reference IQA metrics as well as the state-of-the-art NR-LFIQA methods. The experimental results show that the proposed metric yields overall 42.86% and 45.95% smaller prediction errors than the second-best benchmarking metric in Win5-LID and SMART, respectively. In some challenging cases with particular distortion types, the proposed metric can reduce the errors significantly by more than 60%.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2410.24098.pdf' target='_blank'>https://arxiv.org/pdf/2410.24098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clemens Karner, Janek GrÃ¶hl, Ian Selby, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, James H. F. Rudd, Carola-Bibiane SchÃ¶nlieb, Jonathan R Weir-McCall, Anna Breger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24098">Parameter choices in HaarPSI for IQA with medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When developing machine learning models, image quality assessment (IQA) measures are a crucial component for the evaluation of obtained output images. However, commonly used full-reference IQA (FR-IQA) measures have been primarily developed and optimized for natural images. In many specialized settings, such as medical images, this poses an often overlooked problem regarding suitability. In previous studies, the FR-IQA measure HaarPSI showed promising behavior regarding generalizability. The measure is based on Haar wavelet representations and the framework allows optimization of two parameters. So far, these parameters have been aligned for natural images. Here, we optimize these parameters for two medical image data sets, a photoacoustic and a chest X-ray data set, with IQA expert ratings. We observe that they lead to similar parameter values, different to the natural image data, and are more sensitive to parameter changes. We denote the novel optimized setting as HaarPSI$_{MED}$, which improves the performance of the employed medical images significantly (p<0.05). Additionally, we include an independent CT test data set that illustrates the generalizability of HaarPSI$_{MED}$, as well as visual examples that qualitatively demonstrate the improvement. The results suggest that adapting common IQA measures within their frameworks for medical images can provide a valuable, generalizable addition to employment of more specific task-based measures.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2409.12854.pdf' target='_blank'>https://arxiv.org/pdf/2409.12854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philippe Zhang, Pierre-Henri Conze, Mathieu Lamard, GwenolÃ© Quellec, Mostafa El Habib Daho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12854">Deep Learning-Based Detection of Referable Diabetic Retinopathy and Macular Edema Using Ultra-Widefield Fundus Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy and diabetic macular edema are significant complications of diabetes that can lead to vision loss. Early detection through ultra-widefield fundus imaging enhances patient outcomes but presents challenges in image quality and analysis scale. This paper introduces deep learning solutions for automated UWF image analysis within the framework of the MICCAI 2024 UWF4DR challenge. We detail methods and results across three tasks: image quality assessment, detection of referable DR, and identification of DME. Employing advanced convolutional neural network architectures such as EfficientNet and ResNet, along with preprocessing and augmentation strategies, our models demonstrate robust performance in these tasks. Results indicate that deep learning can significantly aid in the automated analysis of UWF images, potentially improving the efficiency and accuracy of DR and DME detection in clinical settings.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2409.05595.pdf' target='_blank'>https://arxiv.org/pdf/2409.05595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05595">SynMorph: Generating Synthetic Face Morphing Dataset with Mated Samples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face morphing attack detection (MAD) algorithms have become essential to overcome the vulnerability of face recognition systems. To solve the lack of large-scale and public-available datasets due to privacy concerns and restrictions, in this work we propose a new method to generate a synthetic face morphing dataset with 2450 identities and more than 100k morphs. The proposed synthetic face morphing dataset is unique for its high-quality samples, different types of morphing algorithms, and the generalization for both single and differential morphing attack detection algorithms. For experiments, we apply face image quality assessment and vulnerability analysis to evaluate the proposed synthetic face morphing dataset from the perspective of biometric sample quality and morphing attack potential on face recognition systems. The results are benchmarked with an existing SOTA synthetic dataset and a representative non-synthetic and indicate improvement compared with the SOTA. Additionally, we design different protocols and study the applicability of using the proposed synthetic dataset on training morphing attack detection algorithms.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2408.08396.pdf' target='_blank'>https://arxiv.org/pdf/2408.08396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Rege Cambrin, Gabriele Scaffidi Militone, Luca Colomba, Giovanni Malnati, Daniele Apiletti, Paolo Garza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08396">Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective game tutorials is crucial for a smooth learning curve for new players, especially in games with many rules and complex core mechanics. Evaluating the effectiveness of these tutorials usually requires multiple iterations with testers who have no prior knowledge of the game. Recent Vision-Language Models (VLMs) have demonstrated significant capabilities in understanding and interpreting visual content. VLMs can analyze images, provide detailed insights, and answer questions about their content. They can recognize objects, actions, and contexts in visual data, making them valuable tools for various applications, including automated game testing. In this work, we propose an automated game-testing solution to evaluate the quality of game tutorials. Our approach leverages VLMs to analyze frames from video game tutorials, answer relevant questions to simulate human perception, and provide feedback. This feedback is compared with expected results to identify confusing or problematic scenes and highlight potential errors for developers. In addition, we publish complete tutorial videos and annotated frames from different game versions used in our tests. This solution reduces the need for extensive manual testing, especially by speeding up and simplifying the initial development stages of the tutorial to improve the final game experience.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2405.19224.pdf' target='_blank'>https://arxiv.org/pdf/2405.19224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Breger, Clemens Karner, Ian Selby, Janek GrÃ¶hl, SÃ¶ren Dittmer, Edward Lilley, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, Carola-Bibiane SchÃ¶nlieb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19224">A study on the adequacy of common IQA measures for medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) is standard practice in the development stage of novel machine learning algorithms that operate on images. The most commonly used IQA measures have been developed and tested for natural images, but not in the medical setting. Reported inconsistencies arising in medical images are not surprising, as they have different properties than natural images. In this study, we test the applicability of common IQA measures for medical image data by comparing their assessment to manually rated chest X-ray (5 experts) and photoacoustic image data (2 experts). Moreover, we include supplementary studies on grayscale natural images and accelerated brain MRI data. The results of all experiments show a similar outcome in line with previous findings for medical images: PSNR and SSIM in the default setting are in the lower range of the result list and HaarPSI outperforms the other tested measures in the overall performance. Also among the top performers in our experiments are the full reference measures FSIM, LPIPS and MS-SSIM. Generally, the results on natural images yield considerably higher correlations, suggesting that additional employment of tailored IQA measures for medical imaging algorithms is needed.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2405.01273.pdf' target='_blank'>https://arxiv.org/pdf/2405.01273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Praveen Kumar Chandaliya, Kiran Raja, Raghavendra Ramachandra, Zahid Akhtar, Christoph Busch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01273">Towards Inclusive Face Recognition Through Synthetic Ethnicity Alteration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous studies have shown that existing Face Recognition Systems (FRS), including commercial ones, often exhibit biases toward certain ethnicities due to under-represented data. In this work, we explore ethnicity alteration and skin tone modification using synthetic face image generation methods to increase the diversity of datasets. We conduct a detailed analysis by first constructing a balanced face image dataset representing three ethnicities: Asian, Black, and Indian. We then make use of existing Generative Adversarial Network-based (GAN) image-to-image translation and manifold learning models to alter the ethnicity from one to another. A systematic analysis is further conducted to assess the suitability of such datasets for FRS by studying the realistic skin-tone representation using Individual Typology Angle (ITA). Further, we also analyze the quality characteristics using existing Face image quality assessment (FIQA) approaches. We then provide a holistic FRS performance analysis using four different systems. Our findings pave the way for future research works in (i) developing both specific ethnicity and general (any to any) ethnicity alteration models, (ii) expanding such approaches to create databases with diverse skin tones, (iii) creating datasets representing various ethnicities which further can help in mitigating bias while addressing privacy concerns.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2404.18458.pdf' target='_blank'>https://arxiv.org/pdf/2404.18458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luzhe Huang, Yuzhu Li, Nir Pillar, Tal Keidar Haran, William Dean Wallace, Aydogan Ozcan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18458">A robust and scalable framework for hallucination detection in virtual tissue staining and digital pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathological staining of human tissue is essential for disease diagnosis. Recent advances in virtual tissue staining technologies using artificial intelligence (AI) alleviate some of the costly and tedious steps involved in traditional histochemical staining processes, permitting multiplexed staining and tissue preservation. However, potential hallucinations and artifacts in these virtually stained tissue images pose concerns, especially for the clinical uses of these approaches. Quality assessment of histology images by experts can be subjective. Here, we present an autonomous quality and hallucination assessment method, AQuA, for virtual tissue staining and digital pathology. AQuA autonomously achieves 99.8% accuracy when detecting acceptable and unacceptable virtually stained tissue images without access to histochemically stained ground truth, and presents an agreement of 98.5% with the manual assessments made by board-certified pathologists, including identifying realistic-looking images that could mislead diagnosticians. We demonstrate the wide adaptability of AQuA across various virtually and histochemically stained human tissue images. This framework enhances the reliability of virtual tissue staining and provides autonomous quality assurance for image generation and transformation tasks in digital pathology and computational imaging.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2403.17232.pdf' target='_blank'>https://arxiv.org/pdf/2403.17232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathaniel Hanson, Gary Lvov, Vedant Rautela, Samuel Hibbard, Ethan Holand, Charles DiMarzio, TaÅkÄ±n PadÄ±r
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17232">PROSPECT: Precision Robot Spectroscopy Exploration and Characterization Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Near Infrared (NIR) spectroscopy is widely used in industrial quality control and automation to test the purity and grade of items. In this research, we propose a novel sensorized end effector and acquisition strategy to capture spectral signatures from objects and register them with a 3D point cloud. Our methodology first takes a 3D scan of an object generated by a time-of-flight depth camera and decomposes the object into a series of planned viewpoints covering the surface. We generate motion plans for a robot manipulator and end-effector to visit these viewpoints while maintaining a fixed distance and surface normal. This process is enabled by the spherical motion of the end-effector and ensures maximal spectral signal quality. By continuously acquiring surface reflectance values as the end-effector scans the target object, the autonomous system develops a four-dimensional model of the target object: position in an $R^3$ coordinate frame, and a reflectance vector denoting the associated spectral signature. We demonstrate this system in building spectral-spatial object profiles of increasingly complex geometries. We show the proposed system and spectral acquisition planning produce more consistent spectral signals than naive point scanning strategies. Our work represents a significant step towards high-resolution spectral-spatial sensor fusion for automated quality assessment.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2312.15102.pdf' target='_blank'>https://arxiv.org/pdf/2312.15102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Kabbani, Christoph Busch, Kiran Raja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15102">Robust Sclera Segmentation for Skin-tone Agnostic Face Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face image quality assessment (FIQA) is crucial for obtaining good face recognition performance. FIQA algorithms should be robust and insensitive to demographic factors. The eye sclera has a consistent whitish color in all humans regardless of their age, ethnicity and skin-tone. This work proposes a robust sclera segmentation method that is suitable for face images in the enrolment and the border control face recognition scenarios. It shows how the statistical analysis of the sclera pixels produces features that are invariant to skin-tone, age and ethnicity and thus can be incorporated into FIQA algorithms to make them agnostic to demographic factors.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2303.10961.pdf' target='_blank'>https://arxiv.org/pdf/2303.10961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qu, Xiaoming Chen, Yuk Ying Chung, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10961">LFACon: Introducing Anglewise Attention to No-Reference Quality Assessment in Light Field Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Light field imaging can capture both the intensity information and the direction information of light rays. It naturally enables a six-degrees-of-freedom viewing experience and deep user engagement in virtual reality. Compared to 2D image assessment, light field image quality assessment (LFIQA) needs to consider not only the image quality in the spatial domain but also the quality consistency in the angular domain. However, there is a lack of metrics to effectively reflect the angular consistency and thus the angular quality of a light field image (LFI). Furthermore, the existing LFIQA metrics suffer from high computational costs due to the excessive data volume of LFIs. In this paper, we propose a novel concept of "anglewise attention" by introducing a multihead self-attention mechanism to the angular domain of an LFI. This mechanism better reflects the LFI quality. In particular, we propose three new attention kernels, including anglewise self-attention, anglewise grid attention, and anglewise central attention. These attention kernels can realize angular self-attention, extract multiangled features globally or selectively, and reduce the computational cost of feature extraction. By effectively incorporating the proposed kernels, we further propose our light field attentional convolutional neural network (LFACon) as an LFIQA metric. Our experimental results show that the proposed LFACon metric significantly outperforms the state-of-the-art LFIQA metrics. For the majority of distortion types, LFACon attains the best performance with lower complexity and less computational time.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2303.07489.pdf' target='_blank'>https://arxiv.org/pdf/2303.07489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Ke, Tianhao Zhang, Yilin Wang, Peyman Milanfar, Feng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07489">MRET: Multi-resolution Transformer for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference video quality assessment (NR-VQA) for user generated content (UGC) is crucial for understanding and improving visual experience. Unlike video recognition tasks, VQA tasks are sensitive to changes in input resolution. Since large amounts of UGC videos nowadays are 720p or above, the fixed and relatively small input used in conventional NR-VQA methods results in missing high-frequency details for many videos. In this paper, we propose a novel Transformer-based NR-VQA framework that preserves the high-resolution quality information. With the multi-resolution input representation and a novel multi-resolution patch sampling mechanism, our method enables a comprehensive view of both the global video composition and local high-resolution details. The proposed approach can effectively aggregate quality information across different granularities in spatial and temporal dimensions, making the model robust to input resolution variations. Our method achieves state-of-the-art performance on large-scale UGC VQA datasets LSVQ and LSVQ-1080p, and on KoNViD-1k and LIVE-VQC without fine-tuning.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2301.11158.pdf' target='_blank'>https://arxiv.org/pdf/2301.11158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Tigina, Anastasiia Birillo, Yaroslav Golubev, Hieke Keuning, Nikolay Vyahhi, Timofey Bryksin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11158">Analyzing the Quality of Submissions in Online Programming Courses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Programming education should aim to provide students with a broad range of skills that they will later use while developing software. An important aspect in this is their ability to write code that is not only correct but also of high quality. Unfortunately, this is difficult to control in the setting of a massive open online course. In this paper, we carry out an analysis of the code quality of submissions from JetBrains Academy - a platform for studying programming in an industry-like project-based setting with an embedded code quality assessment tool called Hyperstyle. We analyzed more than a million Java submissions and more than 1.3 million Python submissions, studied the most prevalent types of code quality issues and the dynamics of how students fix them. We provide several case studies of different issues, as well as an analysis of why certain issues remain unfixed even after several attempts. Also, we studied abnormally long sequences of submissions, in which students attempted to fix code quality issues after passing the task. Our results point the way towards the improvement of online courses, such as making sure that the task itself does not incentivize students to write code poorly.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2510.04097.pdf' target='_blank'>https://arxiv.org/pdf/2510.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peichao Lai, Jinhui Zhuang, Kexuan Zhang, Ningchang Xiong, Shengjie Wang, Yanwei Xu, Chong Chen, Yilei Wang, Bin Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04097">WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 45.1k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2509.22227.pdf' target='_blank'>https://arxiv.org/pdf/2509.22227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weidan Xiong, Bochuan Zeng, Ziyu Hu, Jianwei Guo, Ke Xie, Hui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22227">Aerial Path Planning for Urban Geometry and Texture Co-Capture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in image acquisition and scene reconstruction have enabled the generation of high-quality structural urban scene geometry, given sufficient site information. However, current capture techniques often overlook the crucial importance of texture quality, resulting in noticeable visual artifacts in the textured models. In this work, we introduce the urban geometry and texture co-capture problem under limited prior knowledge before a site visit. The only inputs are a 2D building contour map of the target area and a safe flying altitude above the buildings. We propose an innovative aerial path planning framework designed to co-capture images for reconstructing both structured geometry and high-fidelity textures. To evaluate and guide view planning, we introduce a comprehensive texture quality assessment system, including two novel metrics tailored for building facades. Firstly, our method generates high-quality vertical dipping views and horizontal planar views to effectively capture both geometric and textural details. A multi-objective optimization strategy is then proposed to jointly maximize texture fidelity, improve geometric accuracy, and minimize the cost associated with aerial views. Furthermore, we present a sequential path planning algorithm that accounts for texture consistency during image capture. Extensive experiments on large-scale synthetic and real-world urban datasets demonstrate that our approach effectively produces image sets suitable for concurrent geometric and texture reconstruction, enabling the creation of realistic, textured scene proxies at low operational cost.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2506.09795.pdf' target='_blank'>https://arxiv.org/pdf/2506.09795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amritha Premkumar, Prajit T Rajendran, Vignesh V Menon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09795">Learning Quality from Complexity and Structure: A Feature-Fused XGBoost Model for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach for reduced-reference video quality assessment (VQA), developed as part of the recent VQA Grand Challenge. Our method leverages low-level complexity and structural information from reference and test videos to predict perceptual quality scores. Specifically, we extract spatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM values from the test video to capture both texture and structural characteristics. These features are aggregated through temporal pooling, and residual features are calculated by comparing the original and distorted feature sets. The combined features are used to train an XGBoost regression model that estimates the overall video quality. The pipeline is fully automated, interpretable, and highly scalable, requiring no deep neural networks or GPU inference. Experimental results on the challenge dataset demonstrate that our proposed method achieves competitive correlation with subjective quality scores while maintaining a low computational footprint. The model's lightweight design and strong generalization performance suit real-time streaming quality monitoring and adaptive encoding scenarios.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2505.14028.pdf' target='_blank'>https://arxiv.org/pdf/2505.14028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, Rui Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14028">OmniStyle: Filtering High Quality Style Transfer Data at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer dataset comprising over one million content-style-stylized image triplets across 1,000 diverse style categories, each enhanced with textual descriptions and instruction prompts. We show that OmniStyle-1M can not only enable efficient and scalable of style transfer models through supervised training but also facilitate precise control over target stylization. Especially, to ensure the quality of the dataset, we introduce OmniFilter, a comprehensive style transfer quality assessment framework, which filters high-quality triplets based on content preservation, style consistency, and aesthetic appeal. Building upon this foundation, we propose OmniStyle, a framework based on the Diffusion Transformer (DiT) architecture designed for high-quality and efficient style transfer. This framework supports both instruction-guided and image-guided style transfer, generating high resolution outputs with exceptional detail. Extensive qualitative and quantitative evaluations demonstrate OmniStyle's superior performance compared to existing approaches, highlighting its efficiency and versatility. OmniStyle-1M and its accompanying methodologies provide a significant contribution to advancing high-quality style transfer, offering a valuable resource for the research community.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2505.11915.pdf' target='_blank'>https://arxiv.org/pdf/2505.11915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davoud Shariat Panah, Dan Barry, Alessandro Ragano, Jan Skoglund, Andrew Hines
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11915">BINAQUAL: A Full-Reference Objective Localization Similarity Metric for Binaural Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial audio enhances immersion in applications such as virtual reality, augmented reality, gaming, and cinema by creating a three-dimensional auditory experience. Ensuring the spatial fidelity of binaural audio is crucial, given that processes such as compression, encoding, or transmission can alter localization cues. While subjective listening tests like MUSHRA remain the gold standard for evaluating spatial localization quality, they are costly and time-consuming. This paper introduces BINAQUAL, a full-reference objective metric designed to assess localization similarity in binaural audio recordings. BINAQUAL adapts the AMBIQUAL metric, originally developed for localization quality assessment in ambisonics audio format to the binaural domain. We evaluate BINAQUAL across five key research questions, examining its sensitivity to variations in sound source locations, angle interpolations, surround speaker layouts, audio degradations, and content diversity. Results demonstrate that BINAQUAL effectively differentiates between subtle spatial variations and correlates strongly with subjective listening tests, making it a reliable metric for binaural localization quality assessment. The proposed metric provides a robust benchmark for ensuring spatial accuracy in binaural audio processing, paving the way for improved objective evaluations in immersive audio applications.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2502.09914.pdf' target='_blank'>https://arxiv.org/pdf/2502.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shixiao Wang, Runsheng Zhang, Junliang Du, Ran Hao, Jiacheng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09914">A Deep Learning Approach to Interface Color Quality Assessment in HCI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a quantitative evaluation model for the color quality of human-computer interaction interfaces is proposed by combining deep convolutional neural networks (CNN). By extracting multidimensional features of interface images, including hue, brightness, purity, etc., CNN is used for efficient feature modeling and quantitative analysis, and the relationship between interface design and user perception is studied. The experiment is based on multiple international mainstream website interface datasets, covering e-commerce platforms, social media, education platforms, etc., and verifies the evaluation effect of the model on indicators such as contrast, clarity, color coordination, and visual appeal. The results show that the CNN evaluation is highly consistent with the user rating, with a correlation coefficient of up to 0.96, and it also shows high accuracy in mean square error and absolute error. Compared with traditional experience-based evaluation methods, the proposed model can efficiently and scientifically capture the visual characteristics of the interface and avoid the influence of subjective factors. Future research can explore the introduction of multimodal data (such as text and interactive behavior) into the model to further enhance the evaluation ability of dynamic interfaces and expand it to fields such as smart homes, medical systems, and virtual reality. This paper provides new methods and new ideas for the scientific evaluation and optimization of interface design.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2411.05794.pdf' target='_blank'>https://arxiv.org/pdf/2411.05794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Ragano, Helard Becerra Martinez, Andrew Hines
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05794">Beyond Correlation: Evaluating Multimedia Quality Models with the Constrained Concordance Index</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson's Correlation Coefficient (PCC), Spearman's Rank Correlation Coefficient (SRCC), and Kendall's Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2410.06675.pdf' target='_blank'>https://arxiv.org/pdf/2410.06675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Ragano, Jan Skoglund, Andrew Hines
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06675">SCOREQ: Speech Quality Assessment with Contrastive Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present SCOREQ, a novel approach for speech quality prediction. SCOREQ is a triplet loss function for contrastive regression that addresses the domain generalisation shortcoming exhibited by state of the art no-reference speech quality metrics. In the paper we: (i) illustrate the problem of L2 loss training failing at capturing the continuous nature of the mean opinion score (MOS) labels; (ii) demonstrate the lack of generalisation through a benchmarking evaluation across several speech domains; (iii) outline our approach and explore the impact of the architectural design decisions through incremental evaluation; (iv) evaluate the final model against state of the art models for a wide variety of data and domains. The results show that the lack of generalisation observed in state of the art speech quality metrics is addressed by SCOREQ. We conclude that using a triplet loss function for contrastive regression improves generalisation for speech quality prediction models but also has potential utility across a wide range of applications using regression-based predictive models.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2404.07206.pdf' target='_blank'>https://arxiv.org/pdf/2404.07206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07206">GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2309.16284.pdf' target='_blank'>https://arxiv.org/pdf/2309.16284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Ragano, Jan Skoglund, Andrew Hines
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16284">NOMAD: Unsupervised Learning of Perceptual Embeddings for Speech Enhancement and Non-matching Reference Audio Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents NOMAD (Non-Matching Audio Distance), a differentiable perceptual similarity metric that measures the distance of a degraded signal against non-matching references. The proposed method is based on learning deep feature embeddings via a triplet loss guided by the Neurogram Similarity Index Measure (NSIM) to capture degradation intensity. During inference, the similarity score between any two audio samples is computed through Euclidean distance of their embeddings. NOMAD is fully unsupervised and can be used in general perceptual audio tasks for audio analysis e.g. quality assessment and generative tasks such as speech enhancement and speech synthesis. The proposed method is evaluated with 3 tasks. Ranking degradation intensity, predicting speech quality, and as a loss function for speech enhancement. Results indicate NOMAD outperforms other non-matching reference approaches in both ranking degradation intensity and quality assessment, exhibiting competitive performance with full-reference audio metrics. NOMAD demonstrates a promising technique that mimics human capabilities in assessing audio quality with non-matching references to learn perceptual embeddings without the need for human-generated labels.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2308.01940.pdf' target='_blank'>https://arxiv.org/pdf/2308.01940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Yang, Joel Jung, Haiqiang Wang, Xiaozhong Xu, Shan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01940">TSMD: A Database for Static Color Mesh Quality Assessment Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Static meshes with texture map are widely used in modern industrial and manufacturing sectors, attracting considerable attention in the mesh compression community due to its huge amount of data. To facilitate the study of static mesh compression algorithm and objective quality metric, we create the Tencent - Static Mesh Dataset (TSMD) containing 42 reference meshes with rich visual characteristics. 210 distorted samples are generated by the lossy compression scheme developed for the Call for Proposals on polygonal static mesh coding, released on June 23 by the Alliance for Open Media Volumetric Visual Media group. Using processed video sequences, a large-scale, crowdsourcing-based, subjective experiment was conducted to collect subjective scores from 74 viewers. The dataset undergoes analysis to validate its sample diversity and Mean Opinion Scores (MOS) accuracy, establishing its heterogeneous nature and reliability. State-of-the-art objective metrics are evaluated on the new dataset. Pearson and Spearman correlations around 0.75 are reported, deviating from results typically observed on less heterogeneous datasets, demonstrating the need for further development of more robust metrics. The TSMD, including meshes, PVSs, bitstreams, and MOS, is made publicly available at the following location: https://multimedia.tencent.com/resources/tsmd.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2306.11900.pdf' target='_blank'>https://arxiv.org/pdf/2306.11900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenbin Qian, Constantin Orasan, Felix do Carmo, Qiuliang Li, Diptesh Kanojia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11900">Evaluation of Chinese-English Machine Translation of Emotion-Loaded Microblog Texts: A Human Annotated Dataset for the Quality Assessment of Emotion Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on how current Machine Translation (MT) tools perform on the translation of emotion-loaded texts by evaluating outputs from Google Translate according to a framework proposed in this paper. We propose this evaluation framework based on the Multidimensional Quality Metrics (MQM) and perform a detailed error analysis of the MT outputs. From our analysis, we observe that about 50% of the MT outputs fail to preserve the original emotion. After further analysis of the errors, we find that emotion carrying words and linguistic phenomena such as polysemous words, negation, abbreviation etc., are common causes for these translation errors.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2304.10234.pdf' target='_blank'>https://arxiv.org/pdf/2304.10234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh V Menon, Reza Farahani, Prajit T Rajendran, Mohammed Ghanbari, Hermann Hellwagner, Christian Timmerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10234">Transcoding Quality Prediction for Adaptive Video Streaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, video streaming applications have proliferated the demand for Video Quality Assessment VQA). Reduced reference video quality assessment (RR-VQA) is a category of VQA where certain features (e.g., texture, edges) of the original video are provided for quality assessment. It is a popular research area for various applications such as social media, online games, and video streaming. This paper introduces a reduced reference Transcoding Quality Prediction Model (TQPM) to determine the visual quality score of the video possibly transcoded in multiple stages. The quality is predicted using Discrete Cosine Transform (DCT)-energy-based features of the video (i.e., the video's brightness, spatial texture information, and temporal activity) and the target bitrate representation of each transcoding stage. To do that, the problem is formulated, and a Long Short-Term Memory (LSTM)-based quality prediction model is presented. Experimental results illustrate that, on average, TQPM yields PSNR, SSIM, and VMAF predictions with an R2 score of 0.83, 0.85, and 0.87, respectively, and Mean Absolute Error (MAE) of 1.31 dB, 1.19 dB, and 3.01, respectively, for single-stage transcoding. Furthermore, an R2 score of 0.84, 0.86, and 0.91, respectively, and MAE of 1.32 dB, 1.33 dB, and 3.25, respectively, are observed for a two-stage transcoding scenario. Moreover, the average processing time of TQPM for 4s segments is 0.328s, making it a practical VQA method in online streaming applications.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2303.15038.pdf' target='_blank'>https://arxiv.org/pdf/2303.15038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Che, Siyu Chen, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15038">Image Quality-aware Diagnosis via Meta-knowledge Co-embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images usually suffer from image degradation in clinical practice, leading to decreased performance of deep learning-based models. To resolve this problem, most previous works have focused on filtering out degradation-causing low-quality images while ignoring their potential value for models. Through effectively learning and leveraging the knowledge of degradations, models can better resist their adverse effects and avoid misdiagnosis. In this paper, we raise the problem of image quality-aware diagnosis, which aims to take advantage of low-quality images and image quality labels to achieve a more accurate and robust diagnosis. However, the diversity of degradations and superficially unrelated targets between image quality assessment and disease diagnosis makes it still quite challenging to effectively leverage quality labels to assist diagnosis. Thus, to tackle these issues, we propose a novel meta-knowledge co-embedding network, consisting of two subnets: Task Net and Meta Learner. Task Net constructs an explicit quality information utilization mechanism to enhance diagnosis via knowledge co-embedding features, while Meta Learner ensures the effectiveness and constrains the semantics of these features via meta-learning and joint-encoding masking. Superior performance on five datasets with four widely-used medical imaging modalities demonstrates the effectiveness and generalizability of our method.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2302.14465.pdf' target='_blank'>https://arxiv.org/pdf/2302.14465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh V Menon, Prajit T Rajendran, Reza Farahani, Klaus Schoeffmann, Christian Timmerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14465">Video Quality Assessment with Texture Information Fusion for Streaming Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise in video streaming applications has increased the demand for video quality assessment (VQA). In 2016, Netflix introduced Video Multi-Method Assessment Fusion (VMAF), a full reference VQA metric that strongly correlates with perceptual quality, but its computation is time-intensive. We propose a Discrete Cosine Transform (DCT)-energy-based VQA with texture information fusion (VQ-TIF) model for video streaming applications that determines the visual quality of the reconstructed video compared to the original video. VQ-TIF extracts Structural Similarity (SSIM) and spatiotemporal features of the frames from the original and reconstructed videos and fuses them using a long short-term memory (LSTM)-based model to estimate the visual quality. Experimental results show that VQ-TIF estimates the visual quality with a Pearson Correlation Coefficient (PCC) of 0.96 and a Mean Absolute Error (MAE) of 2.71, on average, compared to the ground truth VMAF scores. Additionally, VQ-TIF estimates the visual quality at a rate of 9.14 times faster than the state-of-the-art VMAF implementation, along with an 89.44 % reduction in energy consumption, assuming an Ultra HD (2160p) display resolution.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2511.05551.pdf' target='_blank'>https://arxiv.org/pdf/2511.05551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaojie Zheng, Jiucai Zhang, Xiaoli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05551">In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based quality assessment in additive manufacturing often requires dedicated machine learning models and application-specific datasets. However, data collection and model training can be expensive and time-consuming. In this paper, we leverage vision-language models' (VLMs') reasoning capabilities to assess the quality of printed parts and introduce in-context learning (ICL) to provide VLMs with necessary application-specific knowledge and demonstration samples. This method eliminates the requirement for large application-specific datasets for training models. We explored different sampling strategies for ICL to search for the optimal configuration that makes use of limited samples. We evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with quality assessment tasks in wire-laser direct energy deposition processes. The results show that ICL-assisted VLMs can reach quality classification accuracies similar to those of traditional machine learning models while requiring only a minimal number of samples. In addition, unlike traditional classification models that lack transparency, VLMs can generate human-interpretable rationales to enhance trust. Since there are no metrics to evaluate their interpretability in manufacturing applications, we propose two metrics, knowledge relevance and rationale validity, to evaluate the quality of VLMs' supporting rationales. Our results show that ICL-assisted VLMs can address application-specific tasks with limited data, achieving relatively high accuracy while also providing valid supporting rationales for improved decision transparency.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2509.21967.pdf' target='_blank'>https://arxiv.org/pdf/2509.21967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javad Hassannataj Joloudari, Bita Mesbahzadeh, Omid Zare, Emrah Arslan, Roohallah Alizadehsani, Hossein Moosaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21967">No-Reference Image Contrast Assessment with Customized EfficientNet-B0</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image contrast was a fundamental factor in visual perception and played a vital role in overall image quality. However, most no reference image quality assessment NR IQA models struggled to accurately evaluate contrast distortions under diverse real world conditions. In this study, we proposed a deep learning based framework for blind contrast quality assessment by customizing and fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and MobileNetV2, for perceptual Mean Opinion Score, along with an additional model built on a Siamese network, which indicated a limited ability to capture perceptual contrast distortions. Each model is modified with a contrast-aware regression head and trained end to end using targeted data augmentations on two benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic contrast distortions. Performance is evaluated using Pearson Linear Correlation Coefficient and Spearman Rank Order Correlation Coefficient, which assess the alignment between predicted and human rated scores. Among these three models, our customized EfficientNet B0 model achieved state-of-the-art performance with PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369 on CID2013, surpassing traditional methods and outperforming other deep baselines. These results highlighted the models robustness and effectiveness in capturing perceptual contrast distortion. Overall, the proposed method demonstrated that contrast aware adaptation of lightweight pre trained networks can yield a high performing, scalable solution for no reference contrast quality assessment suitable for real time and resource constrained applications.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2509.19370.pdf' target='_blank'>https://arxiv.org/pdf/2509.19370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyu Ma, Yuan Shan, Jiahao Zhao, Nan Xu, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19370">Meow: End-to-End Outline Writing for Automatic Academic Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As academic paper publication numbers grow exponentially, conducting in-depth surveys with LLMs automatically has become an inevitable trend. Outline writing, which aims to systematically organize related works, is critical for automated survey generation. Yet existing automatic survey methods treat outline writing as mere workflow steps in the overall pipeline. Such template-based workflows produce outlines that lack in-depth understanding of the survey topic and fine-grained styles. To address these limitations, we propose Meow, the first metadata-driven outline writing framework that produces organized and faithful outlines efficiently. Specifically, we first formulate outline writing as an end-to-end task that generates hierarchical structured outlines from paper metadata. We then curate a high-quality dataset of surveys from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics for outline quality assessment. Finally, we employ a two-stage training approach combining supervised fine-tuning and reinforcement learning. Our 8B reasoning model demonstrates strong performance with high structural fidelity and stylistic coherence.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2509.17689.pdf' target='_blank'>https://arxiv.org/pdf/2509.17689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Å½iga Babnik, Deepak Kumar Jain, Peter Peer, Vitomir Å truc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17689">FROQ: Observing Face Recognition Models for Efficient Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Recognition (FR) plays a crucial role in many critical (high-stakes) applications, where errors in the recognition process can lead to serious consequences. Face Image Quality Assessment (FIQA) techniques enhance FR systems by providing quality estimates of face samples, enabling the systems to discard samples that are unsuitable for reliable recognition or lead to low-confidence recognition decisions. Most state-of-the-art FIQA techniques rely on extensive supervised training to achieve accurate quality estimation. In contrast, unsupervised techniques eliminate the need for additional training but tend to be slower and typically exhibit lower performance. In this paper, we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised, training-free approach that leverages specific intermediate representations within a given FR model to estimate face-image quality, and combines the efficiency of supervised FIQA models with the training-free approach of unsupervised methods. A simple calibration step based on pseudo-quality labels allows FROQ to uncover specific representations, useful for quality assessment, in any modern FR model. To generate these pseudo-labels, we propose a novel unsupervised FIQA technique based on sample perturbations. Comprehensive experiments with four state-of-the-art FR models and eight benchmark datasets show that FROQ leads to highly competitive results compared to the state-of-the-art, achieving both strong performance and efficient runtime, without requiring explicit training.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2509.13150.pdf' target='_blank'>https://arxiv.org/pdf/2509.13150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shima Mohammadi, Mohsen Jenadeleh, Jon Sneyers, Dietmar Saupe, JoÃ£o Ascenso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13150">Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, image compression solutions are increasingly designed to operate within high-fidelity quality ranges, where preserving even the most subtle details of the original image is essential. In this context, the ability to detect and quantify subtle compression artifacts becomes critically important, as even slight degradations can impact perceptual quality in professional or quality sensitive applications, such as digital archiving, professional editing and web delivery. However, the performance of current objective image quality assessment metrics in this range has not been thoroughly investigated. In particular, it is not well understood how reliably these metrics estimate distortions at or below the threshold of Just Noticeable Difference (JND). This study directly addresses this issue by proposing evaluation methodologies for assessing the performance of objective quality metrics and performing a comprehensive evaluation using the JPEG AIC-3 dataset which is designed for high-fidelity image compression. Beyond conventional criteria, the study introduces Z-RMSE to incorporate subjective score uncertainty and applies novel statistical tests to assess significant differences between metrics. The analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity subsets, examines the impact of cropping in subjective tests, and a public dataset with benchmarks and evaluation tools is released to support further research.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2509.10114.pdf' target='_blank'>https://arxiv.org/pdf/2509.10114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MohammadAli Hamidi, Hadi Amirpour, Luigi Atzori, Christian Timmerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10114">A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face image quality assessment (FIQA) plays a critical role in face recognition and verification systems, especially in uncontrolled, real-world environments. Although several methods have been proposed, general-purpose no-reference image quality assessment techniques often fail to capture face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be computationally intensive, limiting their practical applicability. We propose a lightweight and efficient method for FIQA, designed for the perceptual evaluation of face images in the wild. Our approach integrates an ensemble of two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2, with prediction-level fusion via simple averaging. To enhance alignment with human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss), combining mean squared error (MSE) with a Pearson correlation regularizer. Our method achieves a strong balance between accuracy and computational cost, making it suitable for real-world deployment. Experiments on the VQualA FIQA benchmark demonstrate that our model achieves a Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894, remaining within competition efficiency constraints.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2509.06442.pdf' target='_blank'>https://arxiv.org/pdf/2509.06442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Li, Xiaoyuan Yang, Guanghui Yue, Jun Fu, Qiuping Jiang, Xu Jia, Paul L. Rosin, Hantao Liu, Wei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06442">Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many super-resolution (SR) algorithms have been proposed to increase image resolution. However, full-reference (FR) image quality assessment (IQA) metrics for comparing and evaluating different SR algorithms are limited. In this work, we propose the Perception-oriented Bidirectional Attention Network (PBAN) for image SR FR-IQA, which is composed of three modules: an image encoder module, a perception-oriented bidirectional attention (PBA) module, and a quality prediction module. First, we encode the input images for feature representations. Inspired by the characteristics of the human visual system, we then construct the perception-oriented PBA module. Specifically, different from existing attention-based SR IQA methods, we conceive a Bidirectional Attention to bidirectionally construct visual attention to distortion, which is consistent with the generation and evaluation processes of SR images. To further guide the quality assessment towards the perception of distorted information, we propose Grouped Multi-scale Deformable Convolution, enabling the proposed method to adaptively perceive distortion. Moreover, we design Sub-information Excitation Convolution to direct visual perception to both sub-pixel and sub-channel attention. Finally, the quality prediction module is exploited to integrate quality-aware features and regress quality scores. Extensive experiments demonstrate that our proposed PBAN outperforms state-of-the-art quality assessment methods.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2508.16661.pdf' target='_blank'>https://arxiv.org/pdf/2508.16661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaojie Zheng, Jiucai Zhang, Joy Gockel, Michael B. Wakin, Craig Brice, Xiaoli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16661">QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based quality assessment (QA) in additive manufacturing (AM) often relies heavily on the expertise and constant attention of skilled human operators. While machine learning and deep learning methods have been introduced to assist in this task, they typically provide black-box outputs without interpretable justifications, limiting their trust and adoption in real-world settings. In this work, we introduce a novel QA-VLM framework that leverages the attention mechanisms and reasoning capabilities of vision-language models (VLMs), enriched with application-specific knowledge distilled from peer-reviewed journal articles, to generate human-interpretable quality assessments. Evaluated on 24 single-bead samples produced by laser wire direct energy deposition (DED-LW), our framework demonstrates higher validity and consistency in explanation quality than off-the-shelf VLMs. These results highlight the potential of our approach to enable trustworthy, interpretable quality assessment in AM applications.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2508.08275.pdf' target='_blank'>https://arxiv.org/pdf/2508.08275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyun Guo, ZhiYan Hou, Yu Chen, Jinghan He, Yandu Sun, Yuzhe Zhou, Shujing Guo, Kuan Zhu, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08275">MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) require continual instruction tuning during their post-training phase to adapt to the dynamic real-world demands. However, the absence of rigorous and systematic benchmarks has hindered progress in this area. To bridge this gap, we introduce \textbf{MLLM-CTBench}, a dataset curating seven challenging tasks from six diverse domains with three contributions. First,to enable fine-grained analysis of continual learning ability, we introduce \textbf{multidimensional evaluation metrics}, which combines final answer accuracy with Chain-of-Thought (CoT) reasoning quality assessment through a carefully trained MLLM evaluator. Then, we conduct a \textbf{comprehensive evaluation of continual learning algorithms}, systematically assessing eight algorithms from four major categories to provide actionable insights for algorithm design and adoption. Finally ,we evaluate the efficacy of \textbf{Reinforcement Fine-tuning (RFT) versus Supervised Fine-tuning (SFT)} in maintaining model performance across sequential tasks during continual instruction tuning. Our experiments demonstrate that reasoning processes in MLLMs exhibit greater resilience than final outputs to forgetting during continual learning, aligning with cognitive theories of hierarchical forgetting. We further show that both model capability and task sequence significantly influence continual learning outcomes, with stronger baseline models exhibiting greater resistance to forgetting. Notably, properly regularized RFT emerges as a more robust approach than SFT for maintaining performance across tasks.One of the key contributing factors is KL-divergence regularization, without which RFT leads to even worse forgetting than SFT on old tasks though may perform better on new tasks.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2506.12505.pdf' target='_blank'>https://arxiv.org/pdf/2506.12505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Jenadeleh, Jon Sneyers, Davi Lazzarotto, Shima Mohammadi, Dominik Keller, Atanas Boev, Rakesh Rao Ramachandra Rao, AntÃ³nio Pinheiro, Thomas Richter, Alexander Raake, Touradj Ebrahimi, JoÃ£o Ascenso, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12505">Fine-Grained HDR Image Quality Assessment From Noticeably Distorted to Very High Fidelity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High dynamic range (HDR) and wide color gamut (WCG) technologies significantly improve color reproduction compared to standard dynamic range (SDR) and standard color gamuts, resulting in more accurate, richer, and more immersive images. However, HDR increases data demands, posing challenges for bandwidth efficiency and compression techniques.
  Advances in compression and display technologies require more precise image quality assessment, particularly in the high-fidelity range where perceptual differences are subtle.
  To address this gap, we introduce AIC-HDR2025, the first such HDR dataset, comprising 100 test images generated from five HDR sources, each compressed using four codecs at five compression levels. It covers the high-fidelity range, from visible distortions to compression levels below the visually lossless threshold.
  A subjective study was conducted using the JPEG AIC-3 test methodology, combining plain and boosted triplet comparisons. In total, 34,560 ratings were collected from 151 participants across four fully controlled labs. The results confirm that AIC-3 enables precise HDR quality estimation, with 95\% confidence intervals averaging a width of 0.27 at 1 JND. In addition, several recently proposed objective metrics were evaluated based on their correlation with subjective ratings. The dataset is publicly available.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2506.05395.pdf' target='_blank'>https://arxiv.org/pdf/2506.05395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mert Can Cakmak, Nitin Agarwal, Diwash Poudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05395">TriPSS: A Tri-Modal Keyframe Extraction Framework Using Perceptual, Structural, and Semantic Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient keyframe extraction is critical for video summarization and retrieval, yet capturing the full semantic and visual richness of video content remains challenging. We introduce TriPSS, a tri-modal framework that integrates perceptual features from the CIELAB color space, structural embeddings from ResNet-50, and semantic context from frame-level captions generated by LLaMA-3.2-11B-Vision-Instruct. These modalities are fused using principal component analysis to form compact multi-modal embeddings, enabling adaptive video segmentation via HDBSCAN clustering. A refinement stage incorporating quality assessment and duplicate filtering ensures the final keyframe set is both concise and semantically diverse. Evaluations on the TVSum20 and SumMe benchmarks show that TriPSS achieves state-of-the-art performance, significantly outperforming both unimodal and prior multimodal approaches. These results highlight TriPSS' ability to capture complementary visual and semantic cues, establishing it as an effective solution for video summarization, retrieval, and large-scale multimedia understanding.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2504.12018.pdf' target='_blank'>https://arxiv.org/pdf/2504.12018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinli Yue, JianHui Sun, Junda Lu, Liangchao Yao, Fan Xia, Tianyi Wang, Fengyun Rao, Jing Lyu, Yuetang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12018">Instruction-augmented Multimodal Alignment for Image-Text and Element Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of text-to-image (T2I) generation models, assessing the semantic alignment between generated images and text descriptions has become a significant research challenge. Current methods, including those based on Visual Question Answering (VQA), still struggle with fine-grained assessments and precise quantification of image-text alignment. This paper presents an improved evaluation method named Instruction-augmented Multimodal Alignment for Image-Text and Element Matching (iMatch), which evaluates image-text semantic alignment by fine-tuning multimodal large language models. We introduce four innovative augmentation strategies: First, the QAlign strategy creates a precise probabilistic mapping to convert discrete scores from multimodal large language models into continuous matching scores. Second, a validation set augmentation strategy uses pseudo-labels from model predictions to expand training data, boosting the model's generalization performance. Third, an element augmentation strategy integrates element category labels to refine the model's understanding of image-text matching. Fourth, an image augmentation strategy employs techniques like random lighting to increase the model's robustness. Additionally, we propose prompt type augmentation and score perturbation strategies to further enhance the accuracy of element assessments. Our experimental results show that the iMatch method significantly surpasses existing methods, confirming its effectiveness and practical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025 Text to Image Generation Model Quality Assessment - Track 1 Image-Text Alignment.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2503.14154.pdf' target='_blank'>https://arxiv.org/pdf/2503.14154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Chen, Shuai Wan, Siyu Ren, Fuzheng Yang, Mengting Yu, Junhui Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14154">RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using Radial Basis Function Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the main challenges in point cloud compression (PCC) is how to evaluate the perceived distortion so that the codec can be optimized for perceptual quality. Current standard practices in PCC highlight a primary issue: while single-feature metrics are widely used to assess compression distortion, the classic method of searching point-to-point nearest neighbors frequently fails to adequately build precise correspondences between point clouds, resulting in an ineffective capture of human perceptual features. To overcome the related limitations, we propose a novel assessment method called RBFIM, utilizing radial basis function (RBF) interpolation to convert discrete point features into a continuous feature function for the distorted point cloud. By substituting the geometry coordinates of the original point cloud into the feature function, we obtain the bijective sets of point features. This enables an establishment of precise corresponding features between distorted and original point clouds and significantly improves the accuracy of quality assessments. Moreover, this method avoids the complexity caused by bidirectional searches. Extensive experiments on multiple subjective quality datasets of compressed point clouds demonstrate that our RBFIM excels in addressing human perception tasks, thereby providing robust support for PCC optimization efforts.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2503.13260.pdf' target='_blank'>https://arxiv.org/pdf/2503.13260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Zalcher, Navve Wasserman, Roman Beliy, Oliver Heinimann, Michal Irani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13260">Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual perceptual tasks aim to predict human judgment of images (e.g., emotions invoked by images, image quality assessment). Unlike objective tasks such as object/scene recognition, perceptual tasks rely on subjective human assessments, making its data-labeling difficult. The scarcity of such human-annotated data results in small datasets leading to poor generalization. Typically, specialized models were designed for each perceptual task, tailored to its unique characteristics and its own training dataset. We propose a unified architectural framework for solving multiple different perceptual tasks leveraging CLIP as a prior. Our approach is based on recent cognitive findings which indicate that CLIP correlates well with human judgment. While CLIP was explicitly trained to align images and text, it implicitly also learned human inclinations. We attribute this to the inclusion of human-written image captions in CLIP's training data, which contain not only factual image descriptions, but inevitably also human sentiments and emotions. This makes CLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest that minimal adaptation of CLIP suffices for solving a variety of perceptual tasks. Our simple unified framework employs a lightweight adaptation to fine-tune CLIP to each task, without requiring any task-specific architectural changes. We evaluate our approach on three tasks: (i) Image Memorability Prediction, (ii) No-reference Image Quality Assessment, and (iii) Visual Emotion Analysis. Our model achieves state-of-the-art results on all three tasks, while demonstrating improved generalization across different datasets.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2503.08358.pdf' target='_blank'>https://arxiv.org/pdf/2503.08358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Faizal Karim, Mohammed Saad Hashmi, Shreya Bollimuntha, Mahesh Reddy Tapeti, Gaurav Singh, Nagamanikandan Govindan, K Madhava Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08358">DG16M: A Large-Scale Dataset for Dual-Arm Grasping with Force-Optimized Grasps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dual-arm robotic grasping is crucial for handling large objects that require stable and coordinated manipulation. While single-arm grasping has been extensively studied, datasets tailored for dual-arm settings remain scarce. We introduce a large-scale dataset of 16 million dual-arm grasps, evaluated under improved force-closure constraints. Additionally, we develop a benchmark dataset containing 300 objects with approximately 30,000 grasps, evaluated in a physics simulation environment, providing a better grasp quality assessment for dual-arm grasp synthesis methods. Finally, we demonstrate the effectiveness of our dataset by training a Dual-Arm Grasp Classifier network that outperforms the state-of-the-art methods by 15\%, achieving higher grasp success rates and improved generalization across objects.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2503.00743.pdf' target='_blank'>https://arxiv.org/pdf/2503.00743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dilxat Muhtar, Enzhuo Zhang, Zhenshi Li, Feng Gu, Yanglangxing He, Pengfeng Xiao, Xueliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00743">Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have demonstrated great potential in interpreting remote sensing (RS) images through language-guided semantic. However, the effectiveness of these VLMs critically depends on high-quality image-text training data that captures rich semantic relationships between visual content and language descriptions. Unlike natural images, RS lacks large-scale interleaved image-text pairs from web data, making data collection challenging. While current approaches rely primarily on rule-based methods or flagship VLMs for data synthesis, a systematic framework for automated quality assessment of such synthetically generated RS vision-language data is notably absent. To fill this gap, we propose a novel score model trained on large-scale RS vision-language preference data for automated quality assessment. Our empirical results demonstrate that fine-tuning CLIP or advanced VLMs (e.g., Qwen2-VL) with the top 30% of data ranked by our score model achieves superior accuracy compared to both full-data fine-tuning and CLIP-score-based ranking approaches. Furthermore, we demonstrate applications of our scoring model for reinforcement learning (RL) training and best-of-N (BoN) test-time scaling, enabling significant improvements in VLM performance for RS tasks. Our code, model, and dataset are publicly available
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2503.00625.pdf' target='_blank'>https://arxiv.org/pdf/2503.00625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Hadi Amirpour, Christian Timmerer, Guangtao Zhai, Patrick Le Callet, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00625">Perceptual Visual Quality Assessment: Principles, Methods, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multimedia services such as video streaming, video conferencing, virtual reality (VR), and online gaming continue to expand, ensuring high perceptual visual quality becomes a priority to maintain user satisfaction and competitiveness. However, multimedia content undergoes various distortions during acquisition, compression, transmission, and storage, resulting in the degradation of experienced quality. Thus, perceptual visual quality assessment (PVQA), which focuses on evaluating the quality of multimedia content based on human perception, is essential for optimizing user experiences in advanced communication systems. Several challenges are involved in the PVQA process, including diverse characteristics of multimedia content such as image, video, VR, point cloud, mesh, multimodality, etc., and complex distortion scenarios as well as viewing conditions. In this paper, we first present an overview of PVQA principles and methods. This includes both subjective methods, where users directly rate their experiences, and objective methods, where algorithms predict human perception based on measurable factors such as bitrate, frame rate, and compression levels. Based on the basics of PVQA, quality predictors for different multimedia data are then introduced. In addition to traditional images and videos, immersive multimedia and generative artificial intelligence (GenAI) content are also discussed. Finally, the paper concludes with a discussion on the future directions of PVQA research.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2409.14888.pdf' target='_blank'>https://arxiv.org/pdf/2409.14888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinli Yue, Jianhui Sun, Han Kong, Liangchao Yao, Tianyi Wang, Lei Li, Fengyun Rao, Jing Lv, Fan Xia, Yuetang Deng, Qian Wang, Lingchen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14888">Advancing Video Quality Assessment for AIGC</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, AI generative models have made remarkable progress across various domains, including text generation, image generation, and video generation. However, assessing the quality of text-to-video generation is still in its infancy, and existing evaluation frameworks fall short when compared to those for natural videos. Current video quality assessment (VQA) methods primarily focus on evaluating the overall quality of natural videos and fail to adequately account for the substantial quality discrepancies between frames in generated videos. To address this issue, we propose a novel loss function that combines mean absolute error with cross-entropy loss to mitigate inter-frame quality inconsistencies. Additionally, we introduce the innovative S2CNet technique to retain critical content, while leveraging adversarial training to enhance the model's generalization capabilities. Experimental results demonstrate that our method outperforms existing VQA techniques on the AIGC Video dataset, surpassing the previous state-of-the-art by 3.1% in terms of PLCC.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2409.12218.pdf' target='_blank'>https://arxiv.org/pdf/2409.12218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujan Dutta, Deepak Pandita, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, Ashiqur R. KhudaBukhsh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12218">ARTICLE: Annotator Reliability Through In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring annotator quality in training and evaluation data is a key piece of machine learning in NLP. Tasks such as sentiment analysis and offensive speech detection are intrinsically subjective, creating a challenging scenario for traditional quality assessment approaches because it is hard to distinguish disagreement due to poor work from that due to differences of opinions between sincere annotators. With the goal of increasing diverse perspectives in annotation while ensuring consistency, we propose \texttt{ARTICLE}, an in-context learning (ICL) framework to estimate annotation quality through self-consistency. We evaluate this framework on two offensive speech datasets using multiple LLMs and compare its performance with traditional methods. Our findings indicate that \texttt{ARTICLE} can be used as a robust method for identifying reliable annotators, hence improving data quality.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2404.15163.pdf' target='_blank'>https://arxiv.org/pdf/2404.15163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianwei Zhou, Songbai Tan, Wei Zhou, Yu Luo, Yuan-Gen Wang, Guanghui Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15163">Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc. Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models. In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions, i.e., "visual quality", "authenticity", and "consistency". Specifically, inspired by the characteristics of the human visual system and motivated by the observation that "visual quality" and "authenticity" are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features. After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights. In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment. We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods. The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2403.10406.pdf' target='_blank'>https://arxiv.org/pdf/2403.10406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Li, Xiaoyuan Yang, Jun Fu, Guanghui Yue, Wei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10406">Deep Bi-directional Attention Network for Image Super-Resolution Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has emerged a growing interest in exploring efficient quality assessment algorithms for image super-resolution (SR). However, employing deep learning techniques, especially dual-branch algorithms, to automatically evaluate the visual quality of SR images remains challenging. Existing SR image quality assessment (IQA) metrics based on two-stream networks lack interactions between branches. To address this, we propose a novel full-reference IQA (FR-IQA) method for SR images. Specifically, producing SR images and evaluating how close the SR images are to the corresponding HR references are separate processes. Based on this consideration, we construct a deep Bi-directional Attention Network (BiAtten-Net) that dynamically deepens visual attention to distortions in both processes, which aligns well with the human visual system (HVS). Experiments on public SR quality databases demonstrate the superiority of our proposed BiAtten-Net over state-of-the-art quality assessment methods. In addition, the visualization results and ablation study show the effectiveness of bi-directional attention.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2402.04426.pdf' target='_blank'>https://arxiv.org/pdf/2402.04426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijeet Parida, Zhifan Jiang, Roger J. Packer, Robert A. Avery, Syed M. Anwar, Marius G. Linguraru
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04426">Quantitative Metrics for Benchmarking Medical Image Harmonization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image harmonization is an important preprocessing strategy to address domain shifts arising from data acquired using different machines and scanning protocols in medical imaging. However, benchmarking the effectiveness of harmonization techniques has been a challenge due to the lack of widely available standardized datasets with ground truths. In this context, we propose three metrics: two intensity harmonization metrics and one anatomy preservation metric for medical images during harmonization, where no ground truths are required. Through extensive studies on a dataset with available harmonization ground truth, we demonstrate that our metrics are correlated with established image quality assessment metrics. We show how these novel metrics may be applied to real-world scenarios where no harmonization ground truth exists. Additionally, we provide insights into different interpretations of the metric values, shedding light on their significance in the context of the harmonization process. As a result of our findings, we advocate for the adoption of these quantitative harmonization metrics as a standard for benchmarking the performance of image harmonization techniques.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2311.03850.pdf' target='_blank'>https://arxiv.org/pdf/2311.03850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shima Mohammadi, JoÃ£o Ascenso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03850">Predictive Sampling for Efficient Pairwise Subjective Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective image quality assessment studies are used in many scenarios, such as the evaluation of compression, super-resolution, and denoising solutions. Among the available subjective test methodologies, pair comparison is attracting popularity due to its simplicity, reliability, and robustness to changes in the test conditions, e.g. display resolutions. The main problem that impairs its wide acceptance is that the number of pairs to compare by subjects grows quadratically with the number of stimuli that must be considered. Usually, the paired comparison data obtained is fed into an aggregation model to obtain a final score for each degraded image and thus, not every comparison contributes equally to the final quality score. In the past years, several solutions that sample pairs (from all possible combinations) have been proposed, from random sampling to active sampling based on the past subjects' decisions. This paper introduces a novel sampling solution called \textbf{P}redictive \textbf{S}ampling for \textbf{P}airwise \textbf{C}omparison (PS-PC) which exploits the characteristics of the input data to make a prediction of which pairs should be evaluated by subjects. The proposed solution exploits popular machine learning techniques to select the most informative pairs for subjects to evaluate, while for the other remaining pairs, it predicts the subjects' preferences. The experimental results show that PS-PC is the best choice among the available sampling algorithms with higher performance for the same number of pairs. Moreover, since the choice of the pairs is done \emph{a priori} before the subjective test starts, the algorithm is not required to run during the test and thus much more simple to deploy in online crowdsourcing subjective tests.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2309.05658.pdf' target='_blank'>https://arxiv.org/pdf/2309.05658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yili Jin, Kaiyuan Hu, Junhua Liu, Fangxin Wang, Xue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05658">From Capture to Display: A Survey on Volumetric Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Volumetric video, which offers immersive viewing experiences, is gaining increasing prominence. With its six degrees of freedom, it provides viewers with greater immersion and interactivity compared to traditional videos. Despite their potential, volumetric video services pose significant challenges. This survey conducts a comprehensive review of the existing literature on volumetric video. We firstly provide a general framework of volumetric video services, followed by a discussion on prerequisites for volumetric video, encompassing representations, open datasets, and quality assessment metrics. Then we delve into the current methodologies for each stage of the volumetric video service pipeline, detailing capturing, compression, transmission, rendering, and display techniques. Lastly, we explore various applications enabled by this pioneering technology and we present an array of research challenges and opportunities in the domain of volumetric video services. This survey aspires to provide a holistic understanding of this burgeoning field and shed light on potential future research trajectories, aiming to bring the vision of volumetric video to fruition.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2306.09467.pdf' target='_blank'>https://arxiv.org/pdf/2306.09467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mononito Goswami, Vedant Sanil, Arjun Choudhry, Arvind Srinivasan, Chalisa Udompanyawit, Artur Dubrawski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09467">AQuA: A Benchmarking Tool for Label Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2305.05768.pdf' target='_blank'>https://arxiv.org/pdf/2305.05768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Å½iga Babnik, Peter Peer, Vitomir Å truc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05768">DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern face recognition (FR) models excel in constrained scenarios, but often suffer from decreased performance when deployed in unconstrained (real-world) environments due to uncertainties surrounding the quality of the captured facial data. Face image quality assessment (FIQA) techniques aim to mitigate these performance degradations by providing FR models with sample-quality predictions that can be used to reject low-quality samples and reduce false match errors. However, despite steady improvements, ensuring reliable quality estimates across facial images with diverse characteristics remains challenging. In this paper, we present a powerful new FIQA approach, named DifFIQA, which relies on denoising diffusion probabilistic models (DDPM) and ensures highly competitive results. The main idea behind the approach is to utilize the forward and backward processes of DDPMs to perturb facial images and quantify the impact of these perturbations on the corresponding image embeddings for quality prediction. Because the diffusion-based perturbations are computationally expensive, we also distill the knowledge encoded in DifFIQA into a regression-based quality predictor, called DifFIQA(R), that balances performance and execution time. We evaluate both models in comprehensive experiments on 7 datasets, with 4 target FR models and against 10 state-of-the-art FIQA techniques with highly encouraging results. The source code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2305.00220.pdf' target='_blank'>https://arxiv.org/pdf/2305.00220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohsen Jenadeleh, Johannes Zagermann, Harald Reiterer, Ulf-Dietrich Reips, Raouf Hamzaoui, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00220">Relaxed forced choice improves performance of visual quality assessment methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In image quality assessment, a collective visual quality score for an image or video is obtained from the individual ratings of many subjects. One commonly used format for these experiments is the two-alternative forced choice method. Two stimuli with the same content but differing visual quality are presented sequentially or side-by-side. Subjects are asked to select the one of better quality, and when uncertain, they are required to guess. The relaxed alternative forced choice format aims to reduce the cognitive load and the noise in the responses due to the guessing by providing a third response option, namely, ``not sure''. This work presents a large and comprehensive crowdsourcing experiment to compare these two response formats: the one with the ``not sure'' option and the one without it. To provide unambiguous ground truth for quality evaluation, subjects were shown pairs of images with differing numbers of dots and asked each time to choose the one with more dots. Our crowdsourcing study involved 254 participants and was conducted using a within-subject design. Each participant was asked to respond to 40 pair comparisons with and without the ``not sure'' response option and completed a questionnaire to evaluate their cognitive load for each testing condition. The experimental results show that the inclusion of the ``not sure'' response option in the forced choice method reduced mental load and led to models with better data fit and correspondence to ground truth. We also tested for the equivalence of the models and found that they were different. The dataset is available at http://database.mmsp-kn.de/cogvqa-database.html.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2304.03563.pdf' target='_blank'>https://arxiv.org/pdf/2304.03563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saikat Mondal, Mohammad Masudur Rahman, Chanchal K. Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03563">Do Subjectivity and Objectivity Always Agree? A Case Study with Stack Overflow Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Stack Overflow (SO), the quality of posts (i.e., questions and answers) is subjectively evaluated by users through a voting mechanism. The net votes (upvotes - downvotes) obtained by a post are often considered an approximation of its quality. However, about half of the questions that received working solutions got more downvotes than upvotes. Furthermore, about 18% of the accepted answers (i.e., verified solutions) also do not score the maximum votes. All these counter-intuitive findings cast doubts on the reliability of the evaluation mechanism employed at SO. Moreover, many users raise concerns against the evaluation, especially downvotes to their posts. Therefore, rigorous verification of the subjective evaluation is highly warranted to ensure a non-biased and reliable quality assessment mechanism. In this paper, we compare the subjective assessment of questions with their objective assessment using 2.5 million questions and ten text analysis metrics. According to our investigation, four objective metrics agree with the subjective evaluation, two do not agree, one either agrees or disagrees, and the remaining three neither agree nor disagree with the subjective evaluation. We then develop machine learning models to classify the promoted and discouraged questions. Our models outperform the state-of-the-art models with a maximum of about 76% - 87% accuracy.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2302.09838.pdf' target='_blank'>https://arxiv.org/pdf/2302.09838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamu Sheng, Jiayuan Fan, Peng Ye, Jianjian Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09838">JNDMix: JND-Based Data Augmentation for No-reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in no-reference image quality assessment (NR-IQA), previous training models often suffer from over-fitting due to the limited scale of used datasets, resulting in model performance bottlenecks. To tackle this challenge, we explore the potential of leveraging data augmentation to improve data efficiency and enhance model robustness. However, most existing data augmentation methods incur a serious issue, namely that it alters the image quality and leads to training images mismatching with their original labels. Additionally, although only a few data augmentation methods are available for NR-IQA task, their ability to enrich dataset diversity is still insufficient. To address these issues, we propose a effective and general data augmentation based on just noticeable difference (JND) noise mixing for NR-IQA task, named JNDMix. In detail, we randomly inject the JND noise, imperceptible to the human visual system (HVS), into the training image without any adjustment to its label. Extensive experiments demonstrate that JNDMix significantly improves the performance and data efficiency of various state-of-the-art NR-IQA models and the commonly used baseline models, as well as the generalization ability. More importantly, JNDMix facilitates MANIQA to achieve the state-of-the-art performance on LIVEC and KonIQ-10k.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2301.07681.pdf' target='_blank'>https://arxiv.org/pdf/2301.07681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Guanghui Yue, Ruizeng Zhang, Yipeng Qin, Hantao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07681">Reduced-Reference Quality Assessment of Point Clouds via Content-Oriented Saliency Projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many dense 3D point clouds have been exploited to represent visual objects instead of traditional images or videos. To evaluate the perceptual quality of various point clouds, in this letter, we propose a novel and efficient Reduced-Reference quality metric for point clouds, which is based on Content-oriented sAliency Projection (RR-CAP). Specifically, we make the first attempt to simplify reference and distorted point clouds into projected saliency maps with a downsampling operation. Through this process, we tackle the issue of transmitting large-volume original point clouds to user-ends for quality assessment. Then, motivated by the characteristics of the human visual system (HVS), the objective quality scores of distorted point clouds are produced by combining content-oriented similarity and statistical correlation measurements. Finally, extensive experiments are conducted on SJTU-PCQA and WPC databases. The experimental results demonstrate that our proposed algorithm outperforms existing reduced-reference and no-reference quality metrics, and significantly reduces the performance gap between state-of-the-art full-reference quality assessment methods. In addition, we show the performance variation of each proposed technical component by ablation tests.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2206.06725.pdf' target='_blank'>https://arxiv.org/pdf/2206.06725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Sciarra, Soumick Chatterjee, Max DÃ¼nnwald, Giuseppe Placidi, Andreas NÃ¼rnberger, Oliver Speck, Steffen Oeltze-Jafra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.06725">Automated SSIM Regression for Detection and Quantification of Motion Artefacts in Brain MR Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion artefacts in magnetic resonance brain images can have a strong impact on diagnostic confidence. The assessment of MR image quality is fundamental before proceeding with the clinical diagnosis. Motion artefacts can alter the delineation of structures such as the brain, lesions or tumours and may require a repeat scan. Otherwise, an inaccurate (e.g. correct pathology but wrong severity) or incorrect diagnosis (e.g. wrong pathology) may occur. "\textit{Image quality assessment}" as a fast, automated step right after scanning can assist in deciding if the acquired images are diagnostically sufficient. An automated image quality assessment based on the structural similarity index (SSIM) regression through a residual neural network is proposed in this work. Additionally, a classification into different groups - by subdividing with SSIM ranges - is evaluated. Importantly, this method predicts SSIM values of an input image in the absence of a reference ground truth image. The networks were able to detect motion artefacts, and the best performance for the regression and classification task has always been achieved with ResNet-18 with contrast augmentation. The mean and standard deviation of residuals' distribution were $Î¼=-0.0009$ and $Ï=0.0139$, respectively. Whilst for the classification task in 3, 5 and 10 classes, the best accuracies were 97, 95 and 89\%, respectively. The results show that the proposed method could be a tool for supporting neuro-radiologists and radiographers in evaluating image quality quickly.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2510.19384.pdf' target='_blank'>https://arxiv.org/pdf/2510.19384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19384">Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs) is central to web-scale applications such as search, recommendation, and knowledge discovery. However, existing CLIP-style graph-text aligners face two key limitations: they assume strict one-to-one correspondences between nodes and texts, overlooking the inherent many-to-many relations in real-world graphs; and they rely on static alignment objectives that cannot adapt to varying data quality, making them brittle under noisy supervision. Together, these limitations expose a core dilemma: embracing expressive many-to-many alignment amplifies noise, while reverting to strict one-to-one strategies sacrifices semantic diversity and fails to handle inherently mismatched pairs. To address these challenges, we propose ADAligner, a dynamic, quality-aware graph-text alignment framework that dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. ADAligner estimates batch-level alignment reliability in real time and adapts its optimization accordingly, promoting soft, subgraph-level many-to-many alignment when supervision is clean, while emphasizing reliable one-to-one alignment by dynamically filtering low-confidence pairs under noise. Theoretically, we prove that this dynamic mechanism forms a stable negative feedback process, ensuring convergence and robustness. Comprehensive experiments on nine diverse TAG datasets demonstrate that ADAligner consistently outperforms prior graph-text aligners on zero-/few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines, establishing a scalable and reliable foundation for graph-text representation learning in real-world web environments.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2510.04864.pdf' target='_blank'>https://arxiv.org/pdf/2510.04864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ciem Cornelissen, Sander De Coninck, Axel Willekens, Sam Leroux, Pieter Simoens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04864">In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an end-to-end, IoT-enabled robotic system for the non-destructive, real-time, and spatially-resolved mapping of grape yield and quality (Brix, Acidity) in vineyards. The system features a comprehensive analytical pipeline that integrates two key modules: a high-performance model for grape bunch detection and weight estimation, and a novel deep learning framework for quality assessment from hyperspectral (HSI) data. A critical barrier to in-field HSI is the ``domain shift" caused by variable illumination. To overcome this, our quality assessment is powered by the Light-Invariant Spectral Autoencoder (LISA), a domain-adversarial framework that learns illumination-invariant features from uncalibrated data. We validated the system's robustness on a purpose-built HSI dataset spanning three distinct illumination domains: controlled artificial lighting (lab), and variable natural sunlight captured in the morning and afternoon. Results show the complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$ (0.76) for weight prediction, while the LISA module improves quality prediction generalization by over 20% compared to the baselines. By combining these robust modules, the system successfully generates high-resolution, georeferenced data of both grape yield and quality, providing actionable, data-driven insights for precision viticulture.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2508.09598.pdf' target='_blank'>https://arxiv.org/pdf/2508.09598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Shao, Ke Zhu, Minghao Fu, Guo-hua Wang, Jianxin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09598">Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved remarkable progress in class-to-image generation. However, we observe that despite impressive FID scores, state-of-the-art models often generate distorted or low-quality images, especially in certain classes. This gap arises because FID evaluates global distribution alignment, while ignoring the perceptual quality of individual samples. We further examine the role of CFG, a common technique used to enhance generation quality. While effective in improving metrics and suppressing outliers, CFG can introduce distribution shift and visual artifacts due to its misalignment with both training objectives and user expectations. In this work, we propose FaME, a training-free and inference-efficient method for improving perceptual quality. FaME uses an image quality assessment model to identify low-quality generations and stores their sampling trajectories. These failure modes are then used as negative guidance to steer future sampling away from poor-quality regions. Experiments on ImageNet demonstrate that FaME brings consistent improvements in visual quality without compromising FID. FaME also shows the potential to be extended to improve text-to-image generation.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2508.07818.pdf' target='_blank'>https://arxiv.org/pdf/2508.07818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Song, Chen Hui, Haiqi Zhu, Feng Jiang, Yachun Mi, Wei Zhang, Shaohui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07818">Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-reference image quality assessment (NR-IQA) aims to simulate the process of perceiving image quality aligned with subjective human perception. However, existing NR-IQA methods either focus on global representations that leads to limited insights into the semantically salient regions or employ a uniform weighting for region features that weakens the sensitivity to local quality variations. In this paper, we propose a fine-grained image quality assessment model, named RSFIQA, which integrates region-level distortion information to perceive multi-dimensional quality discrepancies. To enhance regional quality awareness, we first utilize the Segment Anything Model (SAM) to dynamically partition the input image into non-overlapping semantic regions. For each region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract descriptive content and perceive multi-dimensional distortions, enabling a comprehensive understanding of both local semantics and quality degradations. To effectively leverage this information, we introduce Region-Aware Semantic Attention (RSA) mechanism, which generates a global attention map by aggregating fine-grained representations from local regions. In addition, RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep neural network architectures. Extensive experiments demonstrate the robustness and effectiveness of the proposed method, which achieves competitive quality prediction performance across multiple benchmark datasets.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2508.06092.pdf' target='_blank'>https://arxiv.org/pdf/2508.06092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yachun Mi, Yu Li, Yanting Li, Shixin Sun, Chen Hui, Tong Zhang, Yuanyuan Liu, Chenyue Song, Shaohui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06092">Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and efficient Video Quality Assessment (VQA) has long been a key research challenge. Current mainstream VQA methods typically improve performance by pretraining on large-scale classification datasets (e.g., ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this strategy presents two significant challenges: (1) merely transferring semantic knowledge learned from pretraining is insufficient for VQA, as video quality depends on multiple factors (e.g., semantics, distortion, motion, aesthetics); (2) pretraining on large-scale datasets demands enormous computational resources, often dozens or even hundreds of times greater than training directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown remarkable generalization capabilities across a wide range of visual tasks, and have begun to demonstrate promising potential in quality assessment. In this work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP enhances both visual and textual representations through a Shared Cross-Modal Adapter (SCMA), which contains only a minimal number of trainable parameters and is the only component that requires training. This design significantly reduces computational cost. In addition, we introduce a set of five learnable quality-level prompts to guide the VLMs in perceiving subtle quality variations, thereby further enhancing the model's sensitivity to video quality. Furthermore, we investigate the impact of different frame sampling strategies on VQA performance, and find that frame-difference-based sampling leads to better generalization performance across datasets. Extensive experiments demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2506.17969.pdf' target='_blank'>https://arxiv.org/pdf/2506.17969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Song, Chen Hui, Wei Zhang, Haiqi Zhu, Shaohui Liu, Hong Huang, Feng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17969">BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) aims to evaluate the perceptual quality of images based on human subjective perception. Existing methods generally combine multiscale features to achieve high performance, but most rely on straightforward linear fusion of these features, which may not adequately capture the impact of distortions on semantic content. To address this, we propose a bottom-up image quality assessment approach based on the Contrastive Language-Image Pre-training (CLIP, a recently proposed model that aligns images and text in a shared feature space), named BPCLIP, which progressively extracts the impact of low-level distortions on high-level semantics. Specifically, we utilize an encoder to extract multiscale features from the input image and introduce a bottom-up multiscale cross attention module designed to capture the relationships between shallow and deep features. In addition, by incorporating 40 image quality adjectives across six distinct dimensions, we enable the pre-trained CLIP text encoder to generate representations of the intrinsic quality of the image, thereby strengthening the connection between image quality perception and human language. Our method achieves superior results on most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while demonstrating greater robustness.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2506.00178.pdf' target='_blank'>https://arxiv.org/pdf/2506.00178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Nair, Adi Banerjee, Laurent Mombaerts, Matthew Hagen, Tarik Borogovac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00178">Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt engineering represents a critical bottleneck to harness the full potential of Large Language Models (LLMs) for solving complex tasks, as it requires specialized expertise, significant trial-and-error, and manual intervention. This challenge is particularly pronounced for tasks involving subjective quality assessment, where defining explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in these scenarios, as they typically require well-defined task-specific numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a debate-driven evaluation with an Elo-based selection. Contrary to prior work, DEEVOs approach enables exploration of the discrete prompt space while preserving semantic coherence through intelligent crossover and strategic mutation operations that incorporate debate-based feedback, combining elements from both successful and unsuccessful prompts based on identified strengths rather than arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity in the prompt population. Experimental results demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative state-of-the-art optimization approaches on open-ended tasks and close-ended tasks despite using no ground truth feedback. By connecting LLMs reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2505.19696.pdf' target='_blank'>https://arxiv.org/pdf/2505.19696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Nour Aburaed, Alessandro Bruno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19696">Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This position paper argues that Mean Opinion Score (MOS), while historically foundational, is no longer sufficient as the sole supervisory signal for multimedia quality assessment models. MOS reduces rich, context-sensitive human judgments to a single scalar, obscuring semantic failures, user intent, and the rationale behind quality decisions. We contend that modern quality assessment models must integrate three interdependent capabilities: (1) context-awareness, to adapt evaluations to task-specific goals and viewing conditions; (2) reasoning, to produce interpretable, evidence-grounded justifications for quality judgments; and (3) multimodality, to align perceptual and semantic cues using vision-language models. We critique the limitations of current MOS-centric benchmarks and propose a roadmap for reform: richer datasets with contextual metadata and expert rationales, and new evaluation metrics that assess semantic alignment, reasoning fidelity, and contextual sensitivity. By reframing quality assessment as a contextual, explainable, and multimodal modeling task, we aim to catalyze a shift toward more robust, human-aligned, and trustworthy evaluation systems.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2504.16003.pdf' target='_blank'>https://arxiv.org/pdf/2504.16003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yachun Mi, Yu Li, Weicheng Meng, Chaofeng Chen, Chen Hui, Shaohui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16003">MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of long-duration, high-definition videos has made efficient video quality assessment (VQA) a critical challenge. Existing research typically tackles this problem through two main strategies: reducing model parameters and resampling inputs. However, light-weight Convolution Neural Networks (CNN) and Transformers often struggle to balance efficiency with high performance due to the requirement of long-range modeling capabilities. Recently, the state-space model, particularly Mamba, has emerged as a promising alternative, offering linear complexity with respect to sequence length. Meanwhile, efficient VQA heavily depends on resampling long sequences to minimize computational costs, yet current resampling methods are often weak in preserving essential semantic information. In this work, we present MVQA, a Mamba-based model designed for efficient VQA along with a novel Unified Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch sampling from low-resolution videos and distortion patch sampling from original-resolution videos. The former captures semantically dense regions, while the latter retains critical distortion details. To prevent computation increase from dual inputs, we propose a fusion mechanism using pre-defined masks, enabling a unified sampling strategy that captures both semantic and quality information without additional computational burden. Experiments show that the proposed MVQA, equipped with USDS, achieve comparable performance to state-of-the-art methods while being $2\times$ as fast and requiring only $1/5$ GPU memory.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2504.01205.pdf' target='_blank'>https://arxiv.org/pdf/2504.01205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Clark, Hua Shen, Bill Howe, Tanushree Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01205">Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLMs increasingly serve as tools for knowledge acquisition, yet users cannot effectively specify how they want information presented. When users request that LLMs "cite reputable sources," "express appropriate uncertainty," or "include multiple perspectives," they discover that current interfaces provide no structured way to articulate these preferences. The result is prompt sharing folklore: community-specific copied prompts passed through trust relationships rather than based on measured efficacy. We propose the Epistemic Alignment Framework, a set of ten challenges in knowledge transmission derived from the philosophical literature of epistemology, concerning issues such as evidence quality assessment and calibration of testimonial reliance. The framework serves as a structured intermediary between user needs and system capabilities, creating a common vocabulary to bridge the gap between what users want and what systems deliver. Through a thematic analysis of custom prompts and personalization strategies shared on online communities where these issues are actively discussed, we find users develop elaborate workarounds to address each of the challenges. We then apply our framework to two prominent model providers, OpenAI and Anthropic, through content analysis of their documented policies and product features. Our analysis shows that while these providers have partially addressed the challenges we identified, they fail to establish adequate mechanisms for specifying epistemic preferences, lack transparency about how preferences are implemented, and offer no verification tools to confirm whether preferences were followed. For AI developers, the Epistemic Alignment Framework offers concrete guidance for supporting diverse approaches to knowledge; for users, it works toward information delivery that aligns with their specific needs rather than defaulting to one-size-fits-all approaches.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2412.18641.pdf' target='_blank'>https://arxiv.org/pdf/2412.18641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Xiucheng Liang, Zicheng Fan, Yujun Hou, Tianhong Zhao, Rui Ma, Kunihiko Fujiwara, Jiani Ouyang, Matias Quintana, Filip Biljecki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18641">ZenSVI: An Open-Source Software for the Integrated Acquisition, Processing and Analysis of Street View Imagery Towards Scalable Urban Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Street view imagery (SVI) has been instrumental in many studies in the past decade to understand and characterize street features and the built environment. Researchers across a variety of domains, such as transportation, health, architecture, human perception, and infrastructure have employed different methods to analyze SVI. However, these applications and image-processing procedures have not been standardized, and solutions have been implemented in isolation, often making it difficult for others to reproduce existing work and carry out new research. Using SVI for research requires multiple technical steps: accessing APIs for scalable data collection, preprocessing images to standardize formats, implementing computer vision models for feature extraction, and conducting spatial analysis. These technical requirements create barriers for researchers in urban studies, particularly those without extensive programming experience. We developed ZenSVI, a free and open-source Python package that integrates and implements the entire process of SVI analysis, supporting a wide range of use cases. Its end-to-end pipeline includes downloading SVI from multiple platforms (e.g., Mapillary and KartaView) efficiently, analyzing metadata of SVI, applying computer vision models to extract target features, transforming SVI into different projections (e.g., fish-eye and perspective) and different formats (e.g., depth map and point cloud), visualizing analyses with maps and plots, and exporting outputs to other software tools. We demonstrated its use in Singapore through a case study of data quality assessment and clustering analysis in a streamlined manner. Our software improves the transparency, reproducibility, and scalability of research relying on SVI and supports researchers in conducting urban analyses efficiently. Its modular design facilitates extensions of the package for new use cases.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2412.06760.pdf' target='_blank'>https://arxiv.org/pdf/2412.06760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Hsiang Yu, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06760">Ranking-aware adapter for text-driven image ordering with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks. Code is available: github.com/uynaes/RankingAwareCLIP.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2412.02030.pdf' target='_blank'>https://arxiv.org/pdf/2412.02030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02030">NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce NitroFusion, a fundamentally different approach to single-step diffusion that achieves high-quality generation through a dynamic adversarial framework. While one-step methods offer dramatic speed advantages, they typically suffer from quality degradation compared to their multi-step counterparts. Just as a panel of art critics provides comprehensive feedback by specializing in different aspects like composition, color, and technique, our approach maintains a large pool of specialized discriminator heads that collectively guide the generation process. Each discriminator group develops expertise in specific quality aspects at different noise levels, providing diverse feedback that enables high-fidelity one-step generation. Our framework combines: (i) a dynamic discriminator pool with specialized discriminator groups to improve generation quality, (ii) strategic refresh mechanisms to prevent discriminator overfitting, and (iii) global-local discriminator heads for multi-scale quality assessment, and unconditional/conditional training for balanced generation. Additionally, our framework uniquely supports flexible deployment through bottom-up refinement, allowing users to dynamically choose between 1-4 denoising steps with the same model for direct quality-speed trade-offs. Through comprehensive experiments, we demonstrate that NitroFusion significantly outperforms existing single-step methods across multiple evaluation metrics, particularly excelling in preserving fine details and global consistency.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2412.00575.pdf' target='_blank'>https://arxiv.org/pdf/2412.00575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juhyung Ha, Jong Sung Park, David Crandall, Eleftherios Garyfallidis, Xuhong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00575">Multi-resolution Guided 3D GANs for Medical Image Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image translation is the process of converting from one imaging modality to another, in order to reduce the need for multiple image acquisitions from the same patient. This can enhance the efficiency of treatment by reducing the time, equipment, and labor needed. In this paper, we introduce a multi-resolution guided Generative Adversarial Network (GAN)-based framework for 3D medical image translation. Our framework uses a 3D multi-resolution Dense-Attention UNet (3D-mDAUNet) as the generator and a 3D multi-resolution UNet as the discriminator, optimized with a unique combination of loss functions including voxel-wise GAN loss and 2.5D perception loss. Our approach yields promising results in volumetric image quality assessment (IQA) across a variety of imaging modalities, body regions, and age groups, demonstrating its robustness. Furthermore, we propose a synthetic-to-real applicability assessment as an additional evaluation to assess the effectiveness of synthetic data in downstream applications such as segmentation. This comprehensive evaluation shows that our method produces synthetic medical images not only of high-quality but also potentially useful in clinical applications. Our code is available at github.com/juhha/3D-mADUNet.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2405.02266.pdf' target='_blank'>https://arxiv.org/pdf/2405.02266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Zanella, Ismail Ben Ayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02266">On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views of a single image to enhance zero-shot generalization, is emerging as a significant area of interest. This has predominantly directed research efforts toward test-time prompt tuning. In contrast, we introduce a robust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based methods without requiring this intensive training procedure. This positions MTA as an ideal solution for both standalone and API-based applications. Additionally, our method does not rely on ad hoc rules (e.g., confidence threshold) used in some previous test-time augmentation techniques to filter the augmented views. Instead, MTA incorporates a quality assessment variable for each view directly into its optimization process, termed as the inlierness score. This score is jointly optimized with a density mode seeking process, leading to an efficient training- and hyperparameter-free approach. We extensively benchmark our method on 15 datasets and demonstrate MTA's superiority and computational efficiency. Deployed easily as plug-and-play module on top of zero-shot models and state-of-the-art few-shot methods, MTA shows systematic and consistent improvements.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2403.08700.pdf' target='_blank'>https://arxiv.org/pdf/2403.08700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo SÃ¸ndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders Nymark Christensen, Martin Tolsgaard, Aasa Feragen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08700">Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, acquiring high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or fetus dynamics. In this work, we explore diffusion-based counterfactual explainable AI to generate realistic, high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our approach in generating plausible counterfactuals of increased quality. This shows future promise for enhancing training of clinicians by providing visual feedback and potentially improving standard plane quality and acquisition for downstream diagnosis and monitoring.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2402.09444.pdf' target='_blank'>https://arxiv.org/pdf/2402.09444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling-An Zeng, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09444">Multimodal Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between modality-specific branches and the mixed-modality branch, three novel modules are proposed. First, a Modality-specific Feature Decoder module is designed to selectively transfer modality-specific information to the mixed-modality branch. Second, when exploring the interaction between modality-specific information, we argue that using an invariant multimodal fusion policy may lead to suboptimal results, so as to take the potential diversity in different parts of an action into consideration. Therefore, an Adaptive Fusion Module is proposed to learn adaptive multimodal fusion policies in different parts of an action. This module consists of several FusionNets for exploring different multimodal fusion strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a module called Cross-modal Feature Decoder is designed to transfer cross-modal features generated by Adaptive Fusion Module to the mixed-modality branch.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2311.07090.pdf' target='_blank'>https://arxiv.org/pdf/2311.07090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yachun Mi, Yu Li, Yan Shu, Chen Hui, Puchao Zhou, Shaohui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07090">CLiF-VQA: Enhancing Video Quality Assessment by Incorporating High-Level Semantic Information related to Human Feelings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA) aims to simulate the process of perceiving video quality by the human visual system (HVS). The judgments made by HVS are always influenced by human subjective feelings. However, most of the current VQA research focuses on capturing various distortions in the spatial and temporal domains of videos, while ignoring the impact of human feelings. In this paper, we propose CLiF-VQA, which considers both features related to human feelings and spatial features of videos. In order to effectively extract features related to human feelings from videos, we explore the consistency between CLIP and human feelings in video perception for the first time. Specifically, we design multiple objective and subjective descriptions closely related to human feelings as prompts. Further we propose a novel CLIP-based semantic feature extractor (SFE) which extracts features related to human feelings by sliding over multiple regions of the video frame. In addition, we further capture the low-level-aware features of the video through a spatial feature extraction module. The two different features are then aggregated thereby obtaining the quality score of the video. Extensive experiments show that the proposed CLiF-VQA exhibits excellent performance on several VQA datasets.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2511.06947.pdf' target='_blank'>https://arxiv.org/pdf/2511.06947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulin Chen, Zeyuan Wang, Tianyuan Yu, Yingmei Wei, Liang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06947">FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2511.00639.pdf' target='_blank'>https://arxiv.org/pdf/2511.00639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taulant Kerci, Federico Milano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00639">Frequency Quality Assessment of GFM and GFL Converters and Synchronous Condensers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper compares the impact of different conventional and emerging technologies and control strategies on frequency quality. We study, in particular, the long-term dynamic performance of grid-forming (GFM) and grid-following (GFL) inverter-based resources (IBRs) as well as conventional synchronous machines. Extensive simulations and several realistic scenarios consider both short-term and long-term aspects of frequency quality. It is shown that, while overall GFM IBRs significantly improve frequency quality, a combination of GFL IBRs providing frequency support such as wind and batteries, and synchronous condensers, might be enough to meet similar frequency quality standards. Another result of the paper is that the need for automatic generation control (AGC) becomes less clear in GFM IBR-dominated grids from a frequency quality perspective.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2510.00872.pdf' target='_blank'>https://arxiv.org/pdf/2510.00872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kristoffer Christensen, Bo Nørregaard Jørgensen, Zheng Grace Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00872">A Visual Diagnostics Framework for District Heating Data: Enhancing Data Quality for AI-Driven Heat Consumption Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality data is a prerequisite for training reliable Artificial Intelligence (AI) models in the energy domain. In district heating networks, sensor and metering data often suffer from noise, missing values, and temporal inconsistencies, which can significantly degrade model performance. This paper presents a systematic approach for evaluating and improving data quality using visual diagnostics, implemented through an interactive web-based dashboard. The dashboard employs Python-based visualization techniques, including time series plots, heatmaps, box plots, histograms, correlation matrices, and anomaly-sensitive KPIs such as skewness and anomaly detection based on the modified z-scores. These tools al-low human experts to inspect and interpret data anomalies, enabling a human-in-the-loop strategy for data quality assessment. The methodology is demonstrated on a real-world dataset from a Danish district heating provider, covering over four years of hourly data from nearly 7000 meters. The findings show how visual analytics can uncover systemic data issues and, in the future, guide data cleaning strategies that enhance the accuracy, stability, and generalizability of Long Short-Term Memory and Gated Recurrent Unit models for heat demand forecasting. The study contributes to a scalable, generalizable framework for visual data inspection and underlines the critical role of data quality in AI-driven energy management systems.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2509.01411.pdf' target='_blank'>https://arxiv.org/pdf/2509.01411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>UÄur ÃoÄalan, Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Colin Groth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01411">MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2508.21550.pdf' target='_blank'>https://arxiv.org/pdf/2508.21550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Park, Haejun Chung, Ikbeom Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21550">EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2506.19844.pdf' target='_blank'>https://arxiv.org/pdf/2506.19844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Wang, Yash Bhalgat, Ruining Li, Victor Adrian Prisacariu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19844">Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle active view selection in novel view synthesis and 3D reconstruction. Existing methods like FisheRF and ActiveNeRF select the next best view by minimizing uncertainty or maximizing information gain in 3D, but they require specialized designs for different 3D representations and involve complex modelling in 3D space. Instead, we reframe this as a 2D image quality assessment (IQA) task, selecting views where current renderings have the lowest quality. Since ground-truth images for candidate views are unavailable, full-reference metrics like PSNR and SSIM are inapplicable, while no-reference metrics, such as MUSIQ and MANIQA, lack the essential multi-view context. Inspired by a recent cross-referencing quality framework CrossScore, we train a model to predict SSIM within a multi-view setup and use it to guide view selection. Our cross-reference IQA framework achieves substantial quantitative and qualitative improvements across standard benchmarks, while being agnostic to 3D representations, and runs 14-33 times faster than previous methods.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2503.06129.pdf' target='_blank'>https://arxiv.org/pdf/2503.06129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Kangcheng Wu, Junjie Chen, Ziwen Tan, Yuming Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06129">Viewport-Unaware Blind Omnidirectional Image Quality Assessment: A Flexible and Effective Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of existing blind omnidirectional image quality assessment (BOIQA) models rely on viewport generation by modeling user viewing behavior or transforming omnidirectional images (OIs) into varying formats; however, these methods are either computationally expensive or less scalable. To solve these issues, in this paper, we present a flexible and effective paradigm, which is viewport-unaware and can be easily adapted to 2D plane image quality assessment (2D-IQA). Specifically, the proposed BOIQA model includes an adaptive prior-equator sampling module for extracting a patch sequence from the equirectangular projection (ERP) image in a resolution-agnostic manner, a progressive deformation-unaware feature fusion module which is able to capture patch-wise quality degradation in a deformation-immune way, and a local-to-global quality aggregation module to adaptively map local perception to global quality. Extensive experiments across four OIQA databases (including uniformly distorted OIs and non-uniformly distorted OIs) demonstrate that the proposed model achieves competitive performance with low complexity against other state-of-the-art models, and we also verify its adaptive capacity to 2D-IQA.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2503.03255.pdf' target='_blank'>https://arxiv.org/pdf/2503.03255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Ziwen Tan, Jiale Rao, Lei Wu, Yifan Zuo, Yuming Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03255">Computational Analysis of Degradation Modeling in Blind Panoramic Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind panoramic image quality assessment (BPIQA) has recently brought new challenge to the visual quality community, due to the complex interaction between immersive content and human behavior. Although many efforts have been made to advance BPIQA from both conducting psychophysical experiments and designing performance-driven objective algorithms, \textit{limited content} and \textit{few samples} in those closed sets inevitably would result in shaky conclusions, thereby hindering the development of BPIQA, we refer to it as the \textit{easy-database} issue. In this paper, we present a sufficient computational analysis of degradation modeling in BPIQA to thoroughly explore the \textit{easy-database issue}, where we carefully design three types of experiments via investigating the gap between BPIQA and blind image quality assessment (BIQA), the necessity of specific design in BPIQA models, and the generalization ability of BPIQA models. From extensive experiments, we find that easy databases narrow the gap between the performance of BPIQA and BIQA models, which is unconducive to the development of BPIQA. And the easy databases make the BPIQA models be closed to saturation, therefore the effectiveness of the associated specific designs can not be well verified. Besides, the BPIQA models trained on our recently proposed databases with complicated degradation show better generalization ability. Thus, we believe that much more efforts are highly desired to put into BPIQA from both subjective viewpoint and objective viewpoint.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2502.11726.pdf' target='_blank'>https://arxiv.org/pdf/2502.11726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11726">No-reference geometry quality assessment for colorless point clouds via list-wise rank learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometry quality assessment (GQA) of colorless point clouds is crucial for evaluating the performance of emerging point cloud-based solutions (e.g., watermarking, compression, and 3-Dimensional (3D) reconstruction). Unfortunately, existing objective GQA approaches are traditional full-reference metrics, whereas state-of-the-art learning-based point cloud quality assessment (PCQA) methods target both color and geometry distortions, neither of which are qualified for the no-reference GQA task. In addition, the lack of large-scale GQA datasets with subjective scores, which are always imprecise, biased, and inconsistent, also hinders the development of learning-based GQA metrics. Driven by these limitations, this paper proposes a no-reference geometry-only quality assessment approach based on list-wise rank learning, termed LRL-GQA, which comprises of a geometry quality assessment network (GQANet) and a list-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the no-reference GQA as a list-wise rank problem, with the objective of directly optimizing the entire quality ordering. Specifically, a large dataset containing a variety of geometry-only distortions is constructed first, named LRL dataset, in which each sample is label-free but coupled with quality ranking information. Then, the GQANet is designed to capture intrinsic multi-scale patch-wise geometric features in order to predict a quality index for each point cloud. After that, the LRLNet leverages the LRL dataset and a likelihood loss to train the GQANet and ranks the input list of degraded point clouds according to their distortion levels. In addition, the pre-trained GQANet can be fine-tuned further to obtain absolute quality scores. Experimental results demonstrate the superior performance of the proposed no-reference LRL-GQA method compared with existing full-reference GQA metrics.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2502.11710.pdf' target='_blank'>https://arxiv.org/pdf/2502.11710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyong Su, Bingxu Xie, Zheng Li, Jincan Wu, Weiqing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11710">The Worse The Better: Content-Aware Viewpoint Generation Network for Projection-related Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Through experimental studies, however, we observed the instability of final predicted quality scores, which change significantly over different viewpoint settings. Inspired by the "wooden barrel theory", given the default content-independent viewpoints of existing projection-related PCQA approaches, this paper presents a novel content-aware viewpoint generation network (CAVGN) to learn better viewpoints by taking the distribution of geometric and attribute features of degraded point clouds into consideration. Firstly, the proposed CAVGN extracts multi-scale geometric and texture features of the entire input point cloud, respectively. Then, for each default content-independent viewpoint, the extracted geometric and texture features are refined to focus on its corresponding visible part of the input point cloud. Finally, the refined geometric and texture features are concatenated to generate an optimized viewpoint. To train the proposed CAVGN, we present a self-supervised viewpoint ranking network (SSVRN) to select the viewpoint with the worst quality projected image to construct a default-optimized viewpoint dataset, which consists of thousands of paired default viewpoints and corresponding optimized viewpoints. Experimental results show that the projection-related PCQA methods can achieve higher performance using the viewpoints generated by the proposed CAVGN.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2501.09927.pdf' target='_blank'>https://arxiv.org/pdf/2501.09927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangkun Sun, Bowen Qu, Xiaoyu Liang, Songlin Fan, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09927">IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding results different editing methods, and total 3,010 Mean Opinion Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a multi-modality source-aware quality assessment method for text-driven image editing. To the best of our knowledge, IE-Bench offers the first IQA dataset and model tailored for text-driven image editing. Extensive experiments demonstrate IE-QA's superior subjective-alignments on the text-driven image editing task compared with previous metrics. We will make all related data and code available to the public.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2501.07087.pdf' target='_blank'>https://arxiv.org/pdf/2501.07087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiebin Yan, Lei Wu, Yuming Fang, Xuelin Liu, Xue Xia, Weide Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07087">Video Quality Assessment for Online Processing: From Spatial to Temporal Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of multimedia processing and deep learning technologies, especially in the field of video understanding, video quality assessment (VQA) has achieved significant progress. Although researchers have moved from designing efficient video quality mapping models to various research directions, in-depth exploration of the effectiveness-efficiency trade-offs of spatio-temporal modeling in VQA models is still less sufficient. Considering the fact that videos have highly redundant information, this paper investigates this problem from the perspective of joint spatial and temporal sampling, aiming to seek the answer to how little information we should keep at least when feeding videos into the VQA models while with acceptable performance sacrifice. To this end, we drastically sample the video's information from both spatial and temporal dimensions, and the heavily squeezed video is then fed into a stable VQA model. Comprehensive experiments regarding joint spatial and temporal sampling are conducted on six public video quality databases, and the results demonstrate the acceptable performance of the VQA model when throwing away most of the video information. Furthermore, with the proposed joint spatial and temporal sampling strategy, we make an initial attempt to design an online VQA model, which is instantiated by as simple as possible a spatial feature extractor, a temporal feature fusion module, and a global quality regression module. Through quantitative and qualitative experiments, we verify the feasibility of online VQA model by simplifying itself and reducing input.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2406.09762.pdf' target='_blank'>https://arxiv.org/pdf/2406.09762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryosuke Watanabe, Keisuke Nonaka, Eduardo Pavez, Tatsuya Kobayashi, Antonio Ortega
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09762">Full-reference Point Cloud Quality Assessment Using Spectral Graph Wavelets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point clouds in 3D applications frequently experience quality degradation during processing, e.g., scanning and compression. Reliable point cloud quality assessment (PCQA) is important for developing compression algorithms with good bitrate-quality trade-offs and techniques for quality improvement (e.g., denoising). This paper introduces a full-reference (FR) PCQA method utilizing spectral graph wavelets (SGWs). First, we propose novel SGW-based PCQA metrics that compare SGW coefficients of coordinate and color signals between reference and distorted point clouds. Second, we achieve accurate PCQA by integrating several conventional FR metrics and our SGW-based metrics using support vector regression. To our knowledge, this is the first study to introduce SGWs for PCQA. Experimental results demonstrate the proposed PCQA metric is more accurately correlated with subjective quality scores compared to conventional PCQA metrics.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2404.14409.pdf' target='_blank'>https://arxiv.org/pdf/2404.14409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Wang, Wenjing Bian, Victor Adrian Prisacariu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14409">CrossScore: Towards Multi-View Image Evaluation and Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2402.08294.pdf' target='_blank'>https://arxiv.org/pdf/2402.08294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manxi Lin, Jakob Ambsdorf, Emilie Pi Fogtmann Sejer, Zahra Bashir, Chun Kit Wong, Paraskevas Pegios, Alberto Raheli, Morten Bo SÃ¸ndergaard Svendsen, Mads Nielsen, Martin GrÃ¸nnebÃ¦k Tolsgaard, Anders Nymark Christensen, Aasa Feragen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08294">Learning semantic image quality for fetal ultrasound from noisy ranking annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the notion of semantic image quality for applications where image quality relies on semantic requirements. Working in fetal ultrasound, where ranking is challenging and annotations are noisy, we design a robust coarse-to-fine model that ranks images based on their semantic image quality and endow our predicted rankings with an uncertainty estimate. To annotate rankings on training data, we design an efficient ranking annotation scheme based on the merge sort algorithm. Finally, we compare our ranking algorithm to a number of state-of-the-art ranking algorithms on a challenging fetal ultrasound quality assessment task, showing the superior performance of our method on the majority of rank correlation metrics.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2402.00622.pdf' target='_blank'>https://arxiv.org/pdf/2402.00622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vignesh V Menon, Adam Wieckowski, Jens Brandenburg, Benjamin Bross, Thomas Schierl, Detlev Marpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00622">Gain of Grain: A Film Grain Handling Toolchain for VVC-based Open Implementations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Film grain is a distinctive visual characteristic cherished by filmmakers and cinephiles for its ability to evoke nostalgia and artistic aesthetics. However, faithful preservation of film grain during encoding poses unique challenges. Film grain introduces random noise, complicating traditional compression techniques. Consequently, specialized algorithms and encoding strategies have emerged, aiming to strike a harmonious equilibrium. This paper delves into the nuanced realm of film grain handling in Versatile Video Coding (VVC) encoding. We explore the delicate balance between retaining the cinematic charm of film grain and achieving efficient compression. Moreover, we discuss the importance of perceptual quality assessment and adaptive encoding techniques in preserving film grain fidelity. Additionally, we delve into the impact of film grain handling on bitrate control and compression efficiency using VVenC, an open and optimized VVC encoder. Understanding the role of film grain and its nuanced treatment within encoders becomes increasingly pivotal for delivering high-quality, grain-inclusive content in the digital age.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2401.08926.pdf' target='_blank'>https://arxiv.org/pdf/2401.08926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Fan, Wei Gao, Zhineng Chen, Ge Li, Guoqing Liu, Qicheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08926">Stochasticity-aware No-Reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of point cloud processing algorithms necessitates an accurate assessment for their quality. Previous works consistently regard point cloud quality assessment (PCQA) as a MOS regression problem and devise a deterministic mapping, ignoring the stochasticity in generating MOS from subjective tests. This work presents the first probabilistic architecture for no-reference PCQA, motivated by the labeling process of existing datasets. The proposed method can model the quality judging stochasticity of subjects through a tailored conditional variational autoencoder (CVAE) and produces multiple intermediate quality ratings. These intermediate ratings simulate the judgments from different subjects and are then integrated into an accurate quality prediction, mimicking the generation process of a ground truth MOS. Specifically, our method incorporates a Prior Module, a Posterior Module, and a Quality Rating Generator, where the former two modules are introduced to model the judging stochasticity in subjective tests, while the latter is developed to generate diverse quality ratings. Extensive experiments indicate that our approach outperforms previous cutting-edge methods by a large margin and exhibits gratifying cross-dataset robustness. Codes are available at https://git.openi.org.cn/OpenPointCloud/nrpcqa.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2304.05463.pdf' target='_blank'>https://arxiv.org/pdf/2304.05463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Kit Wong, Manxi Lin, Alberto Raheli, Zahra Bashir, Morten Bo SÃ¸ndergaard Svendsen, Martin GrÃ¸nnebÃ¦k Tolsgaard, Aasa Feragen, Anders Nymark Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05463">An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Examination of the umbilical artery with Doppler ultrasonography is performed to investigate blood supply to the fetus through the umbilical cord, which is vital for the monitoring of fetal health. Such examination involves several steps that must be performed correctly: identifying suitable sites on the umbilical artery for the measurement, acquiring the blood flow curve in the form of a Doppler spectrum, and ensuring compliance to a set of quality standards. These steps rely heavily on the operator's skill, and the shortage of experienced sonographers has thus created a demand for machine assistance. In this work, we propose an automatic system to fill the gap. By using a modified Faster R-CNN network, we obtain an algorithm that can suggest locations suitable for Doppler measurement. Meanwhile, we have also developed a method for assessment of the Doppler spectrum's quality. The proposed system is validated on 657 images from a national ultrasound screening database, with results demonstrating its potential as a guidance system.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2304.05359.pdf' target='_blank'>https://arxiv.org/pdf/2304.05359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Di Feola, Lorenzo Tronchin, Paolo Soda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05359">A comparative study between paired and unpaired Image Quality Assessment in Low-Dose CT Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The current deep learning approaches for low-dose CT denoising can be divided into paired and unpaired methods. The former involves the use of well-paired datasets, whilst the latter relaxes this constraint. The large availability of unpaired datasets has raised the interest in deepening unpaired denoising strategies that, in turn, need for robust evaluation techniques going beyond the qualitative evaluation. To this end, we can use quantitative image quality assessment scores that we divided into two categories, i.e., paired and unpaired measures. However, the interpretation of unpaired metrics is not straightforward, also because the consistency with paired metrics has not been fully investigated. To cope with this limitation, in this work we consider 15 paired and unpaired scores, which we applied to assess the performance of low-dose CT denoising. We perform an in-depth statistical analysis that not only studies the correlation between paired and unpaired metrics but also within each category. This brings out useful guidelines that can help researchers and practitioners select the right measure for their applications.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2301.06943.pdf' target='_blank'>https://arxiv.org/pdf/2301.06943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingshan Hou, Peng Cao, Jiaqi Wang, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06943">Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retinal fundus images have been applied for the diagnosis and screening of eye diseases, such as Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). However, both low-quality fundus images and style inconsistency potentially increase uncertainty in the diagnosis of fundus disease and even lead to misdiagnosis by ophthalmologists. Most of the existing image enhancement methods mainly focus on improving the image quality by leveraging the guidance of high-quality images, which is difficult to be collected in medical applications. In this paper, we tackle image quality enhancement in a fully unsupervised setting, i.e., neither paired images nor high-quality images. To this end, we explore the potential of the self-supervised task for improving the quality of fundus images without the requirement of high-quality reference images. Specifically, we construct multiple patch-wise domains via an auxiliary pre-trained quality assessment network and a style clustering. To achieve robust low-quality image enhancement and address style inconsistency, we formulate two self-supervised domain adaptation tasks to disentangle the features of image content, low-quality factor and style information by exploring intrinsic supervision signals within the low-quality images. Extensive experiments are conducted on EyeQ and Messidor datasets, and results show that our DASQE method achieves new state-of-the-art performance when only low-quality images are available.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2211.10630.pdf' target='_blank'>https://arxiv.org/pdf/2211.10630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manxi Lin, Aasa Feragen, Kamil Mikolaj, Zahra Bashir, Martin GrÃ¸nnebÃ¦k Tolsgaard, Anders Nymark Christensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.10630">Explainable fetal ultrasound quality assessment with progressive concept bottleneck models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quality of fetal ultrasound screening scans directly influences the precision of biometric measurements. However, acquiring high-quality scans is labor-intensive and highly relies on the operator's skills. Considering the low contrastiveness and imaging artifacts that widely exist in ultrasound, even a dedicated deep-learning model can be vulnerable to learning from confounding information in the image. In this paper, we propose a holistic and explainable method for fetal ultrasound quality assessment, where we design a hierarchical concept bottleneck model by introducing human-readable ``concepts" into the task and imitating the sequential expert decision-making process. This hierarchical information flow forces the model to learn concepts from semantically meaningful areas: The model first passes through a layer of visual, segmentation-based concepts, and next a second layer of property concepts directly associated with the decision-making task. We consider the quality assessment to be in a more challenging but more realistic setting, with fine-grained image recognition. Experiments show that our model outperforms equivalent concept-free models on an in-house dataset, and shows better generalizability on two public benchmarks, one from Spain and one from Africa, without any fine-tuning.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2511.10250.pdf' target='_blank'>https://arxiv.org/pdf/2511.10250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongji Zhang, Siqi Li, Yue Gao, Yu Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10250">FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2511.06830.pdf' target='_blank'>https://arxiv.org/pdf/2511.06830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianang Chen, Jian Jin, Shilv Cai, Zhuangzi Li, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06830">MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2510.11832.pdf' target='_blank'>https://arxiv.org/pdf/2510.11832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Narine Kokhlikyan, Kamalika Chaudhuri, Saeed Mahloujifar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11832">Z0-Inf: Zeroth Order Approximation for Data Influence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A critical aspect of analyzing and improving modern machine learning systems lies in understanding how individual training examples influence a model's predictive behavior. Estimating this influence enables critical applications, including data selection and model debugging; in particular, self-influence, which quantifies the influence of a training point on itself, has found many uses in data quality assessment and outlier detection. Existing methods for measuring data influence, however, are often impractical for large models due to low accuracy or prohibitive computational costs: most approaches either provide poor approximations or rely on gradients and inverse-Hessian computations that remain challenging to scale. In this work, we introduce a highly efficient zeroth-order approximation for estimating the influence of training data that requires only a fraction of the time and memory footprint of prior methods. Notably, our method relies solely on loss values of intermediate checkpoints on the training and test data, along with the checkpoints themselves, making it broadly applicable even when the loss function of interest is non-differentiable. Beyond its computational efficiency, our approach achieves superior accuracy in estimating self-influence and comparable or improved accuracy in estimating train-test influence for fine-tuned large language models, enabling scalable and practical analysis of how training data shapes model behavior.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2509.12510.pdf' target='_blank'>https://arxiv.org/pdf/2509.12510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Shao, Ruoyu Zhang, Zequan Liang, Ehsan Kourkchi, Setareh Rafatirad, Houman Homayoun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12510">Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wearable photoplethysmography (PPG) is embedded in billions of devices, yet its optical waveform is easily corrupted by motion, perfusion loss, and ambient light, jeopardizing downstream cardiometric analytics. Existing signal-quality assessment (SQA) methods rely either on brittle heuristics or on data-hungry supervised models. We introduce the first fully unsupervised SQA pipeline for wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw, unlabeled data from heterogeneous sources (varying in device and sampling frequency), yielding optical-emitter- and motion-invariant embeddings (i.e., the learned representation is stable across differences in LED wavelength, drive intensity, and device optics, as well as wrist motion). Stage 2 converts each 512-D encoder embedding into a 4-D topological signature via persistent homology (PH) and clusters these signatures with HDBSCAN. To produce a binary signal-quality index (SQI), the acceptable PPG signals are represented by the densest cluster while the remaining clusters are assumed to mainly contain poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette, Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173, respectively, on a stratified sample of 10,000 windows. In this study, we propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA) framework that offers a drop-in, scalable, cross-device quality gate for PPG signals.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2508.17965.pdf' target='_blank'>https://arxiv.org/pdf/2508.17965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangfei Sheng, Zhichao Duan, Xiaofeng Pan, Yipo Huang, Zhichao Yang, Pengfei Chen, Leida Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17965">TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Livestreaming has become increasingly prevalent in modern visual communication, where automatic camera quality tuning is essential for delivering superior user Quality of Experience (QoE). Such tuning requires accurate blind image quality assessment (BIQA) to guide parameter optimization decisions. Unfortunately, the existing BIQA models typically only predict an overall coarse-grained quality score, which cannot provide fine-grained perceptual guidance for precise camera parameter tuning. To bridge this gap, we first establish FGLive-10K, a comprehensive fine-grained BIQA database containing 10,185 high-resolution images captured under varying camera parameter configurations across diverse livestreaming scenarios. The dataset features 50,925 multi-attribute quality annotations and 19,234 fine-grained pairwise preference annotations. Based on FGLive-10K, we further develop TuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which integrates human-aware feature extraction and graph-based camera parameter fusion. Extensive experiments and comparisons demonstrate that TuningIQA significantly outperforms state-of-the-art BIQA methods in both score regression and fine-grained quality ranking, achieving superior performance when deployed for livestreaming camera tuning.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2508.05168.pdf' target='_blank'>https://arxiv.org/pdf/2508.05168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caner Ãzer, Patryk Rygiel, Bram de Wilde, Ä°lkay ÃksÃ¼z, Jelmer M. Wolterink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05168">Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artifacts pose a significant challenge in medical imaging, impacting diagnostic accuracy and downstream analysis. While image-based approaches for detecting artifacts can be effective, they often rely on preprocessing methods that can lead to information loss and high-memory-demand medical images, thereby limiting the scalability of classification models. In this work, we propose the use of implicit neural representations (INRs) for image quality assessment. INRs provide a compact and continuous representation of medical images, naturally handling variations in resolution and image size while reducing memory overhead. We develop deep weight space networks, graph neural networks, and relational attention transformers that operate on INRs to achieve image quality assessment. Our method is evaluated on the ACDC dataset with synthetically generated artifact patterns, demonstrating its effectiveness in assessing image quality while achieving similar performance with fewer parameters.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2508.01460.pdf' target='_blank'>https://arxiv.org/pdf/2508.01460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sikha O K, Meritxell Riera-MarÃ­n, Adrian Galdran, Javier GarcÃ­a Lopez, Julia RodrÃ­guez-Comas, Gemma Piella, Miguel A. GonzÃ¡lez Ballester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01460">Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image segmentation is a critical step in computational biomedical image analysis, typically evaluated using metrics like the Dice coefficient during training and validation. However, in clinical settings without manual annotations, assessing segmentation quality becomes challenging, and models lacking reliability indicators face adoption barriers. To address this gap, we propose a novel framework for predicting segmentation quality without requiring ground truth annotations during test time. Our approach introduces two complementary frameworks: one leveraging predicted segmentation and uncertainty maps, and another integrating the original input image, uncertainty maps, and predicted segmentation maps. We present Bayesian adaptations of two benchmark segmentation models-SwinUNet and Feature Pyramid Network with ResNet50-using Monte Carlo Dropout, Ensemble, and Test Time Augmentation to quantify uncertainty. We evaluate four uncertainty estimates: confidence map, entropy, mutual information, and expected pairwise Kullback-Leibler divergence on 2D skin lesion and 3D liver segmentation datasets, analyzing their correlation with segmentation quality metrics. Our framework achieves an R2 score of 93.25 and Pearson correlation of 96.58 on the HAM10000 dataset, outperforming previous segmentation quality assessment methods. For 3D liver segmentation, Test Time Augmentation with entropy achieves an R2 score of 85.03 and a Pearson correlation of 65.02, demonstrating cross-modality robustness. Additionally, we propose an aggregation strategy that combines multiple uncertainty estimates into a single score per image, offering a more robust and comprehensive assessment of segmentation quality. Finally, we use Grad-CAM and UMAP-based embedding analysis to interpret the model's behavior and reliability, highlighting the impact of uncertainty integration.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2507.20548.pdf' target='_blank'>https://arxiv.org/pdf/2507.20548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lan Yang, Kaiyue Pang, Honggang Zhang, Yi-Zhe Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20548">Annotation-Free Human Sketch Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As lovely as bunnies are, your sketched version would probably not do them justice (Fig.~\ref{fig:intro}). This paper recognises this very problem and studies sketch quality assessment for the first time -- letting you find these badly drawn ones. Our key discovery lies in exploiting the magnitude ($L_2$ norm) of a sketch feature as a quantitative quality metric. We propose Geometry-Aware Classification Layer (GACL), a generic method that makes feature-magnitude-as-quality-metric possible and importantly does it without the need for specific quality annotations from humans. GACL sees feature magnitude and recognisability learning as a dual task, which can be simultaneously optimised under a neat cross-entropy classification loss with theoretic guarantee. This gives GACL a nice geometric interpretation (the better the quality, the easier the recognition), and makes it agnostic to both network architecture changes and the underlying sketch representation. Through a large scale human study of 160,000 \doublecheck{trials}, we confirm the agreement between our GACL-induced metric and human quality perception. We further demonstrate how such a quality assessment capability can for the first time enable three practical sketch applications. Interestingly, we show GACL not only works on abstract visual representations such as sketch but also extends well to natural images on the problem of image quality assessment (IQA). Last but not least, we spell out the general properties of GACL as general-purpose data re-weighting strategy and demonstrate its applications in vertical problems such as noisy label cleansing. Code will be made publicly available at github.com/yanglan0225/SketchX-Quantifying-Sketch-Quality.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2507.05528.pdf' target='_blank'>https://arxiv.org/pdf/2507.05528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05528">Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2506.10125.pdf' target='_blank'>https://arxiv.org/pdf/2506.10125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muqi Zou, Hongyu Cai, Hongwei Wu, Zion Leonahenahe Basque, Arslan Khan, Berkay Celik, Dave, Tian, Antonio Bianchi, Ruoyu, Wang, Dongyan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10125">D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation.
  In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2% of all the functions produced by the native decompiler.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2506.04950.pdf' target='_blank'>https://arxiv.org/pdf/2506.04950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Sun, Yipeng Wang, Junyu Shi, Zhiyuan Zhang, Yanmei Xiao, Lei Zhu, Manxi Jiang, Qiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04950">Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence has recently shown promise in automated embryo selection for In-Vitro Fertilization (IVF). However, current approaches either address partial embryo evaluation lacking holistic quality assessment or target clinical outcomes inevitably confounded by extra-embryonic factors, both limiting clinical utility. To bridge this gap, we propose a new task called Video-Based Embryo Grading - the first paradigm that directly utilizes full-length time-lapse monitoring (TLM) videos to predict embryologists' overall quality assessments. To support this task, we curate a real-world clinical dataset comprising over 2,500 TLM videos, each annotated with a grading label indicating the overall quality of embryos. Grounded in clinical decision-making principles, we propose a Complementary Spatial-Temporal Pattern Mining (CoSTeM) framework that conceptually replicates embryologists' evaluation process. The CoSTeM comprises two branches: (1) a morphological branch using a Mixture of Cross-Attentive Experts layer and a Temporal Selection Block to select discriminative local structural features, and (2) a morphokinetic branch employing a Temporal Transformer to model global developmental trajectories, synergistically integrating static and dynamic determinants for grading embryos. Extensive experimental results demonstrate the superiority of our design. This work provides a valuable methodological framework for AI-assisted embryo selection. The dataset and source code will be publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2505.08537.pdf' target='_blank'>https://arxiv.org/pdf/2505.08537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Lamine Mekhalfi, Paul Chippendale, Fabio Poiesi, Samuele Bonecher, Gilberto Osler, Nicola Zancanella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08537">The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on Hugging Face at: https://huggingface.co/datasets/FBK-TeV/RaspGrade.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2503.02206.pdf' target='_blank'>https://arxiv.org/pdf/2503.02206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Yang, Leida Li, Pengfei Chen, Jinjian Wu, Giuseppe Valenzise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02206">Language-Guided Visual Perception Disentanglement for Image Quality Assessment and Conditional Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive vision-language models, such as CLIP, have demonstrated excellent zero-shot capability across semantic recognition tasks, mainly attributed to the training on a large-scale I&1T (one Image with one Text) dataset. This kind of multimodal representations often blend semantic and perceptual elements, placing a particular emphasis on semantics. However, this could be problematic for popular tasks like image quality assessment (IQA) and conditional image generation (CIG), which typically need to have fine control on perceptual and semantic features. Motivated by the above facts, this paper presents a new multimodal disentangled representation learning framework, which leverages disentangled text to guide image disentanglement. To this end, we first build an I&2T (one Image with a perceptual Text and a semantic Text) dataset, which consists of disentangled perceptual and semantic text descriptions for an image. Then, the disentangled text descriptions are utilized as supervisory signals to disentangle pure perceptual representations from CLIP's original `coarse' feature space, dubbed DeCLIP. Finally, the decoupled feature representations are used for both image quality assessment (technical quality and aesthetic quality) and conditional image generation. Extensive experiments and comparisons have demonstrated the advantages of the proposed method on the two popular tasks. The dataset, code, and model will be available.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2502.05924.pdf' target='_blank'>https://arxiv.org/pdf/2502.05924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengzhu Tang, Zefeng Zhang, Zhiping Li, Zhenyu Zhang, Xing Wu, Li Gao, Suqi Cheng, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05924">Multi-Branch Collaborative Learning Network for Video Quality Assessment in Industrial Video Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA) is vital for large-scale video retrieval systems, aimed at identifying quality issues to prioritize high-quality videos. In industrial systems, low-quality video characteristics fall into four categories: visual-related issues like mosaics and black boxes, textual issues from video titles and OCR content, and semantic issues like frame incoherence and frame-text mismatch from AI-generated videos. Despite their prevalence in industrial settings, these low-quality videos have been largely overlooked in academic research, posing a challenge for accurate identification. To address this, we introduce the Multi-Branch Collaborative Network (MBCN) tailored for industrial video retrieval systems. MBCN features four branches, each designed to tackle one of the aforementioned quality issues. After each branch independently scores videos, we aggregate these scores using a weighted approach and a squeeze-and-excitation mechanism to dynamically address quality issues across different scenarios. We implement point-wise and pair-wise optimization objectives to ensure score stability and reasonableness. Extensive offline and online experiments on a world-level video search engine demonstrate MBCN's effectiveness in identifying video quality issues, significantly enhancing the retrieval system's ranking performance. Detailed experimental analyses confirm the positive contribution of all four evaluation branches. Furthermore, MBCN significantly improves recognition accuracy for low-quality AI-generated videos compared to the baseline.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2410.13216.pdf' target='_blank'>https://arxiv.org/pdf/2410.13216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Felipe Villa-Arenas, Ata Nizamoglu, Qianli Wang, Sebastian MÃ¶ller, Vera Schmitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13216">Anchored Alignment for Self-Explanations Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2410.08534.pdf' target='_blank'>https://arxiv.org/pdf/2410.08534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhijay Ghildyal, Yuanhan Chen, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08534">Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of AI has influenced many aspects of human life, from self-driving cars and intelligent chatbots to text-based image and video generation models capable of creating realistic images and videos based on user prompts (text-to-image, image-to-image, and image-to-video). AI-based methods for image and video super resolution, video frame interpolation, denoising, and compression have already gathered significant attention and interest in the industry and some solutions are already being implemented in real-world products and services. However, to achieve widespread integration and acceptance, AI-generated and enhanced content must be visually accurate, adhere to intended use, and maintain high visual quality to avoid degrading the end user's quality of experience (QoE).
  One way to monitor and control the visual "quality" of AI-generated and -enhanced content is by deploying Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models. However, most existing IQA and VQA models measure visual fidelity in terms of "reconstruction" quality against a pristine reference content and were not designed to assess the quality of "generative" artifacts. To address this, newer metrics and models have recently been proposed, but their performance evaluation and overall efficacy have been limited by datasets that were too small or otherwise lack representative content and/or distortion capacity; and by performance measures that can accurately report the success of an IQA/VQA model for "GenAI". This paper examines the current shortcomings and possibilities presented by AI-generated and enhanced image and video content, with a particular focus on end-user perceived quality. Finally, we discuss open questions and make recommendations for future work on the "GenAI" quality assessment problems, towards further progressing on this interesting and relevant field of research.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2408.16879.pdf' target='_blank'>https://arxiv.org/pdf/2408.16879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16879">MSLIQA: Enhancing Learning Representations for Image Quality Assessment through Multi-Scale Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due to the diversity of distortions and the lack of large annotated datasets. Many studies have attempted to tackle these challenges by developing more accurate NR-IQA models, often employing complex and computationally expensive networks, or by bridging the domain gap between various distortions to enhance performance on test datasets. In our work, we improve the performance of a generic lightweight NR-IQA model by introducing a novel augmentation strategy that boosts its performance by almost 28\%. This augmentation strategy enables the network to better discriminate between different distortions in various parts of the image by zooming in and out. Additionally, the inclusion of test-time augmentation further enhances performance, making our lightweight network's results comparable to the current state-of-the-art models, simply through the use of augmentations.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2403.13030.pdf' target='_blank'>https://arxiv.org/pdf/2403.13030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiang Luo, Yan Wang, Hongwei Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13030">Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learned Image Compression (LIC) has achieved dramatic progress regarding objective and subjective metrics. MSE-based models aim to improve objective metrics while generative models are leveraged to improve visual quality measured by subjective metrics. However, they all suffer from blurring or deformation at low bit rates, especially at below $0.2bpp$. Besides, deformation on human faces and text is unacceptable for visual quality assessment, and the problem becomes more prominent on small faces and text. To solve this problem, we combine the advantage of MSE-based models and generative models by utilizing region of interest (ROI). We propose Hierarchical-ROI (H-ROI), to split images into several foreground regions and one background region to improve the reconstruction of regions containing faces, text, and complex textures. Further, we propose adaptive quantization by non-linear mapping within the channel dimension to constrain the bit rate while maintaining the visual quality. Exhaustive experiments demonstrate that our methods achieve better visual quality on small faces and text with lower bit rates, e.g., $0.7X$ bits of HiFiC and $0.5X$ bits of BPG.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2402.12690.pdf' target='_blank'>https://arxiv.org/pdf/2402.12690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wei Lim, Ekaterina Vylomova, Trevor Cohn, Charles Kemp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12690">Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al., 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off between these dimensions has implications both for assessing translation quality and developing improved MT systems.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2308.16215.pdf' target='_blank'>https://arxiv.org/pdf/2308.16215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Reich, Biplob Debnath, Deep Patel, Tim Prangemeier, Daniel Cremers, Srimat Chakradhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16215">Deep Video Codec Control for Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standardized lossy video coding is at the core of almost all real-world video processing pipelines. Rate control is used to enable standard codecs to adapt to different network bandwidth conditions or storage constraints. However, standard video codecs (e.g., H.264) and their rate control modules aim to minimize video distortion w.r.t. human quality assessment. We demonstrate empirically that standard-coded videos vastly deteriorate the performance of deep vision models. To overcome the deterioration of vision performance, this paper presents the first end-to-end learnable deep video codec control that considers both bandwidth constraints and downstream deep vision performance, while adhering to existing standardization. We demonstrate that our approach better preserves downstream deep vision performance than traditional standard video coding.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2306.07383.pdf' target='_blank'>https://arxiv.org/pdf/2306.07383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MohammadHossein Givkashi, MohammadReza Naderi, Nader Karimi, Shahram Shirani, Shadrokh Samavi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07383">Supervised Deep Learning for Content-Aware Image Retargeting with Fourier Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image retargeting aims to alter the size of the image with attention to the contents. One of the main obstacles to training deep learning models for image retargeting is the need for a vast labeled dataset. Labeled datasets are unavailable for training deep learning models in the image retargeting tasks. As a result, we present a new supervised approach for training deep learning models. We use the original images as ground truth and create inputs for the model by resizing and cropping the original images. A second challenge is generating different image sizes in inference time. However, regular convolutional neural networks cannot generate images of different sizes than the input image. To address this issue, we introduced a new method for supervised learning. In our approach, a mask is generated to show the desired size and location of the object. Then the mask and the input image are fed to the network. Comparing image retargeting methods and our proposed method demonstrates the model's ability to produce high-quality retargeted images. Afterward, we compute the image quality assessment score for each output image based on different techniques and illustrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2306.04335.pdf' target='_blank'>https://arxiv.org/pdf/2306.04335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mbithe Nzomo, Deshendran Moodley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04335">Semantic web technologies in sensor-based personal health monitoring systems: A systematic mapping study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been an increased focus on early detection, prevention, and prediction of diseases. This, together with advances in sensor technology and the Internet of Things, has led to accelerated efforts in the development of personal health monitoring systems. This study analyses the state of the art in the use of Semantic Web technologies in sensor-based personal health monitoring systems. Using a systematic approach, a total of 48 systems are selected as representative of the current state of the art. We critically analyse the extent to which the selected systems address seven key challenges: interoperability, situation detection, situation prediction, decision support, context awareness, explainability, and uncertainty handling. We discuss the role and limitations of Semantic Web technologies in managing each challenge. We then conduct a quality assessment of the selected systems based on the data and devices used, system and components development, rigour of evaluation, and accessibility of research outputs. Finally, we propose a reference architecture to provide guidance for the design and development of new systems. This study provides a comprehensive mapping of the field, identifies inadequacies in the state of the art, and provides recommendations for future research.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2304.11521.pdf' target='_blank'>https://arxiv.org/pdf/2304.11521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Jin, Wu Zhou, Jinyu Wang, Duo Xu, Yiqing Rong, Jialin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11521">An Order-Complexity Model for Aesthetic Quality Assessment of Homophony Music Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although computational aesthetics evaluation has made certain achievements in many fields, its research of music performance remains to be explored. At present, subjective evaluation is still a ultimate method of music aesthetics research, but it will consume a lot of human and material resources. In addition, the music performance generated by AI is still mechanical, monotonous and lacking in beauty. In order to guide the generation task of AI music performance, and to improve the performance effect of human performers, this paper uses Birkhoff's aesthetic measure to propose a method of objective measurement of beauty. The main contributions of this paper are as follows: Firstly, we put forward an objective aesthetic evaluation method to measure the music performance aesthetic; Secondly, we propose 10 basic music features and 4 aesthetic music features. Experiments show that our method performs well on performance assessment.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2304.02859.pdf' target='_blank'>https://arxiv.org/pdf/2304.02859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengzhong Tu, Peyman Milanfar, Hossein Talebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02859">MULLER: Multilayer Laplacian Resizer for Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image resizing operation is a fundamental preprocessing module in modern computer vision. Throughout the deep learning revolution, researchers have overlooked the potential of alternative resizing methods beyond the commonly used resizers that are readily available, such as nearest-neighbors, bilinear, and bicubic. The key question of our interest is whether the front-end resizer affects the performance of deep vision models? In this paper, we present an extremely lightweight multilayer Laplacian resizer with only a handful of trainable parameters, dubbed MULLER resizer. MULLER has a bandpass nature in that it learns to boost details in certain frequency subbands that benefit the downstream recognition models. We show that MULLER can be easily plugged into various training pipelines, and it effectively boosts the performance of the underlying vision task with little to no extra cost. Specifically, we select a state-of-the-art vision Transformer, MaxViT, as the baseline, and show that, if trained with MULLER, MaxViT gains up to 0.6% top-1 accuracy, and meanwhile enjoys 36% inference cost saving to achieve similar top-1 accuracy on ImageNet-1k, as compared to the standard training scheme. Notably, MULLER's performance also scales with model size and training data size such as ImageNet-21k and JFT, and it is widely applicable to multiple vision tasks, including image classification, object detection and segmentation, as well as image quality assessment.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2303.02562.pdf' target='_blank'>https://arxiv.org/pdf/2303.02562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaxuan Liu, Jian Jin, Yuan Xue, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02562">The First Comprehensive Dataset with Multiple Distortion Types for Visual Just-Noticeable Differences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, with the development of deep learning, a number of Just Noticeable Difference (JND) datasets have been built for JND modeling. However, all the existing JND datasets only label the JND points based on the level of compression distortion. Hence, JND models learned from such datasets can only be used for image/video compression. As known, JND is a major characteristic of the human visual system (HVS), which reflects the maximum visual distortion that the HVS can tolerate. Hence, a generalized JND modeling should take more kinds of distortion types into account. To benefit JND modeling, this work establishes a generalized JND dataset with a coarse-to-fine JND selection, which contains 106 source images and 1,642 JND maps, covering 25 distortion types. To this end, we proposed a coarse JND candidate selection scheme to select the distorted images from the existing Image Quality Assessment (IQA) datasets as JND candidates instead of generating JND maps ourselves. Then, a fine JND selection is carried out on the JND candidates with a crowdsourced subjective assessment.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2301.05908.pdf' target='_blank'>https://arxiv.org/pdf/2301.05908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Jin, Wu Zhou, Jinyu Wang, Duo Xu, Yiqing Rong, Shuai Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05908">An Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational aesthetics evaluation has made great achievements in the field of visual arts, but the research work on music still needs to be explored. Although the existing work of music generation is very substantial, the quality of music score generated by AI is relatively poor compared with that created by human composers. The music scores created by AI are usually monotonous and devoid of emotion. Based on Birkhoff's aesthetic measure, this paper proposes an objective quantitative evaluation method for homophony music score aesthetic quality assessment. The main contributions of our work are as follows: first, we put forward a homophony music score aesthetic model to objectively evaluate the quality of music score as a baseline model; second, we put forward eight basic music features and four music aesthetic features.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/1901.05811.pdf' target='_blank'>https://arxiv.org/pdf/1901.05811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinay Kumar, Vivek Singh Bawa, Rahul Upadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1901.05811">No reference image quality assessment metric based on regional mutual information among images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the inclusion of camera in daily life, an automatic no reference image quality evaluation index is required for automatic classification of images. The present manuscripts proposes a new No Reference Regional Mutual Information based technique for evaluating the quality of an image. We use regional mutual information on subsets of the complete image. Proposed technique is tested on four benchmark natural image databases, and one benchmark synthetic database. A comparative analysis with classical and state-of-art methods indicate superiority of the present technique for high quality images and comparable for other images of the respective databases.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2510.15579.pdf' target='_blank'>https://arxiv.org/pdf/2510.15579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Soltaninezhad, Yashar Rouzbahani, Jhonatan Contreras, Rohan Chippalkatti, Daniel Kwaku Abankwa, Christian Eggeling, Thomas Bocklitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15579">Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight deep learning models offer substantial reductions in computational cost and environmental impact, making them crucial for scientific applications. We present a lightweight CycleGAN for modality transfer in fluorescence microscopy (confocal to super-resolution STED/deconvolved STED), addressing the common challenge of unpaired datasets. By replacing the traditional channel-doubling strategy in the U-Net-based generator with a fixed channel approach, we drastically reduce trainable parameters from 41.8 million to approximately nine thousand, achieving superior performance with faster training and lower memory usage. We also introduce the GAN as a diagnostic tool for experimental and labeling quality. When trained on high-quality images, the GAN learns the characteristics of optimal imaging; deviations between its generated outputs and new experimental images can reveal issues such as photobleaching, artifacts, or inaccurate labeling. This establishes the model as a practical tool for validating experimental accuracy and image fidelity in microscopy workflows.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2510.10774.pdf' target='_blank'>https://arxiv.org/pdf/2510.10774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Javad Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10774">ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for Text-to-Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Persian Language, despite being spoken by over 100 million people worldwide, remains severely underrepresented in high-quality speech corpora, particularly for text-to-speech (TTS) synthesis applications. Existing Persian speech datasets are typically smaller than their English counterparts, which creates a key limitation for developing Persian speech technologies. We address this gap by introducing ParsVoice, the largest Persian speech corpus designed specifically for TTS applications. We created an automated pipeline that transforms raw audiobook content into TTS-ready data, incorporating components such as a BERT-based sentence completion detector, a binary search boundary optimization method for precise audio-text alignment, and multi-dimensional quality assessment frameworks tailored to Persian. The pipeline processes 2,000 audiobooks, yielding 3,526 hours of clean speech, which was further filtered into a 1,804-hour high-quality subset suitable for TTS, featuring more than 470 speakers. ParsVoice is the largest high-quality Persian speech dataset, offering speaker diversity and audio quality comparable to major English corpora. The complete dataset has been made publicly available to accelerate the development of Persian speech technologies and to serve as a template for other low-resource languages. The ParsVoice dataset is publicly available at ParsVoice (https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice).
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2510.07636.pdf' target='_blank'>https://arxiv.org/pdf/2510.07636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Gupta, Gregoire Phillips, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07636">PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) have recently enabled considerable advances in the realm of image and video quality assessment, but this progress has yet to be fully explored in the domain of 3D assets. We are interested in using these models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where the aim is to automatically evaluate the perceptual quality of a point cloud in absence of a reference. We begin with the observation that different modalities of data - text descriptions, 2D projections, and 3D point cloud views - provide complementary information about point cloud quality. We then construct PIT-QMM, a novel LMM for NR-PCQA that is capable of consuming text, images and point clouds end-to-end to predict quality scores. Extensive experimentation shows that our proposed method outperforms the state-of-the-art by significant margins on popular benchmarks with fewer training iterations. We also demonstrate that our framework enables distortion localization and identification, which paves a new way forward for model explainability and interactivity. Code and datasets are available at https://www.github.com/shngt/pit-qmm.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2509.17374.pdf' target='_blank'>https://arxiv.org/pdf/2509.17374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Yadav, Ta Duc Huy, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17374">Revisiting Vision Language Foundations for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale vision language pre-training has recently shown promise for no-reference image-quality assessment (NR-IQA), yet the relative merits of modern Vision Transformer foundations remain poorly understood. In this work, we present the first systematic evaluation of six prominent pretrained backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an identical lightweight MLP head. Our study uncovers two previously overlooked factors: (1) SigLIP2 consistently achieves strong performance; and (2) the choice of activation function plays a surprisingly crucial role, particularly for enhancing the generalization ability of image quality assessment models. Notably, we find that simple sigmoid activations outperform commonly used ReLU and GELU on several benchmarks. Motivated by this finding, we introduce a learnable activation selection mechanism that adaptively determines the nonlinearity for each channel, eliminating the need for manual activation design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and AGIQA3K. Extensive ablations confirm the benefits across architectures and regimes, establishing strong, resource-efficient NR-IQA baselines.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2509.14023.pdf' target='_blank'>https://arxiv.org/pdf/2509.14023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sami Ul Haq, Sheila Castilho, Yvette Graham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14023">Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine Translation (MT) has achieved remarkable performance, with growing interest in speech translation and multimodal approaches. However, despite these advancements, MT quality assessment remains largely text centric, typically relying on human experts who read and compare texts. Since many real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK Translator) involve translation being spoken rather printed or read, a more natural way to assess translation quality would be through speech as opposed text-only evaluations. This study compares text-only and audio-based evaluations of 10 MT systems from the WMT General MT Shared Task, using crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally, performed statistical significance testing and self-replication experiments to test reliability and consistency of audio-based approach. Crowd-sourced assessments based on audio yield rankings largely consistent with text only evaluations but, in some cases, identify significant differences between translation systems. We attribute this to speech richer, more natural modality and propose incorporating speech-based assessments into future MT evaluation frameworks.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2507.19418.pdf' target='_blank'>https://arxiv.org/pdf/2507.19418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Lou, Yuanpeng He, Rongchao Zhang, Yongzhi Cao, Hanpin Wang, Yu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19418">DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2506.16601.pdf' target='_blank'>https://arxiv.org/pdf/2506.16601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Azeem Aslam, Muhammad Hamza, Nisar Ahmed, Gulshan Saleem, Zhu Shuangtong, Hu Hongfei, Xu Wei, Saba Aslam, Wang Jun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16601">MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) is a critical task in a wide range of applications but remains challenging due to the subjective nature of human perception and the complexity of real-world image distortions. This study proposes MetaQAP, a novel no-reference IQA model designed to address these challenges by leveraging quality-aware pre-training and meta-learning. The model performs three key contributions: pre-training Convolutional Neural Networks (CNNs) on a quality-aware dataset, implementing a quality-aware loss function to optimize predictions, and integrating a meta-learner to form an ensemble model that effectively combines predictions from multiple base models. Experimental evaluations were conducted on three benchmark datasets: LiveCD, KonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional performance with Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD, 0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing IQA methods. Cross-dataset evaluations further demonstrated the generalizability of the model, with PLCC and SROCC scores ranging from 0.6721 to 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The ablation study confirmed the significance of each model component, revealing substantial performance degradation when critical elements such as the meta-learner or quality-aware loss function were omitted. MetaQAP not only addresses the complexities of authentic distortions but also establishes a robust and generalizable framework for practical IQA applications. By advancing the state-of-the-art in no-reference IQA, this research provides valuable insights and methodologies for future improvements and extensions in the field.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2505.16025.pdf' target='_blank'>https://arxiv.org/pdf/2505.16025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wen, Yaohong Wu, Yue Sheng, Neil Birkbeck, Balu Adsumilli, Yilin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16025">CP-LLM: Context and Pixel Aware Large Language Model for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is a challenging research topic with broad applications. Effective VQA necessitates sensitivity to pixel-level distortions and a comprehensive understanding of video context to accurately determine the perceptual impact of distortions. Traditional hand-crafted and learning-based VQA models mainly focus on pixel-level distortions and lack contextual understanding, while recent LLM-based models struggle with sensitivity to small distortions or handle quality scoring and description as separate tasks. To address these shortcomings, we introduce CP-LLM: a Context and Pixel aware Large Language Model. CP-LLM is a novel multimodal LLM architecture featuring dual vision encoders designed to independently analyze perceptual quality at both high-level (video context) and low-level (pixel distortion) granularity, along with a language decoder subsequently reasons about the interplay between these aspects. This design enables CP-LLM to simultaneously produce robust quality scores and interpretable quality descriptions, with enhanced sensitivity to pixel distortions (e.g. compression artifacts). The model is trained via a multi-task pipeline optimizing for score prediction, description generation, and pairwise comparisons. Experiment results demonstrate that CP-LLM achieves state-of-the-art cross-dataset performance on established VQA benchmarks and superior robustness to pixel distortions, confirming its efficacy for comprehensive and practical video quality assessment in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2505.15631.pdf' target='_blank'>https://arxiv.org/pdf/2505.15631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nick Kocher, Christian Wassermann, Leona Hennig, Jonas Seng, Holger Hoos, Kristian Kersting, Marius Lindauer, Matthias MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15631">Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Architecture Search (NAS) accelerates progress in deep learning through systematic refinement of model architectures. The downside is increasingly large energy consumption during the search process. Surrogate-based benchmarking mitigates the cost of full training by querying a pre-trained surrogate to obtain an estimate for the quality of the model. Specifically, energy-aware benchmarking aims to make it possible for NAS to favourably trade off model energy consumption against accuracy. Towards this end, we propose three design principles for such energy-aware benchmarks: (i) reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic cost reporting. We analyse EA-HAS-Bench based on these principles and find that the choice of GPU measurement API has a large impact on the quality of results. Using the Nvidia System Management Interface (SMI) on top of its underlying library influences the sampling rate during the initial data collection, returning faulty low-power estimations. This results in poor correlation with accurate measurements obtained from an external power meter. With this study, we bring to attention several key considerations when performing energy-aware surrogate-based benchmarking and derive first guidelines that can help design novel benchmarks. We show a narrow usage range of the four GPUs attached to our device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down even further when using all four GPUs. To improve holistic energy reporting, we propose calibration experiments over assumptions made in popular tools, such as Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to 8.9 % without and to 6.6 % with prior estimation of the expected load on the device.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2505.07372.pdf' target='_blank'>https://arxiv.org/pdf/2505.07372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07372">Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2504.13233.pdf' target='_blank'>https://arxiv.org/pdf/2504.13233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Rafiei, Gari D. Clifford, Nasim Katebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13233">Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fetal health monitoring through one-dimensional Doppler ultrasound (DUS) signals offers a cost-effective and accessible approach that is increasingly gaining interest. Despite its potential, the development of machine learning based techniques to assess the health condition of mothers and fetuses using DUS signals remains limited. This scarcity is primarily due to the lack of extensive DUS datasets with a reliable reference for interpretation and data imbalance across different gestational ages. In response, we introduce a novel autoregressive generative model designed to map fetal electrocardiogram (FECG) signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural temporal network based on dilated causal convolutions that operate directly on the waveform level, the model effectively captures both short and long-range dependencies within the signals, preserving the integrity of generated data. Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional generative architectures across both time and frequency domain evaluations, producing DUS signals that closely resemble the morphology of their real counterparts. The realism of these synthesized signals was further gauged using a quality assessment model, which classified all as good quality, and a heart rate estimation model, which produced comparable results for generated and real data, with a Bland-Altman limit of 4.5 beats per minute. This advancement offers a promising solution for mitigating limited data availability and enhancing the training of DUS-based fetal models, making them more effective and generalizable.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2503.09394.pdf' target='_blank'>https://arxiv.org/pdf/2503.09394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhen Qiao, Peng Huang, Jiakang Yuan, Xianda Guo, Bowen Ye, Chaocan Xue, Ye Zheng, Zhe Sun, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09394">Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time adaptation (TTA) is crucial in maintaining performance of Vision Language Models (VLMs) when facing distribution shifts, particularly when the source data or target labels are inaccessible. Existing TTA methods predominantly leverage the output probability distribution of CLIP for feature evaluation, resulting in biases under domain shifts, which cause misclassified features due to text priors or incorrect textual associations. To address these issues, we propose \underline{B}idirectional Prototype-Reward co-Evolution (BPRE), a novel VLMs framework with TTA that integrates feature quality assessment with prototype evolution via a synergistic feedback loop. First, the Multi-dimensional Quality-aware Reward Module (MQRM) is designed to evaluate feature quality and guide prototype refinement precisely. The continuous refinement of prototype quality via Prototype-Reward Interactive Evolution (PRIE) enhances the computation more robust. Through this bidirectional interaction, the precision of rewards and prototype evolution mutually reinforce each other, forming a self-evolving feedback cycle. Extensive experiments conducted on 15 diverse recognition datasets demonstrate that our model consistently achieves superior performance compared to other SOTA methods, and advances VLM generalization capabilities through emphasizing comprehensive feature evaluation.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2501.12536.pdf' target='_blank'>https://arxiv.org/pdf/2501.12536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Li, Zhipeng Bao, Haoming Meng, Haotian Shi, Qianwen Li, Handong Yao, Xiaopeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12536">Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the development of a comprehensive dataset capturing interactions between Autonomous Vehicles (AVs) and traffic control devices, specifically traffic lights and stop signs. Derived from the Waymo Motion dataset, our work addresses a critical gap in the existing literature by providing real-world trajectory data on how AVs navigate these traffic control devices. We propose a methodology for identifying and extracting relevant interaction trajectory data from the Waymo Motion dataset, incorporating over 37,000 instances with traffic lights and 44,000 with stop signs. Our methodology includes defining rules to identify various interaction types, extracting trajectory data, and applying a wavelet-based denoising method to smooth the acceleration and speed profiles and eliminate anomalous values, thereby enhancing the trajectory quality. Quality assessment metrics indicate that trajectories obtained in this study have anomaly proportions in acceleration and jerk profiles reduced to near-zero levels across all interaction categories. By making this dataset publicly available, we aim to address the current gap in datasets containing AV interaction behaviors with traffic lights and signs. Based on the organized and published dataset, we can gain a more in-depth understanding of AVs' behavior when interacting with traffic lights and signs. This will facilitate research on AV integration into existing transportation infrastructures and networks, supporting the development of more accurate behavioral models and simulation tools.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2412.18060.pdf' target='_blank'>https://arxiv.org/pdf/2412.18060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Wen, Yilin Wang, Neil Birkbeck, Balu Adsumilli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18060">An Ensemble Approach to Short-form Video Quality Assessment Using Multimodal LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of short-form videos, characterized by diverse content, editing styles, and artifacts, poses substantial challenges for learning-based blind video quality assessment (BVQA) models. Multimodal large language models (MLLMs), renowned for their superior generalization capabilities, present a promising solution. This paper focuses on effectively leveraging a pretrained MLLM for short-form video quality assessment, regarding the impacts of pre-processing and response variability, and insights on combining the MLLM with BVQA models. We first investigated how frame pre-processing and sampling techniques influence the MLLM's performance. Then, we introduced a lightweight learning-based ensemble method that adaptively integrates predictions from the MLLM and state-of-the-art BVQA models. Our results demonstrated superior generalization performance with the proposed ensemble approach. Furthermore, the analysis of content-aware ensemble weights highlighted that some video characteristics are not fully represented by existing BVQA models, revealing potential directions to improve BVQA models further.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2411.10183.pdf' target='_blank'>https://arxiv.org/pdf/2411.10183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mizuki Miyamoto, Ryugo Morita, Jinjia Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10183">Visual question answering based evaluation metrics for text-to-image generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image generation and text-guided image manipulation have received considerable attention in the field of image generation tasks. However, the mainstream evaluation methods for these tasks have difficulty in evaluating whether all the information from the input text is accurately reflected in the generated images, and they mainly focus on evaluating the overall alignment between the input text and the generated images. This paper proposes new evaluation metrics that assess the alignment between input text and generated images for every individual object. Firstly, according to the input text, chatGPT is utilized to produce questions for the generated images. After that, we use Visual Question Answering(VQA) to measure the relevance of the generated images to the input text, which allows for a more detailed evaluation of the alignment compared to existing methods. In addition, we use Non-Reference Image Quality Assessment(NR-IQA) to evaluate not only the text-image alignment but also the quality of the generated images. Experimental results show that our proposed evaluation approach is the superior metric that can simultaneously assess finer text-image alignment and image quality while allowing for the adjustment of these ratios.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2406.05305.pdf' target='_blank'>https://arxiv.org/pdf/2406.05305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05305">YouTube SFV+HDR Quality Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The popularity of Short form videos (SFV) has grown dramatically in the past few years, and has become a phenomenal video category with billions of viewers. Meanwhile, High Dynamic Range (HDR) as an advanced feature also becomes more and more popular on video sharing platforms. As a hot topic with huge impact, SFV and HDR bring new questions to video quality research: 1) is SFV+HDR quality assessment significantly different from traditional User Generated Content (UGC) quality assessment? 2) do objective quality metrics designed for traditional UGC still work well for SFV+HDR? To answer the above questions, we created the first large scale SFV+HDR dataset with reliable subjective quality scores, covering 10 popular content categories. Further, we also introduce a general sampling framework to maximize the representativeness of the dataset. We provided a comprehensive analysis of subjective quality scores for Short form SDR and HDR videos, and discuss the reliability of state-of-the-art UGC quality metrics and potential improvements.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2405.20392.pdf' target='_blank'>https://arxiv.org/pdf/2405.20392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Egor Kashkarov, Egor Chistov, Ivan Molodetskikh, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20392">Can No-Reference Quality-Assessment Methods Serve as Perceptual Losses for Super-Resolution?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual losses play an important role in constructing deep-neural-network-based methods by increasing the naturalness and realism of processed images and videos. Use of perceptual losses is often limited to LPIPS, a fullreference method. Even though deep no-reference image-qualityassessment methods are excellent at predicting human judgment, little research has examined their incorporation in loss functions. This paper investigates direct optimization of several video-superresolution models using no-reference image-quality-assessment methods as perceptual losses. Our experimental results show that straightforward optimization of these methods produce artifacts, but a special training procedure can mitigate them.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2404.17170.pdf' target='_blank'>https://arxiv.org/pdf/2404.17170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghua Liao, Chen Hui, Lang Yuan, Haiqi Zhu, Feng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17170">Image Quality Assessment With Compressed Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) aims at estimating image quality in accordance with subjective human perception. However, most methods focus on exploring increasingly complex networks to improve the final performance,accompanied by limitations on input images. Especially when applied to high-resolution (HR) images, these methods offen have to adjust the size of original image to meet model input.To further alleviate the aforementioned issue, we propose two networks for NR-IQA with Compressive Sampling (dubbed CL-IQA and CS-IQA). They consist of four components: (1) The Compressed Sampling Module (CSM) to sample the image (2)The Adaptive Embedding Module (AEM). The measurements are embedded by AEM to extract high-level features. (3) The Vision Transformer and Scale Swin TranBlocksformer Moudle(SSTM) to extract deep features. (4) The Dual Branch (DB) to get final quality score. Experiments show that our proposed methods outperform other methods on various datasets with less data usage.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2402.09541.pdf' target='_blank'>https://arxiv.org/pdf/2402.09541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huynh Khanh Vi Tran, Michael Unterkalmsteiner, JÃ¼rgen BÃ¶rstler, Nauman bin Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09541">Assessing test artifact quality -- A tertiary study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context: Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases. Objective: We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives. Method: We have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts. Results: We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and ISO/IEC 25010:2011. Conclusion: The test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furtherm Information and Software Technology 139 (2021): 106620ore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2401.14736.pdf' target='_blank'>https://arxiv.org/pdf/2401.14736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Birchler, Tanzil Kombarabettu Mohammed, Pooja Rani, Teodora Nechita, Timo Kehrer, Sebastiano Panichella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14736">How does Simulation-based Testing for Self-driving Cars match Human Perception?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software metrics such as coverage and mutation scores have been extensively explored for the automated quality assessment of test suites. While traditional tools rely on such quantifiable software metrics, the field of self-driving cars (SDCs) has primarily focused on simulation-based test case generation using quality metrics such as the out-of-bound (OOB) parameter to determine if a test case fails or passes. However, it remains unclear to what extent this quality metric aligns with the human perception of the safety and realism of SDCs, which are critical aspects in assessing SDC behavior. To address this gap, we conducted an empirical study involving 50 participants to investigate the factors that determine how humans perceive SDC test cases as safe, unsafe, realistic, or unrealistic. To this aim, we developed a framework leveraging virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study participants into the virtual environment of SDC simulators. Our findings indicate that the human assessment of the safety and realism of failing and passing test cases can vary based on different factors, such as the test's complexity and the possibility of interacting with the SDC. Especially for the assessment of realism, the participants' age as a confounding factor leads to a different perception. This study highlights the need for more research on SDC simulation testing quality metrics and the importance of human perception in evaluating SDC behavior.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2401.13716.pdf' target='_blank'>https://arxiv.org/pdf/2401.13716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vibeke Binz Vallevik, Aleksandar Babic, Serena Elizabeth Marshall, Severin Elvatun, Helga BrÃ¸gger, Sharmini Alagaratnam, BjÃ¸rn Edwin, Narasimha Raghavan Veeraragavan, Anne Kjersti Befring, Jan Franz NygÃ¥rd
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13716">Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Registry. We present a conceptual framework for quality assurance of SD for AI applications in healthcare that aligns diverging taxonomies, expands on common quality dimensions to include the dimensions of Fairness and Carbon footprint, and proposes stages necessary to support real-life applications. Building trust in synthetic data by increasing transparency and reducing the safety risk will accelerate the development and uptake of trustworthy AI tools for the benefit of patients. Despite the growing emphasis on algorithmic fairness and carbon footprint, these metrics were scarce in the literature review. The overwhelming focus was on statistical similarity using distance metrics while sequential logic detection was scarce. A consensus-backed framework that includes all relevant quality dimensions can provide assurance for safe and responsible real-life applications of SD.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2312.04334.pdf' target='_blank'>https://arxiv.org/pdf/2312.04334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, Jean-FranÃ§ois Lalonde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04334">Towards a Perceptual Evaluation Framework for Lighting Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in lighting estimation is tracked by computing existing image quality assessment (IQA) metrics on images from standard datasets. While this may appear to be a reasonable approach, we demonstrate that doing so does not correlate to human preference when the estimated lighting is used to relight a virtual scene into a real photograph. To study this, we design a controlled psychophysical experiment where human observers must choose their preference amongst rendered scenes lit using a set of lighting estimation algorithms selected from the recent literature, and use it to analyse how these algorithms perform according to human perception. Then, we demonstrate that none of the most popular IQA metrics from the literature, taken individually, correctly represent human perception. Finally, we show that by learning a combination of existing IQA metrics, we can more accurately represent human preference. This provides a new perceptual framework to help evaluate future lighting estimation algorithms.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2310.03118.pdf' target='_blank'>https://arxiv.org/pdf/2310.03118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03118">Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lowering radiation dose per view and utilizing sparse views per scan are two common CT scan modes, albeit often leading to distorted images characterized by noise and streak artifacts. Blind image quality assessment (BIQA) strives to evaluate perceptual quality in alignment with what radiologists perceive, which plays an important role in advancing low-dose CT reconstruction techniques. An intriguing direction involves developing BIQA methods that mimic the operational characteristic of the human visual system (HVS). The internal generative mechanism (IGM) theory reveals that the HVS actively deduces primary content to enhance comprehension. In this study, we introduce an innovative BIQA metric that emulates the active inference process of IGM. Initially, an active inference module, implemented as a denoising diffusion probabilistic model (DDPM), is constructed to anticipate the primary content. Then, the dissimilarity map is derived by assessing the interrelation between the distorted image and its primary content. Subsequently, the distorted image and dissimilarity map are combined into a multi-channel image, which is inputted into a transformer-based image quality evaluator. Remarkably, by exclusively utilizing this transformer-based quality evaluator, we won the second place in the MICCAI 2023 low-dose computed tomography perceptual image quality assessment grand challenge. Leveraging the DDPM-derived primary content, our approach further improves the performance on the challenge dataset.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2308.06055.pdf' target='_blank'>https://arxiv.org/pdf/2308.06055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrupiÅski, Maciej Wielgosz, Szymon Mazurek, Krystian StrzaÅka, PaweÅ Russek, Jakub Caputa, Daria Åukasik, Jakub Grzeszczyk, MichaÅ Karwatowski, RafaÅ Fraczek, Ernest Jamro, Marcin PietroÅ, Sebastian Koryciak, Agnieszka DÄbrowska-Boruch, Kazimierz Wiatr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06055">Computer-Aided Cytology Diagnosis in Animals: CNN-Based Image Quality Assessment for Accurate Disease Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a computer-aided cytology diagnosis system designed for animals, focusing on image quality assessment (IQA) using Convolutional Neural Networks (CNNs). The system's building blocks are tailored to seamlessly integrate IQA, ensuring reliable performance in disease classification. We extensively investigate the CNN's ability to handle various image variations and scenarios, analyzing the impact on detecting low-quality input data. Additionally, the network's capacity to differentiate valid cellular samples from those with artifacts is evaluated. Our study employs a ResNet18 network architecture and explores the effects of input sizes and cropping strategies on model performance. The research sheds light on the significance of CNN-based IQA in computer-aided cytology diagnosis for animals, enhancing the accuracy of disease classification.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2307.12183.pdf' target='_blank'>https://arxiv.org/pdf/2307.12183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Freire-ObregÃ³n, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel HernÃ¡ndez-Sosa, Modesto CastrillÃ³n-Santana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12183">An X3D Neural Network Analysis for Runner's Performance Assessment in a Wild Sporting Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a transfer learning analysis on a sporting environment of the expanded 3D (X3D) neural networks. Inspired by action quality assessment methods in the literature, our method uses an action recognition network to estimate athletes' cumulative race time (CRT) during an ultra-distance competition. We evaluate the performance considering the X3D, a family of action recognition networks that expand a small 2D image classification architecture along multiple network axes, including space, time, width, and depth. We demonstrate that the resulting neural network can provide remarkable performance for short input footage, with a mean absolute error of 12 minutes and a half when estimating the CRT for runners who have been active from 8 to 20 hours. Our most significant discovery is that X3D achieves state-of-the-art performance while requiring almost seven times less memory to achieve better precision than previous work.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2306.15392.pdf' target='_blank'>https://arxiv.org/pdf/2306.15392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Szymon Mazurek, Maciej Wielgosz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15392">Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we delve into the critical aspect of dataset quality assessment in machine learning classification tasks. Leveraging a variety of nine distinct datasets, each crafted for classification tasks with varying complexity levels, we illustrate the profound impact of dataset quality on model training and performance. We further introduce two additional datasets designed to represent specific data conditions - one maximizing entropy and the other demonstrating high redundancy. Our findings underscore the importance of appropriate feature selection, adequate data volume, and data quality in achieving high-performing machine learning models. To aid researchers and practitioners, we propose a comprehensive framework for dataset quality assessment, which can help evaluate if the dataset at hand is sufficient and of the required quality for specific tasks. This research offers valuable insights into data assessment practices, contributing to the development of more accurate and robust machine learning models.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2305.18079.pdf' target='_blank'>https://arxiv.org/pdf/2305.18079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18079">Towards a Robust Framework for NeRF Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Field (NeRF) research has attracted significant attention recently, with 3D modelling, virtual/augmented reality, and visual effects driving its application. While current NeRF implementations can produce high quality visual results, there is a conspicuous lack of reliable methods for evaluating them. Conventional image quality assessment methods and analytical metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of performance since they generalise the ability of the entire NeRF pipeline. Hence, in this paper, we propose a new test framework which isolates the neural rendering network from the NeRF pipeline and then performs a parametric evaluation by training and evaluating the NeRF on an explicit radiance field representation. We also introduce a configurable approach for generating representations specifically for evaluation purposes. This employs ray-casting to transform mesh models into explicit NeRF samples, as well as to "shade" these representations. Combining these two approaches, we demonstrate how different "tasks" (scenes with different visual effects or learning strategies) and types of networks (NeRFs and depth-wise implicit neural representations (INRs)) can be evaluated within this framework. Additionally, we propose a novel metric to measure task complexity of the framework which accounts for the visual parameters and the distribution of the spatial data. Our approach offers the potential to create a comparative objective evaluation framework for NeRF methods.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2305.15907.pdf' target='_blank'>https://arxiv.org/pdf/2305.15907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Luo, Bin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15907">Double Descent of Discrepancy: A Task-, Data-, and Model-Agnostic Phenomenon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we studied two identically-trained neural networks (i.e. networks with the same architecture, trained on the same dataset using the same algorithm, but with different initialization) and found that their outputs discrepancy on the training dataset exhibits a "double descent" phenomenon. We demonstrated through extensive experiments across various tasks, datasets, and network architectures that this phenomenon is prevalent. Leveraging this phenomenon, we proposed a new early stopping criterion and developed a new method for data quality assessment. Our results show that a phenomenon-driven approach can benefit deep learning research both in theoretical understanding and practical applications.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2305.14856.pdf' target='_blank'>https://arxiv.org/pdf/2305.14856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Å½iga Babnik, Naser Damer, Vitomir Å truc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14856">Optimization-Based Improvement of Face Image Quality Assessment Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary face recognition (FR) models achieve near-ideal recognition performance in constrained settings, yet do not fully translate the performance to unconstrained (realworld) scenarios. To help improve the performance and stability of FR systems in such unconstrained settings, face image quality assessment (FIQA) techniques try to infer sample-quality information from the input face images that can aid with the recognition process. While existing FIQA techniques are able to efficiently capture the differences between high and low quality images, they typically cannot fully distinguish between images of similar quality, leading to lower performance in many scenarios. To address this issue, we present in this paper a supervised quality-label optimization approach, aimed at improving the performance of existing FIQA techniques. The developed optimization procedure infuses additional information (computed with a selected FR model) into the initial quality scores generated with a given FIQA technique to produce better estimates of the "actual" image quality. We evaluate the proposed approach in comprehensive experiments with six state-of-the-art FIQA approaches (CR-FIQA, FaceQAN, SER-FIQ, PCNet, MagFace, SDD-FIQA) on five commonly used benchmarks (LFW, CFPFP, CPLFW, CALFW, XQLFW) using three targeted FR models (ArcFace, ElasticFace, CurricularFace) with highly encouraging results.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2305.12280.pdf' target='_blank'>https://arxiv.org/pdf/2305.12280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshan Deshpande, Zhivar Sourati, Filip Ilievski, Fred Morstatter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12280">Contextualizing Argument Quality Assessment with Relevant Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic assessment of the quality of arguments has been recognized as a challenging task with significant implications for misinformation and targeted speech. While real-world arguments are tightly anchored in context, existing computational methods analyze their quality in isolation, which affects their accuracy and generalizability. We propose SPARK: a novel method for scoring argument quality based on contextualization via relevant knowledge. We devise four augmentations that leverage large language models to provide feedback, infer hidden assumptions, supply a similar-quality argument, or give a counter-argument. SPARK uses a dual-encoder Transformer architecture to enable the original argument and its augmentation to be considered jointly. Our experiments in both in-domain and zero-shot setups show that SPARK consistently outperforms existing techniques across multiple metrics.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2304.10066.pdf' target='_blank'>https://arxiv.org/pdf/2304.10066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacky Chen Long Chai, Tiong-Sik Ng, Cheng-Yaw Low, Jaewoo Park, Andrew Beng Jin Teoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10066">Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Very low-resolution face recognition (VLRFR) poses unique challenges, such as tiny regions of interest and poor resolution due to extreme standoff distance or wide viewing angle of the acquisition devices. In this paper, we study principled approaches to elevate the recognizability of a face in the embedding space instead of the visual quality. We first formulate a robust learning-based face recognizability measure, namely recognizability index (RI), based on two criteria: (i) proximity of each face embedding against the unrecognizable faces cluster center and (ii) closeness of each face embedding against its positive and negative class prototypes. We then devise an index diversion loss to push the hard-to-recognize face embedding with low RI away from unrecognizable faces cluster to boost the RI, which reflects better recognizability. Additionally, a perceptibility attention mechanism is introduced to attend to the most recognizable face regions, which offers better explanatory and discriminative traits for embedding learning. Our proposed model is trained end-to-end and simultaneously serves recognizability-aware embedding learning and face quality estimation. To address VLRFR, our extensive evaluations on three challenging low-resolution datasets and face quality assessment demonstrate the superiority of the proposed model over the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2511.13533.pdf' target='_blank'>https://arxiv.org/pdf/2511.13533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeffrey Wen, Rizwan Ahmad, Philip Schniter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13533">Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2510.17932.pdf' target='_blank'>https://arxiv.org/pdf/2510.17932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Tang, Henry Hengyuan Zhao, Lijian Wu, Yifei Tao, Dongxing Mao, Yang Wan, Jingru Tan, Min Zeng, Min Li, Alex Jinpeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17932">From Charts to Code: A Hierarchical Benchmark for Multimodal Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2510.17332.pdf' target='_blank'>https://arxiv.org/pdf/2510.17332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoran Zhao, Xinli Yue, Jianhui Sun, Yuhao Xie, Tao Shao, Liangchao Yao, Fan Xia, Yuetang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17332">iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) has progressed from scalar quality prediction to more interpretable, human-aligned evaluation paradigms. In this work, we address the emerging challenge of detailed and explainable IQA by proposing iDETEX-a unified multimodal large language model (MLLM) capable of simultaneously performing three key tasks: quality grounding, perception, and description. To facilitate efficient and generalizable training across these heterogeneous subtasks, we design a suite of task-specific offline augmentation modules and a data mixing strategy. These are further complemented by online enhancement strategies to fully exploit multi-sourced supervision. We validate our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves state-of-the-art performance across all subtasks. Our model ranks first in the ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its effectiveness and robustness in delivering accurate and interpretable quality assessments.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2509.18546.pdf' target='_blank'>https://arxiv.org/pdf/2509.18546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Liu, Dingquan Li, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18546">SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) models play an important role in various real-world applications. Recently, adversarial attacks against NR-IQA models have attracted increasing attention, as they provide valuable insights for revealing model vulnerabilities and guiding robust system design. Some effective attacks have been proposed against NR-IQA models in white-box settings, where the attacker has full access to the target model. However, these attacks often suffer from poor transferability to unknown target models in more realistic black-box scenarios, where the target model is inaccessible. This work makes the first attempt to address the challenge of low transferability in attacking NR-IQA models by proposing a transferable Signed Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the gradient of the target model by applying Gaussian smoothing to source models and ensembling their smoothed gradients. To ensure the imperceptibility of adversarial perturbations, SEGA further removes inappropriate perturbations using a specially designed perturbation filter mask. Experimental results on the CLIVE dataset demonstrate the superior transferability of SEGA, validating its effectiveness in enabling successful transfer-based black-box attacks against NR-IQA models.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2508.09956.pdf' target='_blank'>https://arxiv.org/pdf/2508.09956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fares Antaki, David Mikhail, Daniel Milad, Danny A Mammo, Sumit Sharma, Sunil K Srivastava, Bing Yu Chen, Samir Touma, Mertcan Sevgi, Jonathan El-Khoury, Pearse A Keane, Qingyu Chen, Yih Chung Tham, Renaud Duval
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09956">Performance of GPT-5 Frontier Models in Ophthalmology Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) such as GPT-5 integrate advanced reasoning capabilities that may improve performance on complex medical question-answering tasks. For this latest generation of reasoning models, the configurations that maximize both accuracy and cost-efficiency have yet to be established. We evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using 260 closed-access multiple-choice questions from the American Academy of Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome was multiple-choice accuracy; secondary outcomes included head-to-head ranking via a Bradley-Terry model, rationale quality assessment using a reference-anchored, pairwise LLM-as-a-judge framework, and analysis of accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high (0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x stronger than o3-high) and rationale quality (1.11x stronger than o3-high). Cost-accuracy analysis identified several GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the most favorable low-cost, high-performance balance. These results benchmark GPT-5 on a high-quality ophthalmology dataset, demonstrate the influence of reasoning effort on accuracy, and introduce an autograder framework for scalable evaluation of LLM-generated answers against reference standards in ophthalmology.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2506.08686.pdf' target='_blank'>https://arxiv.org/pdf/2506.08686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Poddar, Paramita Koley, Janardan Misra, Sanjay Podder, Navveen Balani, Niloy Ganguly, Saptarshi Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08686">Brevity is the soul of sustainability: Characterizing LLM response lengths</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\% by reducing the response length while preserving the quality of LLM responses.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2504.11733.pdf' target='_blank'>https://arxiv.org/pdf/2504.11733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu, Situo Wang, Wei Zhou, Moncef Gabbouj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11733">DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the dual-stream theory of the human visual system (HVS) - where the ventral stream is responsible for object recognition and detail analysis, while the dorsal stream focuses on spatial relationships and motion perception - an increasing number of video quality assessment (VQA) works built upon this framework are proposed. Recent advancements in large multi-modal models, notably Contrastive Language-Image Pretraining (CLIP), have motivated researchers to incorporate CLIP into dual-stream-based VQA methods. This integration aims to harness the model's superior semantic understanding capabilities to replicate the object recognition and detail analysis in ventral stream, as well as spatial relationship analysis in dorsal stream. However, CLIP is originally designed for images and lacks the ability to capture temporal and motion information inherent in videos. To address the limitation, this paper propose a Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples CLIP's visual and textual components, and integrates them into different stages of the NR-VQA pipeline. Specifically, a Video-Based Temporal CLIP module is proposed to explicitly model temporal dynamics and enhance motion perception, aligning with the dorsal stream. Additionally, a Temporal Context Module is developed to refine inter-frame dependencies, further improving motion modeling. On the ventral stream side, a Basic Visual Feature Extraction Module is employed to strengthen detail analysis. Finally, a text-guided adaptive fusion strategy is proposed to enable dynamic weighting of features, facilitating more effective integration of spatial and temporal information.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2503.02797.pdf' target='_blank'>https://arxiv.org/pdf/2503.02797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Drenkow, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02797">A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality plays an important role in the performance of deep neural networks (DNNs) that have been widely shown to exhibit sensitivity to changes in imaging conditions. Conventional image quality assessment (IQA) seeks to measure and align quality relative to human perceptual judgments, but we often need a metric that is not only sensitive to imaging conditions but also well-aligned with DNN sensitivities. We first ask whether conventional IQA metrics are also informative of DNN performance. We show theoretically and empirically that conventional IQA metrics are weak predictors of DNN performance for image classification. Using our causal framework, we then develop metrics that exhibit strong correlation with DNN performance, thus enabling us to effectively estimate the quality distribution of large image datasets relative to targeted vision tasks.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2502.13990.pdf' target='_blank'>https://arxiv.org/pdf/2502.13990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiying Shi, Zhihong Tan, Zhihan Zhang, Hongchen Wei, Yaosi Hu, Yingxue Zhang, Zhenzhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13990">Remote Sensing Semantic Segmentation Quality Assessment based on Vision Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complexity of scenes and variations in image quality result in significant variability in the performance of semantic segmentation methods of remote sensing imagery (RSI) in supervised real-world scenarios. This makes the evaluation of semantic segmentation quality in such scenarios an issue to be resolved. However, most of the existing evaluation metrics are developed based on expert-labeled object-level annotations, which are not applicable in such scenarios. To address this issue, we propose RS-SQA, an unsupervised quality assessment model for RSI semantic segmentation based on vision language model (VLM). This framework leverages a pre-trained RS VLM for semantic understanding and utilizes intermediate features from segmentation methods to extract implicit information about segmentation quality. Specifically, we introduce CLIP-RS, a large-scale pre-trained VLM trained with purified text to reduce textual noise and capture robust semantic information in the RS domain. Feature visualizations confirm that CLIP-RS can effectively differentiate between various levels of segmentation quality. Semantic features and low-level segmentation features are effectively integrated through a semantic-guided approach to enhance evaluation accuracy. To further support the development of RS semantic segmentation quality assessment, we present RS-SQED, a dedicated dataset sampled from four major RS semantic segmentation datasets and annotated with segmentation accuracy derived from the inference results of 8 representative segmentation methods. Experimental results on the established dataset demonstrate that RS-SQA significantly outperforms state-of-the-art quality assessment models. This provides essential support for predicting segmentation accuracy and high-quality semantic segmentation interpretation, offering substantial practical value.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2412.18160.pdf' target='_blank'>https://arxiv.org/pdf/2412.18160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuting Lan, Mingliang Zhou, Jielu Yan, Xuekai Wei, Yueting Huang, Zhaowei Shang, Huayan Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18160">Image Quality Assessment: Exploring Regional Heterogeneity via Response of Adaptive Multiple Quality Factors in Dictionary Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given that the factors influencing image quality vary significantly with scene, content, and distortion type, particularly in the context of regional heterogeneity, we propose an adaptive multi-quality factor (AMqF) framework to represent image quality in a dictionary space, enabling the precise capture of quality features in non-uniformly distorted regions. By designing an adapter, the framework can flexibly decompose quality factors (such as brightness, structure, contrast, etc.) that best align with human visual perception and quantify them into discrete visual words. These visual words respond to the constructed dictionary basis vector, and by obtaining the corresponding coordinate vectors, we can measure visual similarity. Our method offers two key contributions. First, an adaptive mechanism that extracts and decomposes quality factors according to human visual perception principles enhances their representation ability through reconstruction constraints. Second, the construction of a comprehensive and discriminative dictionary space and basis vector allows quality factors to respond effectively to the dictionary basis vector and capture non-uniform distortion patterns in images, significantly improving the accuracy of visual similarity measurement. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches in handling various types of distorted images. The source code is available at https://anonymous.4open.science/r/AMqF-44B2.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2412.16939.pdf' target='_blank'>https://arxiv.org/pdf/2412.16939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Shen, Mingliang Zhou, Yu Chen, Xuekai Wei, Jun Luo, Huayan Pu, Weijia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16939">Image Quality Assessment: Investigating Causal Perceptual Effects with Abductive Counterfactual Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing full-reference image quality assessment (FR-IQA) methods often fail to capture the complex causal mechanisms that underlie human perceptual responses to image distortions, limiting their ability to generalize across diverse scenarios. In this paper, we propose an FR-IQA method based on abductive counterfactual inference to investigate the causal relationships between deep network features and perceptual distortions. First, we explore the causal effects of deep features on perception and integrate causal reasoning with feature comparison, constructing a model that effectively handles complex distortion types across different IQA scenarios. Second, the analysis of the perceptual causal correlations of our proposed method is independent of the backbone architecture and thus can be applied to a variety of deep networks. Through abductive counterfactual experiments, we validate the proposed causal relationships, confirming the model's superior perceptual relevance and interpretability of quality scores. The experimental results demonstrate the robustness and effectiveness of the method, providing competitive quality predictions across multiple benchmarks. The source code is available at https://anonymous.4open.science/r/DeepCausalQuality-25BC.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2411.17390.pdf' target='_blank'>https://arxiv.org/pdf/2411.17390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtong Yue, Xin Lin, Zijiu Yang, Chao Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17390">Dual-Representation Interaction Driven Image Quality Assessment with Restoration Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment for distorted images has always been a challenging problem due to image content variance and distortion diversity. Previous IQA models mostly encode explicit single-quality features of synthetic images to obtain quality-aware representations for quality score prediction. However, performance decreases when facing real-world distortion and restored images from restoration models. The reason is that they do not consider the degradation factors of the low-quality images adequately. To address this issue, we first introduce the DRI method to obtain degradation vectors and quality vectors of images, which separately model the degradation and quality information of low-quality images. After that, we add the restoration network to provide the MOS score predictor with degradation information. Then, we design the Representation-based Semantic Loss (RS Loss) to assist in enhancing effective interaction between representations. Extensive experimental results demonstrate that the proposed method performs favorably against existing state-of-the-art models on both synthetic and real-world datasets.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2411.16885.pdf' target='_blank'>https://arxiv.org/pdf/2411.16885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Falah Jabar, Lill-Tove Rasmussen Busund, Biagio Ricciuti, Masoud Tafavvoghi, Mette PÃ¸hl, Sigve Andersen, Tom Donnem, David J. Kwiatkowski, Mehrdad Rakaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16885">Fully Automatic Content-Aware Tiling Pipeline for Pathology Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the use of deep learning (DL) methods, including convolutional neural networks (CNNs) and vision transformers (ViTs), has significantly advanced computational pathology, enhancing both diagnostic accuracy and efficiency. Hematoxylin and Eosin (H&E) Whole Slide Images (WSI) plays a crucial role by providing detailed tissue samples for the analysis and training of DL models. However, WSIs often contain regions with artifacts such as tissue folds, blurring, as well as non-tissue regions (background), which can negatively impact DL model performance. These artifacts are diagnostically irrelevant and can lead to inaccurate results. This paper proposes a fully automatic supervised DL pipeline for WSI Quality Assessment (WSI-QA) that uses a fused model combining CNNs and ViTs to detect and exclude WSI regions with artifacts, ensuring that only qualified WSI regions are used to build DL-based computational pathology applications. The proposed pipeline employs a pixel-based segmentation model to classify WSI regions as either qualified or non-qualified based on the presence of artifacts. The proposed model was trained on a large and diverse dataset and validated with internal and external data from various human organs, scanners, and H&E staining procedures. Quantitative and qualitative evaluations demonstrate the superiority of the proposed model, which outperforms state-of-the-art methods in WSI artifact detection. The proposed model consistently achieved over 95% accuracy, precision, recall, and F1 score across all artifact types. Furthermore, the WSI-QA pipeline shows strong generalization across different tissue types and scanning conditions.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2411.01271.pdf' target='_blank'>https://arxiv.org/pdf/2411.01271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adit Jain, Vikram Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01271">Interacting Large Language Model Agents. Interpretable Models and Social Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper discusses the theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making involving interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under 2 settings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives. We demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like LLaMA and closed-source models like ChatGPT. The main takeaway of this paper, based on empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2410.24055.pdf' target='_blank'>https://arxiv.org/pdf/2410.24055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lokendra Poudel, Sushant Jha, Ryan Meeker, Duy-Nhat Phan, Rahul Bhowmik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24055">Advanced Predictive Quality Assessment for Ultrasonic Additive Manufacturing with Deep Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond similar or dissimilar metal foils to a substrate, resulting in solid, consolidated metal components. However, certain processing conditions can lead to inter-layer defects, affecting the final product's quality. This study develops a method to monitor in-process quality using deep learning-based convolutional neural networks (CNNs). The CNN models were evaluated on their ability to classify samples with and without embedded thermocouples across five power levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with supervised labeling. Four distinct CNN classification models were created for different scenarios including without (baseline) and with thermocouples, only without thermocouples across power levels, only with thermocouples across power levels, and combined without and with thermocouples across power levels. The models achieved 98.29% accuracy on combined baseline and thermocouple images, 97.10% for baseline images across power levels, 97.43% for thermocouple images, and 97.27% for both types across power levels. The high accuracy, above 97%, demonstrates the system's effectiveness in identifying and classifying conditions within the UAM process, providing a reliable tool for quality assurance and process control in manufacturing environments.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2410.04202.pdf' target='_blank'>https://arxiv.org/pdf/2410.04202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Tarek Hasan, Mohammad Nazmush Shamael, H. M. Mutasim Billah, Arifa Akter, Md Al Emran Hossain, Sumayra Islam, Salekul Islam, Swakkhar Shatabda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04202">Deep Transfer Learning Based Peer Review Aggregation and Meta-review Generation for Scientific Articles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer review is the quality assessment of a manuscript by one or more peer experts. Papers are submitted by the authors to scientific venues, and these papers must be reviewed by peers or other authors. The meta-reviewers then gather the peer reviews, assess them, and create a meta-review and decision for each manuscript. As the number of papers submitted to these venues has grown in recent years, it becomes increasingly challenging for meta-reviewers to collect these peer evaluations on time while still maintaining the quality that is the primary goal of meta-review creation. In this paper, we address two peer review aggregation challenges a meta-reviewer faces: paper acceptance decision-making and meta-review generation. Firstly, we propose to automate the process of acceptance decision prediction by applying traditional machine learning algorithms. We use pre-trained word embedding techniques BERT to process the reviews written in natural language text. For the meta-review generation, we propose a transfer learning model based on the T5 model. Experimental results show that BERT is more effective than the other word embedding techniques, and the recommendation score is an important feature for the acceptance decision prediction. In addition, we figure out that fine-tuned T5 outperforms other inference models. Our proposed system takes peer reviews and other relevant features as input to produce a meta-review and make a judgment on whether or not the paper should be accepted. In addition, experimental results show that the acceptance decision prediction system of our task outperforms the existing models, and the meta-review generation task shows significantly improved scores compared to the existing models. For the statistical test, we utilize the Wilcoxon signed-rank test to assess whether there is a statistically significant improvement between paired observations.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2410.00817.pdf' target='_blank'>https://arxiv.org/pdf/2410.00817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dietmar Saupe, Krzysztof Rusek, David HÃ¤gele, Daniel Weiskopf, Lucjan Janowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00817">Maximum entropy and quantized metric models for absolute category ratings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The datasets of most image quality assessment studies contain ratings on a categorical scale with five levels, from bad (1) to excellent (5). For each stimulus, the number of ratings from 1 to 5 is summarized and given in the form of the mean opinion score. In this study, we investigate families of multinomial probability distributions parameterized by mean and variance that are used to fit the empirical rating distributions. To this end, we consider quantized metric models based on continuous distributions that model perceived stimulus quality on a latent scale. The probabilities for the rating categories are determined by quantizing the corresponding random variables using threshold values. Furthermore, we introduce a novel discrete maximum entropy distribution for a given mean and variance. We compare the performance of these models and the state of the art given by the generalized score distribution for two large data sets, KonIQ-10k and VQEG HDTV. Given an input distribution of ratings, our fitted two-parameter models predict unseen ratings better than the empirical distribution. In contrast to empirical ACR distributions and their discrete models, our continuous models can provide fine-grained estimates of quantiles of quality of experience that are relevant to service providers to satisfy a target fraction of the user population.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2409.05297.pdf' target='_blank'>https://arxiv.org/pdf/2409.05297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyi He, Peng Yang, Tian Qin, Jiawei Hou, Ning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05297">Adaptive Offloading and Enhancement for Low-Light Video Analytics on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore adaptive offloading and enhancement strategies for video analytics tasks on computing-constrained mobile devices in low-light conditions. We observe that the accuracy of low-light video analytics varies from different enhancement algorithms. The root cause could be the disparities in the effectiveness of enhancement algorithms for feature extraction in analytic models. Specifically, the difference in class activation maps (CAMs) between enhanced and low-light frames demonstrates a positive correlation with video analytics accuracy. Motivated by such observations, a novel enhancement quality assessment method is proposed on CAMs to evaluate the effectiveness of different enhancement algorithms for low-light videos. Then, we design a multi-edge system, which adaptively offloads and enhances low-light video analytics tasks from mobile devices. To achieve the trade-off between the enhancement quality and the latency for all system-served mobile devices, we propose a genetic-based scheduling algorithm, which can find a near-optimal solution in a reasonable time to meet the latency requirement. Thereby, the offloading strategies and the enhancement algorithms are properly selected under the condition of limited end-edge bandwidth and edge computation resources. Simulation experiments demonstrate the superiority of the proposed system, improving accuracy up to 20.83\% compared to existing benchmarks.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2407.16889.pdf' target='_blank'>https://arxiv.org/pdf/2407.16889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Modan Tailleur, Pierre Aumond, Vincent Tourre, Mathieu Lagrange
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16889">Towards better visualizations of urban sound environments: insights from interviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban noise maps and noise visualizations traditionally provide macroscopic representations of noise levels across cities. However, those representations fail at accurately gauging the sound perception associated with these sound environments, as perception highly depends on the sound sources involved. This paper aims at analyzing the need for the representations of sound sources, by identifying the urban stakeholders for whom such representations are assumed to be of importance. Through spoken interviews with various urban stakeholders, we have gained insight into current practices, the strengths and weaknesses of existing tools and the relevance of incorporating sound sources into existing urban sound environment representations. Three distinct use of sound source representations emerged in this study: 1) noise-related complaints for industrials and specialized citizens, 2) soundscape quality assessment for citizens, and 3) guidance for urban planners. Findings also reveal diverse perspectives for the use of visualizations, which should use indicators adapted to the target audience, and enable data accessibility.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2407.10817.pdf' target='_blank'>https://arxiv.org/pdf/2407.10817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10817">Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2406.00276.pdf' target='_blank'>https://arxiv.org/pdf/2406.00276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyu Tao, Mengtian Zhang, Zixi Zhao, Haoyang Li, Ruifei Ma, Yunhong Che, Xin Sun, Lin Su, Xiangyu Chen, Zihao Zhou, Heng Chang, Tingwei Cao, Xiao Xiao, Yaojun Liu, Wenjun Yu, Zhongling Xu, Yang Li, Han Hao, Xuan Zhang, Xiaosong Hu, Guangmin ZHou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00276">Non-destructive Degradation Pattern Decoupling for Ultra-early Battery Prototype Verification Using Physics-informed Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manufacturing complexities and uncertainties have impeded the transition from material prototypes to commercial batteries, making prototype verification critical to quality assessment. A fundamental challenge involves deciphering intertwined chemical processes to characterize degradation patterns and their quantitative relationship with battery performance. Here we show that a physics-informed machine learning approach can quantify and visualize temporally resolved losses concerning thermodynamics and kinetics only using electric signals. Our method enables non-destructive degradation pattern characterization, expediting temperature-adaptable predictions of entire lifetime trajectories, rather than end-of-life points. The verification speed is 25 times faster yet maintaining 95.1% accuracy across temperatures. Such advances facilitate more sustainable management of defective prototypes before massive production, establishing a 19.76 billion USD scrap material recycling market by 2060 in China. By incorporating stepwise charge acceptance as a measure of the initial manufacturing variability of normally identical batteries, we can immediately identify long-term degradation variations. We attribute the predictive power to interpreting machine learning insights using material-agnostic featurization taxonomy for degradation pattern decoupling. Our findings offer new possibilities for dynamic system analysis, such as battery prototype degradation, demonstrating that complex pattern evolutions can be accurately predicted in a non-destructive and data-driven fashion by integrating physics-informed machine learning.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2404.13277.pdf' target='_blank'>https://arxiv.org/pdf/2404.13277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Yang, Yujia Liu, Dingquan Li, Yan Zhong, Tingting Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13277">Beyond Score Changes: Adversarial Attack on No-Reference Image Quality Assessment from Two Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have demonstrated impressive success in No-Reference Image Quality Assessment (NR-IQA). However, recent researches highlight the vulnerability of NR-IQA models to subtle adversarial perturbations, leading to inconsistencies between model predictions and subjective ratings. Current adversarial attacks, however, focus on perturbing predicted scores of individual images, neglecting the crucial aspect of inter-score correlation relationships within an entire image set. Meanwhile, it is important to note that the correlation, like ranking correlation, plays a significant role in NR-IQA tasks. To comprehensively explore the robustness of NR-IQA models, we introduce a new framework of correlation-error-based attacks that perturb both the correlation within an image set and score changes on individual images. Our research primarily focuses on ranking-related correlation metrics like Spearman's Rank-Order Correlation Coefficient (SROCC) and prediction error-related metrics like Mean Squared Error (MSE). As an instantiation, we propose a practical two-stage SROCC-MSE-Attack (SMA) that initially optimizes target attack scores for the entire image set and then generates adversarial examples guided by these scores. Experimental results demonstrate that our SMA method not only significantly disrupts the SROCC to negative values but also maintains a considerable change in the scores of individual images. Meanwhile, it exhibits state-of-the-art performance across metrics with different categories. Our method provides a new perspective on the robustness of NR-IQA models.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2404.11236.pdf' target='_blank'>https://arxiv.org/pdf/2404.11236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ² Di Domenico, Guido Borghi, Annalisa Franco, Davide Maltoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11236">ONOT: a High-Quality ICAO-compliant Synthetic Mugshot Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, state-of-the-art AI-based generative models represent a viable solution to overcome privacy issues and biases in the collection of datasets containing personal information, such as faces. Following this intuition, in this paper we introduce ONOT, a synthetic dataset specifically focused on the generation of high-quality faces in adherence to the requirements of the ISO/IEC 39794-5 standards that, following the guidelines of the International Civil Aviation Organization (ICAO), defines the interchange formats of face images in electronic Machine-Readable Travel Documents (eMRTD). The strictly controlled and varied mugshot images included in ONOT are useful in research fields related to the analysis of face images in eMRTD, such as Morphing Attack Detection and Face Quality Assessment. The dataset is publicly released, in combination with the generation procedure details in order to improve the reproducibility and enable future extensions.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2404.03998.pdf' target='_blank'>https://arxiv.org/pdf/2404.03998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reina Kaneko, Takumi Ueda, Hiroshi Higashi, Yuichi Tanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03998">PHISWID: Physics-Inspired Underwater Image Dataset Synthesized from RGB-D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the physics-inspired synthesized underwater image dataset (PHISWID), a dataset tailored for enhancing underwater image processing through physics-inspired image synthesis. For underwater image enhancement, data-driven approaches (e.g., deep neural networks) typically demand extensive datasets, yet acquiring paired clean atmospheric images and degraded underwater images poses significant challenges. Existing datasets have limited contributions to image enhancement due to lack of physics models, publicity, and ground-truth atmospheric images. PHISWID addresses these issues by offering a set of paired atmospheric and underwater images. Specifically, underwater images are synthetically degraded by color degradation and marine snow artifacts from atmospheric RGB-D images. It is enabled based on a physics-based underwater image observation model. Our synthetic approach generates a large quantity of the pairs, enabling effective training of deep neural networks and objective image quality assessment. Through benchmark experiments with some datasets and image enhancement methods, we validate that our dataset can improve the image enhancement performance. Our dataset, which is publicly available, contributes to the development in underwater image processing.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2403.11397.pdf' target='_blank'>https://arxiv.org/pdf/2403.11397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11397">Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information. NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance. However, these models are found to be vulnerable to adversarial attacks, which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores. In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the adversarial robustness of NR-IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\ell_1$ norm of the model's gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on NR-IQA models. Our study offers valuable insights into the adversarial robustness of NR-IQA models and provides a foundation for future research in this area.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2401.05217.pdf' target='_blank'>https://arxiv.org/pdf/2401.05217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Yang, Yujia Liu, Dingquan Li, Tingting Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05217">Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality scores consistent with human perception without relying on pristine reference images, serving as a crucial component in various visual tasks. Ensuring the robustness of NR-IQA methods is vital for reliable comparisons of different image processing techniques and consistent user experiences in recommendations. The attack methods for NR-IQA provide a powerful instrument to test the robustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on the gradient of the NR-IQA model, leading to limitations when the gradient information is unavailable. In this paper, we present a pioneering query-based black box attack against NR-IQA methods. We propose the concept of score boundary and leverage an adaptive iterative approach with multiple score boundaries. Meanwhile, the initial attack directions are also designed to leverage the characteristics of the Human Visual System (HVS). Experiments show our method outperforms all compared state-of-the-art attack methods and is far ahead of previous black-box methods. The effective NR-IQA model DBCNN suffers a Spearman's rank-order correlation coefficient (SROCC) decline of 0.6381 attacked by our method, revealing the vulnerability of NR-IQA models to black-box attacks. The proposed attack method also provides a potent tool for further exploration into NR-IQA robustness.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2312.06240.pdf' target='_blank'>https://arxiv.org/pdf/2312.06240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dazhao Du, Enhan Li, Lingyu Si, Fanjiang Xu, Jianwei Niu, Fuchun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06240">UIEDP:Underwater Image Enhancement with Diffusion Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater image enhancement (UIE) aims to generate clear images from low-quality underwater images. Due to the unavailability of clear reference images, researchers often synthesize them to construct paired datasets for training deep models. However, these synthesized images may sometimes lack quality, adversely affecting training outcomes. To address this issue, we propose UIE with Diffusion Prior (UIEDP), a novel framework treating UIE as a posterior distribution sampling process of clear images conditioned on degraded underwater inputs. Specifically, UIEDP combines a pre-trained diffusion model capturing natural image priors with any existing UIE algorithm, leveraging the latter to guide conditional generation. The diffusion prior mitigates the drawbacks of inferior synthetic images, resulting in higher-quality image generation. Extensive experiments have demonstrated that our UIEDP yields significant improvements across various metrics, especially no-reference image quality assessment. And the generated enhanced images also exhibit a more natural appearance.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2309.04590.pdf' target='_blank'>https://arxiv.org/pdf/2309.04590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arpit Agarwal, Abhiroop Ajith, Chengtao Wen, Veniamin Stryzheus, Brian Miller, Matthew Chen, Micah K. Johnson, Jose Luis Susa Rincon, Justinian Rosca, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04590">Robotic Defect Inspection with Visual and Tactile Perception for Large-scale Components</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In manufacturing processes, surface inspection is a key requirement for quality assessment and damage localization. Due to this, automated surface anomaly detection has become a promising area of research in various industrial inspection systems. A particular challenge in industries with large-scale components, like aircraft and heavy machinery, is inspecting large parts with very small defect dimensions. Moreover, these parts can be of curved shapes. To address this challenge, we present a 2-stage multi-modal inspection pipeline with visual and tactile sensing. Our approach combines the best of both visual and tactile sensing by identifying and localizing defects using a global view (vision) and using the localized area for tactile scanning for identifying remaining defects. To benchmark our approach, we propose a novel real-world dataset with multiple metallic defect types per image, collected in the production environments on real aerospace manufacturing parts, as well as online robot experiments in two environments. Our approach is able to identify 85% defects using Stage I and identify 100% defects after Stage II. The dataset is publicly available at https://zenodo.org/record/8327713
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2304.00451.pdf' target='_blank'>https://arxiv.org/pdf/2304.00451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avinab Saha, Sandeep Mishra, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00451">Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two separate encoders to learn high-level content and low-level image quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA. For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear regression model, which is used to map the image representations to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and synthetic distortions, demonstrating how deep neural networks can be trained in an unsupervised setting to produce perceptually relevant representations. We conclude from our experiments that the low and high-level features obtained are indeed complementary and positively impact the performance of the linear regressor. A public release of all the codes associated with this work will be made available on GitHub.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2303.15206.pdf' target='_blank'>https://arxiv.org/pdf/2303.15206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxue Liang, Tianhao Wu, Param Hanji, Francesco Banterle, Hongyun Gao, Rafal Mantiuk, Cengiz Oztireli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15206">Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2202.06533.pdf' target='_blank'>https://arxiv.org/pdf/2202.06533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Yang, Stephan Mandt, Lucas Theis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.06533">An Introduction to Neural Data Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. The present article aims to introduce this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2511.12256.pdf' target='_blank'>https://arxiv.org/pdf/2511.12256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tolga Demiroglu, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12256">Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2511.07958.pdf' target='_blank'>https://arxiv.org/pdf/2511.07958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoye Liang, Lai Jiang, Minglang Qiao, Yichen Guo, Yue Zhang, Xin Deng, Shengxi Li, Yufan Liu, Mai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07958">Burst Image Quality Assessment: A New Benchmark and Unified Framework for Multiple Downstream Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the development of burst imaging technology has improved the capture and processing capabilities of visual data, enabling a wide range of applications. However, the redundancy in burst images leads to the increased storage and transmission demands, as well as reduced efficiency of downstream tasks. To address this, we propose a new task of Burst Image Quality Assessment (BuIQA), to evaluate the task-driven quality of each frame within a burst sequence, providing reasonable cues for burst image selection. Specifically, we establish the first benchmark dataset for BuIQA, consisting of $7,346$ burst sequences with $45,827$ images and $191,572$ annotated quality scores for multiple downstream scenarios. Inspired by the data analysis, a unified BuIQA framework is proposed to achieve an efficient adaption for BuIQA under diverse downstream scenarios. Specifically, a task-driven prompt generation network is developed with heterogeneous knowledge distillation, to learn the priors of the downstream task. Then, the task-aware quality assessment network is introduced to assess the burst image quality based on the task prompt. Extensive experiments across 10 downstream scenarios demonstrate the impressive BuIQA performance of the proposed approach, outperforming the state-of-the-art. Furthermore, it can achieve $0.33$ dB PSNR improvement in the downstream tasks of denoising and super-resolution, by applying our approach to select the high-quality burst frames.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2510.18923.pdf' target='_blank'>https://arxiv.org/pdf/2510.18923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduard Frankford, Tobias Antensteiner, Michael Vierhauser, Clemens Sauerwein, Vivien Wallner, Iris Groher, Reinhold Plösch, Ruth Breu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18923">A Survey on Feedback Types in Automated Programming Assessment Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent rapid increase in digitization across all major industries, acquiring programming skills has increased the demand for introductory programming courses. This has further resulted in universities integrating programming courses into a wide range of curricula, including not only technical studies but also business and management fields of study. Consequently, additional resources are needed for teaching, grading, and tutoring students with diverse educational backgrounds and skills. As part of this, Automated Programming Assessment Systems (APASs) have emerged, providing scalable and high-quality assessment systems with efficient evaluation and instant feedback. Commonly, APASs heavily rely on predefined unit tests for generating feedback, often limiting the scope and level of detail of feedback that can be provided to students. With the rise of Large Language Models (LLMs) in recent years, new opportunities have emerged as these technologies can enhance feedback quality and personalization. To investigate how different feedback mechanisms in APASs are perceived by students, and how effective they are in supporting problem-solving, we have conducted a large-scale study with over 200 students from two different universities. Specifically, we compare baseline Compiler Feedback, standard Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality and impact on student performance. Results indicate that while students rate unit test feedback as the most helpful, AI-generated feedback leads to significantly better performances. These findings suggest combining unit tests and AI-driven guidance to optimize automated feedback mechanisms and improve learning outcomes in programming education.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2510.13638.pdf' target='_blank'>https://arxiv.org/pdf/2510.13638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Wai Chin, Haniza Yazid, Hoi Leong Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13638">Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image enhancement is crucial for improving the quality and interpretability of diagnostic images, ultimately supporting early detection, accurate diagnosis, and effective treatment planning. Despite advancements in imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images often suffer from challenges like noise, artifacts, and low contrast, which limit their diagnostic potential. Addressing these challenges requires robust preprocessing, denoising algorithms, and advanced enhancement methods, with deep learning techniques playing an increasingly significant role. This systematic literature review, following the PRISMA approach, investigates the key challenges, recent advancements, and evaluation metrics in medical image enhancement. By analyzing findings from 39 peer-reviewed studies, this review provides insights into the effectiveness of various enhancement methods across different imaging modalities and the importance of evaluation metrics in assessing their impact. Key issues like low contrast and noise are identified as the most frequent, with MRI and multi-modal imaging receiving the most attention, while specialized modalities such as histopathology, endoscopy, and bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize conventional mathematical methods, 9 focus on deep learning techniques, and 1 explores a hybrid approach. In terms of image quality assessment, 18 studies employ both reference-based and non-reference-based metrics, 9 rely solely on reference-based metrics, and 12 use only non-reference-based metrics, with a total of 65 IQA metrics introduced, predominantly non-reference-based. This review highlights current limitations, research gaps, and potential future directions for advancing medical image enhancement.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2510.04366.pdf' target='_blank'>https://arxiv.org/pdf/2510.04366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Klugmann, Daniel Kondermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04366">Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-generated categorical annotations frequently produce empirical response distributions (soft labels) that reflect ambiguity rather than simple annotator error. We introduce an ambiguity measure that maps a discrete response distribution to a scalar in the unit interval, designed to quantify aleatoric uncertainty in categorical tasks. The measure bears a close relationship to quadratic entropy (Gini-style impurity) but departs from those indices by treating an explicit "can't solve" category asymmetrically, thereby separating uncertainty arising from class-level indistinguishability from uncertainty due to explicit unresolvability. We analyze the measure's formal properties and contrast its behavior with a representative ambiguity measure from the literature. Moving beyond description, we develop statistical tools for inference: we propose frequentist point estimators for population ambiguity and derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the underlying probability vector, providing a principled account of epistemic uncertainty. Numerical examples illustrate estimation, calibration, and practical use for dataset-quality assessment and downstream machine-learning workflows.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2510.02403.pdf' target='_blank'>https://arxiv.org/pdf/2510.02403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jalil Jalili, Yashraj Gavhane, Evan Walker, Anna Heinke, Christopher Bowd, Akram Belghith, Massimo A. Fazio, Christopher A. Girkin, C. Gustavo De Moraes, Jeffrey M. Liebmann, Sally L. Baxter, Robert N. Weinreb, Linda M. Zangwill, Mark Christopher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02403">Glaucoma Detection and Structured OCT Report Generation via a Fine-tuned Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: To develop an explainable multimodal large language model (MM-LLM) that (1) screens optic nerve head (ONH) OCT circle scans for quality and (2) generates structured clinical reports that include glaucoma diagnosis and sector-wise retinal nerve fiber layer (RNFL) thinning assessments. Design: Retrospective cohort study of 1,310 subjects contributing 43,849 Spectralis ONH OCT circle scans (1,331 glaucomatous and 867 healthy eyes) from the DIGS and ADAGES cohorts. Methods: A MM-LLM (Llama 3.2 Vision-Instruct model) was fine-tuned to generate clinical descriptions of OCT imaging data. Training data included paired OCT images and automatically generated, structured clinical reports that described global and sectoral RNFL thinning. Poor-quality scans were labeled as unusable and paired with a fixed refusal statement. The model was evaluated on a held-out test set for three tasks: quality assessment, glaucoma detection, and RNFL thinning classification across seven anatomical sectors. Evaluation metrics included accuracy, sensitivity, specificity, precision, and F1-score. Model description quality was also evaluated using standard text evaluation metrics. Results: The model achieved 0.90 accuracy and 0.98 specificity for quality triage. For glaucoma detection, accuracy was 0.86 (sensitivity 0.91, specificity 0.73, F1-score 0.91). RNFL thinning prediction accuracy ranged from 0.83 to 0.94, with highest performance in global and temporal sectors. Text generation scores showed strong alignment with reference reports (BLEU: 0.82; ROUGE-1: 0.94; ROUGE-2: 0.87; ROUGE-L: 0.92; BERTScore-F1: 0.99). Conclusions: The fine-tuned MM-LLM generated accurate clinical descriptions based on OCT imaging. The model achieved high accuracy in identifying image quality issues and detecting glaucoma. The model also provided sectoral descriptions of RNFL thinning to help support clinical OCT evaluation.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2509.22326.pdf' target='_blank'>https://arxiv.org/pdf/2509.22326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Israel Jesus Santos Filho, Muhammad Mahboob Ur Rahman, Taous-Meriem Laleg-Kirati, Tareq Al-Naffouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22326">Radio-PPG: photoplethysmogram digital twin synthesis using deep neural representation of 6G/WiFi ISAC signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital twins for 1D bio-signals enable real-time monitoring of physiological processes of a person, which enables early disease diagnosis and personalized treatment. This work introduces a novel non-contact method for digital twin (DT) photoplethysmogram (PPG) signal synthesis under the umbrella of 6G/WiFi integrated sensing and communication (ISAC) systems. We employ a software-defined radio (SDR) operating at 5.23 GHz that illuminates the chest of a nearby person with a wideband 6G/WiFi signal and collects the reflected signals. This allows us to acquire Radio-PPG dataset that consists of 300 minutes worth of near synchronous 64-channel radio data, PPG data, along with the labels (three body vitals) of 30 healthy subjects. With this, we test two artificial intelligence (AI) models for DT-PPG signal synthesis: i) discrete cosine transform followed by a multi-layer perceptron, ii) two U-NET models (Approximation network, Refinement network) in cascade, along with a custom loss function. Experimental results indicate that U-NET model achieves an impressive relative mean absolute error of 0.194 with a small ISAC sensing overhead of 15.62%, for DT-PPG synthesis. Furthermore, we performed quality assessment of the synthetic DT-PPG by computing the accuracy of DT-PPG-based vitals estimation and feature extraction, which turned out to be at par with that of reference PPG-based vitals estimation and feature extraction. This work highlights the potential of generative AI and 6G/WiFi ISAC technologies and serves as a foundational step towards the development of non-contact screening tools for covid-19, cardiovascular diseases and well-being assessment of people with special needs.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2509.10572.pdf' target='_blank'>https://arxiv.org/pdf/2509.10572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashlesha Akella, Akshar Kaul, Krishnasuri Narayanam, Sameep Mehta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10572">Quality Assessment of Tabular Data using Large Language Models and Code Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable data quality is crucial for downstream analysis of tabular datasets, yet rule-based validation often struggles with inefficiency, human intervention, and high computational costs. We present a three-stage framework that combines statistical inliner detection with LLM-driven rule and code generation. After filtering data samples through traditional clustering, we iteratively prompt LLMs to produce semantically valid quality rules and synthesize their executable validators through code-generating LLMs. To generate reliable quality rules, we aid LLMs with retrieval-augmented generation (RAG) by leveraging external knowledge sources and domain-specific few-shot examples. Robust guardrails ensure the accuracy and consistency of both rules and code snippets. Extensive evaluations on benchmark datasets confirm the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2508.17117.pdf' target='_blank'>https://arxiv.org/pdf/2508.17117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Syed Nazmus Sakib, Nafiul Haque, Mohammad Zabed Hossain, Shifat E. Arman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17117">PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PlantVillageVQA is a large-scale visual question answering (VQA) dataset derived from the widely used PlantVillage image corpus. It was designed to advance the development and evaluation of vision-language models for agricultural decision-making and analysis. The PlantVillageVQA dataset comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448 images spanning 14 crop species and 38 disease conditions. Questions are organised into 3 levels of cognitive complexity and 9 distinct categories. Each question category was phrased manually following expert guidance and generated via an automated two-stage pipeline: (1) template-based QA synthesis from image metadata and (2) multi-stage linguistic re-engineering. The dataset was iteratively reviewed by domain experts for scientific accuracy and relevancy. The final dataset was evaluated using three state-of-the-art models for quality assessment. Our objective remains to provide a publicly available, standardised and expert-verified database to enhance diagnostic accuracy for plant disease identifications and advance scientific research in the agricultural domain. Our dataset will be open-sourced at https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2508.03092.pdf' target='_blank'>https://arxiv.org/pdf/2508.03092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikun Cui, Tianyi Huang, Chia-En Chiang, Cuiqianhe Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03092">Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2507.13626.pdf' target='_blank'>https://arxiv.org/pdf/2507.13626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Hung Hu, Yusuke Yasuda, Akifumi Yoshimoto, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13626">Unifying Listener Scoring Scales: Comparison Learning Framework for Speech Quality Assessment and Continuous Speech Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition (CSER) are two key tasks in speech technology, both relying on listener ratings. However, these ratings are inherently biased due to individual listener factors. Previous approaches have introduced a mean listener scoring scale and modeled all listener scoring scales in the training set. However, the mean listener approach is prone to distortion from averaging ordinal data, leading to potential biases. Moreover, learning multiple listener scoring scales while inferring based only on the mean listener scale limits effectiveness. In contrast, our method focuses on modeling a unified listener scoring scale, using comparison scores to correctly capture the scoring relationships between utterances. Experimental results show that our method effectively improves prediction performance in both SQA and CSER tasks, proving its effectiveness and robustness.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2507.12090.pdf' target='_blank'>https://arxiv.org/pdf/2507.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panos Kakoulidis, Iakovi Alexiou, Junkwang Oh, Gunu Jho, Inchul Hwang, Pirros Tsiakoulis, Aimilios Chalamandaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12090">MambaRate: Speech Quality Assessment Across Different Sampling Rates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose MambaRate, which predicts Mean Opinion Scores (MOS) with limited bias regarding the sampling rate of the waveform under evaluation. It is designed for Track 3 of the AudioMOS Challenge 2025, which focuses on predicting MOS for speech in high sampling frequencies. Our model leverages self-supervised embeddings and selective state space modeling. The target ratings are encoded in a continuous representation via Gaussian radial basis functions (RBF). The results of the challenge were based on the system-level Spearman's Rank Correllation Coefficient (SRCC) metric. An initial MambaRate version (T16 system) outperformed the pre-trained baseline (B03) by ~14% in a few-shot setting without pre-training. T16 ranked fourth out of five in the challenge, differing by ~6% from the winning system. We present additional results on the BVCC dataset as well as ablations with different representations as input, which outperform the initial T16 version.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2506.20303.pdf' target='_blank'>https://arxiv.org/pdf/2506.20303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lee Qi Zun, Oscar Wong Jin Hao, Nor Anita Binti Che Omar, Zalifa Zakiah Binti Asnir, Mohamad Sabri bin Sinal Zainal, Goh Man Fye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20303">FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated fundus image quality assessment (FIQA) remains a challenge due to variations in image acquisition and subjective expert evaluations. We introduce FundaQ-8, a novel expert-validated framework for systematically assessing fundus image quality using eight critical parameters, including field coverage, anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a structured scoring reference, we develop a ResNet18-based regression model to predict continuous quality scores in the 0 to 1 range. The model is trained on 1800 fundus images from real-world clinical sources and Kaggle datasets, using transfer learning, mean squared error optimization, and standardized preprocessing. Validation against the EyeQ dataset and statistical analyses confirm the framework's reliability and clinical interpretability. Incorporating FundaQ-8 into deep learning models for diabetic retinopathy grading also improves diagnostic robustness, highlighting the value of quality-aware training in real-world screening applications.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2505.01761.pdf' target='_blank'>https://arxiv.org/pdf/2505.01761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Domhan, Dawei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01761">Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2505.00308.pdf' target='_blank'>https://arxiv.org/pdf/2505.00308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biling Wang, Austen Maniscalco, Ti Bai, Siqiu Wang, Michael Dohopolski, Mu-Han Lin, Chenyang Shen, Dan Nguyen, Junzhou Huang, Steve Jiang, Xinlei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00308">AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2504.21317.pdf' target='_blank'>https://arxiv.org/pdf/2504.21317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Xie, Yaoyao Fiona Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21317">Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of machine learning (ML)-based process monitoring systems has significantly advanced additive manufacturing (AM) by enabling real-time defect detection, quality assessment, and process optimization. However, redundancy is a critical yet often overlooked challenge in the deployment and operation of ML-based AM process monitoring systems. Excessive redundancy leads to increased equipment costs, compromised model performance, and high computational requirements, posing barriers to industrial adoption. However, existing research lacks a unified definition of redundancy and a systematic framework for its evaluation and mitigation. This paper defines redundancy in ML-based AM process monitoring and categorizes it into sample-level, feature-level, and model-level redundancy. A comprehensive multi-level redundancy mitigation (MLRM) framework is proposed, incorporating advanced methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to systematically reduce redundancy while improving model performance. The framework is validated through an ML-based in-situ defect detection case study for directed energy deposition (DED), demonstrating a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements. Additionally, the proposed approach lowers sensor costs and energy consumption, enabling a lightweight, cost-effective, and scalable monitoring system. By defining redundancy and introducing a structured mitigation framework, this study establishes redundancy analysis and mitigation as a key enabler of efficient ML-based process monitoring in production environments.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2504.12976.pdf' target='_blank'>https://arxiv.org/pdf/2504.12976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles O'Neill, Tirthankar Ghosal, Roberta RÄileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, Ioana CiucÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12976">Sparks of Science: Hypothesis Generation Using Structured Paper Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating novel and creative scientific hypotheses is a cornerstone in achieving Artificial General Intelligence. Large language and reasoning models have the potential to aid in the systematic creation, selection, and validation of scientifically informed hypotheses. However, current foundation models often struggle to produce scientific ideas that are both novel and feasible. One reason is the lack of a dedicated dataset that frames Scientific Hypothesis Generation (SHG) as a Natural Language Generation (NLG) task. In this paper, we introduce HypoGen, the first dataset of approximately 5500 structured problem-hypothesis pairs extracted from top-tier computer science conferences structured with a Bit-Flip-Spark schema, where the Bit is the conventional assumption, the Spark is the key insight or conceptual leap, and the Flip is the resulting counterproposal. HypoGen uniquely integrates an explicit Chain-of-Reasoning component that reflects the intellectual process from Bit to Flip. We demonstrate that framing hypothesis generation as conditional language modelling, with the model fine-tuned on Bit-Flip-Spark and the Chain-of-Reasoning (and where, at inference, we only provide the Bit), leads to improvements in the overall quality of the hypotheses. Our evaluation employs automated metrics and LLM judge rankings for overall quality assessment. We show that by fine-tuning on our HypoGen dataset we improve the novelty, feasibility, and overall quality of the generated hypotheses. The HypoGen dataset is publicly available at huggingface.co/datasets/UniverseTBD/hypogen-dr1.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2502.13764.pdf' target='_blank'>https://arxiv.org/pdf/2502.13764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanke Xia, Ruoxin Peng, Haoqi Chu, Xinlei Zhu, Zhiyu Yang, Lili Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13764">An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2501.12082.pdf' target='_blank'>https://arxiv.org/pdf/2501.12082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Hu, Wei Wang, Chunyi Li, Lihuo He, Leida Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12082">A Multi-annotated and Multi-modal Dataset for Wide-angle Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wide-angle video is favored for its wide viewing angle and ability to capture a large area of scenery, making it an ideal choice for sports and adventure recording. However, wide-angle video is prone to deformation, exposure and other distortions, resulting in poor video quality and affecting the perception and experience, which may seriously hinder its application in fields such as competitive sports. Up to now, few explorations focus on the quality assessment issue of wide-angle video. This deficiency primarily stems from the absence of a specialized dataset for wide-angle videos. To bridge this gap, we construct the first Multi-annotated and multi-modal Wide-angle Video quality assessment (MWV) dataset. Then, the performances of state-of-the-art video quality methods on the MWV dataset are investigated by inter-dataset testing and intra-dataset testing. Experimental results show that these methods impose significant limitations on their applicability.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2501.02706.pdf' target='_blank'>https://arxiv.org/pdf/2501.02706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaze Li, Haoran Xu, Shiding Zhu, Junwei He, Haozhao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02706">Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of diffusion models has greatly advanced AI-generated videos in terms of length and consistency recently, yet assessing AI-generated videos still remains challenging. Previous approaches have often focused on User-Generated Content(UGC), but few have targeted AI-Generated Video Quality Assessment methods. In this work, we introduce MSA-VQA, a Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment, which leverages CLIP-based semantic supervision and cross-attention mechanisms. Our hierarchical framework analyzes video content at three levels: frame, segment, and video. We propose a Prompt Semantic Supervision Module using text encoder of CLIP to ensure semantic consistency between videos and conditional prompts. Additionally, we propose the Semantic Mutation-aware Module to capture subtle variations between frames. Extensive experiments demonstrate our method achieves state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2412.06599.pdf' target='_blank'>https://arxiv.org/pdf/2412.06599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Wang, Shengqi Chen, Jianrong Dai, Shirui Qin, Ying Cao, Ruiao Zhao, Guohua Wu, Yuan Tang, Jiayun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06599">A No-Reference Medical Image Quality Assessment Method Based on Automated Distortion Recognition Technology: Application to Preprocessing in MRI-guided Radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective:To develop a no-reference image quality assessment method using automated distortion recognition to boost MRI-guided radiotherapy precision.Methods:We analyzed 106,000 MR images from 10 patients with liver metastasis,captured with the Elekta Unity MR-LINAC.Our No-Reference Quality Assessment Model includes:1)image preprocessing to enhance visibility of key diagnostic features;2)feature extraction and directional analysis using MSCN coefficients across four directions to capture textural attributes and gradients,vital for identifying image features and potential distortions;3)integrative Quality Index(QI)calculation,which integrates features via AGGD parameter estimation and K-means clustering.The QI,based on a weighted MAD computation of directional scores,provides a comprehensive image quality measure,robust against outliers.LOO-CV assessed model generalizability and performance.Tumor tracking algorithm performance was compared with and without preprocessing to verify tracking accuracy enhancements.Results:Preprocessing significantly improved image quality,with the QI showing substantial positive changes and surpassing other metrics.After normalization,the QI's average value was 79.6 times higher than CNR,indicating improved image definition and contrast.It also showed higher sensitivity in detail recognition with average values 6.5 times and 1.7 times higher than Tenengrad gradient and entropy.The tumor tracking algorithm confirmed significant tracking accuracy improvements with preprocessed images,validating preprocessing effectiveness.Conclusions:This study introduces a novel no-reference image quality evaluation method based on automated distortion recognition,offering a new quality control tool for MRIgRT tumor tracking.It enhances clinical application accuracy and facilitates medical image quality assessment standardization, with significant clinical and research value.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2411.16087.pdf' target='_blank'>https://arxiv.org/pdf/2411.16087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jili Xia, Lihuo He, Fei Gao, Kaifan Zhang, Leida Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16087">AI-Generated Image Quality Assessment Based on Task-Specific Prompt and Multi-Granularity Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, AI-generated images (AIGIs) created by given prompts (initial prompts) have garnered widespread attention. Nevertheless, due to technical nonproficiency, they often suffer from poor perception quality and Text-to-Image misalignment. Therefore, assessing the perception quality and alignment quality of AIGIs is crucial to improving the generative model's performance. Existing assessment methods overly rely on the initial prompts in the task prompt design and use the same prompts to guide both perceptual and alignment quality evaluation, overlooking the distinctions between the two tasks. To address this limitation, we propose a novel quality assessment method for AIGIs named TSP-MGS, which designs task-specific prompts and measures multi-granularity similarity between AIGIs and the prompts. Specifically, task-specific prompts are first constructed to describe perception and alignment quality degrees separately, and the initial prompt is introduced for detailed quality perception. Then, the coarse-grained similarity between AIGIs and task-specific prompts is calculated, which facilitates holistic quality awareness. In addition, to improve the understanding of AIGI details, the fine-grained similarity between the image and the initial prompt is measured. Finally, precise quality prediction is acquired by integrating the multi-granularity similarities. Experiments on the commonly used AGIQA-1K and AGIQA-3K benchmarks demonstrate the superiority of the proposed TSP-MGS.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2411.12273.pdf' target='_blank'>https://arxiv.org/pdf/2411.12273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Gong, Zhuo Deng, Run Gan, Zhiyuan Niu, Lu Chen, Canfeng Huang, Jia Liang, Weihao Gao, Fang Li, Shaochong Zhang, Lan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12273">Acquire Precise and Comparable Fundus Image Quality Score: FTHNet and FQS Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The retinal fundus images are utilized extensively in the diagnosis, and their quality can directly affect the diagnosis results. However, due to the insufficient dataset and algorithm application, current fundus image quality assessment (FIQA) methods are not powerful enough to meet ophthalmologists` demands. In this paper, we address the limitations of datasets and algorithms in FIQA. First, we establish a new FIQA dataset, Fundus Quality Score(FQS), which includes 2246 fundus images with two labels: a continuous Mean Opinion Score varying from 0 to 100 and a three-level quality label. Then, we propose a FIQA Transformer-based Hypernetwork (FTHNet) to solve these tasks with regression results rather than classification results in conventional FIQA works. The FTHNet is optimized for the FIQA tasks with extensive experiments. Results on our FQS dataset show that the FTHNet can give quality scores for fundus images with PLCC of 0.9423 and SRCC of 0.9488, significantly outperforming other methods with fewer parameters and less computation complexity.We successfully build a dataset and model addressing the problems of current FIQA methods. Furthermore, the model deployment experiments demonstrate its potential in automatic medical image quality control. All experiments are carried out with 10-fold cross-validation to ensure the significance of the results.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2411.10724.pdf' target='_blank'>https://arxiv.org/pdf/2411.10724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anton Alekseev, Gulnara Kabaeva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10724">HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the key tasks in modern applied computational linguistics is constructing word vector representations (word embeddings), which are widely used to address natural language processing tasks such as sentiment analysis, information extraction, and more. To choose an appropriate method for generating these word embeddings, quality assessment techniques are often necessary. A standard approach involves calculating distances between vectors for words with expert-assessed 'similarity'. This work introduces the first 'silver standard' dataset for such tasks in the Kyrgyz language, alongside training corresponding models and validating the dataset's suitability through quality evaluation metrics.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2411.07503.pdf' target='_blank'>https://arxiv.org/pdf/2411.07503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengqi Chen, Zilin Wang, Jianrong Dai, Shirui Qin, Ying Cao, Ruiao Zhao, Jiayun Chen, Guohua Wu, Yuan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07503">A Novel Automatic Real-time Motion Tracking Method in MRI-guided Radiotherapy Using Enhanced Tracking-Learning-Detection Framework with Automatic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and Purpose: Accurate motion tracking in MRI-guided Radiotherapy (MRIgRT) is essential for effective treatment delivery. This study aimed to enhance motion tracking precision in MRIgRT through an automatic real-time markerless tracking method using an enhanced Tracking-Learning-Detection (ETLD) framework with automatic segmentation. Materials and Methods: We developed a novel MRIgRT motion tracking and segmentation method by integrating the ETLD framework with an improved Chan-Vese model (ICV), named ETLD+ICV. The ETLD framework was upgraded for real-time cine MRI, including advanced image preprocessing, no-reference image quality assessment, an enhanced median-flow tracker, and a refined detector with dynamic search region adjustments. ICV was used for precise target volume coverage, refining the segmented region frame by frame using tracking results, with key parameters optimized. The method was tested on 3.5D MRI scans from 10 patients with liver metastases. Results: Evaluation of 106,000 frames across 77 treatment fractions showed sub-millimeter tracking errors of less than 0.8mm, with over 99% precision and 98% recall for all subjects in the Beam Eye View(BEV)/Beam Path View(BPV) orientation. The ETLD+ICV method achieved a dice global score of more than 82% for all subjects, demonstrating the method's extensibility and precise target volume coverage. Conclusion: This study successfully developed an automatic real-time markerless motion tracking method for MRIgRT that significantly outperforms current methods. The novel method not only delivers exceptional precision in tracking and segmentation but also shows enhanced adaptability to clinical demands, making it an indispensable asset in improving the efficacy of radiotherapy treatments.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2409.15240.pdf' target='_blank'>https://arxiv.org/pdf/2409.15240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqing He, Liang Zhu, Rui Wang, Xi Wang, Reza Haffari, Jiaxing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15240">MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term memory is important for chatbots and dialogue systems (DS) to create consistent and human-like conversations, evidenced by numerous developed memory-augmented DS (MADS). To evaluate the effectiveness of such MADS, existing commonly used evaluation metrics, like retrieval accuracy and perplexity (PPL), mainly focus on query-oriented factualness and language quality assessment. However, these metrics often lack practical value. Moreover, the evaluation dimensions are insufficient for human-like assessment in DS. Regarding memory-recalling paradigms, current evaluation schemes only consider passive memory retrieval while ignoring diverse memory recall with rich triggering factors, e.g., emotions and surroundings, which can be essential in emotional support scenarios. To bridge the gap, we construct a novel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various memory-recalling paradigms based on cognitive science and psychology theories. The benchmark assesses two tasks separately: memory retrieval and memory recognition with the incorporation of both passive and proactive memory recall data. We introduce new scoring criteria to the evaluation, including memory injection, emotion support (ES) proficiency, and intimacy, to comprehensively assess generated responses. Results from cutting-edge embedding models and large language models on this benchmark indicate the potential for further advancement. Extensive testing further reveals correlations between memory injection, ES proficiency, and intimacy.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2407.19082.pdf' target='_blank'>https://arxiv.org/pdf/2407.19082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Xiong, Skylar W. Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19082">Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be rendered along with the data to inform the reconstruction quality, or be integrated into uncertainty-aware volume visualization algorithms. To prevent the misalignment between the quantified variance and the prediction quality, we propose a novel variance regularization loss for ensemble learning that promotes the Regularized multi-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates closely to the true model error. We comprehensively evaluate the quality of variance quantification and data reconstruction of Monte Carlo Dropout, Mean Field Variational Inference, Deep Ensemble, and Predicting Variance compared to the proposed MDSRN and RMDSRN across diverse scalar field datasets. We demonstrate that RMDSRN attains the most accurate data reconstruction and competitive variance-error correlation among uncertain SRNs under the same neural network parameter budgets.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2406.17472.pdf' target='_blank'>https://arxiv.org/pdf/2406.17472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vlad Hosu, Lorenzo Agnolucci, Oliver Wiedemann, Daisuke Iso, Dietmar Saupe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17472">UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073 UHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to existing No-Reference (NR) IQA datasets, ours focuses on highly aesthetic photos of high technical quality, filling a gap in the literature. The images, carefully curated to exclude synthetic content, are sufficiently diverse to train general NR-IQA models. Importantly, the dataset is annotated with perceptual quality ratings obtained through a crowdsourcing study. Ten expert raters, comprising photographers and graphics artists, assessed each image at least twice in multiple sessions spanning several days, resulting in 20 highly reliable ratings per image. Annotators were rigorously selected based on several metrics, including self-consistency, to ensure their reliability. The dataset includes rich metadata with user and machine-generated tags from over 5,000 categories and popularity indicators such as favorites, likes, downloads, and views. With its unique characteristics, such as its focus on high-quality images, reliable crowdsourced annotations, and high annotation resolution, our dataset opens up new opportunities for advancing perceptual image quality assessment research and developing practical NR-IQA models that apply to modern photos. Our dataset is available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2406.06202.pdf' target='_blank'>https://arxiv.org/pdf/2406.06202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuzanna Fendor, Bas H. M. van der Velden, Xinxin Wang, Andrea Jr. Carnoli, Osman Mutlu, Ali HÃ¼rriyetoÄlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06202">Federated learning in food research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in the food domain is at times limited due to data sharing obstacles, such as data ownership, privacy requirements, and regulations. While important, these obstacles can restrict data-driven methods such as machine learning. Federated learning, the approach of training models on locally kept data and only sharing the learned parameters, is a potential technique to alleviate data sharing obstacles. This systematic review investigates the use of federated learning within the food domain, structures included papers in a federated learning framework, highlights knowledge gaps, and discusses potential applications. A total of 41 papers were included in the review. The current applications include solutions to water and milk quality assessment, cybersecurity of water processing, pesticide residue risk analysis, weed detection, and fraud detection, focusing on centralized horizontal federated learning. One of the gaps found was the lack of vertical or transfer federated learning and decentralized architectures.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2406.02302.pdf' target='_blank'>https://arxiv.org/pdf/2406.02302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Farahani, Zoha Azimi, Christian Timmerer, Radu Prodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02302">Towards AI-Assisted Sustainable Adaptive Video Streaming Systems: Tutorial and Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improvements in networking technologies and the steadily increasing numbers of users, as well as the shift from traditional broadcasting to streaming content over the Internet, have made video applications (e.g., live and Video-on-Demand (VoD)) predominant sources of traffic. Recent advances in Artificial Intelligence (AI) and its widespread application in various academic and industrial fields have focused on designing and implementing a variety of video compression and content delivery techniques to improve user Quality of Experience (QoE). However, providing high QoE services results in more energy consumption and carbon footprint across the service delivery path, extending from the end user's device through the network and service infrastructure (e.g., cloud providers). Despite the importance of energy efficiency in video streaming, there is a lack of comprehensive surveys covering state-of-the-art AI techniques and their applications throughout the video streaming lifecycle. Existing surveys typically focus on specific parts, such as video encoding, delivery networks, playback, or quality assessment, without providing a holistic view of the entire lifecycle and its impact on energy consumption and QoE. Motivated by this research gap, this survey provides a comprehensive overview of the video streaming lifecycle, content delivery, energy and Video Quality Assessment (VQA) metrics and models, and AI techniques employed in video streaming. In addition, it conducts an in-depth state-of-the-art analysis focused on AI-driven approaches to enhance the energy efficiency of end-to-end aspects of video streaming systems (i.e., encoding, delivery network, playback, and VQA approaches). Finally, it discusses prospective research directions for developing AI-assisted energy-aware video streaming systems.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2404.18409.pdf' target='_blank'>https://arxiv.org/pdf/2404.18409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiquan Yuan, Fanyi Yang, Jihe Li, Xinyan Cao, Jinming Che, Jinlong Lin, Xixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18409">PKU-AIGIQA-4K: A Perceptual Quality Assessment Database for Both Text-to-Image and Image-to-Image AI-Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, image generation technology has rapidly advanced, resulting in the creation of a vast array of AI-generated images (AIGIs). However, the quality of these AIGIs is highly inconsistent, with low-quality AIGIs severely impairing the visual experience of users. Due to the widespread application of AIGIs, the AI-generated image quality assessment (AIGIQA), aimed at evaluating the quality of AIGIs from the perspective of human perception, has garnered increasing interest among scholars. Nonetheless, current research has not yet fully explored this field. We have observed that existing databases are limited to images generated from single scenario settings. Databases such as AGIQA-1K, AGIQA-3K, and AIGCIQA2023, for example, only include images generated by text-to-image generative models. This oversight highlights a critical gap in the current research landscape, underscoring the need for dedicated databases catering to image-to-image scenarios, as well as more comprehensive databases that encompass a broader range of AI-generated image scenarios. Addressing these issues, we have established a large scale perceptual quality assessment database for both text-to-image and image-to-image AIGIs, named PKU-AIGIQA-4K. We then conduct a well-organized subjective experiment to collect quality labels for AIGIs and perform a comprehensive analysis of the PKU-AIGIQA-4K database. Regarding the use of image prompts during the training process, we propose three image quality assessment (IQA) methods based on pre-trained models that include a no-reference method NR-AIGCIQA, a full-reference method FR-AIGCIQA, and a partial-reference method PR-AIGCIQA. Finally, leveraging the PKU-AIGIQA-4K database, we conduct extensive benchmark experiments and compare the performance of the proposed methods and the current IQA methods.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2404.10155.pdf' target='_blank'>https://arxiv.org/pdf/2404.10155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10155">The Fault in our Stars: Quality Assessment of Code Generation Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2403.01069.pdf' target='_blank'>https://arxiv.org/pdf/2403.01069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhe Yuan, Pengfei Liu, Matthias GallÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01069">LLMCRIT: Teaching Large Language Models to Use Criteria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2312.02632.pdf' target='_blank'>https://arxiv.org/pdf/2312.02632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ane Rahbek VierÃ¸, Anastassia Vybornova, Michael Szell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02632">How Good Is Open Bicycle Infrastructure Data? A Countrywide Case Study of Denmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cycling is a key ingredient for a sustainability shift of Denmark's transportation system. To increase cycling rates, a better nationwide network of bicycle infrastructure is required. Planning such a network requires high-quality infrastructure data, however, the quality of bicycle infrastructure data is severely understudied. Here, we compare Denmark's two largest open data sets on dedicated bicycle infrastructure, OpenStreetMap (OSM) and GeoDanmark, in a countrywide data quality assessment, asking whether data is good enough for network-based analysis of cycling conditions. We find that neither of the data sets is of sufficient quality, and that data set conflation is necessary to obtain a complete dataset. Our analysis of the spatial variation of data quality suggests that rural areas are more likely to suffer from problems with data completeness. We demonstrate that the prevalent method of using infrastructure density as a proxy for data completeness is not suitable for bicycle infrastructure data, and that matching of corresponding features thus is necessary to assess data completeness. Based on our data quality assessment we recommend strategic mapping efforts towards data completeness, consistent standards to support comparability between different data sources, and increased focus on data topology to ensure high-quality bicycle network data.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2310.12241.pdf' target='_blank'>https://arxiv.org/pdf/2310.12241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prasenjit Karmakar, Swadhin Pradhan, Sandip Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12241">Exploring Indoor Health: An In-depth Field Study on the Indoor Air Quality Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor air pollution, a significant driver of respiratory and cardiovascular diseases, claims 3.2 million lives yearly, according to the World Health Organization, highlighting the pressing need to address this global crisis. In contrast to unconstrained outdoor environments, room structures, floor plans, ventilation systems, and occupant activities all impact the accumulation and spread of pollutants. Yet, comprehensive in-the-wild empirical studies exploring these unique indoor air pollution patterns and scope are lacking. To address this, we conducted a three-month-long field study involving over 28 indoor spaces to delve into the complexities of indoor air pollution. Our study was conducted using our custom-built DALTON air quality sensor and monitoring system, an innovative IoT air quality monitoring solution that considers cost, sensor type, accuracy, network connectivity, power, and usability. Our study also revealed that conventional measures, such as the Indoor Air Quality Index (IAQI), don't fully capture complex indoor air quality dynamics. Hence, we proposed the Healthy Home Index (HHI), a new metric considering the context and household activities, offering a more comprehensive understanding of indoor air quality. Our findings suggest that HHI provides a more accurate air quality assessment, underscoring the potential for wide-scale deployment of our indoor air quality monitoring platform.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2308.03698.pdf' target='_blank'>https://arxiv.org/pdf/2308.03698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songlin Fan, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03698">Screen-based 3D Subjective Experiment Software</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, widespread 3D graphics (e.g., point clouds and meshes) have drawn considerable efforts from academia and industry to assess their perceptual quality by conducting subjective experiments. However, lacking a handy software for 3D subjective experiments complicates the construction of 3D graphics quality assessment datasets, thus hindering the prosperity of relevant fields. In this paper, we develop a powerful platform with which users can flexibly design their 3D subjective methodologies and build high-quality datasets, easing a broad spectrum of 3D graphics subjective quality study. To accurately illustrate the perceptual quality differences of 3D stimuli, our software can simultaneously render the source stimulus and impaired stimulus and allows both stimuli to respond synchronously to viewer interactions. Compared with amateur 3D visualization tool-based or image/video rendering-based schemes, our approach embodies typical 3D applications while minimizing cognitive overload during subjective experiments. We organized a subjective experiment involving 40 participants to verify the validity of the proposed software. Experimental analyses demonstrate that subjective tests on our software can produce reasonable subjective quality scores of 3D models. All resources in this paper can be found at https://openi.pcl.ac.cn/OpenDatasets/3DQA.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2307.09279.pdf' target='_blank'>https://arxiv.org/pdf/2307.09279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Wang, Jian Xiong, Hao Gao, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09279">Regression-free Blind Image Quality Assessment with Content-Distortion Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The optimization objective of regression-based blind image quality assessment (IQA) models is to minimize the mean prediction error across the training dataset, which can lead to biased parameter estimation due to potential training data biases. To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is based upon retrieving locally similar instances by incorporating semantic and distortion feature spaces. The approach is motivated by the observation that the human visual system (HVS) exhibits analogous perceptual responses to semantically similar image contents impaired by identical distortions, which we term as content-distortion consistency. The proposed method constructs a hierarchical k-nearest neighbor (k-NN) algorithm for instance retrieval through two classification modules: semantic classification (SC) module and distortion classification (DC) module. Given a test image and an IQA database, the SC module retrieves multiple pristine images semantically similar to the test image. The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image. Finally, quality prediction is obtained by aggregating the subjective scores of the retrieved instances. Without training on subjective quality scores, the proposed regression-free method achieves competitive, even superior performance compared to state-of-the-art regression-based methods on authentic and synthetic distortion IQA benchmarks.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2303.15068.pdf' target='_blank'>https://arxiv.org/pdf/2303.15068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Firas Bayram, Bestoun S. Ahmed, Erik Hallin, Anton Engman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15068">DQSOps: Data Quality Scoring Operations Framework for Data-Driven Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data quality assessment has become a prominent component in the successful execution of complex data-driven artificial intelligence (AI) software systems. In practice, real-world applications generate huge volumes of data at speeds. These data streams require analysis and preprocessing before being permanently stored or used in a learning task. Therefore, significant attention has been paid to the systematic management and construction of high-quality datasets. Nevertheless, managing voluminous and high-velocity data streams is usually performed manually (i.e. offline), making it an impractical strategy in production environments. To address this challenge, DataOps has emerged to achieve life-cycle automation of data processes using DevOps principles. However, determining the data quality based on a fitness scale constitutes a complex task within the framework of DataOps. This paper presents a novel Data Quality Scoring Operations (DQSOps) framework that yields a quality score for production data in DataOps workflows. The framework incorporates two scoring approaches, an ML prediction-based approach that predicts the data quality score and a standard-based approach that periodically produces the ground-truth scores based on assessing several data quality dimensions. We deploy the DQSOps framework in a real-world industrial use case. The results show that DQSOps achieves significant computational speedup rates compared to the conventional approach of data quality scoring while maintaining high prediction performance.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2301.04471.pdf' target='_blank'>https://arxiv.org/pdf/2301.04471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesca Venturini, Michela Sperti, Umberto Michelucci, Arnaud Gucciardi, Vanessa M. Martos, Marco A. Deriu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04471">Dataset of Fluorescence Spectra and Chemical Parameters of Olive Oils</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This dataset encompasses fluorescence spectra and chemical parameters of 24 olive oil samples from the 2019-2020 harvest provided by the producer Conde de Benalua, Granada, Spain. The oils are characterized by different qualities: 10 extra virgin olive oil (EVOO), 8 virgin olive oil (VOO), and 6 lampante olive oil (LOO) samples. For each sample, the dataset includes fluorescence spectra obtained with two excitation wavelengths, oil quality, and five chemical parameters necessary for the quality assessment of olive oil. The fluorescence spectra were obtained by exciting the samples at 365 nm and 395 nm under identical conditions. The dataset includes the values of the following chemical parameters for each olive oil sample: acidity, peroxide value, K270, K232, ethyl esters, and the quality of the samples (EVOO, VOO, or LOO). The dataset offers a unique possibility for researchers in food technology to develop machine learning models based on fluorescence data for the quality assessment of olive oil due to the availability of both spectroscopic and chemical data. The dataset can be used, for example, to predict one or multiple chemical parameters or to classify samples based on their quality from fluorescence spectra.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2203.02458.pdf' target='_blank'>https://arxiv.org/pdf/2203.02458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DÃ¡vid JavorskÃ½, Dominik MachÃ¡Äek, OndÅej Bojar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.02458">Continuous Rating as Reliable Human Evaluation of Simultaneous Speech Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous speech translation (SST) can be evaluated on simulated online events where human evaluators watch subtitled videos and continuously express their satisfaction by pressing buttons (so called Continuous Rating). Continuous Rating is easy to collect, but little is known about its reliability, or relation to comprehension of foreign language document by SST users. In this paper, we contrast Continuous Rating with factual questionnaires on judges with different levels of source language knowledge. Our results show that Continuous Rating is easy and reliable SST quality assessment if the judges have at least limited knowledge of the source language. Our study indicates users' preferences on subtitle layout and presentation style and, most importantly, provides a significant evidence that users with advanced source language knowledge prefer low latency over fewer re-translations.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2108.06144.pdf' target='_blank'>https://arxiv.org/pdf/2108.06144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuseppe Scarpa, Matteo Ciotola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.06144">Full-resolution quality assessment for pansharpening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A reliable quality assessment procedure for pansharpening methods is of critical importance for the development of the related solutions. Unfortunately, the lack of ground-truths to be used as guidance for an objective evaluation has pushed the community to resort to two approaches which can also be jointly applied. Hence, two kinds of indexes can be found in the literature: i) reference-based reduced-resolution indexes aimed to assess the synthesis ability; ii) no-reference subjective quality indexes for full-resolution datasets aimed to assess spectral and spatial consistency. Both reference-based and no-reference indexes present critical shortcomings which motivate the community to explore new solutions. In this work, we propose an alternative no-reference full-resolution assessment framework. On one side we introduce a protocol, namely the reprojection protocol, to take care of the spectral consistency issue. On the other side, a new index of the spatial consistency between the pansharpened image and the panchromatic band at full resolution is also proposed. Experimental results carried out on different datasets/sensors demonstrate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2510.16375.pdf' target='_blank'>https://arxiv.org/pdf/2510.16375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishi Raj Sahoo, Surbhi Saswati Mohanty, Subhankar Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16375">iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road potholes pose significant safety hazards and maintenance challenges, particularly on India's diverse and under-maintained road networks. This paper presents iWatchRoadv2, a fully automated end-to-end platform for real-time pothole detection, GPS-based geotagging, and dynamic road health visualization using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000 dashcam frames capturing diverse Indian road conditions, weather patterns, and lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for accurate pothole detection. The system synchronizes OCR-extracted video timestamps with external GPS logs to precisely geolocate each detected pothole, enriching detections with comprehensive metadata, including road segment attribution and contractor information managed through an optimized backend database. iWatchRoadv2 introduces intelligent governance features that enable authorities to link road segments with contract metadata through a secure login interface. The system automatically sends alerts to contractors and officials when road health deteriorates, supporting automated accountability and warranty enforcement. The intuitive web interface delivers actionable analytics to stakeholders and the public, facilitating evidence-driven repair planning, budget allocation, and quality assessment. Our cost-effective and scalable solution streamlines frame processing and storage while supporting seamless public engagement for urban and rural deployments. By automating the complete pothole monitoring lifecycle, from detection to repair verification, iWatchRoadv2 enables data-driven smart city management, transparent governance, and sustainable improvements in road infrastructure maintenance. The platform and live demonstration are accessible at https://smlab.niser.ac.in/project/iwatchroad.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2510.13349.pdf' target='_blank'>https://arxiv.org/pdf/2510.13349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sipeng Yang, Jiayu Ji, Qingchuan Zhu, Zhiyao Yang, Xiaogang Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13349">No-Reference Rendered Video Quality Assessment: Dataset and Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment of videos is crucial for many computer graphics applications, including video games, virtual reality, and augmented reality, where visual performance has a significant impact on user experience. When test videos cannot be perfectly aligned with references or when references are unavailable, the significance of no-reference video quality assessment (NR-VQA) methods is undeniable. However, existing NR-VQA datasets and metrics are primarily focused on camera-captured videos; applying them directly to rendered videos would result in biased predictions, as rendered videos are more prone to temporal artifacts. To address this, we present a large rendering-oriented video dataset with subjective quality annotations, as well as a designed NR-VQA metric specific to rendered videos. The proposed dataset includes a wide range of 3D scenes and rendering settings, with quality scores annotated for various display types to better reflect real-world application scenarios. Building on this dataset, we calibrate our NR-VQA metric to assess rendered video quality by looking at both image quality and temporal stability. We compare our metric to existing NR-VQA metrics, demonstrating its superior performance on rendered videos. Finally, we demonstrate that our metric can be used to benchmark supersampling methods and assess frame generation strategies in real-time rendering.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2509.20878.pdf' target='_blank'>https://arxiv.org/pdf/2509.20878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabei Zhang, Qi Wang, Siyu Wu, Du Chen, Tianhe Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20878">The Unanticipated Asymmetry Between Perceptual Optimization and Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2509.18965.pdf' target='_blank'>https://arxiv.org/pdf/2509.18965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anukriti Kumar, Tanushree Padath, Lucy Lu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18965">Benchmarking PDF Accessibility Evaluation A Dataset and Framework for Assessing Automated and LLM-Based Approaches for Accessibility Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PDFs remain the dominant format for scholarly communication, despite significant accessibility challenges for blind and low-vision users. While various tools attempt to evaluate PDF accessibility, there is no standardized methodology to evaluate how different accessibility assessment approaches perform. Our work addresses this critical gap by introducing a novel benchmark dataset of scholarly PDFs with expert-validated accessibility annotations across seven criteria (alternative text quality, logical reading order, semantic tagging, table structure, functional hyperlinks, color contrast, and font readability), and a four-category evaluation framework with standardized labels (Passed, Failed, Not Present, Cannot Tell) to systematically assess accessibility evaluation approaches. Using our evaluation framework, we explore whether large language models (LLMs) are capable of supporting automated accessibility evaluation. We benchmark five LLMs, which demonstrate varying capabilities in correctly assessing different accessibility criteria, with GPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models struggled in correctly categorizing documents with Not Present and Cannot Tell accessibility labels, particularly for alt text quality assessment. Our qualitative comparison with standard automated checkers reveals complementary strengths: rule-based tools excel at technical verification, while LLMs better evaluate semantic appropriateness and contextual relevance. Based on our findings, we propose a hybrid approach that would combine automated checkers, LLM evaluation, and human assessment as a future strategy for PDF accessibility evaluation.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2509.13144.pdf' target='_blank'>https://arxiv.org/pdf/2509.13144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingli Cao, Shanshan Li, Ying Fan, Danyang Li, Chenxing Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13144">Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: The rapid advancement of large language models (LLMs) has given rise to AI-native applications, a new paradigm in software engineering that fundamentally redefines how software is designed, developed, and evolved. Despite their growing prominence, AI-native applications still lack a unified engineering definition and architectural blueprint, leaving practitioners without systematic guidance for system design, quality assurance, and technology selection. Objective: This study seeks to establish a comprehensive understanding of AI-native applications by identifying their defining characteristics, key quality attributes, and typical technology stacks, as well as by clarifying the opportunities and challenges they present. Method: We conducted a grey literature review, integrating conceptual perspectives retrieved from targeted Google and Bing searches with practical insights derived from leading open-source projects on GitHub. A structured protocol encompassing source selection, quality assessment, and thematic analysis was applied to synthesize findings across heterogeneous sources. Results: We finally identified 106 studies based on the selection criteria. The analysis reveals that AI-native applications are distinguished by two core pillars: the central role of AI as the system's intelligence paradigm and their inherently probabilistic, non-deterministic nature. Critical quality attributes include reliability, usability, performance efficiency, and AI-specific observability. In addition, a typical technology stack has begun to emerge, comprising LLM orchestration frameworks, vector databases, and AI-native observability platforms. These systems emphasize response quality, cost-effectiveness, and outcome predictability, setting them apart from conventional software systems. Conclusion: This study is the first to propose a dual-layered engineering blueprint...
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2508.15130.pdf' target='_blank'>https://arxiv.org/pdf/2508.15130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaishnav Ramesh, Haining Wang, Md Jahidul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15130">HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in no-reference image quality assessment (NR-IQA), dataset biases and reliance on subjective labels continue to hinder their generalization performance. We propose HiRQA, Hierarchical Ranking and Quality Alignment), a self-supervised, opinion-unaware framework that offers a hierarchical, quality-aware embedding through a combination of ranking and contrastive learning. Unlike prior approaches that depend on pristine references or auxiliary modalities at inference time, HiRQA predicts quality scores using only the input image. We introduce a novel higher-order ranking loss that supervises quality predictions through relational ordering across distortion pairs, along with an embedding distance loss that enforces consistency between feature distances and perceptual differences. A training-time contrastive alignment loss, guided by structured textual prompts, further enhances the learned representation. Trained only on synthetic distortions, HiRQA generalizes effectively to authentic degradations, as demonstrated through evaluation on various distortions such as lens flare, haze, motion blur, and low-light conditions. For real-time deployment, we introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of only 3.5 ms per image. Extensive experiments across synthetic and authentic benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong generalization ability, and scalability.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2508.10860.pdf' target='_blank'>https://arxiv.org/pdf/2508.10860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaokun Jiang, Ziyin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10860">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2507.22305.pdf' target='_blank'>https://arxiv.org/pdf/2507.22305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carolina CortÃ©s, Lisa Ehrlinger, Lorena Etcheverry, Felix Naumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22305">Is SHACL Suitable for Data Quality Assessment?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs have been widely adopted in both enterprises, such as the Google Knowledge Graph, and open platforms like Wikidata, to represent domain knowledge and support artificial intelligence applications. They model real-world information as nodes and edges. To embrace flexibility, knowledge graphs often lack enforced schemas (i.e., ontologies), leading to potential data quality issues, such as semantically overlapping nodes. Yet ensuring their quality is essential, as issues in the data can affect applications relying on them. To assess the quality of knowledge graphs, existing works propose either high-level frameworks comprising various data quality dimensions without concrete implementations, define tools that measure data quality with ad-hoc SPARQL queries, or promote the usage of constraint languages, such as the Shapes Constraint Language (SHACL), to assess and improve the quality of the graph. Although the latter approaches claim to address data quality assessment, none of them comprehensively tries to cover all data quality dimensions. In this paper, we explore this gap by investigating the extent to which SHACL can be used to assess data quality in knowledge graphs. Specifically, we defined SHACL shapes for 69 data quality metrics proposed by Zaveri et al. [1] and implemented a prototype that automatically instantiates these shapes and computes the corresponding data quality measures from their validation results. All resources are provided for repeatability.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2507.12669.pdf' target='_blank'>https://arxiv.org/pdf/2507.12669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananya Raghu, Anisha Raghu, Alice S. Tang, Yannis M. Paulus, Tyson N. Kim, Tomiko T. Oskotsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12669">InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background/Objectives: Age-related macular degeneration, glaucoma, diabetic retinopathy (DR), diabetic macular edema, and pathological myopia affect hundreds of millions of people worldwide. Early screening for these diseases is essential, yet access to medical care remains limited in low- and middle-income countries as well as in resource-limited settings. We develop InSight, an AI-based app that combines patient metadata with fundus images for accurate diagnosis of five common eye diseases to improve accessibility of screenings.
  Methods: InSight features a three-stage pipeline: real-time image quality assessment, disease diagnosis model, and a DR grading model to assess severity. Our disease diagnosis model incorporates three key innovations: (a) Multimodal fusion technique (MetaFusion) combining clinical metadata and images; (b) Pretraining method leveraging supervised and self-supervised loss functions; and (c) Multitask model to simultaneously predict 5 diseases. We make use of BRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets, both of which also contain clinical metadata for model training/evaluation.
  Results: Trained on a dataset of BRSET and mBRSET images, the image quality checker achieves near-100% accuracy in filtering out low-quality fundus images. The multimodal pretrained disease diagnosis model outperforms models using only images by 6% in balanced accuracy for BRSET and 4% for mBRSET.
  Conclusions: The InSight pipeline demonstrates robustness across varied image conditions and has high diagnostic accuracy across all five diseases, generalizing to both smartphone and lab captured images. The multitask model contributes to the lightweight nature of the pipeline, making it five times computationally efficient compared to having five individual models corresponding to each disease.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2507.09732.pdf' target='_blank'>https://arxiv.org/pdf/2507.09732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara Si-Moussi, Stephan Hennekens, Sander Mucher, Stan Los, Wilfried Thuiller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09732">Continental scale habitat modelling with artificial intelligence and multimodal earth observation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Habitats integrate the abiotic conditions and biophysical structures that support biodiversity and sustain nature's contributions to people. As these ecosystems face mounting pressure from human activities, accurate, high-resolution habitat maps are essential for effective conservation and restoration. Yet current maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicate multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat classification over large geographic extents at fine thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled Level 3 EUNIS habitats across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat nomenclatures resolved classification ambiguities, especially in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic aperture radar (SAR) imagery, particularly through Earth Observation Foundation models, enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted accuracy further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of dynamic habitats, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in-situ observations.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2507.03478.pdf' target='_blank'>https://arxiv.org/pdf/2507.03478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Breger, Janek GrÃ¶hl, Clemens Karner, Thomas R Else, Ian Selby, Jonathan Weir-McCall, Carola-Bibiane SchÃ¶nlieb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03478">PhotIQA: A photoacoustic image data set with image quality ratings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) is crucial in the evaluation stage of novel algorithms operating on images, including traditional and machine learning based methods. Due to the lack of available quality-rated medical images, most commonly used IQA methods employing reference images (i.e. full-reference IQA) have been developed and tested for natural images. Reported application inconsistencies arising when employing such measures for medical images are not surprising, as they rely on different properties than natural images. In photoacoustic imaging (PAI), especially, standard benchmarking approaches for assessing the quality of image reconstructions are lacking. PAI is a multi-physics imaging modality, in which two inverse problems have to be solved, which makes the application of IQA measures uniquely challenging due to both, acoustic and optical, artifacts.
  To support the development and testing of full- and no-reference IQA measures we assembled PhotIQA, a data set consisting of 1134 reconstructed photoacoustic (PA) images that were rated by 2 experts across five quality properties (overall quality, edge visibility, homogeneity, inclusion and background intensity), where the detailed rating enables usage beyond PAI. To allow full-reference assessment, highly characterised imaging test objects were used, providing a ground truth. Our baseline experiments show that HaarPSI$_{med}$ significantly outperforms SSIM in correlating with the quality ratings (SRCC: 0.83 vs. 0.62). The dataset is publicly available at https://doi.org/10.5281/zenodo.13325196.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2506.22438.pdf' target='_blank'>https://arxiv.org/pdf/2506.22438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xumin Gao, Mark Stevens, Grzegorz Cielniak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22438">Counting with Confidence: Accurate Pest Monitoring in Water Traps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate pest population monitoring and tracking their dynamic changes are crucial for precision agriculture decision-making. A common limitation in existing vision-based automatic pest counting research is that models are typically evaluated on datasets with ground truth but deployed in real-world scenarios without assessing the reliability of counting results due to the lack of ground truth. To this end, this paper proposed a method for comprehensively evaluating pest counting confidence in the image, based on information related to counting results and external environmental conditions. First, a pest detection network is used for pest detection and counting, extracting counting result-related information. Then, the pest images undergo image quality assessment, image complexity assessment, and pest distribution uniformity assessment. And the changes in image clarity caused by stirring during image acquisition are quantified by calculating the average gradient magnitude. Notably, we designed a hypothesis-driven multi-factor sensitivity analysis method to select the optimal image quality assessment and image complexity assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for pest distribution uniformity assessment. Finally, the obtained information related to counting results and external environmental conditions is input into a regression model for prediction, resulting in the final pest counting confidence. To the best of our knowledge, this is the first study dedicated to comprehensively evaluating counting confidence in counting tasks, and quantifying the relationship between influencing factors and counting confidence through a model. Experimental results show our method reduces MSE by 31.7% and improves R2 by 15.2% on the pest counting confidence test set, compared to the baseline built primarily on information related to counting results.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2506.01655.pdf' target='_blank'>https://arxiv.org/pdf/2506.01655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattson Ogg, Caitlyn Bishop, Han Yi, Sarah Robinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01655">Self-Supervised Speech Quality Assessment (S3QA): Leveraging Speech Foundation Models for a Scalable Speech Quality Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Methods for automatically assessing speech quality are critical for many human language technologies. Behavioral ratings provided by human raters (e.g., mean opinion scores; MOS) are considered the gold standard, but they are susceptible to variability between individual raters, cannot easily be generalized across corpora, and are labor-intensive to collect, thus limiting the acoustic challenges they can quantify. Here, we present a new, scalable method for automatically assessing speech quality: the self-supervised speech quality assessment (S3QA) model. First, we processed high quality utterances from multiple speech corpora, using a wide range of acoustic manipulations intended to emulate common sources of quality degradation in the real-world: frequency filtering, reverberation, background noise, and digital compression. Second, we leveraged an existing, pre-trained speech foundation model, WavLM, to computationally derive a self-supervised training target for the level of signal degradation by calculating the cosine distances between the clean and degraded versions of each utterance in the embedding space. Next, we trained a transformer-based model to predict the cosine distance, or degradation index, given only the degraded versions of these utterances. Finally, the trained model was evaluated on unseen test corpora of synthetic mixtures, NISQA, and VOiCES. We show that the S3QA model trained on this task performs well and is aligned with both behavioral ratings (MOS), speech technology performance (automatic speech recognition) and other important features of the held-out data (e.g., microphone distances). This approach provides an automated, scalable method for assessing speech quality across a wide range of acoustic challenges, and could easily be adapted to other use cases where acoustic simulations are available.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2506.00506.pdf' target='_blank'>https://arxiv.org/pdf/2506.00506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marie KuneÅ¡ovÃ¡, AleÅ¡ PraÅ¾Ã¡k, Jan LeheÄka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00506">Quality Assessment of Noisy and Enhanced Speech with Limited Data: UWB-NTIS System for VoiceMOS 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system for non-intrusive prediction of speech quality in noisy and enhanced speech, developed for Track 3 of the VoiceMOS 2024 Challenge. The task required estimating the ITU-T P.835 metrics SIG, BAK, and OVRL without reference signals and with only 100 subjectively labeled utterances for training. Our approach uses wav2vec 2.0 with a two-stage transfer learning strategy: initial fine-tuning on automatically labeled noisy data, followed by adaptation to the challenge data. The system achieved the best performance on BAK prediction (LCC=0.867) and a very close second place in OVRL (LCC=0.711) in the official evaluation. Post-challenge experiments show that adding artificially degraded data to the first fine-tuning stage substantially improves SIG prediction, raising correlation with ground truth scores from 0.207 to 0.516. These results demonstrate that transfer learning with targeted data generation is effective for predicting P.835 scores under severe data constraints.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2505.24002.pdf' target='_blank'>https://arxiv.org/pdf/2505.24002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaishnav Ramesh, Junliang Liu, Haining Wang, Md Jahidul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24002">DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-held challenge in no-reference image quality assessment (NR-IQA) learning from human subjective perception is the lack of objective generalization to unseen natural distortions. To address this, we integrate a novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which distills scene depth and spatial features into a structure-aware representation for improved NR-IQA. This brings in the knowledge of object saliency and relative contrast of the scene for more discriminative feature learning. Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse high-level global contextual dependencies from a transformer backbone with local spatial features captured by a set of hierarchical CNN (convolutional neural network) layers. We implement TCB and Depth-CAR as multimodal attention-based projection functions to select the most informative features, which also improve training time and inference efficiency. Experimental results demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA) performance on both synthetic and authentic benchmark datasets. More importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well as in assessing natural image distortions such as low-light effects, hazy conditions, and lens flares.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2505.21831.pdf' target='_blank'>https://arxiv.org/pdf/2505.21831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Chen, Cheng-han Lee, Yixu Chen, Zaixi Shang, Hai Wei, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21831">HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HDRSDR-VQA, a large-scale video quality assessment dataset designed to facilitate comparative analysis between High Dynamic Range (HDR) and Standard Dynamic Range (SDR) content under realistic viewing conditions. The dataset comprises 960 videos generated from 54 diverse source sequences, each presented in both HDR and SDR formats across nine distortion levels. To obtain reliable perceptual quality scores, we conducted a comprehensive subjective study involving 145 participants and six consumer-grade HDR-capable televisions. A total of over 22,000 pairwise comparisons were collected and scaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets that focus on a single dynamic range format or use limited evaluation protocols, HDRSDR-VQA enables direct content-level comparison between HDR and SDR versions, supporting detailed investigations into when and why one format is preferred over the other. The open-sourced part of the dataset is publicly available to support further research in video quality assessment, content-adaptive streaming, and perceptual model development.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2504.19155.pdf' target='_blank'>https://arxiv.org/pdf/2504.19155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hussein Harb, Didier Benoit, Axel Rannou, Chi-Hieu Pham, Valentin Tissot, Bahaa Nasr, Julien Bert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19155">Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop a machine learning-based framework for accurately modeling the anode heel effect in Monte Carlo simulations of X-ray imaging systems, enabling realistic beam intensity profiles with minimal experimental calibration. Multiple regression models were trained to predict spatial intensity variations along the anode-cathode axis using experimentally acquired weights derived from beam measurements across different tube potentials. These weights captured the asymmetry introduced by the anode heel effect. A systematic fine-tuning protocol was established to minimize the number of required measurements while preserving model accuracy. The models were implemented in the OpenGATE 10 and GGEMS Monte Carlo toolkits to evaluate their integration feasibility and predictive performance. Among the tested models, gradient boosting regression (GBR) delivered the highest accuracy, with prediction errors remaining below 5% across all energy levels. The optimized fine-tuning strategy required only six detector positions per energy level, reducing measurement effort by 65%. The maximum error introduced through this fine-tuning process remained below 2%. Dose actor comparisons within Monte Carlo simulations demonstrated that the GBR-based model closely replicated clinical beam profiles and significantly outperformed conventional symmetric beam models. This study presents a robust and generalizable method for incorporating the anode heel effect into Monte Carlo simulations using machine learning. By enabling accurate, energy-dependent beam modeling with limited calibration data, the approach enhances simulation realism for applications in clinical dosimetry, image quality assessment, and radiation protection.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2504.18524.pdf' target='_blank'>https://arxiv.org/pdf/2504.18524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengjia Zhang, Samrudhdhi B. Rangrej, Tristan Aumentado-Armstrong, Afsaneh Fazly, Alex Levinshtein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18524">Augmenting Perceptual Super-Resolution via Image Quality Predictors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Super-resolution (SR), a classical inverse problem in computer vision, is inherently ill-posed, inducing a distribution of plausible solutions for every input. However, the desired result is not simply the expectation of this distribution, which is the blurry image obtained by minimizing pixelwise error, but rather the sample with the highest image quality. A variety of techniques, from perceptual metrics to adversarial losses, are employed to this end. In this work, we explore an alternative: utilizing powerful non-reference image quality assessment (NR-IQA) models in the SR context. We begin with a comprehensive analysis of NR-IQA metrics on human-derived SR data, identifying both the accuracy (human alignment) and complementarity of different metrics. Then, we explore two methods of applying NR-IQA models to SR learning: (i) altering data sampling, by building on an existing multi-ground-truth SR framework, and (ii) directly optimizing a differentiable quality score. Our results demonstrate a more human-centric perception-distortion tradeoff, focusing less on non-perceptual pixel-wise distortion, instead improving the balance between perceptual fidelity and human-tuned NR-IQA measures.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2501.14264.pdf' target='_blank'>https://arxiv.org/pdf/2501.14264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Tang, Jingru Wang, Guangwei Huang, Guannan Chen, Rui Zheng, Lian Huai, Yuyu Liu, Xingqun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14264">CDI: Blind Image Restoration Fidelity Evaluation based on Consistency with Degraded Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Blind Image Restoration (BIR) methods, based on Generative Adversarial Networks and Diffusion Models, have significantly improved visual quality. However, they present significant challenges for Image Quality Assessment (IQA), as the existing Full-Reference IQA methods often rate images with high perceptual quality poorly. In this paper, we reassess the Solution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and propose constructing a specific BIR IQA system. In stead of directly comparing a restored image with a reference image, the BIR IQA evaluates fidelity by calculating the Consistency with Degraded Image (CDI). Specifically, we propose a wavelet domain Reference Guided CDI algorithm, which can acquire the consistency with a degraded image for various types without requiring knowledge of degradation parameters. The supported degradation types include down sampling, blur, noise, JPEG and complex combined degradations etc. In addition, we propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without reference images. Finally, in order to validate the rationality of CDI, we create a new Degraded Images Switch Display Comparison Dataset (DISDCD) for subjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify that CDI is markedly superior to common Full Reference IQA methods for BIR fidelity evaluation. The source code and the DISDCD dataset will be publicly available shortly.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2501.13014.pdf' target='_blank'>https://arxiv.org/pdf/2501.13014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrii Zahorodnii, Jasper J. F. van den Bosch, Ian Charest, Christopher Summerfield, Ila R. Fiete
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13014">Paper Quality Assessment based on Individual Wisdom Metrics from Open Peer Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a data-driven framework for enhancing the accuracy and efficiency of scientific peer review through an open, bottom-up process that estimates reviewer quality. Traditional closed peer review systems, while essential for quality control, are often slow, costly, and subject to biases that can impede scientific progress. Here, we introduce a method that evaluates individual reviewer reliability by quantifying agreement with community consensus scores and applying Bayesian weighting to refine paper quality assessments. We analyze open peer review data from two major scientific conferences, and demonstrate that reviewer-specific quality scores significantly improve the reliability of paper quality estimation. Perhaps surprisingly, we find that reviewer quality scores are unrelated to authorship quality. Our model incorporates incentive structures to recognize high-quality reviewers and encourage broader coverage of submitted papers, thereby mitigating the common "rich-get-richer" pitfall of social media. These findings suggest that open peer review, with mechanisms for estimating and incentivizing reviewer quality, offers a scalable and equitable alternative for scientific publishing, with potential to enhance the speed, fairness, and transparency of the peer review process.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2501.08072.pdf' target='_blank'>https://arxiv.org/pdf/2501.08072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08072">Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two groundbreaking technologies that have revolutionized the field of Novel View Synthesis (NVS), enabling immersive photorealistic rendering and user experiences by synthesizing multiple viewpoints from a set of images of sparse views. The potential applications of NVS, such as high-quality virtual and augmented reality, detailed 3D modeling, and realistic medical organ imaging, underscore the importance of quality assessment of NVS methods from the perspective of human perception. Although some previous studies have explored subjective quality assessments for NVS technology, they still face several challenges, especially in NVS methods selection, scenario coverage, and evaluation methodology. To address these challenges, we conducted two subjective experiments for the quality assessment of NVS technologies containing both GS-based and NeRF-based methods, focusing on dynamic and real-world scenes. This study covers 360Â°, front-facing, and single-viewpoint videos while providing a richer and greater number of real scenes. Meanwhile, it's the first time to explore the impact of NVS methods in dynamic scenes with moving objects. The two types of subjective experiments help to fully comprehend the influences of different viewing paths from a human perception perspective and pave the way for future development of full-reference and no-reference quality metrics. In addition, we established a comprehensive benchmark of various state-of-the-art objective metrics on the proposed database, highlighting that existing methods still struggle to accurately capture subjective quality. The results give us some insights into the limitations of existing NVS methods and may promote the development of new NVS methods.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2411.07322.pdf' target='_blank'>https://arxiv.org/pdf/2411.07322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arianna Bunnell, Dustin Valdez, Fredrik Strand, Yannik Glaser, Peter Sadowski, John A. Shepherd
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07322">Artificial Intelligence-Informed Handheld Breast Ultrasound for Screening: A Systematic Review of Diagnostic Test Accuracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background. Breast cancer screening programs using mammography have led to significant mortality reduction in high-income countries. However, many low- and middle-income countries lack resources for mammographic screening. Handheld breast ultrasound (BUS) is a low-cost alternative but requires substantial training. Artificial intelligence (AI) enabled BUS may aid in both the detection (perception) and classification (interpretation) of breast cancer. Materials and Methods. This review (CRD42023493053) is reported in accordance with the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) and SWiM (Synthesis Without Meta-analysis) guidelines. PubMed and Google Scholar were searched from January 1, 2016 to December 12, 2023. A meta-analysis was not attempted. Studies are grouped according to their AI task type, application time, and AI task. Study quality is assessed using the QUality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool. Results. Of 763 candidate studies, 314 total full texts were reviewed. 34 studies are included. The AI tasks of included studies are as follows: 1 frame selection, 6 detection, 11 segmentation, and 16 classification. In total, 5.7 million BUS images from over 185,000 patients were used for AI training or validation. A single study included a prospective testing set. 79% of studies were at high or unclear risk of bias. Conclusion. There has been encouraging development of AI for BUS. Despite studies demonstrating high performance across all identified tasks, the evidence supporting AI-enhanced BUS generally lacks robustness. High-quality model validation will be key to realizing the potential for AI-enhanced BUS in increasing access to screening in resource-limited environments.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2409.20137.pdf' target='_blank'>https://arxiv.org/pdf/2409.20137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roland Kammerbauer, Thomas H. Schmitt, Tobias Bocklet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20137">Segmenting Wood Rot using Computer Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the woodworking industry, a huge amount of effort has to be invested into the initial quality assessment of the raw material. In this study we present an AI model to detect, quantify and localize defects on wooden logs. This model aims to both automate the quality control process and provide a more consistent and reliable quality assessment. For this purpose a dataset of 1424 sample images of wood logs is created. A total of 5 annotators possessing different levels of expertise is involved in dataset creation. An inter-annotator agreement analysis is conducted to analyze the impact of expertise on the annotation task and to highlight subjective differences in annotator judgement. We explore, train and fine-tune the state-of-the-art InternImage and ONE-PEACE architectures for semantic segmentation. The best model created achieves an average IoU of 0.71, and shows detection and quantification capabilities close to the human annotators.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2407.06780.pdf' target='_blank'>https://arxiv.org/pdf/2407.06780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Hao, Chunlin Zhong, He Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06780">CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The depth/thermal information is beneficial for detecting salient object with conventional RGB images. However, in dual-modal salient object detection (SOD) model, the robustness against noisy inputs and modality missing is crucial but rarely studied. To tackle this problem, we introduce \textbf{Co}nditional Dropout and \textbf{LA}nguage-driven(\textbf{CoLA}) framework comprising two core components. 1) Language-driven Quality Assessment (LQA): Leveraging a pretrained vision-language model with a prompt learner, the LQA recalibrates image contributions without requiring additional quality annotations. This approach effectively mitigates the impact of noisy inputs. 2) Conditional Dropout (CD): A learning method to strengthen the model's adaptability in scenarios with missing modalities, while preserving its performance under complete modalities. The CD serves as a plug-in training scheme that treats modality-missing as conditions, strengthening the overall robustness of various dual-modal SOD models. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art dual-modal SOD models, under both modality-complete and modality-missing conditions. We will release source code upon acceptance.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2407.06331.pdf' target='_blank'>https://arxiv.org/pdf/2407.06331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karthika N J, Krishnakant Bhatt, Ganesh Ramakrishnan, Preethi Jyothi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06331">LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating technical terms into lexically similar, low-resource Indian languages remains a challenge due to limited parallel data and the complexity of linguistic structures. We propose a novel use-case of Sanskrit-based segments for linguistically informed translation of such terms, leveraging subword-level similarity and morphological alignment across related languages. Our approach uses character-level segmentation to identify meaningful subword units, facilitating more accurate and context-aware translation. To enable this, we utilize a Character-level Transformer model for Sanskrit Word Segmentation (CharSS), which addresses the complexities of sandhi and morpho-phonemic changes during segmentation. We observe consistent improvements in two experimental settings for technical term translation using Sanskrit-derived segments, averaging 8.46 and 6.79 chrF++ scores, respectively. Further, we conduct a post hoc human evaluation to verify the quality assessment of the translated technical terms using automated metrics. This work has important implications for the education field, especially in creating accessible, high-quality learning materials in Indian languages. By supporting the accurate and linguistically rooted translation of technical content, our approach facilitates inclusivity and aids in bridging the resource gap for learners in low-resource language communities.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2407.04928.pdf' target='_blank'>https://arxiv.org/pdf/2407.04928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengchuang Xing, Mingjie Li, Yuan-Gen Wang, Guopu Zhu, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04928">CLIPVQA:Video Quality Assessment via CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In learning vision-language representations from web-scale data, the contrastive language-image pre-training (CLIP) mechanism has demonstrated a remarkable performance in many vision tasks. However, its application to the widely studied video quality assessment (VQA) task is still an open issue. In this paper, we propose an efficient and effective CLIP-based Transformer method for the VQA problem (CLIPVQA). Specifically, we first design an effective video frame perception paradigm with the goal of extracting the rich spatiotemporal quality and content information among video frames. Then, the spatiotemporal quality features are adequately integrated together using a self-attention mechanism to yield video-level quality representation. To utilize the quality language descriptions of videos for supervision, we develop a CLIP-based encoder for language embedding, which is then fully aggregated with the generated content information via a cross-attention module for producing video-language representation. Finally, the video-level quality and video-language representations are fused together for final video quality prediction, where a vectorized regression loss is employed for efficient end-to-end optimization. Comprehensive experiments are conducted on eight in-the-wild video datasets with diverse resolutions to evaluate the performance of CLIPVQA. The experimental results show that the proposed CLIPVQA achieves new state-of-the-art VQA performance and up to 37% better generalizability than existing benchmark VQA methods. A series of ablation studies are also performed to validate the effectiveness of each module in CLIPVQA.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2406.12448.pdf' target='_blank'>https://arxiv.org/pdf/2406.12448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophie Loizillon, Simona Bottani, StÃ©phane Mabille, Yannick Jacob, AurÃ©lien Maire, Sebastian StrÃ¶er, Didier Dormont, Olivier Colliot, Ninon Burgos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12448">Automated MRI Quality Assessment of Brain T1-weighted MRI in Clinical Data Warehouses: A Transfer Learning Approach Relying on Artefact Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of clinical data warehouses (CDWs), which contain the medical data of millions of patients, has paved the way for vast data sharing for research. The quality of MRIs gathered in CDWs differs greatly from what is observed in research settings and reflects a certain clinical reality. Consequently, a significant proportion of these images turns out to be unusable due to their poor quality. Given the massive volume of MRIs contained in CDWs, the manual rating of image quality is impossible. Thus, it is necessary to develop an automated solution capable of effectively identifying corrupted images in CDWs. This study presents an innovative transfer learning method for automated quality control of 3D gradient echo T1-weighted brain MRIs within a CDW, leveraging artefact simulation. We first intentionally corrupt images from research datasets by inducing poorer contrast, adding noise and introducing motion artefacts. Subsequently, three artefact-specific models are pre-trained using these corrupted images to detect distinct types of artefacts. Finally, the models are generalised to routine clinical data through a transfer learning technique, utilising 3660 manually annotated images. The overall image quality is inferred from the results of the three models, each designed to detect a specific type of artefact. Our method was validated on an independent test set of 385 3D gradient echo T1-weighted MRIs. Our proposed approach achieved excellent results for the detection of bad quality MRIs, with a balanced accuracy of over 87%, surpassing our previous approach by 3.5 percent points. Additionally, we achieved a satisfactory balanced accuracy of 79% for the detection of moderate quality MRIs, outperforming our previous performance by 5 percent points. Our framework provides a valuable tool for exploiting the potential of MRIs in CDWs.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2406.08526.pdf' target='_blank'>https://arxiv.org/pdf/2406.08526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangjing Huang, Qiong Wu, Jingyi Li, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08526">IMFL-AIGC: Incentive Mechanism Design for Federated Learning Empowered by Artificial Intelligence Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) has emerged as a promising paradigm that enables clients to collaboratively train a shared global model without uploading their local data. To alleviate the heterogeneous data quality among clients, artificial intelligence-generated content (AIGC) can be leveraged as a novel data synthesis technique for FL model performance enhancement. Due to various costs incurred by AIGC-empowered FL (e.g., costs of local model computation and data synthesis), however, clients are usually reluctant to participate in FL without adequate economic incentives, which leads to an unexplored critical issue for enabling AIGC-empowered FL. To fill this gap, we first devise a data quality assessment method for data samples generated by AIGC and rigorously analyze the convergence performance of FL model trained using a blend of authentic and AI-generated data samples. We then propose a data quality-aware incentive mechanism to encourage clients' participation. In light of information asymmetry incurred by clients' private multi-dimensional attributes, we investigate clients' behavior patterns and derive the server's optimal incentive strategies to minimize server's cost in terms of both model accuracy loss and incentive payments for both complete and incomplete information scenarios. Numerical results demonstrate that our proposed mechanism exhibits highest training accuracy and reduces up to 53.34% of the server's cost with real-world datasets, compared with existing benchmark mechanisms.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2404.13484.pdf' target='_blank'>https://arxiv.org/pdf/2404.13484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinau K. Venkataramanan, Cosmin Stejerean, Ioannis Katsavounidis, Hassene Tmar, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13484">Joint Quality Assessment and Example-Guided Image Processing by Disentangling Picture Appearance from Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deep learning revolution has strongly impacted low-level image processing tasks such as style/domain transfer, enhancement/restoration, and visual quality assessments. Despite often being treated separately, the aforementioned tasks share a common theme of understanding, editing, or enhancing the appearance of input images without modifying the underlying content. We leverage this observation to develop a novel disentangled representation learning method that decomposes inputs into content and appearance features. The model is trained in a self-supervised manner and we use the learned features to develop a new quality prediction model named DisQUE. We demonstrate through extensive evaluations that DisQUE achieves state-of-the-art accuracy across quality prediction tasks and distortion types. Moreover, we demonstrate that the same features may also be used for image processing tasks such as HDR tone mapping, where the desired output characteristics may be tuned using example input-output pairs.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2404.07814.pdf' target='_blank'>https://arxiv.org/pdf/2404.07814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Bott, Horacio Saggion, Nelson PerÃ©z Rojas, Martin Solis Salazar, Saul Calderon Ramirez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07814">Lexical Complexity Prediction and Lexical Simplification for Catalan and Spanish: Resource Creation, Quality Assessment, and Ethical Considerations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic lexical simplification is a task to substitute lexical items that may be unfamiliar and difficult to understand with easier and more common words. This paper presents the description and analysis of two novel datasets for lexical simplification in Spanish and Catalan. This dataset represents the first of its kind in Catalan and a substantial addition to the sparse data on automatic lexical simplification which is available for Spanish. Specifically, it is the first dataset for Spanish which includes scalar ratings of the understanding difficulty of lexical items. In addition, we present a detailed analysis aiming at assessing the appropriateness and ethical dimensions of the data for the lexical simplification task.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2404.00252.pdf' target='_blank'>https://arxiv.org/pdf/2404.00252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanglong Fan, Wen Wen, Mu Li, Yifan Peng, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00252">Learned Scanpaths Aid Blind Panoramic Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic videos have the advantage of providing an immersive and interactive viewing experience. Nevertheless, their spherical nature gives rise to various and uncertain user viewing behaviors, which poses significant challenges for panoramic video quality assessment (PVQA). In this work, we propose an end-to-end optimized, blind PVQA method with explicit modeling of user viewing patterns through visual scanpaths. Our method consists of two modules: a scanpath generator and a quality assessor. The scanpath generator is initially trained to predict future scanpaths by minimizing their expected code length and then jointly optimized with the quality assessor for quality prediction. Our blind PVQA method enables direct quality assessment of panoramic images by treating them as videos composed of identical frames. Experiments on three public panoramic image and video quality datasets, encompassing both synthetic and authentic distortions, validate the superiority of our blind PVQA model over existing methods.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2403.15061.pdf' target='_blank'>https://arxiv.org/pdf/2403.15061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinau K. Venkataramanan, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15061">Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High Dynamic Range (HDR) videos are able to represent wider ranges of contrasts and colors than Standard Dynamic Range (SDR) videos, giving more vivid experiences. Due to this, HDR videos are expected to grow into the dominant video modality of the future. However, HDR videos are incompatible with existing SDR displays, which form the majority of affordable consumer displays on the market. Because of this, HDR videos must be processed by tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited video consumers. Here, we analyze the impact of tone-mapping operators on the visual quality of streaming HDR videos. To this end, we built the first large-scale subjectively annotated open-source database of compressed tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40 unique HDR source contents. The videos in the database were labeled with more than 750,000 subjective quality annotations, collected from more than 1,600 unique human observers. We demonstrate the usefulness of the new subjective database by benchmarking objective models of visual quality on it. We envision that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant progress on HDR video tone mapping and quality assessment in the future. To this end, we make the database freely available to the community at https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2403.12024.pdf' target='_blank'>https://arxiv.org/pdf/2403.12024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo-Han Lu, Yi-Hsuan Lin, En-Shiun Annie Lee, Richard Tzong-Han Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12024">Enhancing Taiwanese Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. The study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA 2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien as well as between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus still further improves the model's Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we introduce an evaluation method incorporating back-translation and GPT-4 to ensure reliable translation quality assessment even for LRLs. The study contributes to narrowing the resource gap for Taiwanese Hokkien and empirically investigates the advantages and limitations of pre-training and fine-tuning based on LLaMA 2.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2403.00526.pdf' target='_blank'>https://arxiv.org/pdf/2403.00526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sedir Mohammed, Lisa Ehrlinger, Hazar Harmouch, Felix Naumann, Divesh Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00526">Data Quality Assessment: Challenges and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-oriented applications, their users, and even the law require data of high quality. Research has divided the rather vague notion of data quality into various dimensions, such as accuracy, consistency, and reputation. To achieve the goal of high data quality, many tools and techniques exist to clean and otherwise improve data. Yet, systematic research on actually assessing data quality in its dimensions is largely absent, and with it, the ability to gauge the success of any data cleaning effort.
  We propose five facets as ingredients to assess data quality: data, source, system, task, and human. Tapping each facet for data quality assessment poses its own challenges. We show how overcoming these challenges helps data quality assessment for those data quality dimensions mentioned in Europe's AI Act. Our work concludes with a proposal for a comprehensive data quality assessment framework.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2401.09828.pdf' target='_blank'>https://arxiv.org/pdf/2401.09828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhili Zhang, Xiangyun Hu, Jiabo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09828">Enhanced Automated Quality Assessment Network for Interactive Building Segmentation in High-Resolution Remote Sensing Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this research, we introduce the enhanced automated quality assessment network (IBS-AQSNet), an innovative solution for assessing the quality of interactive building segmentation within high-resolution remote sensing imagery. This is a new challenge in segmentation quality assessment, and our proposed IBS-AQSNet allievate this by identifying missed and mistaken segment areas. First of all, to acquire robust image features, our method combines a robust, pre-trained backbone with a lightweight counterpart for comprehensive feature extraction from imagery and segmentation results. These features are then fused through a simple combination of concatenation, convolution layers, and residual connections. Additionally, ISR-AQSNet incorporates a multi-scale differential quality assessment decoder, proficient in pinpointing areas where segmentation result is either missed or mistaken. Experiments on a newly-built EVLab-BGZ dataset, which includes over 39,198 buildings, demonstrate the superiority of the proposed method in automating segmentation quality assessment, thereby setting a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2401.03854.pdf' target='_blank'>https://arxiv.org/pdf/2401.03854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiquan Yuan, Xinyan Cao, Jinming Che, Qinyuan Wang, Sen Liang, Wei Ren, Jinlong Lin, Xixin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03854">TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images (AIGIs) from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, \textit{etc.}, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text-image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to extract features from these text prompts and generated images, respectively. To demonstrate the effectiveness of our proposed TIER method, we conduct extensive experiments on several mainstream AIGCIQA databases, including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed TIER method generally demonstrates superior performance compared to baseline in most cases.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2312.08524.pdf' target='_blank'>https://arxiv.org/pdf/2312.08524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinau K. Venkataramanan, Cosmin Stejerean, Ioannis Katsavounidis, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08524">A FUNQUE Approach to the Quality Assessment of Compressed HDR Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen steady growth in the popularity and availability of High Dynamic Range (HDR) content, particularly videos, streamed over the internet. As a result, assessing the subjective quality of HDR videos, which are generally subjected to compression, is of increasing importance. In particular, we target the task of full-reference quality assessment of compressed HDR videos. The state-of-the-art (SOTA) approach HDRMAX involves augmenting off-the-shelf video quality models, such as VMAF, with features computed on non-linearly transformed video frames. However, HDRMAX increases the computational complexity of models like VMAF. Here, we show that an efficient class of video quality prediction models named FUNQUE+ achieves SOTA accuracy. This shows that the FUNQUE+ models are flexible alternatives to VMAF that achieve higher HDR video quality prediction accuracy at lower computational cost.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2312.07852.pdf' target='_blank'>https://arxiv.org/pdf/2312.07852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Leo, Michael R. Crusoe, Laura RodrÃ­guez-Navas, RaÃ¼l Sirvent, Alexander Kanitz, Paul De Geest, Rudolf Wittner, Luca Pireddu, Daniel Garijo, JosÃ© M. FernÃ¡ndez, Iacopo Colonnelli, Matej Gallo, Tazro Ohta, Hirotaka Suetake, Salvador Capella-Gutierrez, Renske de Wit, Bruno P. Kinoshita, Stian Soiland-Reyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07852">Recording provenance of workflow runs with RO-Crate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recording the provenance of scientific computation results is key to the support of traceability, reproducibility and quality assessment of data products. Several data models have been explored to address this need, providing representations of workflow plans and their executions as well as means of packaging the resulting information for archiving and sharing. However, existing approaches tend to lack interoperable adoption across workflow management systems. In this work we present Workflow Run RO-Crate, an extension of RO-Crate (Research Object Crate) and Schema.org to capture the provenance of the execution of computational workflows at different levels of granularity and bundle together all their associated objects (inputs, outputs, code, etc.). The model is supported by a diverse, open community that runs regular meetings, discussing development, maintenance and adoption aspects. Workflow Run RO-Crate is already implemented by several workflow management systems, allowing interoperable comparisons between workflow runs from heterogeneous systems. We describe the model, its alignment to standards such as W3C PROV, and its implementation in six workflow systems. Finally, we illustrate the application of Workflow Run RO-Crate in two use cases of machine learning in the digital image analysis domain.
  A corresponding RO-Crate for this article is at https://w3id.org/ro/doi/10.5281/zenodo.10368989
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2311.15582.pdf' target='_blank'>https://arxiv.org/pdf/2311.15582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Heng Lin, Wen-Hsuan Tseng, Li-Chin Chen, Ching-Ting Tan, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15582">Lightly Weighted Automatic Audio Parameter Extraction for the Quality Assessment of Consensus Auditory-Perceptual Evaluation of Voice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Consensus Auditory-Perceptual Evaluation of Voice is a widely employed tool in clinical voice quality assessment that is significant for streaming communication among clinical professionals and benchmarking for the determination of further treatment. Currently, because the assessment relies on experienced clinicians, it tends to be inconsistent, and thus, difficult to standardize. To address this problem, we propose to leverage lightly weighted automatic audio parameter extraction, to increase the clinical relevance, reduce the complexity, and enhance the interpretability of voice quality assessment. The proposed method utilizes age, sex, and five audio parameters: jitter, absolute jitter, shimmer, harmonic-to-noise ratio (HNR), and zero crossing. A classical machine learning approach is employed. The result reveals that our approach performs similar to state-of-the-art (SOTA) methods, and outperforms the latent representation obtained by using popular audio pre-trained models. This approach provide insights into the feasibility of different feature extraction approaches for voice evaluation. Audio parameters such as jitter and the HNR are proven to be suitable for characterizing voice quality attributes, such as roughness and strain. Conversely, pre-trained models exhibit limitations in effectively addressing noise-related scorings. This study contributes toward more comprehensive and precise voice quality evaluations, achieved by a comprehensively exploring diverse assessment methodologies.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2311.15437.pdf' target='_blank'>https://arxiv.org/pdf/2311.15437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinau K. Venkataramanan, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15437">Quality Modeling Under A Relaxed Natural Scene Statistics Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information-theoretic image quality assessment (IQA) models such as Visual Information Fidelity (VIF) and Spatio-temporal Reduced Reference Entropic Differences (ST-RRED) have enjoyed great success by seamlessly integrating natural scene statistics (NSS) with information theory. The Gaussian Scale Mixture (GSM) model that governs the wavelet subband coefficients of natural images forms the foundation for these algorithms. However, the explosion of user-generated content on social media, which is typically distorted by one or more of many possible unknown impairments, has revealed the limitations of NSS-based IQA models that rely on the simple GSM model. Here, we seek to elaborate the VIF index by deriving useful properties of the Multivariate Generalized Gaussian Distribution (MGGD), and using them to study the behavior of VIF under a Generalized GSM (GGSM) model.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2311.10113.pdf' target='_blank'>https://arxiv.org/pdf/2311.10113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashvala Vinay, Alexander Lerch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10113">AQUATK: An Audio Quality Assessment Toolkit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2309.16801.pdf' target='_blank'>https://arxiv.org/pdf/2309.16801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huynh Khanh Vi Tran, Nauman Bin Ali, JÃ¼rgen BÃ¶rstler, Michael Unterkalmsteiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16801">Test-Case Quality -- Understanding Practitioners' Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Test-case quality has always been one of the major concerns in software testing. To improve test-case quality, it is important to better understand how practitioners perceive the quality of test-cases. Objective: Motivated by that need, we investigated how practitioners define test-case quality and which aspects of test-cases are important for quality assessment. Method: We conducted semi-structured interviews with professional developers, testers and test architects from a multinational software company in Sweden. Before the interviews, we asked participants for actual test cases (written in natural language) that they perceive as good, normal, and bad respectively together with rationales for their assessment. We also compared their opinions on shared test cases and contrasted their views with the relevant literature. Results: We present a quality model which consists of 11 test-case quality attributes. We also identify a misalignment in defining test-case quality among practitioners and between academia and industry, along with suggestions for improving test-case quality in industry. Conclusion: The results show that practitioners' background, including roles and working experience, are critical dimensions of how test-case quality is defined and assessed.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2309.02777.pdf' target='_blank'>https://arxiv.org/pdf/2309.02777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>VÃ­ctor M. Batlle, JosÃ© M. M. Montiel, Pascal Fua, Juan D. TardÃ³s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02777">LightNeuS: Neural Surface Reconstruction in Endoscopy using Illumination Decline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new approach to 3D reconstruction from sequences of images acquired by monocular endoscopes. It is based on two key insights. First, endoluminal cavities are watertight, a property naturally enforced by modeling them in terms of a signed distance function. Second, the scene illumination is variable. It comes from the endoscope's light sources and decays with the inverse of the squared distance to the surface. To exploit these insights, we build on NeuS, a neural implicit surface reconstruction technique with an outstanding capability to learn appearance and a SDF surface model from multiple views, but currently limited to scenes with static illumination. To remove this limitation and exploit the relation between pixel brightness and depth, we modify the NeuS architecture to explicitly account for it and introduce a calibrated photometric model of the endoscope's camera and light source. Our method is the first one to produce watertight reconstructions of whole colon sections. We demonstrate excellent accuracy on phantom imagery. Remarkably, the watertight prior combined with illumination decline, allows to complete the reconstruction of unseen portions of the surface with acceptable accuracy, paving the way to automatic quality assessment of cancer screening explorations, measuring the global percentage of observed mucosa.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2308.02756.pdf' target='_blank'>https://arxiv.org/pdf/2308.02756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jitesh Joshi, Katherine Wang, Youngjun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02756">PhysioKit: Open-source, Low-cost Physiological Computing Toolkit for Single and Multi-user Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of physiological sensors opens new opportunities to explore interactions, conduct experiments and evaluate the user experience with continuous monitoring of bodily functions. Commercial devices, however, can be costly or limit access to raw waveform data, while low-cost sensors are efforts-intensive to setup. To address these challenges, we introduce PhysioKit, an open-source, low-cost physiological computing toolkit. PhysioKit provides a one-stop pipeline consisting of (i) a sensing and data acquisition layer that can be configured in a modular manner per research needs, (ii) a software application layer that enables data acquisition, real-time visualization and machine learning (ML)-enabled signal quality assessment. This also supports basic visual biofeedback configurations and synchronized acquisition for co-located or remote multi-user settings. In a validation study with 16 participants, PhysioKit shows strong agreement with research-grade sensors on measuring heart rate and heart rate variability metrics data. Furthermore, we report usability survey results from 10 small-project teams (44 individual members in total) who used PhysioKit for 4-6 weeks, providing insights into its use cases and research benefits. Lastly, we discuss the extensibility and potential impact of the toolkit on the research community.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2307.09857.pdf' target='_blank'>https://arxiv.org/pdf/2307.09857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Azeem Aslam, Xu Wei, Hassan Khalid, Nisar Ahmed, Zhu Shuangtong, Xin Liu, Yimei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09857">Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2307.08766.pdf' target='_blank'>https://arxiv.org/pdf/2307.08766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08766">Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to assess quality of PPG signals that were labeled as good or poor quality. We used the PPG time series from a publicly available dataset and evaluated the algorithm s performance using Sensitivity (Se), Positive Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV, and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are comparable to state-of-the-art reported in the literature but using a much simpler model, indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2307.02462.pdf' target='_blank'>https://arxiv.org/pdf/2307.02462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Raina, Dimitrios Ntentia, SH Chandrashekhara, Richard Voyles, Subir Kumar Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02462">Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasound imaging is a commonly used modality for several diagnostic and therapeutic procedures. However, the diagnosis by ultrasound relies heavily on the quality of images assessed manually by sonographers, which diminishes the objectivity of the diagnosis and makes it operator-dependent. The supervised learning-based methods for automated quality assessment require manually annotated datasets, which are highly labour-intensive to acquire. These ultrasound images are low in quality and suffer from noisy annotations caused by inter-observer perceptual variations, which hampers learning efficiency. We propose an UnSupervised UltraSound image Quality assessment Network, US2QNet, that eliminates the burden and uncertainty of manual annotations. US2QNet uses the variational autoencoder embedded with the three modules, pre-processing, clustering and post-processing, to jointly enhance, extract, cluster and visualize the quality feature representation of ultrasound images. The pre-processing module uses filtering of images to point the network's attention towards salient quality features, rather than getting distracted by noise. Post-processing is proposed for visualizing the clusters of feature representations in 2D space. We validated the proposed framework for quality assessment of the urinary bladder ultrasound images. The proposed framework achieved 78% accuracy and superior performance to state-of-the-art clustering methods.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2306.14432.pdf' target='_blank'>https://arxiv.org/pdf/2306.14432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vibhoothi, Angeliki Katsenou, FranÃ§ois PitiÃ©, Katarina Domijan, Anil Kokaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14432">Subjective assessment of the impact of a content adaptive optimiser for compressing 4K HDR content with AV1</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since 2015 video dimensionality has expanded to higher spatial and temporal resolutions and a wider colour gamut. This High Dynamic Range (HDR) content has gained traction in the consumer space as it delivers an enhanced quality of experience. At the same time, the complexity of codecs is growing. This has driven the development of tools for content-adaptive optimisation that achieve optimal rate-distortion performance for HDR video at 4K resolution. While improvements of just a few percentage points in BD-Rate (1-5\%) are significant for the streaming media industry, the impact on subjective quality has been less studied especially for HDR/AV1. In this paper, we conduct a subjective quality assessment (42 subjects) of 4K HDR content with a per-clip optimisation strategy. We correlate these subjective scores with existing popular objective metrics used in standard development and show that some perceptual metrics correlate surprisingly well even though they are not tuned for HDR. We find that the DSQCS protocol is too insensitive to categorically compare the methods but the data allows us to make recommendations about the use of experts vs non-experts in HDR studies, and explain the subjective impact of film grain in HDR content under compression.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2306.12298.pdf' target='_blank'>https://arxiv.org/pdf/2306.12298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengchuang Xing, Yuan-Gen Wang, Weixuan Tang, Guopu Zhu, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12298">StarVQA+: Co-training Space-Time Attention for Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-attention based Transformer has achieved great success in many computer vision tasks. However, its application to video quality assessment (VQA) has not been satisfactory so far. Evaluating the quality of in-the-wild videos is challenging due to the unknown of pristine reference and shooting distortion. This paper presents a co-trained Space-Time Attention network for the VQA problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately concatenating the divided space-time attention. Then, to facilitate the training of StarVQA+, we design a vectorized regression loss by encoding the mean opinion score (MOS) to the probability vector and embedding a special token as the learnable variable of MOS, leading to better fitting of human's rating process. Finally, to solve the data hungry problem with Transformer, we propose to co-train the spatial and temporal attention weights using both images and videos. Various experiments are conducted on the de-facto in-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k, YouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate the superiority of the proposed StarVQA+ over the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2305.17260.pdf' target='_blank'>https://arxiv.org/pdf/2305.17260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avinab Saha, Yu-Chih Chen, Chase Davis, Bo Qiu, Xiaoming Wang, Rahul Gowda, Ioannis Katsavounidis, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17260">Study of Subjective and Objective Quality Assessment of Mobile Cloud Gaming Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the outcomes of a recent large-scale subjective study of Mobile Cloud Gaming Video Quality Assessment (MCG-VQA) on a diverse set of gaming videos. Rapid advancements in cloud services, faster video encoding technologies, and increased access to high-speed, low-latency wireless internet have all contributed to the exponential growth of the Mobile Cloud Gaming industry. Consequently, the development of methods to assess the quality of real-time video feeds to end-users of cloud gaming platforms has become increasingly important. However, due to the lack of a large-scale public Mobile Cloud Gaming Video dataset containing a diverse set of distorted videos with corresponding subjective scores, there has been limited work on the development of MCG-VQA models. Towards accelerating progress towards these goals, we created a new dataset, named the LIVE-Meta Mobile Cloud Gaming (LIVE-Meta-MCG) video quality database, composed of 600 landscape and portrait gaming videos, on which we collected 14,400 subjective quality ratings from an in-lab subjective study. Additionally, to demonstrate the usefulness of the new resource, we benchmarked multiple state-of-the-art VQA algorithms on the database. The new database will be made publicly available on our website: \url{https://live.ece.utexas.edu/research/LIVE-Meta-Mobile-Cloud-Gaming/index.html}
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2305.15127.pdf' target='_blank'>https://arxiv.org/pdf/2305.15127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenz Diener, Marju Purin, Sten Sootla, Ando Saabas, Robert Aichner, Ross Cutler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15127">PLCMOS -- a data-driven non-intrusive metric for the evaluation of packet loss concealment algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment is a problem for every researcher working on models that produce or process speech. Human subjective ratings, the gold standard in speech quality assessment, are expensive and time-consuming to acquire in a quantity that is sufficient to get reliable data, while automated objective metrics show a low correlation with gold standard ratings. This paper presents PLCMOS, a non-intrusive data-driven tool for generating a robust, accurate estimate of the mean opinion score a human rater would assign an audio file that has been processed by being transmitted over a degraded packet-switched network with missing packets being healed by a packet loss concealment algorithm. Our new model shows a model-wise Pearson's correlation of ~0.97 and rank correlation of ~0.95 with human ratings, substantially above all other available intrusive and non-intrusive metrics. The model is released as an ONNX model for other researchers to use when building PLC systems.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2305.14935.pdf' target='_blank'>https://arxiv.org/pdf/2305.14935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timon Ziegenbein, Shahbaz Syed, Felix Lange, Martin Potthast, Henning Wachsmuth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14935">Modeling Appropriate Language in Argumentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the first time. In particular, we model appropriateness through the absence of flaws, grounded in research on argument quality assessment, especially in aspects from rhetoric. From these, we derive a new taxonomy of 14 dimensions that determine inappropriate language in online discussions. Building on three argument quality corpora, we then create a corpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses support that the taxonomy covers the concept of appropriateness comprehensively, showing several plausible correlations with argument quality dimensions. Moreover, results of baseline approaches to assessing appropriateness suggest that all dimensions can be modeled computationally on the corpus.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2304.13156.pdf' target='_blank'>https://arxiv.org/pdf/2304.13156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua P. Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Sriram Sethuraman, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13156">HDR-ChipQA: No-Reference Quality Assessment on High Dynamic Range Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a no-reference video quality model and algorithm that delivers standout performance for High Dynamic Range (HDR) videos, which we call HDR-ChipQA. HDR videos represent wider ranges of luminances, details, and colors than Standard Dynamic Range (SDR) videos. The growing adoption of HDR in massively scaled video networks has driven the need for video quality assessment (VQA) algorithms that better account for distortions on HDR content. In particular, standard VQA models may fail to capture conspicuous distortions at the extreme ends of the dynamic range, because the features that drive them may be dominated by distortions {that pervade the mid-ranges of the signal}. We introduce a new approach whereby a local expansive nonlinearity emphasizes distortions occurring at the higher and lower ends of the {local} luma range, allowing for the definition of additional quality-aware features that are computed along a separate path. These features are not HDR-specific, and also improve VQA on SDR video contents, albeit to a reduced degree. We show that this preprocessing step significantly boosts the power of distortion-sensitive natural video statistics (NVS) features when used to predict the quality of HDR content. In similar manner, we separately compute novel wide-gamut color features using the same nonlinear processing steps. We have found that our model significantly outperforms SDR VQA algorithms on the only publicly available, comprehensive HDR database, while also attaining state-of-the-art performance on SDR content.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2304.13092.pdf' target='_blank'>https://arxiv.org/pdf/2304.13092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua P. Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Sriram Sethuraman, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13092">Making Video Quality Assessment Models Robust to Bit Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel feature set, which we call HDRMAX features, that when included into Video Quality Assessment (VQA) algorithms designed for Standard Dynamic Range (SDR) videos, sensitizes them to distortions of High Dynamic Range (HDR) videos that are inadequately accounted for by these algorithms. While these features are not specific to HDR, and also augment the equality prediction performances of VQA models on SDR content, they are especially effective on HDR. HDRMAX features modify powerful priors drawn from Natural Video Statistics (NVS) models by enhancing their measurability where they visually impact the brightest and darkest local portions of videos, thereby capturing distortions that are often poorly accounted for by existing VQA models. As a demonstration of the efficacy of our approach, we show that, while current state-of-the-art VQA models perform poorly on 10-bit HDR databases, their performances are greatly improved by the inclusion of HDRMAX features when tested on HDR and 10-bit distorted videos.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2304.05464.pdf' target='_blank'>https://arxiv.org/pdf/2304.05464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Ebel, Vivien Sainte Fare Garnot, Michael Schmitt, Jan Dirk Wegner, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05464">UnCRtainTS: Uncertainty Quantification for Cloud Removal in Optical Satellite Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clouds and haze often occlude optical satellite images, hindering continuous, dense monitoring of the Earth's surface. Although modern deep learning methods can implicitly learn to ignore such occlusions, explicit cloud removal as pre-processing enables manual interpretation and allows training models when only few annotations are available. Cloud removal is challenging due to the wide range of occlusion scenarios -- from scenes partially visible through haze, to completely opaque cloud coverage. Furthermore, integrating reconstructed images in downstream applications would greatly benefit from trustworthy quality assessment. In this paper, we introduce UnCRtainTS, a method for multi-temporal cloud removal combining a novel attention-based architecture, and a formulation for multivariate uncertainty prediction. These two components combined set a new state-of-the-art performance in terms of image reconstruction on two public cloud removal datasets. Additionally, we show how the well-calibrated predicted uncertainties enable a precise control of the reconstruction quality.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2210.02885.pdf' target='_blank'>https://arxiv.org/pdf/2210.02885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quentin Garrido, Randall Balestriero, Laurent Najman, Yann Lecun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.02885">RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2209.08165.pdf' target='_blank'>https://arxiv.org/pdf/2209.08165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pooja Rani, Arianna Blasi, Nataliia Stulova, Sebastiano Panichella, Alessandra Gorla, Oscar Nierstrasz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08165">A Decade of Code Comment Quality Assessment: A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code comments are important artifacts in software systems and play a paramount role in many software engineering (SE) tasks related to maintenance and program comprehension. However, while it is widely accepted that high quality matters in code comments just as it matters in source code, assessing comment quality in practice is still an open problem. First and foremost, there is no unique definition of quality when it comes to evaluating code comments. The few existing studies on this topic rather focus on specific attributes of quality that can be easily quantified and measured. Existing techniques and corresponding tools may also focus on comments bound to a specific programming language, and may only deal with comments with specific scopes and clear goals (e.g., Javadoc comments at the method level, or in-body comments describing TODOs to be addressed). In this paper, we present a Systematic Literature Review (SLR) of the last decade of research in SE to answer the following research questions: (i) What types of comments do researchers focus on when assessing comment quality? (ii) What quality attributes (QAs) do they consider? (iii) Which tools and techniques do they use to assess comment quality?, and (iv) How do they evaluate their studies on comment quality assessment in general? Our evaluation, based on the analysis of 2353 papers and the actual review of 47 relevant ones, shows that (i) most studies and techniques focus on comments in Java code, thus may not be generalizable to other languages, and (ii) the analyzed studies focus on four main QAs of a total of 21 QAs identified in the literature, with a clear predominance of checking consistency between comments and the code. We observe that researchers rely on manual assessment and specific heuristics rather than the automated assessment of the comment quality attributes.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2205.01676.pdf' target='_blank'>https://arxiv.org/pdf/2205.01676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Or Abramovich, Hadas Pizem, Jan Van Eijgen, Ilan Oren, Joshua Melamed, Ingeborg Stalmans, Eytan Z. Blumenthal, Joachim A. Behar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.01676">FundusQ-Net: a Regression Quality Assessment Deep Learning Algorithm for Fundus Images Quality Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Ophthalmological pathologies such as glaucoma, diabetic retinopathy and age-related macular degeneration are major causes of blindness and vision impairment. There is a need for novel decision support tools that can simplify and speed up the diagnosis of these pathologies. A key step in this process is to automatically estimate the quality of the fundus images to make sure these are interpretable by a human operator or a machine learning model. We present a novel fundus image quality scale and deep learning (DL) model that can estimate fundus image quality relative to this new scale.
  Methods: A total of 1,245 images were graded for quality by two ophthalmologists within the range 1-10, with a resolution of 0.5. A DL regression model was trained for fundus image quality assessment. The architecture used was Inception-V3. The model was developed using a total of 89,947 images from 6 databases, of which 1,245 were labeled by the specialists and the remaining 88,702 images were used for pre-training and semi-supervised learning. The final DL model was evaluated on an internal test set (n=209) as well as an external test set (n=194).
  Results: The final DL model, denoted FundusQ-Net, achieved a mean absolute error of 0.61 (0.54-0.68) on the internal test set. When evaluated as a binary classification model on the public DRIMDB database as an external test set the model obtained an accuracy of 99%.
  Significance: the proposed algorithm provides a new robust tool for automated quality grading of fundus images.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2107.12003.pdf' target='_blank'>https://arxiv.org/pdf/2107.12003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Se-Yun Um, Jihyun Kim, Jihyun Lee, Hong-Goo Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.12003">Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results. Specifically, we evaluate the performances of linguistic features by measuring their accuracy on an automatic speech recognition task. In addition, we estimate speaker and gender similarity for multi-speaker and unseen conditions, respectively. We also evaluate the aturalness of the synthesized speech waveforms using a mean opinion score (MOS) test and non-intrusive objective speech quality assessment (NISQA).The demo samples of the proposed and other models are available at https://sam-0927.github.io/
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2106.04852.pdf' target='_blank'>https://arxiv.org/pdf/2106.04852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoyun Peng, Min Liu, Zhaoning Zhang, Kai Xu, Dongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.04852">Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face recognition has made significant progress in recent years due to deep convolutional neural networks (CNN). In many face recognition (FR) scenarios, face images are acquired from a sequence with huge intra-variations. These intra-variations, which are mainly affected by the low-quality face images, cause instability of recognition performance. Previous works have focused on ad-hoc methods to select frames from a video or use face image quality assessment (FIQA) methods, which consider only a particular or combination of several distortions.
  In this work, we present an efficient non-reference image quality assessment for FR that directly links image quality assessment (IQA) and FR. More specifically, we propose a new measurement to evaluate image quality without any reference. Based on the proposed quality measurement, we propose a deep Tiny Face Quality network (tinyFQnet) to learn a quality prediction function from data.
  We evaluate the proposed method for different powerful FR models on two classical video-based (or template-based) benchmark: IJB-B and YTF. Extensive experiments show that, although the tinyFQnet is much smaller than the others, the proposed method outperforms state-of-the-art quality assessment methods in terms of effectiveness and efficiency.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2103.07666.pdf' target='_blank'>https://arxiv.org/pdf/2103.07666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simeng Sun, Tao Yu, Jiahua Xu, Wei Zhou, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.07666">GraphIQA: Learning Distortion Graph Representations for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A good distortion representation is crucial for the success of deep blind image quality assessment (BIQA). However, most previous methods do not effectively model the relationship between distortions or the distribution of samples with the same distortion type but different distortion levels. In this work, we start from the analysis of the relationship between perceptual image quality and distortion-related factors, such as distortion types and levels. Then, we propose a Distortion Graph Representation (DGR) learning framework for IQA, named GraphIQA, in which each distortion is represented as a graph, i.e., DGR. One can distinguish distortion types by learning the contrast relationship between these different DGRs, and infer the ranking distribution of samples from different levels in a DGR. Specifically, we develop two sub-networks to learn the DGRs: a) Type Discrimination Network (TDN) that aims to embed DGR into a compact code for better discriminating distortion types and learning the relationship between types; b) Fuzzy Prediction Network (FPN) that aims to extract the distributional characteristics of the samples in a DGR and predicts fuzzy degrees based on a Gaussian prior. Experiments show that our GraphIQA achieves the state-of-the-art performance on many benchmark datasets of both synthetic and authentic distortions.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2511.04628.pdf' target='_blank'>https://arxiv.org/pdf/2511.04628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04628">NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2510.20210.pdf' target='_blank'>https://arxiv.org/pdf/2510.20210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hualei Wang, Na Li, Chuke Wang, Shu Wu, Zhifeng Li, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20210">Vox-Evaluator: Enhancing Stability and Fidelity for Zero-shot TTS with A Multi-Level Evaluator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in zero-shot text-to-speech (TTS), driven by language models, diffusion models and masked generation, have achieved impressive naturalness in speech synthesis. Nevertheless, stability and fidelity remain key challenges, manifesting as mispronunciations, audible noise, and quality degradation. To address these issues, we introduce Vox-Evaluator, a multi-level evaluator designed to guide the correction of erroneous speech segments and preference alignment for TTS systems. It is capable of identifying the temporal boundaries of erroneous segments and providing a holistic quality assessment of the generated speech. Specifically, to refine erroneous segments and enhance the robustness of the zero-shot TTS model, we propose to automatically identify acoustic errors with the evaluator, mask the erroneous segments, and finally regenerate speech conditioning on the correct portions. In addition, the fine-gained information obtained from Vox-Evaluator can guide the preference alignment for TTS model, thereby reducing the bad cases in speech synthesis. Due to the lack of suitable training datasets for the Vox-Evaluator, we also constructed a synthesized text-speech dataset annotated with fine-grained pronunciation errors or audio quality issues. The experimental results demonstrate the effectiveness of the proposed Vox-Evaluator in enhancing the stability and fidelity of TTS systems through the speech correction mechanism and preference optimization. The demos are shown.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2510.13624.pdf' target='_blank'>https://arxiv.org/pdf/2510.13624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13624">Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2510.08407.pdf' target='_blank'>https://arxiv.org/pdf/2510.08407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Anderson, Lucas Chatelain, Nicolas Tremblay, Kathryn Grandfield, David Rousseau, Aurélien Gourrier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08407">Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2510.05053.pdf' target='_blank'>https://arxiv.org/pdf/2510.05053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad-Ali Mahmoudpour, Saeed Mahmoudpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05053">No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrast change is an important factor that affects the quality of images. During image capturing, unfavorable lighting conditions can cause contrast change and visual quality loss. While various methods have been proposed to assess the quality of images under different distortions such as blur and noise, contrast distortion has been largely overlooked as its visual impact and properties are different from other conventional types of distortions. In this paper, we propose a no-reference image quality assessment (NR-IQA) metric for contrast-distorted images. Using a set of contrast enhancement algorithms, we aim to generate pseudo-reference images that are visually close to the actual reference image, such that the NR problem is transformed to a Full-reference (FR) assessment with higher accuracy. To this end, a large dataset of contrast-enhanced images is produced to train a classification network that can select the most suitable contrast enhancement algorithm based on image content and distortion for pseudo-reference image generation. Finally, the evaluation is performed in the FR manner to assess the quality difference between the contrast-enhanced (pseudoreference) and degraded images. Performance evaluation of the proposed method on three databases containing contrast distortions (CCID2014, TID2013, and CSIQ), indicates the promising performance of the proposed method.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2510.04859.pdf' target='_blank'>https://arxiv.org/pdf/2510.04859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Corbetta, Thomas Bocklitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04859">μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical microscopy is one of the most widely used techniques in research studies for life sciences and biomedicine. These applications require reliable experimental pipelines to extract valuable knowledge from the measured samples and must be supported by image quality assessment (IQA) to ensure correct processing and analysis of the image data. IQA methods are implemented with variable complexity. However, while most quality metrics have a straightforward implementation, they might be time consuming and computationally expensive when evaluating a large dataset. In addition, quality metrics are often designed for well-defined image features and may be unstable for images out of the ideal domain. To overcome these limitations, recent works have proposed deep learning-based IQA methods, which can provide superior performance, increased generalizability and fast prediction. Our method, named $\mathrmμ$DeepIQA, is inspired by previous studies and applies a deep convolutional neural network designed for IQA on natural images to optical microscopy measurements. We retrained the same architecture to predict individual quality metrics and global quality scores for optical microscopy data. The resulting models provide fast and stable predictions of image quality by generalizing quality estimation even outside the ideal range of standard methods. In addition, $\mathrmμ$DeepIQA provides patch-wise prediction of image quality and can be used to visualize spatially varying quality in a single image. Our study demonstrates that optical microscopy-based studies can benefit from the generalizability of deep learning models due to their stable performance in the presence of outliers, the ability to assess small image patches, and rapid predictions.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2510.00255.pdf' target='_blank'>https://arxiv.org/pdf/2510.00255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Monishwaran Maheswaran, Marco Carini, Christian Federmann, Tony Diaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00255">TASER: Translation Assessment via Systematic Evaluation and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce TASER (Translation Assessment via Systematic Evaluation and Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated translation quality assessment. TASER harnesses the explicit reasoning capabilities of LRMs to conduct systematic, step-by-step evaluation of translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across both reference-based and reference-free scenarios, demonstrating state-of-the-art performance. In system-level evaluation, TASER achieves the highest soft pairwise accuracy in both reference-based and reference-free settings, outperforming all existing metrics. At the segment level, TASER maintains competitive performance with our reference-free variant ranking as the top-performing metric among all reference-free approaches. Our experiments reveal that structured prompting templates yield superior results with LRMs compared to the open-ended approaches that proved optimal for traditional LLMs. We evaluate o3, a large reasoning model from OpenAI, with varying reasoning efforts, providing insights into the relationship between reasoning depth and evaluation quality. The explicit reasoning process in LRMs offers interpretability and visibility, addressing a key limitation of existing automated metrics. Our results demonstrate that Large Reasoning Models show a measurable advancement in translation quality assessment, combining improved accuracy with transparent evaluation across diverse language pairs.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2509.24420.pdf' target='_blank'>https://arxiv.org/pdf/2509.24420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei-Han Chen, Szu-Chi Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24420">A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In machine learning, research has traditionally focused on model development, with relatively less attention paid to training data. As model architectures have matured and marginal gains from further refinements diminish, data quality has emerged as a critical factor. However, systematic studies on evaluating and ensuring dataset quality in the image domain remain limited. This study investigates methods for systematically assessing image dataset quality and examines how various image quality factors influence model performance. Using the publicly available and relatively clean CIFAKE dataset, we identify common quality issues and quantify their impact on training. Building on these findings, we develop a pipeline that integrates two community-developed tools, CleanVision and Fastdup. We analyze their underlying mechanisms and introduce several enhancements, including automatic threshold selection to detect problematic images without manual tuning. Experimental results demonstrate that not all quality issues exert the same level of impact. While convolutional neural networks show resilience to certain distortions, they are particularly vulnerable to degradations that obscure critical visual features, such as blurring and severe downscaling. To assess the performance of existing tools and the effectiveness of our proposed enhancements, we formulate the detection of low-quality images as a binary classification task and use the F1 score as the evaluation metric. Our automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under single perturbations and from 0.7447 to 0.8557 under dual perturbations. For near-duplicate detection, our deduplication strategy increases the F1 score from 0.4576 to 0.7928. These results underscore the effectiveness of our workflow and provide a foundation for advancing data quality assessment in image-based machine learning.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2509.20028.pdf' target='_blank'>https://arxiv.org/pdf/2509.20028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cas Steigstra, Sergey Milyaev, Shaodi You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20028">Predictive Quality Assessment for Mobile Secure Graphics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reliability of secure graphic verification, a key anti-counterfeiting tool, is undermined by poor image acquisition on smartphones. Uncontrolled user captures of these high-entropy patterns cause high false rejection rates, creating a significant 'reliability gap'. To bridge this gap, we depart from traditional perceptual IQA and introduce a framework that predictively estimates a frame's utility for the downstream verification task. We propose a lightweight model to predict a quality score for a video frame, determining its suitability for a resource-intensive oracle model. Our framework is validated using re-contextualized FNMR and ISRR metrics on a large-scale dataset of 32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis on graphics from different industrial printing presses reveals a key finding: a lightweight probe on a frozen, ImageNet-pretrained network generalizes better to an unseen printing technology than a fully fine-tuned model. This provides a key insight for real-world generalization: for domain shifts from physical manufacturing, a frozen general-purpose backbone can be more robust than full fine-tuning, which can overfit to source-domain artifacts.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2509.18108.pdf' target='_blank'>https://arxiv.org/pdf/2509.18108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Viktorin, Tomas Kadavy, Jozef Kovac, Michal Pluhacek, Roman Senkerik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18108">Solve it with EASE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents EASE (Effortless Algorithmic Solution Evolution), an open-source and fully modular framework for iterative algorithmic solution generation leveraging large language models (LLMs). EASE integrates generation, testing, analysis, and evaluation into a reproducible feedback loop, giving users full control over error handling, analysis, and quality assessment. Its architecture supports the orchestration of multiple LLMs in complementary roles-such as generator, analyst, and evaluator. By abstracting the complexity of prompt design and model management, EASE provides a transparent and extensible platform for researchers and practitioners to co-design algorithms and other generative solutions across diverse domains.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2509.16715.pdf' target='_blank'>https://arxiv.org/pdf/2509.16715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Llave, Emma Granier, GrÃ©gory Pallone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16715">QASTAnet: A DNN-based Quality Metric for Spatial Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the development of spatial audio technologies, reliable and shared methods for evaluating audio quality are essential. Listening tests are currently the standard but remain costly in terms of time and resources. Several models predicting subjective scores have been proposed, but they do not generalize well to real-world signals. In this paper, we propose QASTAnet (Quality Assessment for SpaTial Audio network), a new metric based on a deep neural network, specialized on spatial audio (ambisonics and binaural). As training data is scarce, we aim for the model to be trainable with a small amount of data. To do so, we propose to rely on expert modeling of the low-level auditory system and use a neurnal network to model the high-level cognitive function of the quality judgement. We compare its performance to two reference metrics on a wide range of content types (speech, music, ambiance, anechoic, reverberated) and focusing on codec artifacts. Results demonstrate that QASTAnet overcomes the aforementioned limitations of the existing methods. The strong correlation between the proposed metric prediction and subjective scores makes it a good candidate for comparing codecs in their development.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2509.08149.pdf' target='_blank'>https://arxiv.org/pdf/2509.08149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Alexander Lee, Alexis Leconte, Alice Wu, Jonathan Poree, Maxence Laplante-Berthier, Simon Desrocher, Pierre-Olivier Bouchard, Joshua Kinugasa, Samuel Mihelic, Andreas Linninger, Jean Provost
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08149">The-Bodega: A Matlab Toolbox for Biologically Dynamic Microbubble Simulations on Realistic Hemodynamic Microvascular Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The-Bodega is a Matlab-based toolbox for simulating ground-truth datasets for Ultrasound Localization Microscopy (ULM)-a super resolution imaging technique that resolves microvessels by systematically tracking microbubbles flowing through the microvasculature. The-Bodega enables open-source simulation of stochastic microbubble dynamics through anatomically complex vascular graphs and features a quasi-automated pipeline for generating ground-truth ultrasound data from simple vascular inputs. It incorporates sequential Monte Carlo simulations augmented with Poiseuille flow distributions and dynamic pulsatile flow. A key novelty of our framework is its flexibility to accommodate arbitrary vascular architectures and benchmark common ULM algorithms, such as Fourier Ring Correlation and Singular Value Decomposition (SVD) spatiotemporal filtering, on realistic hemodynamic digital phantoms. The-Bodega supports consistent microbubble-to-ultrasound simulations across domains ranging from mouse brains to human hearts and automatically leverages available CPU/GPU parallelization to improve computational efficiency. We demonstrate its versatility in applications including image quality assessment, motion artifact analysis, and the simulation of novel ULM modalities, such as capillary imaging, myocardial reconstruction under beating heart motion, and simulating neurovascular evoked responses.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2509.06994.pdf' target='_blank'>https://arxiv.org/pdf/2509.06994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srihari Bandraupalli, Anupam Purwar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06994">VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-source Vision-Language Models show immense promise for enterprise applications, yet a critical disconnect exists between academic evaluation and enterprise deployment requirements. Current benchmarks rely heavily on multiple-choice questions and synthetic data, failing to capture the complexity of real-world business applications like social media content analysis. This paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge this gap by evaluating VLMs on operational enterprise requirements. We define ten business-critical tasks: logo detection, OCR, object detection, human presence and demographic analysis, human activity and appearance analysis, scene detection, camera perspective and media quality assessment, dominant colors, comprehensive description, and NSFW detection. To this framework, we bring an innovative BlockWeaver Algorithm that solves the challenging problem of comparing unordered, variably-grouped OCR outputs from VLMs without relying on embeddings or LLMs, achieving remarkable speed and reliability. To demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500 diverse samples, carefully stratified from a corpus of one million real-world images and videos. ViLD provides actionable insights by combining semantic matching (both embedding-based and LLM-as-a-judge approaches), traditional metrics, and novel methods to measure the completeness and faithfulness of descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and InternVL) against a powerful proprietary baseline as per ViLD framework, we provide one of the first industry-grounded, task-driven assessment of VLMs capabilities, offering actionable insights for their deployment in enterprise environments.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2509.03494.pdf' target='_blank'>https://arxiv.org/pdf/2509.03494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahya Benmahane, Mohammed El Hassouni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03494">Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel parameter-efficient adaptation method for No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base model), while keeping the underlying model fully frozen. During inference, these visual prompts are combined with images via addition and processed by mPLUG-Owl2 with the textual query "Rate the technical quality of the image." Evaluations across distortion types (synthetic, realistic, AI-generated) on KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on KADID-10k. To our knowledge, this is the first work to leverage pixel-space visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level vision tasks. The source code is publicly available at https: // github. com/ yahya-ben/ mplug2-vp-for-nriqa.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2508.14074.pdf' target='_blank'>https://arxiv.org/pdf/2508.14074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Zhang, Ruilin Zhang, Biaokai Zhu, Xun Han, Jun Xiao, Yifan Liu, Zhe Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14074">GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography has been established as an effective method for detecting Parkinson's disease, typically diagnosed early.Current Parkinson's disease detection methods have shown significant success within individual datasets, however, the variability in detection methods across different EEG datasets and the small size of each dataset pose challenges for training a generalizable model for cross-dataset scenarios. To address these issues, this paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for EEG-based cross-dataset classification of Parkinson's disease.First, we design a generative network that creates fusion EEG data by controlling the distribution similarity between generated data and real data.In addition, an EEG signal quality assessment model is designed to ensure the quality of generated data great.Second, we design a classification network that utilizes a combination of multiple convolutional neural networks to effectively capture the time-frequency characteristics of EEG signals, while maintaining a generalizable structure and ensuring easy convergence.This work is dedicated to utilizing intelligent methods to study pathological manifestations, aiming to facilitate the diagnosis and monitoring of neurological diseases.The evaluation results demonstrate that our model performs comparably to state-of-the-art models in cross-dataset settings, achieving an accuracy of 84.3% and an F1-score of 84.0%, showcasing the generalizability of the proposed model.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2508.13927.pdf' target='_blank'>https://arxiv.org/pdf/2508.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Lopes Temporao, Mickael TemporÃ£o, Corentin Vande Kerckhove, Flavio Abreu Araujo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13927">Towards a general diffusion-based information quality assessment model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid and unregulated dissemination of information in the digital era has amplified the global "infodemic," complicating the identification of high quality information. We present a lightweight, interpretable and non-invasive framework for assessing information quality based solely on diffusion dynamics, demonstrated here in the context of academic publications. Using a heterogeneous dataset of 29,264 sciences, technology, engineering, mathematics (STEM) and social science papers from ArnetMiner and OpenAlex, we model the diffusion network of each paper as a set of three theoretically motivated features: diversity, timeliness, and salience. A Generalized Additive Model (GAM) trained on these features achieved Pearson correlations of 0.834 for next-year citation gain and up to 95.62% accuracy in predicting high-impact papers. Feature relevance studies reveal timeliness and salience as the most robust predictors, while diversity offers less stable benefits in the academic setting but may be more informative in social media contexts. The framework's transparency, domain-agnostic design, and minimal feature requirements position it as a scalable tool for global information quality assessment, opening new avenues for moving beyond binary credibility labels toward richer, diffusion-informed evaluation metrics.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2508.11170.pdf' target='_blank'>https://arxiv.org/pdf/2508.11170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baihong Qian, Haotian Fan, Wenjie Liao, Yunqiu Wang, Tao Li, Junhui Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11170">Better Supervised Fine-tuning for VQA: Integer-Only Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of vision language models(VLM), their ability to assess visual content based on specific criteria and dimensions has become increasingly critical for applications such as video-theme consistency assessment and visual quality scoring. However, existing methods often suffer from imprecise results and inefficient loss calculation, which limit the focus of the model on key evaluation indicators. To address this, we propose IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to enhance their performance in video quality assessment tasks. The key innovation of IOVQA lies in its label construction and its targeted loss calculation mechanism. Specifically, during dataset curation, we constrain the model's output to integers within the range of [10,50], ensuring numerical stability, and convert decimal Overall_MOS to integer before using them as labels. We also introduce a target-mask strategy: when computing the loss, only the first two-digit-integer of the label is unmasked, forcing the model to learn the critical components of the numerical evaluation. After fine-tuning the Qwen2.5-VL model using the constructed dataset, experimental results demonstrate that the proposed method significantly improves the model's accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work highlights the effectiveness of merely leaving integer labels during fine-tuning, providing an effective idea for optimizing VLMs in quantitative evaluation scenarios.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2508.07590.pdf' target='_blank'>https://arxiv.org/pdf/2508.07590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiongwei Xiao, Baoying Chen, Jishen Zeng, Jianquan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07590">MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately assessing the perceptual quality of face images is crucial, especially with the rapid progress in face restoration and generation. Traditional quality assessment methods often struggle with the unique characteristics of face images, limiting their generalizability. While learning-based approaches demonstrate superior performance due to their strong fitting capabilities, their high complexity typically incurs significant computational and storage costs, hindering practical deployment. To address this, we propose a lightweight face quality assessment network with Multi-Stage Progressive Training (MSPT). Our network employs a three-stage progressive training strategy that gradually introduces more diverse data samples and increases input image resolution. This novel approach enables lightweight networks to achieve high performance by effectively learning complex quality features while significantly mitigating catastrophic forgetting. Our MSPT achieved the second highest score on the VQualA 2025 face image quality assessment benchmark dataset, demonstrating that MSPT achieves comparable or better performance than state-of-the-art methods while maintaining efficient inference.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2507.14454.pdf' target='_blank'>https://arxiv.org/pdf/2507.14454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Gong, Qiyue Li, Jie Li, Zhi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14454">Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian splatting video (3DGS) streaming has recently emerged as a research hotspot in both academia and industry, owing to its impressive ability to deliver immersive 3D video experiences. However, research in this area is still in its early stages, and several fundamental challenges, such as tiling, quality assessment, and bitrate adaptation, require further investigation. In this paper, we tackle these challenges by proposing a comprehensive set of solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by saliency analysis, which integrates both spatial and temporal features. Each tile is encoded into versions possessing dedicated deformation fields and multiple quality levels for adaptive selection. We also introduce a novel quality assessment framework for 3DGS video that jointly evaluates spatial-domain degradation in 3DGS representations during streaming and the quality of the resulting 2D rendered images. Additionally, we develop a meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS video streaming, achieving optimal performance across varying network conditions. Extensive experiments demonstrate that our proposed approaches significantly outperform state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2507.01076.pdf' target='_blank'>https://arxiv.org/pdf/2507.01076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vanja StojanoviÄ, Bor PangerÅ¡iÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01076">Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms -- a direct random heuristic, a hypergraph-based approximation, and a genetic algorithm -- on diverse synthetic graph datasets, including those with analytically known $Î¼(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2506.14568.pdf' target='_blank'>https://arxiv.org/pdf/2506.14568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14568">QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2506.10331.pdf' target='_blank'>https://arxiv.org/pdf/2506.10331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Zhao, Da Pan, Zelu Qi, Ping Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10331">Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In response to the rising prominence of the Metaverse, omnidirectional videos (ODVs) have garnered notable interest, gradually shifting from professional-generated content (PGC) to user-generated content (UGC). However, the study of audio-visual quality assessment (AVQA) within ODVs remains limited. To address this, we construct a dataset of UGC omnidirectional audio and video (A/V) content. The videos are captured by five individuals using two different types of omnidirectional cameras, shooting 300 videos covering 10 different scene types. A subjective AVQA experiment is conducted on the dataset to obtain the Mean Opinion Scores (MOSs) of the A/V sequences. After that, to facilitate the development of UGC-ODV AVQA fields, we construct an effective AVQA baseline model on the proposed dataset, of which the baseline model consists of video feature extraction module, audio feature extraction and audio-visual fusion module. The experimental results demonstrate that our model achieves optimal performance on the proposed dataset.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2505.24514.pdf' target='_blank'>https://arxiv.org/pdf/2505.24514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janek GrÃ¶hl, Leonid Kunyansky, Jenni Poimala, Thomas R. Else, Francesca Di Cecio, Sarah E. Bohndiek, Ben T. Cox, Andreas Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24514">Digital twins enable full-reference quality assessment of photoacoustic image reconstructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative comparison of the quality of photoacoustic image reconstruction algorithms remains a major challenge. No-reference image quality measures are often inadequate, but full-reference measures require access to an ideal reference image. While the ground truth is known in simulations, it is unknown in vivo, or in phantom studies, as the reference depends on both the phantom properties and the imaging system. We tackle this problem by using numerical digital twins of tissue-mimicking phantoms and the imaging system to perform a quantitative calibration to reduce the simulation gap. The contributions of this paper are two-fold: First, we use this digital-twin framework to compare multiple state-of-the-art reconstruction algorithms. Second, among these is a Fourier transform-based reconstruction algorithm for circular detection geometries, which we test on experimental data for the first time. Our results demonstrate the usefulness of digital phantom twins by enabling assessment of the accuracy of the numerical forward model and enabling comparison of image reconstruction schemes with full-reference image quality assessment. We show that the Fourier transform-based algorithm yields results comparable to those of iterative time reversal, but at a lower computational cost. All data and code are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2505.23301.pdf' target='_blank'>https://arxiv.org/pdf/2505.23301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-HÃ©lÃ¨ne Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23301">Quality assessment of 3D human animation: Subjective and objective evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2505.12028.pdf' target='_blank'>https://arxiv.org/pdf/2505.12028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupei Ren, Xinyi Zhou, Ning Zhang, Shangqing Zhao, Man Lan, Xiaopeng Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12028">Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2505.03638.pdf' target='_blank'>https://arxiv.org/pdf/2505.03638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawan Li, Fei Zhou, Zhipeng Zhong, Jiongzhi Lin, Guoping Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03638">Towards Smart Point-and-Shoot Photography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2504.05076.pdf' target='_blank'>https://arxiv.org/pdf/2504.05076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Liu, Qingyu Mao, Chao Li, Jiacong Chen, Fanyang Meng, Yonghong Tian, Yongsheng Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05076">Content-Distortion High-Order Interaction for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The content and distortion are widely recognized as the two primary factors affecting the visual quality of an image. While existing No-Reference Image Quality Assessment (NR-IQA) methods have modeled these factors, they fail to capture the complex interactions between content and distortions. This shortfall impairs their ability to accurately perceive quality. To confront this, we analyze the key properties required for interaction modeling and propose a robust NR-IQA approach termed CoDI-IQA (Content-Distortion high-order Interaction for NR-IQA), which aggregates local distortion and global content features within a hierarchical interaction framework. Specifically, a Progressive Perception Interaction Module (PPIM) is proposed to explicitly simulate how content and distortions independently and jointly influence image quality. By integrating internal interaction, coarse interaction, and fine interaction, it achieves high-order interaction modeling that allows the model to properly represent the underlying interaction patterns. To ensure sufficient interaction, multiple PPIMs are employed to hierarchically fuse multi-level content and distortion features at different granularities. We also tailor a training strategy suited for CoDI-IQA to maintain interaction stability. Extensive experiments demonstrate that the proposed method notably outperforms the state-of-the-art methods in terms of prediction accuracy, data efficiency, and generalization ability.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2504.01190.pdf' target='_blank'>https://arxiv.org/pdf/2504.01190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwen Zhu, Yixu Chen, Hai Wei, Sriram Sethuraman, Yongjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01190">Video Quality Assessment for Resolution Cross-Over in Live Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In adaptive bitrate streaming, resolution cross-over refers to the point on the convex hull where the encoding resolution should switch to achieve better quality. Accurate cross-over prediction is crucial for streaming providers to optimize resolution at given bandwidths. Most existing works rely on objective Video Quality Metrics (VQM), particularly VMAF, to determine the resolution cross-over. However, these metrics have limitations in accurately predicting resolution cross-overs. Furthermore, widely used VQMs are often trained on subjective datasets collected using the Absolute Category Rating (ACR) methodologies, which we demonstrate introduces significant uncertainty and errors in resolution cross-over predictions. To address these problems, we first investigate different subjective methodologies and demonstrate that Pairwise Comparison (PC) achieves better cross-over accuracy than ACR. We then propose a novel metric, Resolution Cross-over Quality Loss (RCQL), to measure the quality loss caused by resolution cross-over errors. Furthermore, we collected a new subjective dataset (LSCO) focusing on live streaming scenarios and evaluated widely used VQMs, by benchmarking their resolution cross-over accuracy.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2503.19295.pdf' target='_blank'>https://arxiv.org/pdf/2503.19295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanglu Dong, Xiangyu Liao, Mingyang Li, Guihuan Guo, Chao Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19295">Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Adversarial Networks (GANs) have been widely applied to image super-resolution (SR) to enhance the perceptual quality. However, most existing GAN-based SR methods typically perform coarse-grained discrimination directly on images and ignore the semantic information of images, making it challenging for the super resolution networks (SRN) to learn fine-grained and semantic-related texture details. To alleviate this issue, we propose a semantic feature discrimination method, SFD, for perceptual SR. Specifically, we first design a feature discriminator (Feat-D), to discriminate the pixel-wise middle semantic features from CLIP, aligning the feature distributions of SR images with that of high-quality images. Additionally, we propose a text-guided discrimination method (TG-D) by introducing learnable prompt pairs (LPP) in an adversarial manner to perform discrimination on the more abstract output feature of CLIP, further enhancing the discriminative ability of our method. With both Feat-D and TG-D, our SFD can effectively distinguish between the semantic feature distributions of low-quality and high-quality images, encouraging SRN to generate more realistic and semantic-relevant textures. Furthermore, based on the trained Feat-D and LPP, we propose a novel opinion-unaware no-reference image quality assessment (OU NR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any additional targeted training. Extensive experiments on classical SISR, real-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our proposed methods.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2503.17937.pdf' target='_blank'>https://arxiv.org/pdf/2503.17937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Zhang, Minfu Li, Lu Li, Daoyi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17937">Cross-Domain Underwater Image Enhancement Guided by No-Reference Image Quality Assessment: A Transfer Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single underwater image enhancement (UIE) is a challenging ill-posed problem, but its development is hindered by two major issues: (1) The labels in underwater reference datasets are pseudo labels, relying on these pseudo ground truths in supervised learning leads to domain discrepancy. (2) Underwater reference datasets are scarce, making training on such small datasets prone to overfitting and distribution shift. To address these challenges, we propose Trans-UIE, a transfer learning-based UIE model that captures the fundamental paradigms of UIE through pretraining and utilizes a dataset composed of both reference and non-reference datasets for fine-tuning. However, fine-tuning the model using only reconstruction loss may introduce confirmation bias. To mitigate this, our method leverages no-reference image quality assessment (NR-IQA) metrics from above-water scenes to guide the transfer learning process across domains while generating enhanced images with the style of the above-water image domain. Additionally, to reduce the risk of overfitting during the pretraining stage, we introduce Pearson correlation loss. Experimental results on both full-reference and no-reference underwater benchmark datasets demonstrate that Trans-UIE significantly outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2502.13196.pdf' target='_blank'>https://arxiv.org/pdf/2502.13196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Martin, AntÃ³nio Rodrigues, JoÃ£o Ascenso, Maria Paula Queluz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13196">GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2502.02199.pdf' target='_blank'>https://arxiv.org/pdf/2502.02199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Drinkall, Janet B. Pierrehumbert, Stefan Zohren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02199">When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2412.19553.pdf' target='_blank'>https://arxiv.org/pdf/2412.19553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19553">Structural Similarity in Deep Features: Image Quality Assessment Robust to Geometrically Disparate Reference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) with references plays an important role in optimizing and evaluating computer vision tasks. Traditional methods assume that all pixels of the reference and test images are fully aligned. Such Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world problems with various geometric deformations between the two images. Although significant effort has been made to attack Geometrically-Disparate-Reference IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for example, by dedicated designs for image super-resolution and retargeting, or by assuming the geometric distortions to be small that can be countered by translation-robust filters or by explicit image registrations. Here we rethink this problem and propose a unified, non-training-based Deep Structural Similarity (DeepSSIM) approach to address the above problems in a single framework, which assesses structural similarity of deep features in a simple but efficient way and uses an attention calibration strategy to alleviate attention deviation. The proposed method, without application-specific design, achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows strong robustness to various GDR-IQA test cases. Interestingly, our test also shows the effectiveness of DeepSSIM as an optimization tool for training image super-resolution, enhancement and restoration, implying an even wider generalizability. \footnote{Source code will be made public after the review is completed.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2412.11779.pdf' target='_blank'>https://arxiv.org/pdf/2412.11779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eren Onaran, Erdi SarÄ±taÅ, HazÄ±m Kemal Ekenel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11779">Impact of Face Alignment on Face Image Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face alignment is a crucial step in preparing face images for feature extraction in facial analysis tasks. For applications such as face recognition, facial expression recognition, and facial attribute classification, alignment is widely utilized during both training and inference to standardize the positions of key landmarks in the face. It is well known that the application and method of face alignment significantly affect the performance of facial analysis models. However, the impact of alignment on face image quality has not been thoroughly investigated. Current FIQA studies often assume alignment as a prerequisite but do not explicitly evaluate how alignment affects quality metrics, especially with the advent of modern deep learning-based detectors that integrate detection and landmark localization. To address this need, our study examines the impact of face alignment on face image quality scores. We conducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN and RetinaFace models for face detection and alignment. To evaluate face image quality, we utilized several assessment methods, including SER-FIQ, FaceQAN, DifFIQA, and SDD-FIQA. Our analysis included examining quality score distributions for the LFW and IJB-B datasets and analyzing average quality scores at varying distances in the SCFace dataset. Our findings reveal that face image quality assessment methods are sensitive to alignment. Moreover, this sensitivity increases under challenging real-life conditions, highlighting the importance of evaluating alignment's role in quality assessment.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2412.00970.pdf' target='_blank'>https://arxiv.org/pdf/2412.00970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Wang, Ruiwei Xiao, Ying-Jui Tseng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00970">Generating AI Literacy MCQs: A Multi-Agent LLM Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is transforming society, making it crucial to prepare the next generation through AI literacy in K-12 education. However, scalable and reliable AI literacy materials and assessment resources are lacking. To address this gap, our study presents a novel approach to generating multiple-choice questions (MCQs) for AI literacy assessments. Our method utilizes large language models (LLMs) to automatically generate scalable, high-quality assessment questions. These questions align with user-provided learning objectives, grade levels, and Bloom's Taxonomy levels. We introduce an iterative workflow incorporating LLM-powered critique agents to ensure the generated questions meet pedagogical standards. In the preliminary evaluation, experts expressed strong interest in using the LLM-generated MCQs, indicating that this system could enrich existing AI literacy materials and provide a valuable addition to the toolkit of K-12 educators.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2411.03708.pdf' target='_blank'>https://arxiv.org/pdf/2411.03708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehedi Hasan Raju, Samantha Aziz, Michael J. Proulx, Oleg V. Komogortsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03708">Evaluating Eye Tracking Signal Quality with Real-time Gaze Interaction Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a real-time gaze-based interaction simulation methodology using an offline dataset to evaluate the eye-tracking signal quality. This study employs three fundamental eye-movement classification algorithms to identify physiological fixations from the eye-tracking data. We introduce the Rank-1 fixation selection approach to identify the most stable fixation period nearest to a target, referred to as the trigger-event. Our evaluation explores how varying constraints impact the definition of trigger-events and evaluates the eye-tracking signal quality of defined trigger-events. Results show that while the dispersion threshold-based algorithm identifies trigger-events more accurately, the Kalman filter-based classification algorithm performs better in eye-tracking signal quality, as demonstrated through a user-centric quality assessment using user- and error-percentile tiers. Despite median user-level performance showing minor differences across algorithms, significant variability in signal quality across participants highlights the importance of algorithm selection to ensure system reliability.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2411.01268.pdf' target='_blank'>https://arxiv.org/pdf/2411.01268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Hoffstaedter, NicolÃ¡s Nieto, Simon B. Eickhoff, Kaustubh R. Patil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01268">The impact of MRI image quality on statistical and predictive analysis on voxel based morphology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality of MRI brain scans is strongly influenced by within scanner head movements and the resulting image artifacts alter derived measures like brain volume and cortical thickness. Automated image quality assessment is key to controlling for confounding effects of poor image quality. In this study, we systematically test for the influence of image quality on univariate statistics and machine learning classification. We analyzed group effects of sex/gender on local brain volume and made predictions of sex/gender using logistic regression, while correcting for brain size. From three large publicly available datasets, two age and sex-balanced samples were derived to test the generalizability of the effect for pooled sample sizes of n=760 and n=1094. Results of the Bonferroni corrected t-tests over 3747 gray matter features showed a strong influence of low-quality data on the ability to find significant sex/gender differences for the smaller sample. Increasing sample size and more so image quality showed a stark increase in detecting significant effects in univariate group comparisons. For the classification of sex/gender using logistic regression, both increasing sample size and image quality had a marginal effect on the Area under the Receiver Operating Characteristic Curve for most datasets and subsamples. Our results suggest a more stringent quality control for univariate approaches than for multivariate classification with a leaning towards higher quality for classical group statistics and bigger sample sizes for machine learning applications in neuroimaging.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2410.08576.pdf' target='_blank'>https://arxiv.org/pdf/2410.08576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikhil Bangad, Vivekananda Jayaram, Manjunatha Sughaturu Krishnappa, Amey Ram Banarse, Darshan Mohan Bidkar, Akshay Nagpal, Vidyasagar Parlapalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08576">A Theoretical Framework for AI-driven data quality monitoring in high-volume data environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a theoretical framework for an AI-driven data quality monitoring system designed to address the challenges of maintaining data quality in high-volume environments. We examine the limitations of traditional methods in managing the scale, velocity, and variety of big data and propose a conceptual approach leveraging advanced machine learning techniques. Our framework outlines a system architecture that incorporates anomaly detection, classification, and predictive analytics for real-time, scalable data quality management. Key components include an intelligent data ingestion layer, adaptive preprocessing mechanisms, context-aware feature extraction, and AI-based quality assessment modules. A continuous learning paradigm is central to our framework, ensuring adaptability to evolving data patterns and quality requirements. We also address implications for scalability, privacy, and integration within existing data ecosystems. While practical results are not provided, it lays a robust theoretical foundation for future research and implementations, advancing data quality management and encouraging the exploration of AI-driven solutions in dynamic environments.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2410.06866.pdf' target='_blank'>https://arxiv.org/pdf/2410.06866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao-Xiang Zhang, Yuan-Gen Wang, Yu Ran, Weixuan Tang, Qingxiao Guan, Chunsheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06866">Secure Video Quality Assessment Resisting Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential surge in video traffic has intensified the imperative for Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current VQA models have achieved human-comparable accuracy. However, recent studies have revealed the vulnerability of existing VQA models against adversarial attacks. To establish a reliable and practical assessment system, a secure VQA model capable of resisting such malicious attacks is urgently demanded. Unfortunately, no attempt has been made to explore this issue. This paper first attempts to investigate general adversarial defense principles, aiming at endowing existing VQA models with security. Specifically, we first introduce random spatial grid sampling on the video frame for intra-frame defense. Then, we design pixel-wise randomization through a guardian map, globally neutralizing adversarial perturbations. Meanwhile, we extract temporal information from the video sequence as compensation for inter-frame defense. Building upon these principles, we present a novel VQA framework from the security-oriented perspective, termed SecureVQA. Extensive experiments indicate that SecureVQA sets a new benchmark in security while achieving competitive VQA performance compared with state-of-the-art models. Ablation studies delve deeper into analyzing the principles of SecureVQA, demonstrating their generalization and contributions to the security of leading VQA models.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2409.19679.pdf' target='_blank'>https://arxiv.org/pdf/2409.19679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Long, Wenkang Su, Zixuan Li, Lei Cai, Mingjie Li, Yuan-Gen Wang, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19679">SemiDDM-Weather: A Semi-supervised Learning Framework for All-in-one Adverse Weather Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adverse weather removal aims to restore clear vision under adverse weather conditions. Existing methods are mostly tailored for specific weather types and rely heavily on extensive labeled data. In dealing with these two limitations, this paper presents a pioneering semi-supervised all-in-one adverse weather removal framework built on the teacher-student network with a Denoising Diffusion Model (DDM) as the backbone, termed SemiDDM-Weather. As for the design of DDM backbone in our SemiDDM-Weather, we adopt the SOTA Wavelet Diffusion Model-Wavediff with customized inputs and loss functions, devoted to facilitating the learning of many-to-one mapping distributions for efficient all-in-one adverse weather removal with limited label data. To mitigate the risk of misleading model training due to potentially inaccurate pseudo-labels generated by the teacher network in semi-supervised learning, we introduce quality assessment and content consistency constraints to screen the "optimal" outputs from the teacher network as the pseudo-labels, thus more effectively guiding the student network training with unlabeled data. Experimental results show that on both synthetic and real-world datasets, our SemiDDM-Weather consistently delivers high visual quality and superior adverse weather removal, even when compared to fully supervised competitors. Our code and pre-trained model are available at this repository.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2409.16695.pdf' target='_blank'>https://arxiv.org/pdf/2409.16695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mike Thelwall, Abdallah Yaghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16695">In which fields can ChatGPT detect journal article quality? An evaluation of REF2021 results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time spent by academics on research quality assessment might be reduced if automated approaches can help. Whilst citation-based indicators have been extensively developed and evaluated for this, they have substantial limitations and Large Language Models (LLMs) like ChatGPT provide an alternative approach. This article assesses whether ChatGPT 4o-mini can be used to estimate the quality of journal articles across academia. It samples up to 200 articles from all 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework (REF) 2021, comparing ChatGPT scores with departmental average scores. There was an almost universally positive Spearman correlation between ChatGPT scores and departmental averages, varying between 0.08 (Philosophy) and 0.78 (Psychology, Psychiatry and Neuroscience), except for Clinical Medicine (rho=-0.12). Although other explanations are possible, especially because REF score profiles are public, the results suggest that LLMs can provide reasonable research quality estimates in most areas of science, and particularly the physical and health sciences and engineering, even before citation data is available. Nevertheless, ChatGPT assessments seem to be more positive for most health and physical sciences than for other fields, a concern for multidisciplinary assessments, and the ChatGPT scores are only based on titles and abstracts, so cannot be research evaluations.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2408.05697.pdf' target='_blank'>https://arxiv.org/pdf/2408.05697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghazal Kaviani, Reza Marzban, Ghassan AlRegib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05697">Evaluating BM3D and NBNet: A Comprehensive Study of Image Denoising Across Multiple Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates image denoising, comparing traditional non-learning-based techniques, represented by Block-Matching 3D (BM3D), with modern learning-based methods, exemplified by NBNet. We assess these approaches across diverse datasets, including CURE-OR, CURE-TSR, SSID+, Set-12, and Chest-Xray, each presenting unique noise challenges. Our analysis employs seven Image Quality Assessment (IQA) metrics and examines the impact on object detection performance. We find that while BM3D excels in scenarios like blur challenges, NBNet is more effective in complex noise environments such as under-exposure and over-exposure. The study reveals the strengths and limitations of each method, providing insights into the effectiveness of different denoising strategies in varied real-world applications.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2407.11141.pdf' target='_blank'>https://arxiv.org/pdf/2407.11141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amol S. Joshi, Ali Dabouei, Jeremy Dawson, Nasser Nasrabadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11141">UFQA: Utility guided Fingerphoto Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment of fingerprints captured using digital cameras and smartphones, also called fingerphotos, is a challenging problem in biometric recognition systems. As contactless biometric modalities are gaining more attention, their reliability should also be improved. Many factors, such as illumination, image contrast, camera angle, etc., in fingerphoto acquisition introduce various types of distortion that may render the samples useless. Current quality estimation methods developed for fingerprints collected using contact-based sensors are inadequate for fingerphotos. We propose Utility guided Fingerphoto Quality Assessment (UFQA), a self-supervised dual encoder framework to learn meaningful feature representations to assess fingerphoto quality. A quality prediction model is trained to assess fingerphoto quality with additional supervision of quality maps. The quality metric is a predictor of the utility of fingerphotos in matching scenarios. Therefore, we use a holistic approach by including fingerphoto utility and local quality when labeling the training data. Experimental results verify that our approach performs better than the widely used fingerprint quality metric NFIQ2.2 and state-of-the-art image quality assessment algorithms on multiple publicly available fingerphoto datasets.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2406.19256.pdf' target='_blank'>https://arxiv.org/pdf/2406.19256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaveen Hiniduma, Suren Byna, Jean Luca Bez, Ravi Madduri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19256">AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Garbage In Garbage Out" is a universally agreed quote by computer scientists from various domains, including Artificial Intelligence (AI). As data is the fuel for AI, models trained on low-quality, biased data are often ineffective. Computer scientists who use AI invest a considerable amount of time and effort in preparing the data for AI. However, there are no standard methods or frameworks for assessing the "readiness" of data for AI. To provide a quantifiable assessment of the readiness of data for AI processes, we define parameters of AI data readiness and introduce AIDRIN (AI Data Readiness Inspector). AIDRIN is a framework covering a broad range of readiness dimensions available in the literature that aid in evaluating the readiness of data quantitatively and qualitatively. AIDRIN uses metrics in traditional data quality assessment such as completeness, outliers, and duplicates for data evaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI, such as feature importance, feature correlations, class imbalance, fairness, privacy, and FAIR (Findability, Accessibility, Interoperability, and Reusability) principle compliance. AIDRIN provides visualizations and reports to assist data scientists in further investigating the readiness of data. The AIDRIN framework enhances the efficiency of the machine learning pipeline to make informed decisions on data readiness for AI applications.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2406.17265.pdf' target='_blank'>https://arxiv.org/pdf/2406.17265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Zhang, Azim Eskandarian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17265">Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR is one of the most crucial sensors for autonomous vehicle perception. However, current LiDAR-based point cloud perception algorithms lack comprehensive and rigorous LiDAR quality assessment methods, leading to uncertainty in detection performance. Additionally, existing point cloud quality assessment algorithms are predominantly designed for indoor environments or single-object scenarios. In this paper, we introduce a novel image-guided point cloud quality assessment algorithm for outdoor autonomous driving environments, named the Image-Guided Outdoor Point Cloud Quality Assessment (IGO-PQA) algorithm. Our proposed algorithm comprises two main components. The first component is the IGO-PQA generation algorithm, which leverages point cloud data, corresponding RGB surrounding view images, and agent objects' ground truth annotations to generate an overall quality score for a single-frame LiDAR-based point cloud. The second component is a transformer-based IGO-PQA regression algorithm for no-reference outdoor point cloud quality assessment. This regression algorithm allows for the direct prediction of IGO-PQA scores in an online manner, without requiring image data and object ground truth annotations. We evaluate our proposed algorithm using the nuScenes and Waymo open datasets. The IGO-PQA generation algorithm provides consistent and reasonable perception quality indices. Furthermore, our proposed IGO-PQA regression algorithm achieves a Pearson Linear Correlation Coefficient (PLCC) of 0.86 on the nuScenes dataset and 0.97 on the Waymo dataset.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2406.09858.pdf' target='_blank'>https://arxiv.org/pdf/2406.09858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Zhou, Tianhao Gu, Zhicong Huang, Guoping Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09858">Vision Language Modeling of Content, Distortion and Appearance for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The visual quality of an image is confounded by a number of intertwined factors including its semantic content, distortion characteristics and appearance properties such as brightness, contrast, sharpness, and colourfulness. Distilling high level knowledge about all these quality bearing attributes is crucial for developing objective Image Quality Assessment (IQA).While existing solutions have modeled some of these aspects, a comprehensive solution that involves all these important quality related attributes has not yet been developed. In this paper, we present a new blind IQA (BIQA) model termed Self-supervision and Vision-Language supervision Image QUality Evaluator (SLIQUE) that features a joint vision-language and visual contrastive representation learning framework for acquiring high level knowledge about the images semantic contents, distortion characteristics and appearance properties for IQA. For training SLIQUE, we have developed a systematic approach to constructing a first of its kind large image database annotated with all three categories of quality relevant texts. The Text Annotated Distortion, Appearance and Content (TADAC) database has over 1.6 million images annotated with textual descriptions of their semantic contents, distortion characteristics and appearance properties. The method for constructing TADAC and the database itself will be particularly useful for exploiting vision-language modeling for advanced IQA applications. Extensive experimental results show that SLIQUE has superior performances over state of the art, demonstrating the soundness of its design principle and the effectiveness of its implementation.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2406.04654.pdf' target='_blank'>https://arxiv.org/pdf/2406.04654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diptanu De, Shankhanil Mitra, Rajiv Soundararajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04654">GenzIQA: Generalized Image Quality Assessment using Prompt-Guided Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of no-reference (NR) image quality assessment (IQA) algorithms is extremely important to benchmark and calibrate user experiences in modern visual systems. A major drawback of state-of-the-art NR-IQA methods is their limited ability to generalize across diverse IQA settings with reasonable distribution shifts. Recent text-to-image generative models such as latent diffusion models generate meaningful visual concepts with fine details related to text concepts. In this work, we leverage the denoising process of such diffusion models for generalized IQA by understanding the degree of alignment between learnable quality-aware text prompts and images. In particular, we learn cross-attention maps from intermediate layers of the denoiser of latent diffusion models to capture quality-aware representations of images. In addition, we also introduce learnable quality-aware text prompts that enable the cross-attention features to be better quality-aware. Our extensive cross database experiments across various user-generated, synthetic, and low-light content-based benchmarking databases show that latent diffusion models can achieve superior generalization in IQA when compared to other methods in the literature.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2406.01020.pdf' target='_blank'>https://arxiv.org/pdf/2406.01020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daekyu Kwon, Dongyoung Kim, Sehwan Ki, Younghyun Jo, Hyong-Euk Lee, Seon Joo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01020">ATTIQA: Generalizable Image Quality Feature Extractor using Attribute-aware Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In no-reference image quality assessment (NR-IQA), the challenge of limited dataset sizes hampers the development of robust and generalizable models. Conventional methods address this issue by utilizing large datasets to extract rich representations for IQA. Also, some approaches propose vision language models (VLM) based IQA, but the domain gap between generic VLM and IQA constrains their scalability. In this work, we propose a novel pretraining framework that constructs a generalizable representation for IQA by selectively extracting quality-related knowledge from VLM and leveraging the scalability of large datasets. Specifically, we select optimal text prompts for five representative image quality attributes and use VLM to generate pseudo-labels. Numerous attribute-aware pseudo-labels can be generated with large image datasets, allowing our IQA model to learn rich representations about image quality. Our approach achieves state-of-the-art performance on multiple IQA datasets and exhibits remarkable generalization capabilities. Leveraging these strengths, we propose several applications, such as evaluating image generation models and training image enhancement models, demonstrating our model's real-world applicability.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2405.20078.pdf' target='_blank'>https://arxiv.org/pdf/2405.20078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Martin, Antonio Rodrigues, Joao Ascenso, Maria Paula Queluz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20078">NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural radiance fields (NeRF) are a groundbreaking computer vision technology that enables the generation of high-quality, immersive visual content from multiple viewpoints. This capability has significant advantages for applications such as virtual/augmented reality, 3D modelling, and content creation for the film and entertainment industry. However, the evaluation of NeRF methods poses several challenges, including a lack of comprehensive datasets, reliable assessment methodologies, and objective quality metrics. This paper addresses the problem of NeRF view synthesis (NVS) quality assessment thoroughly, by conducting a rigorous subjective quality assessment test that considers several scene classes and recently proposed NVS methods. Additionally, the performance of a wide range of state-of-the-art conventional and learning-based full-reference 2D image and video quality assessment metrics is evaluated against the subjective scores of the subjective study. This study found that errors in camera pose estimation can result in spatial misalignments between synthesized and reference images, which need to be corrected before applying an objective quality metric. The experimental results are analyzed in depth, providing a comparative evaluation of several NVS methods and objective quality metrics, across different classes of visual scenes, including real and synthetic content for front-face and 360-degree camera trajectories.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2405.14221.pdf' target='_blank'>https://arxiv.org/pdf/2405.14221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhibo Chen, Heming Sun, Li Zhang, Fan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14221">Survey on Visual Signal Coding and Processing with Generative Models: Technologies, Standards and Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides a survey of the latest developments in visual signal coding and processing with generative models. Specifically, our focus is on presenting the advancement of generative models and their influence on research in the domain of visual signal coding and processing. This survey study begins with a brief introduction of well-established generative models, including the Variational Autoencoder (VAE) models, Generative Adversarial Network (GAN) models, Autoregressive (AR) models, Normalizing Flows and Diffusion models. The subsequent section of the paper explores the advancements in visual signal coding based on generative models, as well as the ongoing international standardization activities. In the realm of visual signal processing, our focus lies on the application and development of various generative models in the research of visual signal restoration. We also present the latest developments in generative visual signal synthesis and editing, along with visual signal quality assessment using generative models and quality assessment for generative models. The practical implementation of these studies is closely linked to the investigation of fast optimization. This paper additionally presents the latest advancements in fast optimization on visual signal coding and processing with generative models. We hope to advance this field by providing researchers and practitioners a comprehensive literature review on the topic of visual signal coding and processing with generative models.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2405.11240.pdf' target='_blank'>https://arxiv.org/pdf/2405.11240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Rathgeb, Mathias Ibsen, Denise Hartmann, Simon Hradetzky, Berglind ÃlafsdÃ³ttir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11240">Testing the Performance of Face Recognition for People with Down Syndrome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fairness of biometric systems, in particular facial recognition, is often analysed for larger demographic groups, e.g. female vs. male or black vs. white. In contrast to this, minority groups are commonly ignored. This paper investigates the performance of facial recognition algorithms on individuals with Down syndrome, a common chromosomal abnormality that affects approximately one in 1,000 births per year. To do so, a database of 98 individuals with Down syndrome, each represented by at least five facial images, is semi-automatically collected from YouTube. Subsequently, two facial image quality assessment algorithms and five recognition algorithms are evaluated on the newly collected database and on the public facial image databases CelebA and FRGCv2. The results show that the quality scores of facial images for individuals with Down syndrome are comparable to those of individuals without Down syndrome captured under similar conditions. Furthermore, it is observed that face recognition performance decreases significantly for individuals with Down syndrome, which is largely attributed to the increased likelihood of false matches.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2404.19666.pdf' target='_blank'>https://arxiv.org/pdf/2404.19666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Desen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19666">Beyond MOS: Subjective Image Quality Score Preprocessing Method Based on Perceptual Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment often relies on raw opinion scores provided by subjects in subjective experiments, which can be noisy and unreliable. To address this issue, postprocessing procedures such as ITU-R BT.500, ITU-T P.910, and ITU-T P.913 have been standardized to clean up the original opinion scores. These methods use annotator-based statistical priors, but they do not take into account extensive information about the image itself, which limits their performance in less annotated scenarios. Generally speaking, image quality datasets usually contain similar scenes or distortions, and it is inevitable for subjects to compare images to score a reasonable score when scoring. Therefore, In this paper, we proposed Subjective Image Quality Score Preprocessing Method perceptual similarity Subjective Preprocessing (PSP), which exploit the perceptual similarity between images to alleviate subjective bias in less annotated scenarios. Specifically, we model subjective scoring as a conditional probability model based on perceptual similarity with previously scored images, called subconscious reference scoring. The reference images are stored by a neighbor dictionary, which is obtained by a normalized vector dot-product based nearest neighbor search of the images' perceptual depth features. Then the preprocessed score is updated by the exponential moving average (EMA) of the subconscious reference scoring, called similarity regularized EMA. Our experiments on multiple datasets (LIVE, TID2013, CID2013) show that this method can effectively remove the bias of the subjective scores. Additionally, Experiments prove that the Preprocesed dataset can improve the performance of downstream IQA tasks very well.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2404.19595.pdf' target='_blank'>https://arxiv.org/pdf/2404.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Desen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19595">Perceptual Constancy Constrained Single Opinion Score Calibration for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS). Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS. More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images. Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality. Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3). Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively. Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2404.19567.pdf' target='_blank'>https://arxiv.org/pdf/2404.19567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Wang, Desen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19567">Causal Perception Inspired Representation Learning for Trustworthy Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite great success in modeling visual perception, deep neural network based image quality assessment (IQA) still remains unreliable in real-world applications due to its vulnerability to adversarial perturbations and the inexplicit black-box structure. In this paper, we propose to build a trustworthy IQA model via Causal Perception inspired Representation Learning (CPRL), and a score reflection attack method for IQA model. More specifically, we assume that each image is composed of Causal Perception Representation (CPR) and non-causal perception representation (N-CPR). CPR serves as the causation of the subjective quality label, which is invariant to the imperceptible adversarial perturbations. Inversely, N-CPR presents spurious associations with the subjective quality label, which may significantly change with the adversarial perturbations. To extract the CPR from each input image, we develop a soft ranking based channel-wise activation function to mediate the causally sufficient (beneficial for high prediction accuracy) and necessary (beneficial for high robustness) deep features, and based on intervention employ minimax game to optimize. Experiments on four benchmark databases show that the proposed CPRL method outperforms many state-of-the-art adversarial defense methods and provides explicit model interpretation.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2404.09696.pdf' target='_blank'>https://arxiv.org/pdf/2404.09696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nailia Mirzakhmedova, Marcel Gohsen, Chia Hao Chang, Benno Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09696">Are Large Language Models Reliable Argument Quality Annotators?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining. However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators. Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task. In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators. To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions. Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions. Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators. These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2404.01799.pdf' target='_blank'>https://arxiv.org/pdf/2404.01799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixiang Fang, Daniel L. Oberski, Dong Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01799">PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers'. While such benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics -- a field dedicated to the measurement of latent variables like academic proficiency -- into LLM benchmarking. We make four primary contributions. First, we reflect on current LLM benchmark developments and contrast them with psychometrics-based test development. Second, we introduce PATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of LLMs. PATCH addresses the aforementioned limitations. In particular, PATCH enables valid comparison between LLMs and human populations. Third, we demonstrate PATCH by measuring several LLMs' proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on current benchmarking practices. Fourth, we release 4 high-quality datasets to support measuring and comparing LLM proficiency in grade school mathematics and science with human populations.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2403.15853.pdf' target='_blank'>https://arxiv.org/pdf/2403.15853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesheng Wang, Kunhui Xu, Xiaoyu Chen, Chunlei He, Jianfeng Zhang, Dexing Kong, Qi Dai, Shoujun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15853">An edge detection-based deep learning approach for tear meniscus height measurement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic measurements of tear meniscus height (TMH) have been achieved by using deep learning techniques; however, annotation is significantly influenced by subjective factors and is both time-consuming and labor-intensive. In this paper, we introduce an automatic TMH measurement technique based on edge detection-assisted annotation within a deep learning framework. This method generates mask labels less affected by subjective factors with enhanced efficiency compared to previous annotation approaches. For improved segmentation of the pupil and tear meniscus areas, the convolutional neural network Inceptionv3 was first implemented as an image quality assessment model, effectively identifying higher-quality images with an accuracy of 98.224%. Subsequently, by using the generated labels, various algorithms, including Unet, ResUnet, Deeplabv3+FcnResnet101, Deeplabv3+FcnResnet50, FcnResnet50, and FcnResnet101 were trained, with Unet demonstrating the best performance. Finally, Unet was used for automatic pupil and tear meniscus segmentation to locate the center of the pupil and calculate TMH,respectively. An evaluation of the mask quality predicted by Unet indicated a Mean Intersection over Union of 0.9362, a recall of 0.9261, a precision of 0.9423, and an F1-Score of 0.9326. Additionally, the TMH predicted by the model was assessed, with the fitting curve represented as y= 0.982x-0.862, an overall correlation coefficient of r^2=0.961 , and an accuracy of 94.80% (237/250). In summary, the algorithm can automatically screen images based on their quality,segment the pupil and tear meniscus areas, and automatically measure TMH. Measurement results using the AI algorithm demonstrate a high level of consistency with manual measurements, offering significant support to clinical doctors in diagnosing dry eye disease.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2403.09746.pdf' target='_blank'>https://arxiv.org/pdf/2403.09746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Chahine, Sira Ferradans, Jean Ponce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09746">Pairwise Comparisons Are All You Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) approaches, while promising for automating image quality evaluation, often fall short in real-world scenarios due to their reliance on a generic quality standard applied uniformly across diverse images. This one-size-fits-all approach overlooks the crucial perceptual relationship between image content and quality, leading to a 'domain shift' challenge where a single quality metric inadequately represents various content types. Furthermore, BIQA techniques typically overlook the inherent differences in the human visual system among different observers. In response to these challenges, this paper introduces PICNIQ, a pairwise comparison framework designed to bypass the limitations of conventional BIQA by emphasizing relative, rather than absolute, quality assessment. PICNIQ is specifically designed to estimate the preference likelihood of quality between image pairs. By employing psychometric scaling algorithms, PICNIQ transforms pairwise comparisons into just-objectionable-difference (JOD) quality scores, offering a granular and interpretable measure of image quality. The proposed framework implements a deep learning architecture in combination with a specialized loss function, and a training strategy optimized for sparse pairwise comparison settings. We conduct our research using comparison matrices from the PIQ23 dataset, which are published in this paper. Our extensive experimental analysis showcases PICNIQ's broad applicability and competitive performance, highlighting its potential to set new standards in the field of BIQA.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2403.08256.pdf' target='_blank'>https://arxiv.org/pdf/2403.08256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Gi Pyo Nam, Haksub Kim, Haesol Park, Ig-Jae Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08256">IG-FIQA: Improving Face Image Quality Assessment through Intra-class Variance Guidance robust to Inaccurate Pseudo-Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of face image quality assesment (FIQA), method based on sample relative classification have shown impressive performance. However, the quality scores used as pseudo-labels assigned from images of classes with low intra-class variance could be unrelated to the actual quality in this method. To address this issue, we present IG-FIQA, a novel approach to guide FIQA training, introducing a weight parameter to alleviate the adverse impact of these classes. This method involves estimating sample intra-class variance at each iteration during training, ensuring minimal computational overhead and straightforward implementation. Furthermore, this paper proposes an on-the-fly data augmentation methodology for improved generalization performance in FIQA. On various benchmark datasets, our proposed method, IG-FIQA, achieved novel state-of-the-art (SOTA) performance.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2402.17533.pdf' target='_blank'>https://arxiv.org/pdf/2402.17533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17533">Black-box Adversarial Attacks Against Image Quality Assessment Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation. To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement. This paper makes the first attempt to explore the black-box adversarial attacks on NR-IQA models. Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation. Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of adversarial examples towards an opposite direction with maximum deviation. On this basis, we finally develop an efficient and effective black-box attack method against NR-IQA models. Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method. And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2401.10107.pdf' target='_blank'>https://arxiv.org/pdf/2401.10107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny, Michel Walti, Elias Meier, Francesca Pentimalli Biscaretti di Ruffia, Mark Melnykowycz, Athina Tzovara, Valentina Agostini, Francesca Dalia Faraci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10107">Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort makes long-term monitoring unfeasible, leading to bias in sleep quality assessment. Hence, less invasive, cost-effective, and portable alternatives need to be explored. One promising contender is the in-ear-EEG sensor. This study aims to establish a methodology to assess the similarity between the single-channel in-ear-EEG and standard PSG derivations.
  Methods: The study involves four-hour signals recorded from ten healthy subjects aged 18 to 60 years. Recordings are analyzed following two complementary approaches: (i) a hypnogram-based analysis aimed at assessing the agreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a feature-based analysis based on time- and frequency- domain feature extraction, unsupervised feature selection, and definition of Feature-based Similarity Index via Jensen-Shannon Divergence (JSD-FSI).
  Results: We find large variability between PSG and in-ear-EEG hypnograms scored by the same sleep expert according to Cohen's kappa metric, with significantly greater agreements for PSG scorers than for in-ear-EEG scorers (p < 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high similarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/- 0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the similarity values computed independently on standard PSG-channel-combinations.
  Conclusions: In-ear-EEG is a valuable solution for home-based sleep monitoring, however further studies with a larger and more heterogeneous dataset are needed.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2311.06093.pdf' target='_blank'>https://arxiv.org/pdf/2311.06093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shima Mohammadi, Joao Ascenso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06093">Evaluation of Sampling Algorithms for a Pairwise Subjective Assessment Methodology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subjective assessment tests are often employed to evaluate image processing systems, notably image and video compression, super-resolution among others and have been used as an indisputable way to provide evidence of the performance of an algorithm or system. While several methodologies can be used in a subjective quality assessment test, pairwise comparison tests are nowadays attracting a lot of attention due to their accuracy and simplicity. However, the number of comparisons in a pairwise comparison test increases quadratically with the number of stimuli and thus often leads to very long tests, which is impractical for many cases. However, not all the pairs contribute equally to the final score and thus, it is possible to reduce the number of comparisons without degrading the final accuracy. To do so, pairwise sampling methods are often used to select the pairs which provide more information about the quality of each stimuli. In this paper, a reliable and much-needed evaluation procedure is proposed and used for already available methods in the literature, especially considering the case of subjective evaluation of image and video codecs. The results indicate that an appropriate selection of the pairs allows to achieve very reliable scores while requiring the comparison of a much lower number of pairs.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2311.06084.pdf' target='_blank'>https://arxiv.org/pdf/2311.06084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shima Mohammadi, Joao Ascenso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06084">Perceptual impact of the loss function on deep-learning image coding performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, deep-learning image coding solutions have shown similar or better compression efficiency than conventional solutions based on hand-crafted transforms and spatial prediction techniques. These deep-learning codecs require a large training set of images and a training methodology to obtain a suitable model (set of parameters) for efficient compression. The training is performed with an optimization algorithm which provides a way to minimize the loss function. Therefore, the loss function plays a key role in the overall performance and includes a differentiable quality metric that attempts to mimic human perception. The main objective of this paper is to study the perceptual impact of several image quality metrics that can be used in the loss function of the training process, through a crowdsourcing subjective image quality assessment study. From this study, it is possible to conclude that the choice of the quality metric is critical for the perceptual performance of the deep-learning codec and that can vary depending on the image content.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2310.12877.pdf' target='_blank'>https://arxiv.org/pdf/2310.12877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peibei Cao, Rafal K. Mantiuk, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12877">Perceptual Assessment and Optimization of HDR Image Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High dynamic range (HDR) rendering has the ability to faithfully reproduce the wide luminance ranges in natural scenes, but how to accurately assess the rendering quality is relatively underexplored. Existing quality models are mostly designed for low dynamic range (LDR) images, and do not align well with human perception of HDR image quality. To fill this gap, we propose a family of HDR quality metrics, in which the key step is employing a simple inverse display model to decompose an HDR image into a stack of LDR images with varying exposures. Subsequently, these decomposed images are assessed through well-established LDR quality metrics. Our HDR quality models present three distinct benefits. First, they directly inherit the recent advancements of LDR quality metrics. Second, they do not rely on human perceptual data of HDR image quality for re-calibration. Third, they facilitate the alignment and prioritization of specific luminance ranges for more accurate and detailed quality assessment. Experimental results show that our HDR quality metrics consistently outperform existing models in terms of quality assessment on four HDR image quality datasets and perceptual optimization of HDR novel view synthesis.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2307.14735.pdf' target='_blank'>https://arxiv.org/pdf/2307.14735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14735">Test Time Adaptation for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the design of blind image quality assessment (IQA) algorithms has improved significantly, the distribution shift between the training and testing scenarios often leads to a poor performance of these methods at inference time. This motivates the study of test time adaptation (TTA) techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2306.14918.pdf' target='_blank'>https://arxiv.org/pdf/2306.14918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat Tran, Benjamin Pierce, Diane Litman, Richard Correnti, Lindsay Clare Matsumura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14918">Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2305.17477.pdf' target='_blank'>https://arxiv.org/pdf/2305.17477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Alutis, Egor Chistov, Mikhail Dremin, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17477">BASED: Benchmarking, Analysis, and Structural Estimation of Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper discusses the challenges of evaluating deblurring-methods quality and proposes a reduced-reference metric based on machine learning. Traditional quality-assessment metrics such as PSNR and SSIM are common for this task, but not only do they correlate poorly with subjective assessments, they also require ground-truth (GT) frames, which can be difficult to obtain in the case of deblurring. To develop and evaluate our metric, we created a new motion-blur dataset using a beam splitter. The setup captured various motion types using a static camera, as most scenes in existing datasets include blur due to camera motion. We also conducted two large subjective comparisons to aid in metric development. Our resulting metric requires no GT frames, and it correlates well with subjective human perception of blur.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2305.03176.pdf' target='_blank'>https://arxiv.org/pdf/2305.03176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Martin, AntÃ³nio Rodrigues, JoÃ£o Ascenso, Maria Paula Queluz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03176">NeRF-QA: Neural Radiance Fields Quality Assessment Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This short paper proposes a new database - NeRF-QA - containing 48 videos synthesized with seven NeRF based methods, along with their perceived quality scores, resulting from subjective assessment tests; for the videos selection, both real and synthetic, 360 degrees scenes were considered. This database will allow to evaluate the suitability, to NeRF based synthesized views, of existing objective quality metrics and also the development of new quality metrics, specific for this case.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2303.00630.pdf' target='_blank'>https://arxiv.org/pdf/2303.00630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert P. Spang, Karl El Hajal, Sebastian MÃ¶ller, Milos Cernak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00630">Personalized Task Load Prediction in Speech Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the quality of remote speech communication is a complex task influenced by the speaker, transmission channel, and listener. For example, the degradation of transmission quality can increase listeners' cognitive load, which can influence the overall perceived quality of the conversation. This paper presents a framework that isolates quality-dependent changes and controls most outside influencing factors like personal preference in a simulated conversational environment. The performed statistical analysis finds significant relationships between stimulus quality and the listener's valence and personality (agreeableness and openness) and, similarly, between the perceived task load during the listening task and the listener's personality and frustration intolerance. The machine learning model of the task load prediction improves the correlation coefficients from 0.48 to 0.76 when listeners' individuality is considered. The proposed evaluation framework and results pave the way for personalized audio quality assessment that includes speakers' and listeners' individuality beyond conventional channel modeling.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2302.14520.pdf' target='_blank'>https://arxiv.org/pdf/2302.14520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Kocmi, Christian Federmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14520">Large Language Models Are State-of-the-Art Evaluators of Translation Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT~3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2301.01069.pdf' target='_blank'>https://arxiv.org/pdf/2301.01069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liqun Lin, Yang Zheng, Weiling Chen, Chengdong Lan, Tiesong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01069">Saliency-Aware Spatio-Temporal Artifact Detection for Compressed Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compressed videos often exhibit visually annoying artifacts, known as Perceivable Encoding Artifacts (PEAs), which dramatically degrade video visual quality. Subjective and objective measures capable of identifying and quantifying various types of PEAs are critical in improving visual quality. In this paper, we investigate the influence of four spatial PEAs (i.e. blurring, blocking, bleeding, and ringing) and two temporal PEAs (i.e. flickering and floating) on video quality. For spatial artifacts, we propose a visual saliency model with a low computational cost and higher consistency with human visual perception. In terms of temporal artifacts, self-attention based TimeSFormer is improved to detect temporal artifacts. Based on the six types of PEAs, a quality metric called Saliency-Aware Spatio-Temporal Artifacts Measurement (SSTAM) is proposed. Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics. We believe that SSTAM will be beneficial for optimizing video coding techniques.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2212.10013.pdf' target='_blank'>https://arxiv.org/pdf/2212.10013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forrest Sheng Bao, Ruixuan Tu, Ge Luo, Yinfei Yang, Hebi Li, Minghui Qiu, Youbiao He, Cen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10013">DocAsRef: An Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated summary quality assessment falls into two categories: reference-based and reference-free. Reference-based metrics, historically deemed more accurate due to the additional information provided by human-written references, are limited by their reliance on human input. In this paper, we hypothesize that the comparison methodologies used by some reference-based metrics to evaluate a system summary against its corresponding reference can be effectively adapted to assess it against its source document, thereby transforming these metrics into reference-free ones. Experimental results support this hypothesis. After being repurposed reference-freely, the zero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of <0.5B parameters consistently outperforms its original reference-based version across various aspects on the SummEval and Newsroom datasets. It also excels in comparison to most existing reference-free metrics and closely competes with zero-shot summary evaluators based on GPT-3.5.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2207.14769.pdf' target='_blank'>https://arxiv.org/pdf/2207.14769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peibei Cao, Dingquan Li, Kede Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14769">Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based image quality assessment (IQA) has made remarkable progress in the past decade, but nearly all consider the two key components -- model and data -- in isolation. Specifically, model-centric IQA focuses on developing ``better'' objective quality methods on fixed and extensively reused datasets, with a great danger of overfitting. Data-centric IQA involves conducting psychophysical experiments to construct ``better'' human-annotated datasets, which unfortunately ignores current IQA models during dataset creation. In this paper, we first design a series of experiments to probe computationally that such isolation of model and data impedes further progress of IQA. We then describe a computational framework that integrates model-centric and data-centric IQA. As a specific example, we design computational modules to quantify the sampling-worthiness of candidate images. Experimental results show that the proposed sampling-worthiness module successfully spots diverse failures of the examined blind IQA models, which are indeed worthy samples to be included in next-generation datasets.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2205.09392.pdf' target='_blank'>https://arxiv.org/pdf/2205.09392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannan Zheng, Weiling Chen, Rongfu Lin, Tiesong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.09392">UIF: An Objective Quality Assessment for Underwater Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to complex and volatile lighting environment, underwater imaging can be readily impaired by light scattering, warping, and noises. To improve the visual quality, Underwater Image Enhancement (UIE) techniques have been widely studied. Recent efforts have also been contributed to evaluate and compare the UIE performances with subjective and objective methods. However, the subjective evaluation is time-consuming and uneconomic for all images, while existing objective methods have limited capabilities for the newly-developed UIE approaches based on deep learning. To fill this gap, we propose an Underwater Image Fidelity (UIF) metric for objective evaluation of enhanced underwater images. By exploiting the statistical features of these images, we present to extract naturalness-related, sharpness-related, and structure-related features. Among them, the naturalness-related and sharpness-related features evaluate visual improvement of enhanced images; the structure-related feature indicates structural similarity between images before and after UIE. Then, we employ support vector regression to fuse the above three features into a final UIF metric. In addition, we have also established a large-scale UIE database with subjective scores, namely Underwater Image Enhancement Database (UIED), which is utilized as a benchmark to compare all objective metrics. Experimental results confirm that the proposed UIF outperforms a variety of underwater and general-purpose image quality metrics.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2202.04986.pdf' target='_blank'>https://arxiv.org/pdf/2202.04986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Krahmer, Anna Veselovska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.04986">Enhanced Digital Halftoning via Weighted Sigma-Delta Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study error diffusion techniques for digital halftoning from the perspective of 1-bit Sigma-Delta quantization. We introduce a method to generate Sigma-Delta schemes for two-dimensional signals as a weighted combination of its one-dimensional counterparts and show that various error diffusion schemes proposed in the literature can be represented in this framework via Sigma-Delta schemes of first order. Under the model of two-dimensional bandlimited signals, which is motivated by a mathematical model of human visual perception, we derive quantitative error bounds for such weighted Sigma-Delta schemes. We see these bounds as a step towards a mathematical understanding of the good empirical performance of error diffusion, even though they are formulated in the supremum norm, which is known to not fully capture the visual similarity of images.
  Motivated by the correspondence between existing error diffusion algorithms and first-order Sigma-Delta schemes, we study the performance of the analogous weighted combinations of second-order Sigma-Delta schemes and show that they exhibit a superior performance in terms of guaranteed error decay for two-dimensional bandlimited signals. In extensive numerical simulations for real world images, we demonstrate that with some modifications to enhance stability this superior performance also translates to the problem of digital halftoning.
  More concretely, we find that certain second-order weighted Sigma-Delta schemes exhibit competitive performance for digital halftoning of real world images in terms of the Feature Similarity Index (FSIM), a state-of-the-art measure for image quality assessment.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2510.17821.pdf' target='_blank'>https://arxiv.org/pdf/2510.17821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Lin, Pablo Peiro-Corbacho, Pablo Ávila, Alejandro Carta-Bergaz, Ángel Arenal, Gonzalo R. Ríos-Muñoz, Carlos Sevilla-Salcedo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17821">CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracavitary atrial electrograms (EGMs) provide high-resolution insights into cardiac electrophysiology but are often contaminated by noise and remain high-dimensional, limiting real-time analysis. We introduce CLARAE (CLArity-preserving Reconstruction AutoEncoder), a one-dimensional encoder--decoder designed for atrial EGMs, which achieves both high-fidelity reconstruction and a compact 64-dimensional latent representation. CLARAE is designed to preserve waveform morphology, mitigate reconstruction artifacts, and produce interpretable embeddings through three principles: downsampling with pooling, a hybrid interpolation--convolution upsampling path, and a bounded latent space. We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29 patients across three rhythm types (AF, SR300, SR600). Performance was benchmarked against six state-of-the-art autoencoders using reconstruction metrics, rhythm classification, and robustness across signal-to-noise ratios from -5 to 15 dB. In downstream rhythm classification, CLARAE achieved F1-scores above 0.97 for all rhythm types, and its latent space showed clear clustering by rhythm. In denoising tasks, it consistently ranked among the top performers for both unipolar and bipolar signals. In order to promote reproducibility and enhance accessibility, we offer an interactive web-based application. This platform enables users to explore pre-trained CLARAE models, visualize the reconstructions, and compute metrics in real time. Overall, CLARAE combines robust denoising with compact, discriminative representations, offering a practical foundation for clinical workflows such as rhythm discrimination, signal quality assessment, and real-time mapping.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2510.17056.pdf' target='_blank'>https://arxiv.org/pdf/2510.17056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis F. G. Campos, Leonardo C. Marques, Walter T. Nakamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17056">Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Usability inspection is a well-established technique for identifying interaction issues in software interfaces, thereby contributing to improved product quality. However, it is a costly process that requires time and specialized knowledge from inspectors. With advances in Artificial Intelligence (AI), new opportunities have emerged to support this task, particularly through generative models capable of interpreting interfaces and performing inspections more efficiently. This study examines the performance of generative AIs in identifying usability problems, comparing them to those of experienced human inspectors. A software prototype was evaluated by four specialists and two AI models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall, and F1-score. While inspectors achieved the highest levels of precision and overall coverage, the AIs demonstrated high individual performance and discovered many novel defects, but with a higher rate of false positives and redundant reports. The combination of AIs and human inspectors produced the best results, revealing their complementarity. These findings suggest that AI, in its current stage, cannot replace human inspectors but can serve as a valuable augmentation tool to improve efficiency and expand defect coverage. The results provide evidence based on quantitative analysis to inform the discussion on the role of AI in usability inspections, pointing to viable paths for its complementary use in software quality assessment contexts.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2510.16179.pdf' target='_blank'>https://arxiv.org/pdf/2510.16179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Giro-i-Nieto, Nefeli Andreou, Anqi Liang, Manel Baradad, Francesc Moreno-Noguer, Aleix Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16179">Cost Savings from Automatic Quality Assessment of Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have shown impressive progress in recent years, making it possible to produce high quality images with a simple text prompt or a reference image. However, state of the art technology does not yet meet the quality standards offered by traditional photographic methods. For this reason, production pipelines that use generated images often include a manual stage of image quality assessment (IQA). This process is slow and expensive, especially because of the low yield of automatically generated images that pass the quality bar. The IQA workload can be reduced by introducing an automatic pre-filtering stage, that will increase the overall quality of the images sent to review and, therefore, reduce the average cost required to obtain a high quality image. We present a formula that estimates the cost savings depending on the precision and pass yield of a generic IQA engine. This formula is applied in a use case of background inpainting, showcasing a significant cost saving of 51.61% obtained with a simple AutoML solution.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2508.02131.pdf' target='_blank'>https://arxiv.org/pdf/2508.02131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behnaz Kavoosighafi, Rafal K. Mantiuk, Saghi Hajisharif, Ehsan Miandji, Jonas Unger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02131">A Neural Quality Metric for BRDF Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately evaluating the quality of bidirectional reflectance distribution function (BRDF) models is essential for photo-realistic rendering. Traditional BRDF-space metrics often employ numerical error measures that fail to capture perceptual differences evident in rendered images. In this paper, we introduce the first perceptually informed neural quality metric for BRDF evaluation that operates directly in BRDF space, eliminating the need for rendering during quality assessment. Our metric is implemented as a compact multi-layer perceptron (MLP), trained on a dataset of measured BRDFs supplemented with synthetically generated data and labelled using a perceptually validated image-space metric. The network takes as input paired samples of reference and approximated BRDFs and predicts their perceptual quality in terms of just-objectionable-difference (JOD) scores. We show that our neural metric achieves significantly higher correlation with human judgments than existing BRDF-space metrics. While its performance as a loss function for BRDF fitting remains limited, the proposed metric offers a perceptually grounded alternative for evaluating BRDF models.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2507.23356.pdf' target='_blank'>https://arxiv.org/pdf/2507.23356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Avi Ziv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23356">Quality Evaluation of COBOL to Java Code Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an automated evaluation system for assessing COBOL-to-Java code translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system addresses key challenges in evaluating LLM-based translators, including model opacity and the complexity of translation quality assessment. Our approach combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver scalable, multi-faceted evaluations. The system supports continuous integration workflows, enables large-scale benchmarking, and reduces reliance on manual review. We describe the system architecture, evaluation strategies, and reporting mechanisms that provide actionable insights for developers and project managers, facilitating the evolution of high-quality, modernized codebases.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2507.17507.pdf' target='_blank'>https://arxiv.org/pdf/2507.17507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Papastergios, Lisa Ehrlinger, Anastasios Gounaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17507">Unfolding Data Quality Dimensions in Practice: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data quality describes the degree to which data meet specific requirements and are fit for use by humans and/or downstream tasks (e.g., artificial intelligence). Data quality can be assessed across multiple high-level concepts called dimensions, such as accuracy, completeness, consistency, or timeliness. While extensive research and several attempts for standardization (e.g., ISO/IEC 25012) exist for data quality dimensions, their practical application often remains unclear. In parallel to research endeavors, a large number of tools have been developed that implement functionalities for the detection and mitigation of specific data quality issues, such as missing values or outliers. With this paper, we aim to bridge this gap between data quality theory and practice by systematically connecting low-level functionalities offered by data quality tools with high-level dimensions, revealing their many-to-many relationships. Through an examination of seven open-source data quality tools, we provide a comprehensive mapping between their functionalities and the data quality dimensions, demonstrating how individual functionalities and their variants partially contribute to the assessment of single dimensions. This systematic survey provides both practitioners and researchers with a unified view on the fragmented landscape of data quality checks, offering actionable insights for quality assessment across multiple dimensions.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2507.15961.pdf' target='_blank'>https://arxiv.org/pdf/2507.15961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Aman Ibrahim, Hamad Mansour Alawar, Abdulnasser Abbas Zehi, Ahmed Mohammad Alkendi, Bilal Shafi Ashfaq Ahmed Mirza, Shan Ullah, Ismail Lujain Jaleel, Hassan Ugail
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15961">A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face image quality plays a critical role in determining the accuracy and reliability of face verification systems, particularly in real-time screening applications such as surveillance, identity verification, and access control. Low-quality face images, often caused by factors such as motion blur, poor lighting conditions, occlusions, and extreme pose variations, significantly degrade the performance of face recognition models, leading to higher false rejection and false acceptance rates. In this work, we propose a lightweight yet effective framework for automatic face quality assessment, which aims to pre-filter low-quality face images before they are passed to the verification pipeline. Our approach utilises normalised facial landmarks in conjunction with a Random Forest Regression classifier to assess image quality, achieving an accuracy of 96.67%. By integrating this quality assessment module into the face verification process, we observe a substantial improvement in performance, including a comfortable 99.7% reduction in the false rejection rate and enhanced cosine similarity scores when paired with the ArcFace face verification model. To validate our approach, we have conducted experiments on a real-world dataset collected comprising over 600 subjects captured from CCTV footage in unconstrained environments within Dubai Police. Our results demonstrate that the proposed framework effectively mitigates the impact of poor-quality face images, outperforming existing face quality assessment techniques while maintaining computational efficiency. Moreover, the framework specifically addresses two critical challenges in real-time screening: variations in face resolution and pose deviations, both of which are prevalent in practical surveillance scenarios.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2506.00103.pdf' target='_blank'>https://arxiv.org/pdf/2506.00103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruipeng Jia, Yunyi Yang, Yongbo Gai, Kai Luo, Shihao Huang, Jianhe Lin, Xiaoxi Jiang, Guanjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00103">Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning with verifiable rewards (RLVR) has enabled large language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks with objective ground-truth answers, such as mathematics and code generation. However, a significant gap remains for non-verifiable tasks, like creative writing and open-ended dialogue, where quality assessment is inherently subjective and lacks definitive references. Existing approaches for these domains often rely on scalar reward models trained with human preferences, which suffer from limited generalization and are prone to reward hacking, such as over-explanation and length bias. In this work, we propose a unified RLVR-based training paradigm that bridges the gap between non-verifiable tasks and verifiable rewards. We introduce a writing-principle-based pairwise Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy Optimization (BRPO) algorithm. The pairwise writing GenRM leverages self-principled critique to transform subjective assessments into reliable, verifiable rewards, while BRPO enables dynamic, reference-free pairwise comparison by leveraging a bootstrapped response as temporary reference from within group rollouts during RL training. Our approach empowers LLMs to develop robust writing capabilities without supervised fine-tuning, as demonstrated by Writing-Zero, which shows consistent improvement and strong resistance to reward hacking compared to scalar reward baselines. Furthermore, our method achieves competitive results on both in-house and open-source writing benchmarks. Our findings suggest the potential to unify rule-based, reference-based, and reference-free reward modeling under the RLVR framework, thus paving the way for a comprehensive and scalable RL training paradigm applicable across all language tasks.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2505.22065.pdf' target='_blank'>https://arxiv.org/pdf/2505.22065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikko ImpiÃ¶, Philipp M. Rehsen, Tiina Laamanen, Arne J. Beermann, Florian Leese, Jenni Raitoharju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22065">AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the AquaMonitor dataset, the first large computer vision dataset of aquatic invertebrates collected during routine environmental monitoring. While several large species identification datasets exist, they are rarely collected using standardized collection protocols, and none focus on aquatic invertebrates, which are particularly laborious to collect. For AquaMonitor, we imaged all specimens from two years of monitoring whenever imaging was possible given practical limitations. The dataset enables the evaluation of automated identification methods for real-life monitoring purposes using a realistically challenging and unbiased setup. The dataset has 2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry mass and size measurements for 1494 specimens, making it also one of the largest biological multi-view and multimodal datasets to date. We define three benchmark tasks and provide strong baselines for these: 1) Monitoring benchmark, reflecting real-life deployment challenges such as open-set recognition, distribution shift, and extreme class imbalance, 2) Classification benchmark, which follows a standard fine-grained visual categorization setup, and 3) Few-shot benchmark, which targets classes with only few training examples from very fine-grained categories. Advancements on the Monitoring benchmark can directly translate to improvement of aquatic biodiversity monitoring, which is an important component of regular legislative water quality assessment in many countries.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2503.19570.pdf' target='_blank'>https://arxiv.org/pdf/2503.19570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olgica Zaric, Carmen Leser, Vladimir Juras, Alex Farr, Malina Gologan, Stanislas Rapacchi, Laura Villazan Garcia, Christian Singer, Siegfried Trattnig, Christian Licht, Ramona Woitek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19570">Improved tissue sodium concentration quantification in breast cancer by reducing partial volume effects: a preliminary study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduction: In sodium (23Na) MRI, partial volume effects (PVE) are one of the most common causes of errors in the quantification of tissue sodium concentration (TSC) in vivo. Advanced image reconstruction algorithms, such as compressed sensing (CS), have been shown to potentially reduce PVE. Therefore, we investigated the feasibility of CS-based methods for image quality and TSC quantification accuracy improvement in patients with breast cancer (BC). Subjects and Methods: Three healthy participants and 12 female participants with BC were examined on a 7T MRI scanner in this study. We reconstructed 23Na-MRI images using the weighted total variation (wTV) and directional total variation (dTV), anatomically guided total variation (AG-TV), and adaptive combine (ADC) reconstruction and performed image quality assessment. We evaluated agreement in tumor volumes delineated on sodium data using the Dice score and performed TSC quantification for different image reconstruction approaches. Results: All methods provided sodium images of the breast with good quality. The mean Dice scores for wTV, dTV, and AG-TV were 65%, 72%, and 75%, respectively. In the breast tumors, average TSC values were 83.0, 72.0, 80.0, and 84.0 mmol/L, respectively. There was a significant difference between dTV and wTV (p<0.001), as well as between dTV and AG-TV (p<0.001) and dTV and ADC algorithm (p<0.001). Conclusion: The results of this study showed that there are differences in tumor appearance and TSC estimations that might be depending on the type of image reconstruction and parameters used, most likely due to differences in their robustness in reducing PVE.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2502.13198.pdf' target='_blank'>https://arxiv.org/pdf/2502.13198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manal Rahal, Bestoun S. Ahmed, Gergely Szabados, Torgny Fornstedt, Jorgen Samuelsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13198">Enhancing Machine Learning Performance through Intelligent Data Quality Assessment: An Unsupervised Data-centric Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poor data quality limits the advantageous power of Machine Learning (ML) and weakens high-performing ML software systems. Nowadays, data are more prone to the risk of poor quality due to their increasing volume and complexity. Therefore, tedious and time-consuming work goes into data preparation and improvement before moving further in the ML pipeline. To address this challenge, we propose an intelligent data-centric evaluation framework that can identify high-quality data and improve the performance of an ML system. The proposed framework combines the curation of quality measurements and unsupervised learning to distinguish high- and low-quality data. The framework is designed to integrate flexible and general-purpose methods so that it is deployed in various domains and applications. To validate the outcomes of the designed framework, we implemented it in a real-world use case from the field of analytical chemistry, where it is tested on three datasets of anti-sense oligonucleotides. A domain expert is consulted to identify the relevant quality measurements and evaluate the outcomes of the framework. The results show that the quality-centric data evaluation framework identifies the characteristics of high-quality data that guide the conduct of efficient laboratory experiments and consequently improve the performance of the ML system.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2502.01253.pdf' target='_blank'>https://arxiv.org/pdf/2502.01253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oshani Seneviratne, Brendan Capuzzo, William Van Woensel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01253">Explainability-Driven Quality Assessment for Rule-Based Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an explanation framework designed to enhance the quality of rules in knowledge-based reasoning systems based on dataset-driven insights. The traditional method for rule induction from data typically requires labor-intensive labeling and data-driven learning. This framework provides an alternative and instead allows for the data-driven refinement of existing rules: it generates explanations of rule inferences and leverages human interpretation to refine rules. It leverages four complementary explanation types: trace-based, contextual, contrastive, and counterfactual, providing diverse perspectives for debugging, validating, and ultimately refining rules. By embedding explainability into the reasoning architecture, the framework enables knowledge engineers to address inconsistencies, optimize thresholds, and ensure fairness, transparency, and interpretability in decision-making processes. Its practicality is demonstrated through a use case in finance.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2501.11203.pdf' target='_blank'>https://arxiv.org/pdf/2501.11203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenli Yang, Yanyu Chen, Andrew Trotter, Byeong Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11203">Advancing Oyster Phenotype Segmentation with Multi-Network Ensemble and Multi-Scale mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phenotype segmentation is pivotal in analysing visual features of living organisms, enhancing our understanding of their characteristics. In the context of oysters, meat quality assessment is paramount, focusing on shell, meat, gonad, and muscle components. Traditional manual inspection methods are time-consuming and subjective, prompting the adoption of machine vision technology for efficient and objective evaluation. We explore machine vision's capacity for segmenting oyster components, leading to the development of a multi-network ensemble approach with a global-local hierarchical attention mechanism. This approach integrates predictions from diverse models and addresses challenges posed by varying scales, ensuring robust instance segmentation across components. Finally, we provide a comprehensive evaluation of the proposed method's performance using different real-world datasets, highlighting its efficacy and robustness in enhancing oyster phenotype segmentation.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2501.07154.pdf' target='_blank'>https://arxiv.org/pdf/2501.07154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Novoneel Chakraborty, Abhay Sharma, Jyotirmoy Dutta, Hari Dilip Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07154">Privacy-Preserving Data Quality Assessment for Time-Series IoT Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data from Internet of Things (IoT) sensors has emerged as a key contributor to decision-making processes in various domains. However, the quality of the data is crucial to the effectiveness of applications built on it, and assessment of the data quality is heavily context-dependent. Further, preserving the privacy of the data during quality assessment is critical in domains where sensitive data is prevalent. This paper proposes a novel framework for automated, objective, and privacy-preserving data quality assessment of time-series data from IoT sensors deployed in smart cities. We leverage custom, autonomously computable metrics that parameterise the temporal performance and adherence to a declarative schema document to achieve objectivity. Additionally, we utilise a trusted execution environment to create a "data-blind" model that ensures individual privacy, eliminates assessee bias, and enhances adaptability across data types. This paper describes this data quality assessment methodology for IoT sensors, emphasising its relevance within the smart-city context while addressing the growing need for privacy in the face of extensive data collection practices.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2412.15847.pdf' target='_blank'>https://arxiv.org/pdf/2412.15847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuekai Wei, Junyu Zhang, Qinlin Hu, Mingliang Zhou\\Yong Feng, Weizhi Xian, Huayan Pu, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15847">Image Quality Assessment: Enhancing Perceptual Exploration and Interpretation with Collaborative Feature Refinement and Hausdorff distance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current full-reference image quality assessment (FR-IQA) methods often fuse features from reference and distorted images, overlooking that color and luminance distortions occur mainly at low frequencies, whereas edge and texture distortions occur at high frequencies. This work introduces a pioneering training-free FR-IQA method that accurately predicts image quality in alignment with the human visual system (HVS) by leveraging a novel perceptual degradation modelling approach to address this limitation. First, a collaborative feature refinement module employs a carefully designed wavelet transform to extract perceptually relevant features, capturing multiscale perceptual information and mimicking how the HVS analyses visual information at various scales and orientations in the spatial and frequency domains. Second, a Hausdorff distance-based distribution similarity measurement module robustly assesses the discrepancy between the feature distributions of the reference and distorted images, effectively handling outliers and variations while mimicking the ability of HVS to perceive and tolerate certain levels of distortion. The proposed method accurately captures perceptual quality differences without requiring training data or subjective quality scores. Extensive experiments on multiple benchmark datasets demonstrate superior performance compared with existing state-of-the-art approaches, highlighting its ability to correlate strongly with the HVS.\footnote{The code is available at \url{https://anonymous.4open.science/r/CVPR2025-F339}.}
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2411.10924.pdf' target='_blank'>https://arxiv.org/pdf/2411.10924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priyabrata Karmakar, Manzur Murshed, Shyh Wei Teng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10924">Hyperspectral Imaging-Based Grain Quality Assessment With Limited Labelled Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently hyperspectral imaging (HSI)-based grain quality assessment has gained research attention. However, unlike other imaging modalities, HSI data lacks sufficient labelled samples required to effectively train deep convolutional neural network (DCNN)-based classifiers. In this paper, we present a novel approach to grain quality assessment using HSI combined with few-shot learning (FSL) techniques. Traditional methods for grain quality evaluation, while reliable, are invasive, time-consuming, and costly. HSI offers a non-invasive, real-time alternative by capturing both spatial and spectral information. However, a significant challenge in applying DCNNs for HSI-based grain classification is the need for large labelled databases, which are often difficult to obtain. To address this, we explore the use of FSL, which enables models to perform well with limited labelled data, making it a practical solution for real-world applications where rapid deployment is required. We also explored the application of FSL for the classification of hyperspectral images of bulk grains to enable rapid quality assessment at various receival points in the grain supply chain. We evaluated the performance of few-shot classifiers in two scenarios: first, classification of grain types seen during training, and second, generalisation to unseen grain types, a crucial feature for real-world applications. In the first scenario, we introduce a novel approach using pre-computed collective class prototypes (CCPs) to enhance inference efficiency and robustness. In the second scenario, we assess the model's ability to classify novel grain types using limited support examples. Our experimental results show that despite using very limited labelled data for training, our FSL classifiers accuracy is comparable to that of a fully trained classifier trained using a significantly larger labelled database.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2411.00252.pdf' target='_blank'>https://arxiv.org/pdf/2411.00252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxwell Meyer, Jack Spruyt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00252">IO Transformer: Evaluating SwinV2-Based Reward Models for Computer Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers and their derivatives have achieved state-of-the-art performance across text, vision, and speech recognition tasks. However, minimal effort has been made to train transformers capable of evaluating the output quality of other models. This paper examines SwinV2-based reward models, called the Input-Output Transformer (IO Transformer) and the Output Transformer. These reward models can be leveraged for tasks such as inference quality evaluation, data categorization, and policy optimization. Our experiments demonstrate highly accurate model output quality assessment across domains where the output is entirely dependent on the input, with the IO Transformer achieving perfect evaluation accuracy on the Change Dataset 25 (CD25). We also explore modified Swin V2 architectures. Ultimately Swin V2 remains on top with a score of 95.41 % on the IO Segmentation Dataset, outperforming the IO Transformer in scenarios where the output is not entirely dependent on the input. Our work expands the application of transformer architectures to reward modeling in computer vision and provides critical insights into optimizing these models for various tasks.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2410.14161.pdf' target='_blank'>https://arxiv.org/pdf/2410.14161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renguang Chen, Guolong Zheng, Xu Yang, Zhide Chen, Jiwu Shu, Wencheng Yang, Kexin Zhu, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14161">Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive Constrained Dynamic Time Warping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing popularity of online sports and exercise necessitates effective methods for evaluating the quality of online exercise executions. Previous action quality assessment methods, which relied on labeled scores from motion videos, exhibited slightly lower accuracy and discriminability. This limitation hindered their rapid application to newly added exercises. To address this problem, this paper presents an unlabeled Multi-Dimensional Exercise Distance Adaptive Constrained Dynamic Time Warping (MED-ACDTW) method for action quality assessment. Our approach uses an athletic version of DTW to compare features from template and test videos, eliminating the need for score labels during training. The result shows that utilizing both 2D and 3D spatial dimensions, along with multiple human body features, improves the accuracy by 2-3% compared to using either 2D or 3D pose estimation alone. Additionally, employing MED for score calculation enhances the precision of frame distance matching, which significantly boosts overall discriminability. The adaptive constraint scheme enhances the discriminability of action quality assessment by approximately 30%. Furthermore, to address the absence of a standardized perspective in sports class evaluations, we introduce a new dataset called BGym.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2410.08250.pdf' target='_blank'>https://arxiv.org/pdf/2410.08250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuan Nguyen, Corinne Fredouille, Alain Ghio, Mathieu Balaguer, Virginie Woisard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08250">Exploring ASR-Based Wav2Vec2 for Automated Speech Disorder Assessment: Insights and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of SSL and ASR technologies, the Wav2Vec2 ASR-based model has been fine-tuned for automated speech disorder quality assessment tasks, yielding impressive results and setting a new baseline for Head and Neck Cancer speech contexts. This demonstrates that the ASR dimension from Wav2Vec2 closely aligns with assessment dimensions. Despite its effectiveness, this system remains a black box with no clear interpretation of the connection between the model ASR dimension and clinical assessments. This paper presents the first analysis of this baseline model for speech quality assessment, focusing on intelligibility and severity tasks. We conduct a layer-wise analysis to identify key layers and compare different SSL and ASR Wav2Vec2 models based on pre-trained data. Additionally, post-hoc XAI methods, including Canonical Correlation Analysis (CCA) and visualization techniques, are used to track model evolution and visualize embeddings for enhanced interpretability.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2410.08118.pdf' target='_blank'>https://arxiv.org/pdf/2410.08118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Chen, Ameenat L. Solebo, Weiye Bao, Paul Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08118">Medical Image Quality Assessment based on Probability of Necessity and Sufficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image quality assessment (MIQA) is essential for reliable medical image analysis. While deep learning has shown promise in this field, current models could be misled by spurious correlations learned from data and struggle with out-of-distribution (OOD) scenarios. To that end, we propose an MIQA framework based on a concept from causal inference: Probability of Necessity and Sufficiency (PNS). PNS measures how likely a set of features is to be both necessary (always present for an outcome) and sufficient (capable of guaranteeing an outcome) for a particular result. Our approach leverages this concept by learning hidden features from medical images with high PNS values for quality prediction. This encourages models to capture more essential predictive information, enhancing their robustness to OOD scenarios. We evaluate our framework on an Anterior Segment Optical Coherence Tomography (AS-OCT) dataset for the MIQA task and experimental results demonstrate the effectiveness of our framework.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2408.10134.pdf' target='_blank'>https://arxiv.org/pdf/2408.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10134">Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth perception plays an essential role in the viewer experience for immersive virtual reality (VR) visual environments. However, previous research investigations in the depth quality of 3D/stereoscopic images are rather limited, and in particular, are largely lacking for 3D viewing of 360-degree omnidirectional content. In this work, we make one of the first attempts to develop an objective quality assessment model named depth quality index (DQI) for efficient no-reference (NR) depth quality assessment of stereoscopic omnidirectional images. Motivated by the perceptual characteristics of the human visual system (HVS), the proposed DQI is built upon multi-color-channel, adaptive viewport selection, and interocular discrepancy features. Experimental results demonstrate that the proposed method outperforms state-of-the-art image quality assessment (IQA) and depth quality assessment (DQA) approaches in predicting the perceptual depth quality when tested using both single-viewport and omnidirectional stereoscopic image databases. Furthermore, we demonstrate that combining the proposed depth quality model with existing IQA methods significantly boosts the performance in predicting the overall quality of 3D omnidirectional images.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2408.03885.pdf' target='_blank'>https://arxiv.org/pdf/2408.03885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Wang, Yun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03885">No-Reference Image Quality Assessment with Global-Local Progressive Integration and Semantic-Aligned Quality Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate measurement of image quality without reference signals remains a fundamental challenge in low-level visual perception applications. In this paper, we propose a global-local progressive integration model that addresses this challenge through three key contributions: 1) We develop a dual-measurement framework that combines vision Transformer (ViT)-based global feature extractor and convolutional neural networks (CNNs)-based local feature extractor to comprehensively capture and quantify image distortion characteristics at different granularities. 2) We propose a progressive feature integration scheme that utilizes multi-scale kernel configurations to align global and local features, and progressively aggregates them via an interactive stack of channel-wise self-attention and spatial interaction modules for multi-grained quality-aware representations. 3) We introduce a semantic-aligned quality transfer method that extends the training data by automatically labeling the quality scores of diverse image content with subjective opinion scores. Experimental results demonstrate that our model yields 5.04% and 5.40% improvements in Spearman's rank-order correlation coefficient (SROCC) for cross-authentic and cross-synthetic dataset generalization tests, respectively. Furthermore, the proposed semantic-aligned quality transfer further yields 2.26% and 13.23% performance gains in evaluations on single-synthetic and cross-synthetic datasets.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2405.13331.pdf' target='_blank'>https://arxiv.org/pdf/2405.13331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Toukir Ahmed, Arthur Villordon, Mohammed Kamruzzaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13331">Comparative Analysis of Hyperspectral Image Reconstruction Using Deep Learning for Agricultural and Biological Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imaging (HSI) has become a key technology for non-invasive quality evaluation in various fields, offering detailed insights through spatial and spectral data. Despite its efficacy, the complexity and high cost of HSI systems have hindered their widespread adoption. This study addressed these challenges by exploring deep learning-based hyperspectral image reconstruction from RGB (Red, Green, Blue) images, particularly for agricultural products. Specifically, different hyperspectral reconstruction algorithms, such as Hyperspectral Convolutional Neural Network - Dense (HSCNN-D), High-Resolution Network (HRNET), and Multi-Scale Transformer Plus Plus (MST++), were compared to assess the dry matter content of sweet potatoes. Among the tested reconstruction methods, HRNET demonstrated superior performance, achieving the lowest mean relative absolute error (MRAE) of 0.07, root mean square error (RMSE) of 0.03, and the highest peak signal-to-noise ratio (PSNR) of 32.28 decibels (dB). Some key features were selected using the genetic algorithm (GA), and their importance was interpreted using explainable artificial intelligence (XAI). Partial least squares regression (PLSR) models were developed using the RGB, reconstructed, and ground truth (GT) data. The visual and spectra quality of these reconstructed methods was compared with GT data, and predicted maps were generated. The results revealed the prospect of deep learning-based hyperspectral image reconstruction as a cost-effective and efficient quality assessment tool for agricultural and biological applications.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2405.12313.pdf' target='_blank'>https://arxiv.org/pdf/2405.12313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Toukir Ahmed, Ocean Monjur, Mohammed Kamruzzaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12313">Deep learning-based hyperspectral image reconstruction for quality assessment of agro-product</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data. Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems. Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications. Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes. The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth. The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes. These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2405.05742.pdf' target='_blank'>https://arxiv.org/pdf/2405.05742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Smith, Zheming Zuo, Jonathan Stonehouse, Boguslaw Obara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05742">How Quality Affects Deep Neural Networks in Fine-Grained Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a No-Reference Image Quality Assessment (NRIQA) guided cut-off point selection (CPS) strategy to enhance the performance of a fine-grained classification system. Scores given by existing NRIQA methods on the same image may vary and not be as independent of natural image augmentations as expected, which weakens their connection and explainability to fine-grained image classification. Taking the three most commonly adopted image augmentation configurations -- cropping, rotating, and blurring -- as the entry point, we formulate a two-step mechanism for selecting the most discriminative subset from a given image dataset by considering both the confidence of model predictions and the density distribution of image qualities over several NRIQA methods. Concretely, the cut-off points yielded by those methods are aggregated via majority voting to inform the process of image subset selection. The efficacy and efficiency of such a mechanism have been confirmed by comparing the models being trained on high-quality images against a combination of high- and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial product dataset in terms of mean accuracy through four deep neural classifiers. The robustness of the mechanism has been proven by the observations that all the selected high-quality images can work jointly with 70% low-quality images with 1.3% of classification precision sacrificed when using ResNet34 in an ablation study.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2405.02208.pdf' target='_blank'>https://arxiv.org/pdf/2405.02208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Cui, Alfredo De Goyeneche, Efrat Shimron, Boyuan Ma, Michael Lustig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02208">Reference-Free Image Quality Metric for Degradation and Reconstruction Artifacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) is essential in various Computer Vision tasks such as image deblurring and super-resolution. However, most IQA methods require reference images, which are not always available. While there are some reference-free IQA metrics, they have limitations in simulating human perception and discerning subtle image quality variations. We hypothesize that the JPEG quality factor is representatives of image quality measurement, and a well-trained neural network can learn to accurately evaluate image quality without requiring a clean reference, as it can recognize image degradation artifacts based on prior knowledge. Thus, we developed a reference-free quality evaluation network, dubbed "Quality Factor (QF) Predictor", which does not require any reference. Our QF Predictor is a lightweight, fully convolutional network comprising seven layers. The model is trained in a self-supervised manner: it receives JPEG compressed image patch with a random QF as input, is trained to accurately predict the corresponding QF. We demonstrate the versatility of the model by applying it to various tasks. First, our QF Predictor can generalize to measure the severity of various image artifacts, such as Gaussian Blur and Gaussian noise. Second, we show that the QF Predictor can be trained to predict the undersampling rate of images reconstructed from Magnetic Resonance Imaging (MRI) data.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2404.09764.pdf' target='_blank'>https://arxiv.org/pdf/2404.09764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paramita Das, Isaac Johnson, Diego Saez-Trumper, Pablo AragÃ³n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09764">Language-Agnostic Modeling of Wikipedia Articles for Content Quality Assessment across Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wikipedia is the largest web repository of free knowledge. Volunteer editors devote time and effort to creating and expanding articles in more than 300 language editions. As content quality varies from article to article, editors also spend substantial time rating articles with specific criteria. However, keeping these assessments complete and up-to-date is largely impossible given the ever-changing nature of Wikipedia. To overcome this limitation, we propose a novel computational framework for modeling the quality of Wikipedia articles.
  State-of-the-art approaches to model Wikipedia article quality have leveraged machine learning techniques with language-specific features. In contrast, our framework is based on language-agnostic structural features extracted from the articles, a set of universal weights, and a language version-specific normalization criterion. Therefore, we ensure that all language editions of Wikipedia can benefit from our framework, even those that do not have their own quality assessment scheme. Using this framework, we have built datasets with the feature values and quality scores of all revisions of all articles in the existing language versions of Wikipedia. We provide a descriptive analysis of these resources and a benchmark of our framework. In addition, we discuss possible downstream tasks to be addressed with these datasets, which are released for public use.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2403.20184.pdf' target='_blank'>https://arxiv.org/pdf/2403.20184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuan Nguyen, Corinne Fredouille, Alain Ghio, Mathieu Balaguer, Virginie Woisard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20184">Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intelligibility and severity scores respectively, using only 95 training samples. It shows that the ASR based Wav2Vec2 model brings the best results and may indicate a strong correlation between ASR and speech quality assessment. We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2403.09167.pdf' target='_blank'>https://arxiv.org/pdf/2403.09167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Sun, Chaoyang Mei, Linlin Wei, Kaiyu Zheng, Na Liu, Ming Cui, Tianyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09167">Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general LLMs can be enhanced through fine-tuning with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for fine-tuning.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2403.08836.pdf' target='_blank'>https://arxiv.org/pdf/2403.08836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Irwin, Marco Dossena, Giorgio Leonardi, Stefania Montani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08836">Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive process monitoring is a process mining task aimed at forecasting information about a running process trace, such as the most correct next activity to be executed. In medical domains, predictive process monitoring can provide valuable decision support in atypical and nontrivial situations. Decision support and quality assessment in medicine cannot ignore domain knowledge, in order to be grounded on all the available information (which is not limited to data) and to be really acceptable by end users.
  In this paper, we propose a predictive process monitoring approach relying on the use of a {\em transformer}, a deep learning architecture based on the attention mechanism. A major contribution of our work lies in the incorporation of ontological domain-specific knowledge, carried out through a graph positional encoding technique. The paper presents and discusses the encouraging experimental result we are collecting in the domain of stroke management.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2402.07258.pdf' target='_blank'>https://arxiv.org/pdf/2402.07258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samiha Mirza, Vuong D. Nguyen, Pranav Mantini, Shishir K. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07258">Data Quality Aware Approaches for Addressing Model Drift of Semantic Segmentation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the midst of the rapid integration of artificial intelligence (AI) into real world applications, one pressing challenge we confront is the phenomenon of model drift, wherein the performance of AI models gradually degrades over time, compromising their effectiveness in real-world, dynamic environments. Once identified, we need techniques for handling this drift to preserve the model performance and prevent further degradation. This study investigates two prominent quality aware strategies to combat model drift: data quality assessment and data conditioning based on prior model knowledge. The former leverages image quality assessment metrics to meticulously select high-quality training data, improving the model robustness, while the latter makes use of learned feature vectors from existing models to guide the selection of future data, aligning it with the model's prior knowledge. Through comprehensive experimentation, this research aims to shed light on the efficacy of these approaches in enhancing the performance and reliability of semantic segmentation models, thereby contributing to the advancement of computer vision capabilities in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2402.00993.pdf' target='_blank'>https://arxiv.org/pdf/2402.00993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Farhad Hosseini-Benvidi, Hossein Motamednia, Azadeh Mansouri, Mohammadreza Raei, Ahmad Mahmoudi-Aznaveh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00993">Compressed image quality assessment using stacking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is well-known that there is no universal metric for image quality evaluation. In this case, distortion-specific metrics can be more reliable. The artifact imposed by image compression can be considered as a combination of various distortions. Depending on the image context, this combination can be different. As a result, Generalization can be regarded as the major challenge in compressed image quality assessment. In this approach, stacking is employed to provide a reliable method. Both semantic and low-level information are employed in the presented IQA to predict the human visual system. Moreover, the results of the Full-Reference (FR) and No-Reference (NR) models are aggregated to improve the proposed Full-Reference method for compressed image quality evaluation. The accuracy of the quality benchmark of the clic2024 perceptual image challenge was achieved 79.6\%, which illustrates the effectiveness of the proposed fusion-based approach.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2402.00803.pdf' target='_blank'>https://arxiv.org/pdf/2402.00803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chufan Gao, Nicholas Gisolfi, Artur Dubrawski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00803">Signal Quality Auditing for Time-series Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing "silent failures" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2312.10817.pdf' target='_blank'>https://arxiv.org/pdf/2312.10817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Na Li, Yiyang Qi, Ruyue Xin, Zhiming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10817">Ocean Data Quality Assessment through Outlier Detection-enhanced Active Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an ODEAL framework for ocean data quality assessment, employing AL to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9% using the initial set built with outlier detectors.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2311.15698.pdf' target='_blank'>https://arxiv.org/pdf/2311.15698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico A. Galatolo, Mario G. C. A. Cimino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15698">Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel approach for generating high-quality, language-specific chat corpora using a self-chat mechanism. We combine a generator LLM for creating new samples and an embedder LLM to ensure diversity. A new Masked Language Modelling (MLM) model-based quality assessment metric is proposed for evaluating and filtering the corpora. Utilizing the llama2-70b as the generator and a multilingual sentence transformer as embedder, we generate an Italian chat corpus and refine the Fauno corpus, which is based on translated English ChatGPT self-chat data. The refinement uses structural assertions and Natural Language Processing techniques. Both corpora undergo a comprehensive quality evaluation using the proposed MLM model-based quality metric. The Italian LLM fine-tuned with these corpora demonstrates significantly enhanced language comprehension and question-answering skills. The resultant model, cerbero-7b, establishes a new state-of-the-art for Italian LLMs. This approach marks a substantial advancement in the development of language-specific LLMs, with a special emphasis on augmenting corpora for underrepresented languages like Italian.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2311.11057.pdf' target='_blank'>https://arxiv.org/pdf/2311.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diana Koldasbayeva, Polina Tregubova, Mikhail Gasanov, Alexey Zaytsev, Anna Petrovskaia, Evgeny Burnaev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11057">Challenges in data-based geospatial modeling for environmental research and practice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of electronic data, particularly Earth observation data, data-based geospatial modelling using machine learning (ML) has gained popularity in environmental research. Accurate geospatial predictions are vital for domain research based on ecosystem monitoring and quality assessment and for policy-making and action planning, considering effective management of natural resources. The accuracy and computation speed of ML has generally proved efficient. However, many questions have yet to be addressed to obtain precise and reproducible results suitable for further use in both research and practice. A better understanding of the ML concepts applicable to geospatial problems enhances the development of data science tools providing transparent information crucial for making decisions on global challenges such as biosphere degradation and climate change. This survey reviews common nuances in geospatial modelling, such as imbalanced data, spatial autocorrelation, prediction errors, model generalisation, domain specificity, and uncertainty estimation. We provide an overview of techniques and popular programming tools to overcome or account for the challenges. We also discuss prospects for geospatial Artificial Intelligence in environmental applications.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2309.14868.pdf' target='_blank'>https://arxiv.org/pdf/2309.14868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Chen, Zhiliang Ma, Yang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14868">Cross-Dataset-Robust Method for Blind Real-World Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although many effective models and real-world datasets have been presented for blind image quality assessment (BIQA), recent BIQA models usually tend to fit specific training set. Hence, it is still difficult to accurately and robustly measure the visual quality of an arbitrary real-world image. In this paper, a robust BIQA method, is designed based on three aspects, i.e., robust training strategy, large-scale real-world dataset, and powerful backbone. First, many individual models based on popular and state-of-the-art (SOTA) Swin-Transformer (SwinT) are trained on different real-world BIQA datasets respectively. Then, these biased SwinT-based models are jointly used to generate pseudo-labels, which adopts the probability of relative quality of two random images instead of fixed quality score. A large-scale real-world image dataset with 1,000,000 image pairs and pseudo-labels is then proposed for training the final cross-dataset-robust model. Experimental results on cross-dataset tests show that the performance of the proposed method is even better than some SOTA methods that are directly trained on these datasets, thus verifying the robustness and generalization of our method.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2309.01480.pdf' target='_blank'>https://arxiv.org/pdf/2309.01480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01480">EventTrojan: Manipulating Non-Intrusive Speech Quality Assessment via Imperceptible Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting speech's mean opinion score (MOS) without requiring the reference speech. Researchers have gradually started to apply NISQA to various practical scenarios. However, little attention has been paid to the security of NISQA models. Backdoor attacks represent the most serious threat to deep neural networks (DNNs) due to the fact that backdoors possess a very high attack success rate once embedded. However, existing backdoor attacks assume that the attacker actively feeds samples containing triggers into the model during the inference phase. This is not adapted to the specific scenario of NISQA. And current backdoor attacks on regression tasks lack an objective metric to measure the attack performance. To address these issues, we propose a novel backdoor triggering approach (EventTrojan) that utilizes an event during the usage of the NISQA model as a trigger. Moreover, we innovatively provide an objective metric for backdoor attacks on regression tasks. Extensive experiments on four benchmark datasets demonstrate the effectiveness of the EventTrojan attack. Besides, it also has good resistance to several defense methods.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2305.09353.pdf' target='_blank'>https://arxiv.org/pdf/2305.09353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsong Shi, Pan Gao, Aljosa Smolic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09353">Blind Image Quality Assessment via Transformer Predicted Error Map and Perceptual Quality Token</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment is a fundamental problem in the field of image processing, and due to the lack of reference images in most practical scenarios, no-reference image quality assessment (NR-IQA), has gained increasing attention recently. With the development of deep learning technology, many deep neural network-based NR-IQA methods have been developed, which try to learn the image quality based on the understanding of database information. Currently, Transformer has achieved remarkable progress in various vision tasks. Since the characteristics of the attention mechanism in Transformer fit the global perceptual impact of artifacts perceived by a human, Transformer is thus well suited for image quality assessment tasks. In this paper, we propose a Transformer based NR-IQA model using a predicted objective error map and perceptual quality token. Specifically, we firstly generate the predicted error map by pre-training one model consisting of a Transformer encoder and decoder, in which the objective difference between the distorted and the reference images is used as supervision. Then, we freeze the parameters of the pre-trained model and design another branch using the vision Transformer to extract the perceptual quality token for feature fusion with the predicted error map. Finally, the fused features are regressed to the final image quality score. Extensive experiments have shown that our proposed method outperforms the current state-of-the-art in both authentic and synthetic image databases. Moreover, the attentional map extracted by the perceptual quality token also does conform to the characteristics of the human visual system.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2305.02142.pdf' target='_blank'>https://arxiv.org/pdf/2305.02142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nabajeet Barman, Yuriy Reznik, Maria Martini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02142">Datasheet for Subjective and Objective Quality Assessment Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the years, many subjective and objective quality assessment datasets have been created and made available to the research community. However, there is no standard process for documenting the various aspects of the dataset, such as details about the source sequences, number of test subjects, test methodology, encoding settings, etc. Such information is often of great importance to the users of the dataset as it can help them get a quick understanding of the motivation and scope of the dataset. Without such a template, it is left to each reader to collate the information from the relevant publication or website, which is a tedious and time-consuming process. In some cases, the absence of a template to guide the documentation process can result in an unintentional omission of some important information.
  This paper addresses this simple but significant gap by proposing a datasheet template for documenting various aspects of subjective and objective quality assessment datasets for multimedia data. The contributions presented in this work aim to simplify the documentation process for existing and new datasets and improve their reproducibility. The proposed datasheet template is available on GitHub, along with a few sample datasheets of a few open-source audiovisual subjective and objective datasets.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2305.01965.pdf' target='_blank'>https://arxiv.org/pdf/2305.01965.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MarÃ­a Andrea Cruz BlandÃ³n, Alejandrina Cristia, Okko RÃ¤sÃ¤nen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01965">Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modelling of early language acquisition aims to understand how infants bootstrap their language skills. The modelling encompasses properties of the input data used for training the models, the cognitive hypotheses and their algorithmic implementations being tested, and the evaluation methodologies to compare models to human data. Recent developments have enabled the use of more naturalistic training data for computational models. This also motivates development of more naturalistic tests of model behaviour. A crucial step towards such an aim is to develop representative speech datasets consisting of speech heard by infants in their natural environments. However, a major drawback of such recordings is that they are typically noisy, and it is currently unclear how the sound quality could affect analyses and modelling experiments conducted on such data. In this paper, we explore this aspect for the case of infant-directed speech (IDS) and adult-directed speech (ADS) analysis. First, we manually and automatically annotated audio quality of utterances extracted from two corpora of child-centred long-form recordings (in English and French). We then compared acoustic features of IDS and ADS in an in-lab dataset and across different audio quality subsets of naturalistic data. Finally, we assessed how the audio quality and recording environment may change the conclusions of a modelling analysis using a recent self-supervised learning model. Our results show that the use of modest and high audio quality naturalistic speech data result in largely similar conclusions on IDS and ADS in terms of acoustic analyses and modelling experiments. We also found that an automatic sound quality assessment tool can be used to screen out useful parts of long-form recordings for a closer analysis with comparable results to that of manual quality annotation.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2305.01855.pdf' target='_blank'>https://arxiv.org/pdf/2305.01855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changrong Xiao, Sean Xin Xu, Kunpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01855">Multimodal Data Augmentation for Image Captioning using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning, an important vision-language task, often requires a tremendous number of finely labeled image-caption pairs for learning the underlying alignment between images and texts. In this paper, we proposed a multimodal data augmentation method, leveraging a recent text-to-image model called Stable Diffusion, to expand the training set via high-quality generation of image-caption pairs. Extensive experiments on the MS COCO dataset demonstrate the advantages of our approach over several benchmark methods, and particularly a significant boost when having fewer training instances. In addition, models trained on our augmented datasets also outperform prior unpaired image captioning methods by a large margin. Finally, further improvement regarding the training efficiency and effectiveness can be obtained after intentionally filtering the generated data based on quality assessment.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2304.10701.pdf' target='_blank'>https://arxiv.org/pdf/2304.10701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Yang, Wenglong Deng, Benlin Liu, Yangsibo Huang, James Zou, Xiaoxiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10701">GMValuator: Similarity-based Data Valuation for Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data valuation plays a crucial role in machine learning. Existing data valuation methods, mainly focused on discriminative models, overlook generative models that have gained attention recently. In generative models, data valuation measures the impact of training data on generated datasets. Very few existing attempts at data valuation methods designed for deep generative models either concentrate on specific models or lack robustness in their outcomes. Moreover, efficiency still reveals vulnerable shortcomings. We formulate the data valuation problem in generative models from a similarity matching perspective to bridge the gaps. Specifically, we introduce Generative Model Valuator (GMValuator), the first training-free and model-agnostic approach to providing data valuation for image generation tasks. It empowers efficient data valuation through our innovative similarity matching module, calibrates biased contributions by incorporating image quality assessment, and attributes credits to all training samples based on their contributions to the generated samples. Additionally, we introduce four evaluation criteria for assessing data valuation methods in generative models. GMValuator is extensively evaluated on benchmark and high-resolution datasets and various mainstream generative architectures to demonstrate its effectiveness.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2303.01950.pdf' target='_blank'>https://arxiv.org/pdf/2303.01950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dasa Kusnirakova, Mouzhi Ge, Leonard Walletzky, Barbora Buhnova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01950">Interoperability-oriented Quality Assessment for Czech Open Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid increase of published open datasets, it is crucial to support the open data progress in smart cities while considering the open data quality. In the Czech Republic, and its National Open Data Catalogue (NODC), the open datasets are usually evaluated based on their metadata only, while leaving the content and the adherence to the recommended data structure to the sole responsibility of the data providers. The interoperability of open datasets remains unknown. This paper therefore aims to propose a novel content-aware quality evaluation framework that assesses the quality of open datasets based on five data quality dimensions. With the proposed framework, we provide a fundamental view on the interoperability-oriented data quality of Czech open datasets, which are published in NODC. Our evaluations find that domain-specific open data quality assessments are able to detect data quality issues beyond traditional heuristics used for determining Czech open data quality, increase their interoperability, and thus increase their potential to bring value for the society. The findings of this research are beneficial not only for the case of the Czech Republic, but also can be applied in other countries that intend to enhance their open data quality evaluation processes.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2302.14062.pdf' target='_blank'>https://arxiv.org/pdf/2302.14062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoliang Wu, Peter Bell, Ajitha Rajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14062">Explanations for Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address quality assessment for neural network based ASR by providing explanations that help increase our understanding of the system and ultimately help build trust in the system. Compared to simple classification labels, explaining transcriptions is more challenging as judging their correctness is not straightforward and transcriptions as a variable-length sequence is not handled by existing interpretable machine learning models. We provide an explanation for an ASR transcription as a subset of audio frames that is both a minimal and sufficient cause of the transcription. To do this, we adapt existing explainable AI (XAI) techniques from image classification-Statistical Fault Localisation(SFL) and Causal. Additionally, we use an adapted version of Local Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in our experiments. We evaluate the quality of the explanations generated by the proposed techniques over three different ASR ,Google API, the baseline model of Sphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2302.13770.pdf' target='_blank'>https://arxiv.org/pdf/2302.13770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Xiao, Shuai He, Limin Liu, Anlong Ming
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13770">Mask Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding semantic information is an essential step in knowing what is being learned in both full-reference (FR) and no-reference (NR) image quality assessment (IQA) methods. However, especially for many severely distorted images, even if there is an undistorted image as a reference (FR-IQA), it is difficult to perceive the lost semantic and texture information of distorted images directly. In this paper, we propose a Mask Reference IQA (MR-IQA) method that masks specific patches of a distorted image and supplements missing patches with the reference image patches. In this way, our model only needs to input the reconstructed image for quality assessment. First, we design a mask generator to select the best candidate patches from reference images and supplement the lost semantic information in distorted images, thus providing more reference for quality assessment; in addition, the different masked patches imply different data augmentations, which favors model training and reduces overfitting. Second, we provide a Mask Reference Network (MRNet): the dedicated modules can prevent disturbances due to masked patches and help eliminate the patch discontinuity in the reconstructed image. Our method achieves state-of-the-art performances on the benchmark KADID-10k, LIVE and CSIQ datasets and has better generalization performance across datasets. The code and results are available in the supplementary material.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2302.12393.pdf' target='_blank'>https://arxiv.org/pdf/2302.12393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12393">Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\times$360$^{\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global omnidirectional image, respectively. A quality regression along with a weighting process is then followed that maps the extracted quality-aware features to a perceptual quality prediction. Experimental results demonstrate that the proposed S$^2$ method offers highly competitive performance against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2301.09496.pdf' target='_blank'>https://arxiv.org/pdf/2301.09496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Simone, Davide Bacciu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09496">ECGAN: Self-supervised generative adversarial network for electrocardiography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality synthetic data can support the development of effective predictive models for biomedical tasks, especially in rare diseases or when subject to compelling privacy constraints. These limitations, for instance, negatively impact open access to electrocardiography datasets about arrhythmias. This work introduces a self-supervised approach to the generation of synthetic electrocardiography time series which is shown to promote morphological plausibility. Our model (ECGAN) allows conditioning the generative process for specific rhythm abnormalities, enhancing synchronization and diversity across samples with respect to literature models. A dedicated sample quality assessment framework is also defined, leveraging arrhythmia classifiers. The empirical results highlight a substantial improvement against state-of-the-art generative models for sequences and audio synthesis.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2301.01469.pdf' target='_blank'>https://arxiv.org/pdf/2301.01469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Min Hyun, Tae Jun Jang, Jeongchan Nam, Hyeuknam Kwon, Kiwan Jeon, Kyunghun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01469">Machine Learning-based Signal Quality Assessment for Cardiac Volume Monitoring in Electrical Impedance Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Owing to recent advances in thoracic electrical impedance tomography, a patient's hemodynamic function can be noninvasively and continuously estimated in real-time by surveilling a cardiac volume signal associated with stroke volume and cardiac output. In clinical applications, however, a cardiac volume signal is often of low quality, mainly because of the patient's deliberate movements or inevitable motions during clinical interventions. This study aims to develop a signal quality indexing method that assesses the influence of motion artifacts on transient cardiac volume signals. The assessment is performed on each cardiac cycle to take advantage of the periodicity and regularity in cardiac volume changes. Time intervals are identified using the synchronized electrocardiography system. We apply divergent machine-learning methods, which can be sorted into discriminative-model and manifold-learning approaches. The use of machine-learning could be suitable for our real-time monitoring application that requires fast inference and automation as well as high accuracy. In the clinical environment, the proposed method can be utilized to provide immediate warnings so that clinicians can minimize confusion regarding patients' conditions, reduce clinical resource utilization, and improve the confidence level of the monitoring system. Numerous experiments using actual EIT data validate the capability of cardiac volume signals degraded by motion artifacts to be accurately and automatically assessed in real-time by machine learning. The best model achieved an accuracy of 0.95, positive and negative predictive values of 0.96 and 0.86, sensitivity of 0.98, specificity of 0.77, and AUC of 0.96.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2207.14513.pdf' target='_blank'>https://arxiv.org/pdf/2207.14513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caixia Zhou, Yaping Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14513">Uncertainty-Driven Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic action quality assessment (AQA) has attracted increasing attention due to its wide applications. However, most existing AQA methods employ deterministic models to predict the final score for each action, while overlooking the subjectivity and diversity among expert judges during the scoring process. In this paper, we propose a novel probabilistic model, named Uncertainty-Driven AQA (UD-AQA), to utilize and capture the diversity among multiple judge scores. Specifically, we design a Conditional Variational Auto-Encoder (CVAE)-based module to encode the uncertainty in expert assessment, where multiple judge scores can be produced by sampling latent features from the learned latent space multiple times. To further utilize the uncertainty, we generate the estimation of uncertainty for each prediction, which is employed to re-weight AQA regression loss, effectively reducing the influence of uncertain samples during training. Moreover, we further design an uncertainty-guided training strategy to dynamically adjust the learning order of the samples from low uncertainty to high uncertainty. The experiments show that our proposed method achieves competitive results on three benchmarks including the Olympic events MTL-AQA and FineDiving, and the surgical skill JIGSAWS datasets.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2207.04523.pdf' target='_blank'>https://arxiv.org/pdf/2207.04523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Knott, Fernando Perez-Cruz, Thijs Defraeye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.04523">Facilitated machine learning for image-based fruit quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based machine learning models can be used to make the sorting and grading of agricultural products more efficient. In many regions, implementing such systems can be difficult due to the lack of centralization and automation of postharvest supply chains. Stakeholders are often too small to specialize in machine learning, and large training data sets are unavailable. We propose a machine learning procedure for images based on pre-trained Vision Transformers. It is easier to implement than the current standard approach of training Convolutional Neural Networks (CNNs) as we do not (re-)train deep neural networks. We evaluate our approach based on two data sets for apple defect detection and banana ripeness estimation. Our model achieves a competitive classification accuracy equal to or less than one percent below the best-performing CNN. At the same time, it requires three times fewer training samples to achieve a 90% accuracy.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2203.00756.pdf' target='_blank'>https://arxiv.org/pdf/2203.00756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Rybakov, Marco Tagliasacchi, Yunpeng Li, Liyang Jiang, Xia Zhang, Fadi Biadsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.00756">Real time spectrogram inversion on mobile phone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present two methods of real time magnitude spectrogram inversion: streaming Griffin Lim(GL) and streaming MelGAN. We demonstrate the impact of looking ahead on perceptual quality of MelGAN. As little as one hop size (12.5ms) of lookahead is able to significantly improve perceptual quality in comparison to its causal version. We compare streaming GL with the streaming MelGAN and show different trade-offs in terms of perceptual quality, on-device latency, algorithmic delay, memory footprint and noise sensitivity. For fair quality assessment of the GL approach, we use input log magnitude spectrogram without mel transformation. We evaluate presented real time spectrogram inversion approaches on clean, noisy and atypical speech. We specified conditions when streaming GL has comparable quality with MelGAN: noisy audio and no mel transformation. Streaming GL is 2.4x faster than real time on the ARM CPU of a Pixel4 and it uses 4.5x times less memory than MelGAN.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2007.10213.pdf' target='_blank'>https://arxiv.org/pdf/2007.10213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joao Caldeira, Fernando Brito e Abreu, Jorge Cardoso, Rachel SimÃµes, Toacy Oliveira, JosÃ© Reis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.10213">Software Development Analytics in Practice: A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context:Software Development Analytics is a research area concerned with providing insights to improve product deliveries and processes. Many types of studies, data sources and mining methods have been used for that purpose. Objective:This systematic literature review aims at providing an aggregate view of the relevant studies on Software Development Analytics in the past decade, with an emphasis on its application in practical settings. Method:Definition and execution of a search string upon several digital libraries, followed by a quality assessment criteria to identify the most relevant papers. On those, we extracted a set of characteristics (study type, data source, study perspective, development life-cycle activities covered, stakeholders, mining methods, and analytics scope) and classified their impact against a taxonomy. Results:Source code repositories, experimental case studies, and developers are the most common data sources, study types, and stakeholders, respectively. Product and project managers are also often present, but less than expected. Mining methods are evolving rapidly and that is reflected in the long list identified. Descriptive statistics are the most usual method followed by correlation analysis. Being software development an important process in every organization, it was unexpected to find that process mining was present in only one study. Most contributions to the software development life cycle were given in the quality dimension. Time management and costs control were lightly debated. The analysis of security aspects suggests it is an increasing topic of concern for practitioners. Risk management contributions are scarce. Conclusions:There is a wide improvement margin for software development analytics in practice. For instance, mining and analyzing the activities performed by software developers in their actual workbench, the IDE.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2511.13353.pdf' target='_blank'>https://arxiv.org/pdf/2511.13353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Gabriel Telesco, Danila Nejamkin, Estefanía Mata, Francisco Filizzola, Kevin Wignall, Lucía Franco Troilo, María de los Angeles Cenoz, Melissa Thompson, Mercedes Leguía, Ignacio Larrabide, José Ignacio Orlando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13353">Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2511.08061.pdf' target='_blank'>https://arxiv.org/pdf/2511.08061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditi Singhania, Arushi Jain, Krutik Malani, Riddhi Dhawan, Souymodip Chakraborty, Vineet Batra, Ankit Phogat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08061">Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2511.05853.pdf' target='_blank'>https://arxiv.org/pdf/2511.05853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyang Guo, Qiang Zuo, Ruiyun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05853">Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2511.05393.pdf' target='_blank'>https://arxiv.org/pdf/2511.05393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05393">PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2511.02827.pdf' target='_blank'>https://arxiv.org/pdf/2511.02827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Almukhtar, Anwar Ghammam, Marouane Kessentini, Hua Ming
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02827">From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an era shaped by Generative Artificial Intelligence for code generation and the rising adoption of Python-based Machine Learning systems (MLS), software quality has emerged as a major concern. As these systems grow in complexity and importance, a key obstacle lies in understanding exactly how specific code changes affect overall quality-a shortfall aggravated by the lack of quality assessment tools and a clear mapping between ML systems code changes and their quality effects. Although prior work has explored code changes in MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of the relationship between code changes and the MLS quality. To address this gap, we conducted a large-scale empirical study of 3,340 open-source Python ML projects, encompassing more than 3.7 million commits and 2.7 trillion lines of code. We introduce PyQu, a novel tool that leverages low level software metrics to identify quality-enhancing commits with an average accuracy, precision, and recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic analysis, we identified 61 code changes, each demonstrating a direct impact on enhancing software quality, and we classified them into 13 categories based on contextual characteristics. 41% of the changes are newly discovered by our study and have not been identified by state-of-the-art Python changes detection tools. Our work offers a vital foundation for researchers, practitioners, educators, and tool developers, advancing the quest for automated quality assessment and best practices in Python-based ML software.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2510.23941.pdf' target='_blank'>https://arxiv.org/pdf/2510.23941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Satyadharma, Fatemeh Sheikholeslami, Swati Kaul, Aziz Umit Batur, Suleiman A. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23941">Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel, training free cascade for auto-prompting Large Language Models (LLMs) to assess product quality in e-commerce. Our system requires no training labels or model fine-tuning, instead automatically generating and refining prompts for evaluating attribute quality across tens of thousands of product category-attribute pairs. Starting from a seed of human-crafted prompts, the cascade progressively optimizes instructions to meet catalog-specific requirements. This approach bridges the gap between general language understanding and domain-specific knowledge at scale in complex industrial catalogs. Our extensive empirical evaluations shows the auto-prompt cascade improves precision and recall by $8-10\%$ over traditional chain-of-thought prompting. Notably, it achieves these gains while reducing domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$ reduction. Additionally, the cascade generalizes effectively across five languages and multiple quality assessment tasks, consistently maintaining performance gains.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2508.17397.pdf' target='_blank'>https://arxiv.org/pdf/2508.17397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoqi Li, Yanghui Song, Jichao Dao, Chengfu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17397">Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenging problem of image enhancement in complex underwater scenes by proposing a solution based on deep learning. The proposed method skillfully integrates two deep convolutional neural network models, VGG19 and ResNet50, leveraging their powerful feature extraction capabilities to perform multi-scale and multi-level deep feature analysis of underwater images. By constructing a unified model, the complementary advantages of the two models are effectively integrated, achieving a more comprehensive and accurate image enhancement effect.To objectively evaluate the enhancement effect, this paper introduces image quality assessment metrics such as PSNR, UCIQE, and UIQM to quantitatively compare images before and after enhancement and deeply analyzes the performance of different models in different scenarios.Furthermore, to improve the practicality and stability of the underwater visual enhancement system, this paper also provides practical suggestions from aspects such as model optimization, multi-model fusion, and hardware selection, aiming to provide strong technical support for visual enhancement tasks in complex underwater environments.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2508.16657.pdf' target='_blank'>https://arxiv.org/pdf/2508.16657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyuan Hong, Huimin Zhao, Ying Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16657">Leveraging Multi-Source Textural UGC for Neighbourhood Housing Quality Assessment: A GPT-Enhanced Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study leverages GPT-4o to assess neighbourhood housing quality using multi-source textural user-generated content (UGC) from Dianping, Weibo, and the Government Message Board. The analysis involves filtering relevant texts, extracting structured evaluation units, and conducting sentiment scoring. A refined housing quality assessment system with 46 indicators across 11 categories was developed, highlighting an objective-subjective method gap and platform-specific differences in focus. GPT-4o outperformed rule-based and BERT models, achieving 92.5% accuracy in fine-tuned settings. The findings underscore the value of integrating UGC and GPT-driven analysis for scalable, resident-centric urban assessments, offering practical insights for policymakers and urban planners.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2508.16255.pdf' target='_blank'>https://arxiv.org/pdf/2508.16255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Loizou, Dimitrios Tsoumakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16255">Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the volume and diversity of available datasets continue to increase, assessing data quality has become crucial for reliable and efficient Machine Learning analytics. A modern, game-theoretic approach for evaluating data quality is the notion of Data Shapley which quantifies the value of individual data points within a dataset. State-of-the-art methods to scale the NP-hard Shapley computation also face severe challenges when applied to large-scale datasets, limiting their practical use. In this work, we present a Data Shapley approach to identify a dataset's high-quality data tuples, Chunked Data Shapley (C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and estimates the contribution of each chunk using optimized subset selection and single-iteration stochastic gradient descent. This approach drastically reduces computation time while preserving high quality results. We empirically benchmark our method on diverse real-world classification and regression tasks, demonstrating that C-DaSh outperforms existing Shapley approximations in both computational efficiency (achieving speedups between 80x - 2300x) and accuracy in detecting low-quality data regions. Our method enables practical measurement of dataset quality on large tabular datasets, supporting both classification and regression pipelines.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2508.15646.pdf' target='_blank'>https://arxiv.org/pdf/2508.15646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Swann Emilien CÃ©leste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15646">Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tree instance segmentation of airborne laser scanning (ALS) data is of utmost importance for forest monitoring, but remains challenging due to variations in the data caused by factors such as sensor resolution, vegetation state at acquisition time, terrain characteristics, etc. Moreover, obtaining a sufficient amount of precisely labeled data to train fully supervised instance segmentation methods is expensive. To address these challenges, we propose a weakly supervised approach where labels of an initial segmentation result obtained either by a non-finetuned model or a closed form algorithm are provided as a quality rating by a human operator. The labels produced during the quality assessment are then used to train a rating model, whose task is to classify a segmentation output into the same classes as specified by the human operator. Finally, the segmentation model is finetuned using feedback from the rating model. This in turn improves the original segmentation model by 34\% in terms of correctly identified tree instances while considerably reducing the number of non-tree instances predicted. Challenges still remain in data over sparsely forested regions characterized by small trees (less than two meters in height) or within complex surroundings containing shrubs, boulders, etc. which can be confused as trees where the performance of the proposed method is reduced.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2508.07306.pdf' target='_blank'>https://arxiv.org/pdf/2508.07306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Zahurul Haquea, Yeahyea Sarker, Muhammed Farhan Sadique Mahi, Syed Jubayer Jaman, Md Robiul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07306">DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dragon fruit, renowned for its nutritional benefits and economic value, has experienced rising global demand due to its affordability and local availability. As dragon fruit cultivation expands, efficient pre- and post-harvest quality inspection has become essential for improving agricultural productivity and minimizing post-harvest losses. This study presents DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN) optimized for real-time quality assessment of dragon fruits on mobile devices. We curated a diverse dataset of 13,789 images, integrating self-collected samples with public datasets (dataset from Mendeley Data), and classified them into four categories: fresh, immature, mature, and defective fruits to ensure robust model training. The proposed model achieves an impressive 93.98% accuracy, outperforming existing methods in fruit quality classification. To facilitate practical adoption, we embedded the model into an intuitive mobile application, enabling farmers and agricultural stakeholders to conduct on-device, real-time quality inspections. This research provides an accurate, efficient, and scalable AI-driven solution for dragon fruit quality control, supporting digital agriculture and empowering smallholder farmers with accessible technology. By bridging the gap between research and real-world application, our work advances post-harvest management and promotes sustainable farming practices.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2507.21557.pdf' target='_blank'>https://arxiv.org/pdf/2507.21557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunling Fan, Yun Zhang, Dietmar Saupe, Raouf Hamzaoui, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21557">PC-JND: Subjective Study and Dataset on Just Noticeable Difference for Point Clouds in 6DoF Virtual Reality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Just Noticeable Difference (JND) accounts for the minimum distortion at which humans can perceive a difference between a pristine stimulus and its distorted version. The JND concept has been widely applied in visual signal processing tasks, including coding, transmission, rendering, and quality assessment, to optimize human-centric media experiences. A point cloud is a mainstream volumetric data representation consisting of both geometry information and attributes (e.g. color). Point clouds are used for advanced immersive 3D media such as Virtual Reality (VR). However, the JND characteristics of viewing point clouds in VR have not been explored before. In this paper, we study the point cloud-wise JND (PCJND) characteristics in a Six Degrees of Freedom (6DoF) VR environment using a head-mounted display. Our findings reveal that the texture PCJND of human eyes is smaller than the geometry PCJND for most point clouds. Furthermore, we identify a correlation between colorfulness and texture PCJND. However, there is no significant correlation between colorfulness and the geometry PCJND, nor between the number of points and neither the texture or geometry PCJND. To support future research in JND prediction and perception-driven signal processing, we introduce PC-JND, a novel point cloud-based JND dataset. This dataset will be made publicly available to facilitate advancements in perceptual optimization for immersive media.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2507.17240.pdf' target='_blank'>https://arxiv.org/pdf/2507.17240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Srikar Durbha, Asvin Kumar Venkataramanan, Rajesh Sureddi, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17240">Perceptual Classifiers: Detecting Generative Images using Perceptual Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) models are employed in many practical image and video processing pipelines to reduce storage, minimize transmission costs, and improve the Quality of Experience (QoE) of millions of viewers. These models are sensitive to a diverse range of image distortions and can accurately predict image quality as judged by human viewers. Recent advancements in generative models have resulted in a significant influx of "GenAI" content on the internet. Existing methods for detecting GenAI content have progressed significantly with improved generalization performance on images from unseen generative models. Here, we leverage the capabilities of existing IQA models, which effectively capture the manifold of real images within a bandpass statistical space, to distinguish between real and AI-generated images. We investigate the generalization ability of these perceptual classifiers to the task of GenAI image detection and evaluate their robustness against various image degradations. Our results show that a two-layer network trained on the feature space of IQA models demonstrates state-of-the-art performance in detecting fake images across generative models, while maintaining significant robustness against image degradations.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2507.12624.pdf' target='_blank'>https://arxiv.org/pdf/2507.12624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiankai Wang, James E. D. Tweel, Parsin Haji Reza, Anita Layton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12624">Pathology-Guided Virtual Staining Metric for Evaluation and Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual staining has emerged as a powerful alternative to traditional histopathological staining techniques, enabling rapid, reagent-free image transformations. However, existing evaluation methods predominantly rely on full-reference image quality assessment (FR-IQA) metrics such as structural similarity, which are originally designed for natural images and often fail to capture pathology-relevant features. Expert pathology reviews have also been used, but they are inherently subjective and time-consuming.
  In this study, we introduce PaPIS (Pathology-Aware Perceptual Image Similarity), a novel FR-IQA metric specifically tailored for virtual staining evaluation. PaPIS leverages deep learning-based features trained on cell morphology segmentation and incorporates Retinex-inspired feature decomposition to better reflect histological perceptual quality. Comparative experiments demonstrate that PaPIS more accurately aligns with pathology-relevant visual cues and distinguishes subtle cellular structures that traditional and existing perceptual metrics tend to overlook. Furthermore, integrating PaPIS as a guiding loss function in a virtual staining model leads to improved histological fidelity.
  This work highlights the critical need for pathology-aware evaluation frameworks to advance the development and clinical readiness of virtual staining technologies.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2507.08671.pdf' target='_blank'>https://arxiv.org/pdf/2507.08671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Ge, Juan Zhai, Minxue Pan, Fusen He, Ziyue Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08671">LLMCup: Ranking-Enhanced Comment Updating with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While comments are essential for enhancing code readability and maintainability in modern software projects, developers are often motivated to update code but not comments, leading to outdated or inconsistent documentation that hinders future understanding and maintenance. Recent approaches such as CUP and HebCup have attempted automatic comment updating using neural sequence-to-sequence models and heuristic rules, respectively. However, these methods can miss or misinterpret crucial information during comment updating, resulting in inaccurate comments, and they often struggle with complex update scenarios. Given these challenges, a promising direction lies in leveraging large language models (LLMs), which have shown impressive performance in software engineering tasks such as comment generation, code synthesis, and program repair. This suggests their strong potential to capture the logic behind code modifications - an ability that is crucial for the task of comment updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on each update case remains challenging. To address this, we propose a novel comment updating framework, LLMCup, which first uses multiple prompt strategies to provide diverse candidate updated comments via an LLM, and then employs a ranking model, CupRank, to select the best candidate as final updated comment. Experimental results demonstrate the effectiveness of LLMCup, with improvements over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy, 10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in SentenceBert similarity. Furthermore, a user study shows that comments updated by LLMCup sometimes surpass human-written updates, highlighting the importance of incorporating human evaluation in comment quality assessment.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2507.07764.pdf' target='_blank'>https://arxiv.org/pdf/2507.07764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haokun Tian, Stefan Lattner, Charalampos Saitis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07764">Assessing the Alignment of Audio Representations with Timbre Similarity Ratings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Psychoacoustical so-called "timbre spaces" map perceptual similarity ratings of instrument sounds onto low-dimensional embeddings via multidimensional scaling, but suffer from scalability issues and are incapable of generalization. Recent results from audio (music and speech) quality assessment as well as image similarity have shown that deep learning is able to produce embeddings that align well with human perception while being largely free from these constraints. Although the existing human-rated timbre similarity data is not large enough to train deep neural networks (2,614 pairwise ratings on 334 audio samples), it can serve as test-only data for audio models. In this paper, we introduce metrics to assess the alignment of diverse audio representations with human judgments of timbre similarity by comparing both the absolute values and the rankings of embedding distances to human similarity ratings. Our evaluation involves three signal-processing-based representations, twelve representations extracted from pre-trained models, and three representations extracted from a novel sound matching model. Among them, the style embeddings inspired by image style transfer, extracted from the CLAP model and the sound matching model, remarkably outperform the others, showing their potential in modeling timbre similarity.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2506.14199.pdf' target='_blank'>https://arxiv.org/pdf/2506.14199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junghwan Kim, Kieun Park, Sohee Park, Hyunggug Kim, Bongwon Suh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14199">MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2506.04081.pdf' target='_blank'>https://arxiv.org/pdf/2506.04081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelouahed Laazoufi, Mohammed El Hassouni, Hocine Cherifi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04081">Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for evaluating 3D content in real-world applications where reference models are unavailable.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2505.21592.pdf' target='_blank'>https://arxiv.org/pdf/2505.21592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Chen, Shaode Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21592">Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong function approximation capability. In our previous work, KAN and its variants were explored in score regression for blind image quality assessment (BIQA). However, these models encounter challenges when processing high-dimensional features, leading to limited performance gains and increased computational cost. To address these issues, we propose TaylorKAN that leverages the Taylor expansions as learnable activation functions to enhance local approximation capability. To improve the computational efficiency, network depth reduction and feature dimensionality compression are integrated into the TaylorKAN-based score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and FLIVE) with authentic distortions, extensive experiments demonstrate that TaylorKAN consistently outperforms the other KAN-related models, indicating that the local approximation via Taylor expansions is more effective than global approximation using orthogonal functions. Its generalization capacity is validated through inter-database experiments. The findings highlight the potential of TaylorKAN as an efficient and robust model for high-dimensional score regression.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2505.11724.pdf' target='_blank'>https://arxiv.org/pdf/2505.11724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhu, Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11724">Semantically-Aware Game Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assessing the visual quality of video game graphics presents unique challenges due to the absence of reference images and the distinct types of distortions, such as aliasing, texture blur, and geometry level of detail (LOD) issues, which differ from those in natural images or user-generated content. Existing no-reference image and video quality assessment (NR-IQA/VQA) methods fail to generalize to gaming environments as they are primarily designed for distortions like compression artifacts. This study introduces a semantically-aware NR-IQA model tailored to gaming. The model employs a knowledge-distilled Game distortion feature extractor (GDFE) to detect and quantify game-specific distortions, while integrating semantic gating via CLIP embeddings to dynamically weight feature importance based on scene content. Training on gameplay data recorded across graphical quality presets enables the model to produce quality scores that align with human perception. Our results demonstrate that the GDFE, trained through knowledge distillation from binary classifiers, generalizes effectively to intermediate distortion levels unseen during training. Semantic gating further improves contextual relevance and reduces prediction variance. In the absence of in-domain NR-IQA baselines, our model outperforms out-of-domain methods and exhibits robust, monotonic quality trends across unseen games in the same genre. This work establishes a foundation for automated graphical quality assessment in gaming, advancing NR-IQA methods in this domain.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2504.17234.pdf' target='_blank'>https://arxiv.org/pdf/2504.17234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Lao, Heather Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17234">Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of artificial intelligence and widespread use of smartphones have resulted in an exponential growth of image data, both real (camera-captured) and virtual (AI-generated). This surge underscores the critical need for robust image quality assessment (IQA) methods that accurately reflect human visual perception. Traditional IQA techniques primarily rely on spatial features - such as signal-to-noise ratio, local structural distortions, and texture inconsistencies - to identify artifacts. While effective for unprocessed or conventionally altered images, these methods fall short in the context of modern image post-processing powered by deep neural networks (DNNs). The rise of DNN-based models for image generation, enhancement, and restoration has significantly improved visual quality, yet made accurate assessment increasingly complex. To address this, we propose a novel IQA approach that bridges the gap between deep learning methods and human perception. Our model disentangles deep features into high-level semantic information and low-level perceptual details, treating each stream separately. These features are then combined with conventional IQA metrics to provide a more comprehensive evaluation framework. This hybrid design enables the model to assess both global context and intricate image details, better reflecting the human visual process, which first interprets overall structure before attending to fine-grained elements. The final stage employs a multilayer perceptron (MLP) to map the integrated features into a concise quality score. Experimental results demonstrate that our method achieves improved consistency with human perceptual judgments compared to existing IQA models.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2504.13866.pdf' target='_blank'>https://arxiv.org/pdf/2504.13866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksa Marusic, Sao Mai Nguyen, Adriana Tapus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13866">Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical rehabilitation exercises suggested by healthcare professionals can help recovery from various musculoskeletal disorders and prevent re-injury. However, patients' engagement tends to decrease over time without direct supervision, which is why there is a need for an automated monitoring system. In recent years, there has been great progress in quality assessment of physical rehabilitation exercises. Most of them only provide a binary classification if the performance is correct or incorrect, and a few provide a continuous score. This information is not sufficient for patients to improve their performance. In this work, we propose an algorithm for error classification of rehabilitation exercises, thus making the first step toward more detailed feedback to patients. We focus on skeleton-based exercise assessment, which utilizes human pose estimation to evaluate motion. Inspired by recent algorithms for quality assessment during rehabilitation exercises, we propose a Transformer-based model for the described classification. Our model is inspired by the HyperFormer method for human action recognition, and adapted to our problem and dataset. The evaluation is done on the KERAAL dataset, as it is the only medical dataset with clear error labels for the exercises, and our model significantly surpasses state-of-the-art methods. Furthermore, we bridge the gap towards better feedback to the patients by presenting a way to calculate the importance of joints for each exercise.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2504.12664.pdf' target='_blank'>https://arxiv.org/pdf/2504.12664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srijan Kumar Pal, Shashank Sharma, Nikil Krishnakumar, Jiarong Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12664">Autonomous Drone for Dynamic Smoke Plume Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel autonomous drone-based smoke plume tracking system capable of navigating and tracking plumes in highly unsteady atmospheric conditions. The system integrates advanced hardware and software and a comprehensive simulation environment to ensure robust performance in controlled and real-world settings. The quadrotor, equipped with a high-resolution imaging system and an advanced onboard computing unit, performs precise maneuvers while accurately detecting and tracking dynamic smoke plumes under fluctuating conditions. Our software implements a two-phase flight operation, i.e., descending into the smoke plume upon detection and continuously monitoring the smoke movement during in-plume tracking. Leveraging Proportional Integral-Derivative (PID) control and a Proximal Policy Optimization based Deep Reinforcement Learning (DRL) controller enables adaptation to plume dynamics. Unreal Engine simulation evaluates performance under various smoke-wind scenarios, from steady flow to complex, unsteady fluctuations, showing that while the PID controller performs adequately in simpler scenarios, the DRL-based controller excels in more challenging environments. Field tests corroborate these findings. This system opens new possibilities for drone-based monitoring in areas like wildfire management and air quality assessment. The successful integration of DRL for real-time decision-making advances autonomous drone control for dynamic environments.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2504.02663.pdf' target='_blank'>https://arxiv.org/pdf/2504.02663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuka Haruki, Kei Kato, Yuki Enami, Hiroaki Takeuchi, Daiki Kazuno, Kotaro Yamada, Teruaki Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02663">Development of Automated Data Quality Assessment and Evaluation Indices by Analytical Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The societal need to leverage third-party data has driven the data-distribution market and increased the importance of data quality assessment (DQA) in data transactions between organizations. However, DQA requires expert knowledge of raw data and related data attributes, which hinders consensus-building in data purchasing. This study focused on the differences in DQAs between experienced and inexperienced data handlers. We performed two experiments: The first was a questionnaire survey involving 41 participants with varying levels of data-handling experience, who evaluated 12 data samples using 10 predefined indices with and without quality metadata generated by the automated tool. The second was an eye-tracking experiment to reveal the viewing behavior of participants during data evaluation. It was revealed that using quality metadata generated by the automated tool can reduce misrecognition in DQA. While experienced data handlers rated the quality metadata highly, semi-experienced users gave it the lowest ratings. This study contributes to enhancing data understanding within organizations and promoting the distribution of valuable data by proposing an automated tool to support DQAs.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2503.23370.pdf' target='_blank'>https://arxiv.org/pdf/2503.23370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxing Sun, Jing Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23370">Map Feature Perception Metric for Map Generation Quality Assessment and Loss Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In intelligent cartographic generation tasks empowered by generative models, the authenticity of synthesized maps constitutes a critical determinant. Concurrently, the selection of appropriate evaluation metrics to quantify map authenticity emerges as a pivotal research challenge. Current methodologies predominantly adopt computer vision-based image assessment metrics to compute discrepancies between generated and reference maps. However, conventional visual similarity metrics-including L1, L2, SSIM, and FID-primarily operate at pixel-level comparisons, inadequately capturing cartographic global features and spatial correlations, consequently inducing semantic-structural artifacts in generated outputs. This study introduces a novel Map Feature Perception Metric designed to evaluate global characteristics and spatial congruence between synthesized and target maps. Diverging from pixel-wise metrics, our approach extracts elemental-level deep features that comprehensively encode cartographic structural integrity and topological relationships. Experimental validation demonstrates MFP's superior capability in evaluating cartographic semantic features, with classification-enhanced implementations outperforming conventional loss functions across diverse generative frameworks. When employed as optimization objectives, our metric achieves performance gains ranging from 2% to 50% across multiple benchmarks compared to traditional L1, L2, and SSIM baselines. This investigation concludes that explicit consideration of cartographic global attributes and spatial coherence substantially enhances generative model optimization, thereby significantly improving the geographical plausibility of synthesized maps.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2503.06699.pdf' target='_blank'>https://arxiv.org/pdf/2503.06699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Cao, Nicolas Folastre, Gozde Oney, Edgar Rauch, Stavros Nicolopoulos, Partha Pratim Das, Arnaud DemortiÃ¨re
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06699">Unsupervised Multi-Clustering and Decision-Making Strategies for 4D-STEM Orientation Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a novel integration of unsupervised learning and decision-making strategies for the advanced analysis of 4D-STEM datasets, with a focus on non-negative matrix factorization (NMF) as the primary clustering method. Our approach introduces a systematic framework to determine the optimal number of components (k) required for robust and interpretable orientation mapping. By leveraging the K-Component Loss method and Image Quality Assessment (IQA) metrics, we effectively balance reconstruction fidelity and model complexity. Additionally, we highlight the critical role of dataset preprocessing in improving clustering stability and accuracy. Furthermore, our spatial weight matrix analysis provides insights into overlapping regions within the dataset by employing threshold-based visualization, facilitating a detailed understanding of cluster interactions. The results demonstrate the potential of combining NMF with advanced IQA metrics and preprocessing techniques for reliable orientation mapping and structural analysis in 4D-STEM datasets, paving the way for future applications in multi-dimensional material characterization.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2502.14501.pdf' target='_blank'>https://arxiv.org/pdf/2502.14501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia Romberg, Maximilian Maurer, Henning Wachsmuth, Gabriella Lapesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14501">Towards a Perspectivist Turn in Argument Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable datasets. We fill this gap by conducting a systematic review of argument quality datasets. We assign them to a multi-layered categorization targeting two aspects: (a) What has been annotated: we collect the quality dimensions covered in datasets and consolidate them in an overarching taxonomy, increasing dataset comparability and interoperability. (b) Who annotated: we survey what information is given about annotators, enabling perspectivist research and grounding our recommendations for future actions. To this end, we discuss datasets suitable for developing perspectivist models (i.e., those containing individual, non-aggregated annotations), and we showcase the importance of a controlled selection of annotators in a pilot study.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2502.10908.pdf' target='_blank'>https://arxiv.org/pdf/2502.10908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sevim Cengiz, Ibraheem Hamdi, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10908">Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fetal gestational age (GA) is vital clinical information that is estimated during pregnancy in order to assess fetal growth. This is usually performed by measuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan which is then correlated with fetal age and growth trajectory. A major issue when performing the CRL measurement is ensuring that the image is acquired at the correct view, otherwise it could be misleading. Although clinical guidelines specify the criteria for the correct CRL view, sonographers may not regularly adhere to such rules. In this paper, we propose a new deep learning-based solution that is able to verify the adherence of a CRL image to clinical guidelines in order to assess image quality and facilitate accurate estimation of GA. We first segment out important fetal structures then use the localized structures to perform a clinically-guided mapping that verifies the adherence of criteria. The segmentation method combines the benefits of Convolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment fetal structures in ultrasound images and localize important fetal landmarks. For segmentation purposes, we compare our proposed work with UNet and show that our CNN/ViT-based method outperforms an optimized version of UNet. Furthermore, we compare the output of the mapping with classification CNNs when assessing the clinical criteria and the overall acceptability of CRL images. We show that the proposed mapping is not only explainable but also more accurate than the best performing classification CNNs.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2501.03619.pdf' target='_blank'>https://arxiv.org/pdf/2501.03619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laurin Jonientz, Johannes Merkle, Christian Rathgeb, Benjamin Tams, Georg Merz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03619">Deep Learning-based Compression Detection for explainable Face Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The assessment of face image quality is crucial to ensure reliable face recognition. In order to provide data subjects and operators with explainable and actionable feedback regarding captured face images, relevant quality components have to be measured. Quality components that are known to negatively impact the utility of face images include JPEG and JPEG 2000 compression artefacts, among others. Compression can result in a loss of important image details which may impair the recognition performance. In this work, deep neural networks are trained to detect the compression artefacts in a face images. For this purpose, artefact-free facial images are compressed with the JPEG and JPEG 2000 compression algorithms. Subsequently, the PSNR and SSIM metrics are employed to obtain training labels based on which neural networks are trained using a single network to detect JPEG and JPEG 2000 artefacts, respectively. The evaluation of the proposed method shows promising results: in terms of detection accuracy, error rates of 2-3% are obtained for utilizing PSNR labels during training. In addition, we show that error rates of different open-source and commercial face recognition systems can be significantly reduced by discarding face images exhibiting severe compression artefacts. To minimize resource consumption, EfficientNetV2 serves as basis for the presented algorithm, which is available as part of the OFIQ software.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2501.03499.pdf' target='_blank'>https://arxiv.org/pdf/2501.03499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pritisha Sarkar, Duranta Durbaar Vishal Saha, Mousumi Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03499">Can Deep Learning Trigger Alerts from Mobile-Captured Images?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our research presents a comprehensive approach to leveraging mobile camera image data for real-time air quality assessment and recommendation. We develop a regression-based Convolutional Neural Network model and tailor it explicitly for air quality prediction by exploiting the inherent relationship between output parameters. As a result, the Mean Squared Error of 0.0077 and 0.0112 obtained for 2 and 5 pollutants respectively outperforms existing models. Furthermore, we aim to verify the common practice of augmenting the original dataset with a view to introducing more variation in the training phase. It is one of our most significant contributions that our experimental results demonstrate minimal accuracy differences between the original and augmented datasets. Finally, a real-time, user-friendly dashboard is implemented which dynamically displays the Air Quality Index and pollutant values derived from captured mobile camera images. Users' health conditions are considered to recommend whether a location is suitable based on current air quality metrics. Overall, this research contributes to verification of data augmentation techniques, CNN-based regression modelling for air quality prediction, and user-centric air quality monitoring through mobile technology. The proposed system offers practical solutions for individuals to make informed environmental health and well-being decisions.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2412.15527.pdf' target='_blank'>https://arxiv.org/pdf/2412.15527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhi Xian, Mingliang Zhou, Leong Hou U, Lang Shujun, Bin Fang, Tao Xiang, Zhaowei Shang, Weijia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15527">PIGUIQA: A Physical Imaging Guided Perceptual Framework for Underwater Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a Physical Imaging Guided perceptual framework for Underwater Image Quality Assessment (UIQA), termed PIGUIQA. First, we formulate UIQA as a comprehensive problem that considers the combined effects of direct transmission attenuation and backward scattering on image perception. By leveraging underwater radiative transfer theory, we systematically integrate physics-based imaging estimations to establish quantitative metrics for these distortions. Second, recognizing spatial variations in image content significance and human perceptual sensitivity to distortions, we design a module built upon a neighborhood attention mechanism for local perception of images. This module effectively captures subtle features in images, thereby enhancing the adaptive perception of distortions on the basis of local information. Third, by employing a global perceptual aggregator that further integrates holistic image scene with underwater distortion information, the proposed model accurately predicts image quality scores. Extensive experiments across multiple benchmarks demonstrate that PIGUIQA achieves state-of-the-art performance while maintaining robust cross-dataset generalizability. The implementation is publicly available at https://anonymous.4open.science/r/PIGUIQA-A465/
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2412.09829.pdf' target='_blank'>https://arxiv.org/pdf/2412.09829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quang-Anh N. D., Minh-Duc Pham, Thai Kim Dinh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09829">Speech-based Multimodel Pipeline for Vietnamese Services Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving landscape of customer service within the digital economy, traditional methods of service quality assessment have shown significant limitations, this research proposes a novel deep-learning approach to service quality assessment, focusing on the Vietnamese service sector. By leveraging a multi-modal pipeline that transcends traditional evaluation methods, the research addresses the limitations of conventional assessments by analyzing speech, speaker interactions and emotional content, offering a more comprehensive and objective means of understanding customer service interactions. This aims to provide organizations with a sophisticated tool for evaluating and improving service quality in the digital economy.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2412.06003.pdf' target='_blank'>https://arxiv.org/pdf/2412.06003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymen Sekhri, Seyed Ali Amirshahi, Mohamed-Chaker Larabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06003">Enhancing Content Representation for AR Image Quality Assessment Using Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented Reality (AR) is a major immersive media technology that enriches our perception of reality by overlaying digital content (the foreground) onto physical environments (the background). It has far-reaching applications, from entertainment and gaming to education, healthcare, and industrial training. Nevertheless, challenges such as visual confusion and classical distortions can result in user discomfort when using the technology. Evaluating AR quality of experience becomes essential to measure user satisfaction and engagement, facilitating the refinement necessary for creating immersive and robust experiences. Though, the scarcity of data and the distinctive characteristics of AR technology render the development of effective quality assessment metrics challenging. This paper presents a deep learning-based objective metric designed specifically for assessing image quality for AR scenarios. The approach entails four key steps, (1) fine-tuning a self-supervised pre-trained vision transformer to extract prominent features from reference images and distilling this knowledge to improve representations of distorted images, (2) quantifying distortions by computing shift representations, (3) employing cross-attention-based decoders to capture perceptual quality features, and (4) integrating regularization techniques and label smoothing to address the overfitting problem. To validate the proposed approach, we conduct extensive experiments on the ARIQA dataset. The results showcase the superior performance of our proposed approach across all model variants, namely TransformAR, TransformAR-KD, and TransformAR-KD+ in comparison to existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2412.01986.pdf' target='_blank'>https://arxiv.org/pdf/2412.01986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Shafiee Sarvestani, Sheyang Tang, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01986">HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mesh quality assessment (MQA) models play a critical role in the design, optimization, and evaluation of mesh operation systems in a wide variety of applications. Current MQA models, whether model-based methods using topology-aware features or projection-based approaches working on rendered 2D projections, often fail to capture the intricate interactions between texture and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid full-reference colored MQA framework that integrates model-based and projection-based approaches, capturing complex interactions between textural information and 3D structures for enriched quality representations. Our method employs graph learning to extract detailed 3D representations, which are then projected to 2D using a novel feature rendering process that precisely aligns them with colored projections. This enables the exploration of geometry-texture interactions via cross-attention, producing comprehensive mesh quality representations. Extensive experiments demonstrate HybridMQA's superior performance across diverse datasets, highlighting its ability to effectively leverage geometry-texture interactions for a thorough understanding of mesh quality. Our implementation will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2411.18222.pdf' target='_blank'>https://arxiv.org/pdf/2411.18222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo M. Delgado, JÃ¼rgen Herre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18222">Towards Improved Objective Perceptual Audio Quality Assessment -- Part 1: A Novel Data-Driven Cognitive Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient audio quality assessment is vital for streamlining audio codec development. Objective assessment tools have been developed over time to algorithmically predict quality ratings from subjective assessments, the gold standard for quality judgment. Many of these tools use perceptual auditory models to extract audio features that are mapped to a basic audio quality score prediction using machine learning algorithms and subjective scores as training data. However, existing tools struggle with generalization in quality prediction, especially when faced with unknown signal and distortion types. This is particularly evident in the presence of signals coded using non-waveform-preserving parametric techniques. Addressing these challenges, this two-part work proposes extensions to the Perceptual Evaluation of Audio Quality (PEAQ - ITU-R BS.1387-1) recommendation. Part 1 focuses on increasing generalization, while Part 2 targets accurate spatial audio quality measurement in audio coding.
  To enhance prediction generalization, this paper (Part 1) introduces a novel machine learning approach that uses subjective data to model cognitive aspects of audio quality perception. The proposed method models the perceived severity of audible distortions by adaptively weighting different distortion metrics. The weights are determined using an interaction cost function that captures relationships between distortion salience and cognitive effects. Compared to other machine learning methods and established tools, the proposed architecture achieves higher prediction accuracy on large databases of previously unseen subjective quality scores. The perceptually-motivated model offers a more manageable alternative to general-purpose machine learning algorithms, allowing potential extensions and improvements to multi-dimensional quality measurement without complete retraining.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2410.22323.pdf' target='_blank'>https://arxiv.org/pdf/2410.22323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seetharam Killivalavan, Durairaj Thenmozhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22323">Enhancing Code Annotation Reliability: Generative AI's Role in Comment Quality Assessment Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores a novel method for enhancing binary classification models that assess code comment quality, leveraging Generative Artificial Intelligence to elevate model performance. By integrating 1,437 newly generated code-comment pairs, labeled as "Useful" or "Not Useful" and sourced from various GitHub repositories, into an existing C-language dataset of 9,048 pairs, we demonstrate substantial model improvements. Using an advanced Large Language Model, our approach yields a 5.78% precision increase in the Support Vector Machine (SVM) model, improving from 0.79 to 0.8478, and a 2.17% recall boost in the Artificial Neural Network (ANN) model, rising from 0.731 to 0.7527. These results underscore Generative AI's value in advancing code comment classification models, offering significant potential for enhanced accuracy in software development and quality control. This study provides a promising outlook on the integration of generative techniques for refining machine learning models in practical software engineering settings.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2410.13227.pdf' target='_blank'>https://arxiv.org/pdf/2410.13227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rittwika Kansabanik, Adrian Barbu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13227">Latent Image and Video Resolution Prediction using Convolutional Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a Video Quality Assessment (VQA) problem that has received little attention in the literature, called the latent resolution prediction problem. The problem arises when images or videos are upscaled from their native resolution and are reported as having a higher resolution than their native resolution. This paper formulates the problem, constructs a dataset for training and evaluation, and introduces several machine learning algorithms, including two Convolutional Neural Networks (CNNs), to address this problem. Experiments indicate that some proposed methods can predict the latent video resolution with about 95% accuracy.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2410.11511.pdf' target='_blank'>https://arxiv.org/pdf/2410.11511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiyu Yuan, Tristan Whitmarsh, Dimitri A Kessler, Otso Arponen, Mary A McLean, Gabrielle Baxter, Frank Riemer, Aneurin J Kennerley, William J Brackenbury, Fiona J Gilbert, Joshua D Kaggie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11511">Rician Denoising Diffusion Probabilistic Models For Sodium Breast MRI Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sodium MRI is an imaging technique used to visualize and quantify sodium concentrations in vivo, playing a role in many biological processes and potentially aiding in breast cancer characterization. Sodium MRI, however, suffers from inherently low signal-to-noise ratios (SNR) and spatial resolution, compared with conventional proton MRI. A deep-learning method, the Denoising Diffusion Probabilistic Models (DDPM), has demonstrated success across a wide range of denoising tasks, yet struggles with sodium MRI's unique noise profile, as DDPM primarily targets Gaussian noise. DDPM can distort features when applied to sodium MRI. This paper advances the DDPM by introducing the Rician Denoising Diffusion Probabilistic Models (RDDPM) for sodium MRI denoising. RDDPM converts Rician noise to Gaussian noise at each timestep during the denoising process. The model's performance is evaluated using three non-reference image quality assessment metrics, where RDDPM consistently outperforms DDPM and other CNN-based denoising methods.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2410.00667.pdf' target='_blank'>https://arxiv.org/pdf/2410.00667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Yang, Guangyu Zhang, Xiaodong Lu, Yuan Zhang, Jian Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00667">Contribution of soundscape appropriateness to soundscape quality assessment in space: a mediating variable affecting acoustic comfort</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Soundscape appropriateness (SA) provides supplemental information on the matching degree between auditory information and the surrounding scene in soundscape perception. This indicator has been integrated into the standard ISO process for collecting soundscape data, forming a component of the sound quality assessment questionnaire. However, its role in soundscape quality assessment has not been fully understood. Herein, we present the findings from soundscape data collected from Beiling Park in Shenyang, China. A method was developed that integrates mediation effect models with multiscale geographically weighted regression models to explore the mediating role of SA in the impact of sound source types on soundscape quality, as well as the spatial heterogeneity of this mediation effect. The results confirm that SA does mediates the influence of sound source types on acoustics comfort (AC). Specifically, natural sounds (indirect effect/total effect = .19/.19), traffic sounds (indirect effect/total effect = -.46/-.65), and commercial sounds (indirect effect/total effect = -.25/-.12) impact the perception of AC by either enhancing or reducing SA. Moreover, the relationships among variables depicted in this model demonstrate spatial heterogeneity, demonstrating that in urban open spaces with complex constructures, local spatial models may be needed for soundscape assessment. The research reaffirms the significance of SA in urban open spaces. In terms of practical implications for urban and landscape planners, when sound sources cannot be controlled or altered, coordinating between the sound and the surrounding environment through landscape optimisation could also improve the quality of the soundscape through enhancing SA and help achieve the goal of creating healthy urban open spaces.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2409.17451.pdf' target='_blank'>https://arxiv.org/pdf/2409.17451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongrok Kim, Junha Shin, Juhyun Lee, Hyunsuk Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17451">Study of Subjective and Objective Quality in Super-Resolution Enhanced Broadcast Images on a Novel SR-IQA Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To display low-quality broadcast content on high-resolution screens in full-screen format, the application of Super-Resolution (SR), a key consumer technology, is essential. Recently, SR methods have been developed that not only increase resolution while preserving the original image information but also enhance the perceived quality. However, evaluating the quality of SR images generated from low-quality sources, such as SR-enhanced broadcast content, is challenging due to the need to consider both distortions and improvements. Additionally, assessing SR image quality without original high-quality sources presents another significant challenge. Unfortunately, there has been a dearth of research specifically addressing the Image Quality Assessment (IQA) of SR images under these conditions. In this work, we introduce a new IQA dataset for SR broadcast images in both 2K and 4K resolutions. We conducted a subjective quality evaluation to obtain the Mean Opinion Score (MOS) for these SR images and performed a comprehensive human study to identify the key factors influencing the perceived quality. Finally, we evaluated the performance of existing IQA metrics on our dataset. This study reveals the limitations of current metrics, highlighting the need for a more robust IQA metric that better correlates with the perceived quality of SR images.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2409.10246.pdf' target='_blank'>https://arxiv.org/pdf/2409.10246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saif Khalid, Hatem A. Rashwan, Saddam Abdulwahab, Mohamed Abdel-Nasser, Facundo Manuel Quiroga, Domenec Puig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10246">FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of diagnostic Computer-Aided Design (CAD) systems for retinal diseases depends on the quality of the retinal images being screened. Thus, many studies have been developed to evaluate and assess the quality of such retinal images. However, most of them did not investigate the relationship between the accuracy of the developed models and the quality of the visualization of interpretability methods for distinguishing between gradable and non-gradable retinal images. Consequently, this paper presents a novel framework called FGR-Net to automatically assess and interpret underlying fundus image quality by merging an autoencoder network with a classifier network. The FGR-Net model also provides an interpretable quality assessment through visualizations. In particular, FGR-Net uses a deep autoencoder to reconstruct the input image in order to extract the visual characteristics of the input fundus images based on self-supervised learning. The extracted features by the autoencoder are then fed into a deep classifier network to distinguish between gradable and ungradable fundus images. FGR-Net is evaluated with different interpretability methods, which indicates that the autoencoder is a key factor in forcing the classifier to focus on the relevant structures of the fundus images, such as the fovea, optic disk, and prominent blood vessels. Additionally, the interpretability methods can provide visual feedback for ophthalmologists to understand how our model evaluates the quality of fundus images. The experimental results showed the superiority of FGR-Net over the state-of-the-art quality assessment methods, with an accuracy of 89% and an F1-score of 87%.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2409.10210.pdf' target='_blank'>https://arxiv.org/pdf/2409.10210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arijit Biswas, Guanxin Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10210">RF-GML: Reference-Free Generative Machine Listener</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel reference-free (RF) audio quality metric called the RF-Generative Machine Listener (RF-GML), designed to evaluate coded mono, stereo, and binaural audio at a 48 kHz sample rate. RF-GML leverages transfer learning from a state-of-the-art full-reference (FR) Generative Machine Listener (GML) with minimal architectural modifications. The term "generative" refers to the model's ability to generate an arbitrary number of simulated listening scores. Unlike existing RF models, RF-GML accurately predicts subjective quality scores across diverse content types and codecs. Extensive evaluations demonstrate its superiority in rating unencoded audio and distinguishing different levels of coding artifacts. RF-GML's performance and versatility make it a valuable tool for coded audio quality assessment and monitoring in various applications, all without the need for a reference signal.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2409.07762.pdf' target='_blank'>https://arxiv.org/pdf/2409.07762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaode Yu, Ze Chen, Zhimu Yang, Jiacheng Gu, Bizu Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07762">Exploring Kolmogorov-Arnold networks for realistic image sharpness assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Score prediction is crucial in evaluating realistic image sharpness based on collected informative features. Recently, Kolmogorov-Arnold networks (KANs) have been developed and witnessed remarkable success in data fitting. This study introduces the Taylor series-based KAN (TaylorKAN). Then, different KANs are explored in four realistic image databases (BID2011, CID2013, CLIVE, and KonIQ-10k) to predict the scores by using 15 mid-level features and 2048 high-level features. Compared to support vector regression, results show that KANs are generally competitive or superior, and TaylorKAN is the best one when mid-level features are used. This is the first study to investigate KANs on image quality assessment that sheds some light on how to select and further improve KANs in related tasks.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2408.16481.pdf' target='_blank'>https://arxiv.org/pdf/2408.16481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaiyu Yuan, Tristan Whitmarsh, Dimitri A Kessler, Otso Arponen, Mary A McLean, Gabrielle Baxter, Frank Riemer, Aneurin J Kennerley, William J Brackenbury, Fiona J Gilbert, Joshua D Kaggie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16481">A Deep-Learning-Based Label-free No-Reference Image Quality Assessment Metric: Application in Sodium MRI Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>New multinuclear MRI techniques, such as sodium MRI, generally suffer from low image quality due to an inherently low signal. Postprocessing methods, such as image denoising, have been developed for image enhancement. However, the assessment of these enhanced images is challenging especially considering when there is a lack of high resolution and high signal images as reference, such as in sodium MRI. No-reference Image Quality Assessment (NR-IQA) metrics are approaches to solve this problem. Existing learning-based NR-IQA metrics rely on labels derived from subjective human opinions or metrics like Signal-to-Noise Ratio (SNR), which are either time-consuming or lack accurate ground truths, resulting in unreliable assessment. We note that deep learning (DL) models have a unique characteristic in that they are specialized to a characteristic training set, meaning that deviations between the input testing data from the training data will reduce prediction accuracy. Therefore, we propose a novel DL-based NR-IQA metric, the Model Specialization Metric (MSM), which does not depend on ground-truth images or labels. MSM measures the difference between the input image and the model's prediction for evaluating the quality of the input image. Experiments conducted on both simulated distorted proton T1-weighted MR images and denoised sodium MR images demonstrate that MSM exhibits a superior evaluation performance on various simulated noises and distortions. MSM also has a substantial agreement with the expert evaluations, achieving an averaged Cohen's Kappa coefficient of 0.6528, outperforming the existing NR-IQA metrics.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2408.12279.pdf' target='_blank'>https://arxiv.org/pdf/2408.12279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoxiang Dang, Tetsuya Matsumoto, Yoshinori Takeuchi, Takashi Tsuboi, Yasuhiro Tanaka, Daisuke Nakatsubo, Satoshi Maesawa, Ryuta Saito, Masahisa Katsuno, Hiroaki Kudo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12279">Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The potential of deep learning in clinical speech processing is immense, yet the hurdles of limited and imbalanced clinical data samples loom large. This article addresses these challenges by showcasing the utilization of automatic speech recognition and self-supervised learning representations, pre-trained on extensive datasets of normal speech. This innovative approach aims to estimate voice quality of patients with impaired vocal systems. Experiments involve checks on PVQD dataset, covering various causes of vocal system damage in English, and a Japanese dataset focusing on patients with Parkinson's disease before and after undergoing subthalamic nucleus deep brain stimulation (STN-DBS) surgery. The results on PVQD reveal a notable correlation (>0.8 on PCC) and an extraordinary accuracy (<0.5 on MSE) in predicting Grade, Breathy, and Asthenic indicators. Meanwhile, progress has been achieved in predicting the voice quality of patients in the context of STN-DBS.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2408.05840.pdf' target='_blank'>https://arxiv.org/pdf/2408.05840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Gorbulev, Vasiliy Alekseev, Konstantin Vorontsov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05840">Iterative Improvement of an Additively Regularized Topic Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Topic modelling is fundamentally a soft clustering problem (of known objects -- documents, over unknown clusters -- topics). That is, the task is incorrectly posed. In particular, the topic models are unstable and incomplete. All this leads to the fact that the process of finding a good topic model (repeated hyperparameter selection, model training, and topic quality assessment) can be particularly long and labor-intensive. We aim to simplify the process, to make it more deterministic and provable. To this end, we present a method for iterative training of a topic model. The essence of the method is that a series of related topic models are trained so that each subsequent model is at least as good as the previous one, i.e., that it retains all the good topics found earlier. The connection between the models is achieved by additive regularization. The result of this iterative training is the last topic model in the series, which we call the iteratively updated additively regularized topic model (ITAR). Experiments conducted on several collections of natural language texts show that the proposed ITAR model performs better than other popular topic models (LDA, ARTM, BERTopic), its topics are diverse, and its perplexity (ability to "explain" the underlying data) is moderate.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2407.14994.pdf' target='_blank'>https://arxiv.org/pdf/2407.14994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karl Van Eeden Risager, Torkan Gholamalizadeh, Mostafa Mehdipour Ghazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14994">Non-Reference Quality Assessment for Medical Imaging: Application to Synthetic Brain MRIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-quality synthetic data is crucial for addressing challenges in medical imaging, such as domain adaptation, data scarcity, and privacy concerns. Existing image quality metrics often rely on reference images, are tailored for group comparisons, or are intended for 2D natural images, limiting their efficacy in complex domains like medical imaging. This study introduces a novel deep learning-based non-reference approach to assess brain MRI quality by training a 3D ResNet. The network is designed to estimate quality across six distinct artifacts commonly encountered in MRI scans. Additionally, a diffusion model is trained on diverse datasets to generate synthetic 3D images of high fidelity. The approach leverages several datasets for training and comprehensive quality assessment, benchmarking against state-of-the-art metrics for real and synthetic images. Results demonstrate superior performance in accurately estimating distortions and reflecting image quality from multiple perspectives. Notably, the method operates without reference images, indicating its applicability for evaluating deep generative models. Besides, the quality scores in the [0, 1] range provide an intuitive assessment of image quality across heterogeneous datasets. Evaluation of generated images offers detailed insights into specific artifacts, guiding strategies for improving generative models to produce high-quality synthetic images. This study presents the first comprehensive method for assessing the quality of real and synthetic 3D medical images in MRI contexts without reliance on reference images.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2407.10413.pdf' target='_blank'>https://arxiv.org/pdf/2407.10413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungri Yoon, Yunseong Cho, Tae In Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10413">Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring and managing the growth and quality of fruits are very important tasks. To effectively train deep learning models like YOLO for real-time fruit detection, high-quality image datasets are essential. However, such datasets are often lacking in agriculture. Generative AI models can help create high-quality images. In this study, we used MidJourney and Firefly tools to generate images of melon greenhouses and post-harvest fruits through text-to-image, pre-harvest image-to-image, and post-harvest image-to-image methods. We evaluated these AIgenerated images using PSNR and SSIM metrics and tested the detection performance of the YOLOv9 model. We also assessed the net quality of real and generated fruits. Our results showed that generative AI could produce images very similar to real ones, especially for post-harvest fruits. The YOLOv9 model detected the generated images well, and the net quality was also measurable. This shows that generative AI can create realistic images useful for fruit detection and quality assessment, indicating its great potential in agriculture. This study highlights the potential of AI-generated images for data augmentation in melon fruit detection and quality assessment and envisions a positive future for generative AI applications in agriculture.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2407.07673.pdf' target='_blank'>https://arxiv.org/pdf/2407.07673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feixiang Zhou, Bryan Williams, Hossein Rahmani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07673">Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised Temporal Action Localization (SS-TAL). Existing methods often filter pseudo labels based on strict conditions, but they typically assess classification and localization quality separately, leading to suboptimal pseudo-label ranking and selection. In particular, there might be inaccurate pseudo labels within selected positives, alongside reliable counterparts erroneously assigned to negatives. To tackle these problems, we propose a novel Adaptive Pseudo-label Learning (APL) framework to facilitate better pseudo-label selection. Specifically, to improve the ranking quality, Adaptive Label Quality Assessment (ALQA) is proposed to jointly learn classification confidence and localization reliability, followed by dynamically selecting pseudo labels based on the joint score. Additionally, we propose an Instance-level Consistency Discriminator (ICD) for eliminating ambiguous positives and mining potential positives simultaneously based on inter-instance intrinsic consistency, thereby leading to a more precise selection. We further introduce a general unsupervised Action-aware Contrastive Pre-training (ACP) to enhance the discrimination both within actions and between actions and backgrounds, which benefits SS-TAL. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our method achieves state-of-the-art performance under various semi-supervised settings.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2405.15453.pdf' target='_blank'>https://arxiv.org/pdf/2405.15453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Munief Hassan Tahir, Sana Shams, Layba Fiaz, Farah Adeeba, Sarmad Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15453">Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks. However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity. In addition these benchmarks lack quality assessment against the respective state-of the art models. This study presents an in-depth examination of 7 prominent LLMs: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B, Bloomz 7B1, Ministral-8B and Whisper (Large, medium and small variant) across 17 tasks using 22 datasets, 13.8 hours of speech, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analyzed. Our experiments show that SOTA models currently outperform encoder-decoder models in majority of Urdu NLP tasks under zero-shot settings. However, comparing Llama 3.1-8B over prior version Llama 2-7B-Chat, we can deduce that with improved language coverage, LLMs can surpass these SOTA models. Our results emphasize that models with fewer parameters but richer language-specific data, like Llama 3.1-8B, often outperform larger models with lower language diversity, such as GPT-3.5, in several tasks.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2405.04167.pdf' target='_blank'>https://arxiv.org/pdf/2405.04167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04167">Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised Domain Adaptation for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images. Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment. To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains. Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA. Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2404.18178.pdf' target='_blank'>https://arxiv.org/pdf/2404.18178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Raviv, Gal Chechik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18178">Assessing Image Quality Using a Simple Generative Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as perceived by a human observer. Current state-of-the-art techniques are based on deep representations trained in discriminative manner. Such representations may ignore visually important features, if they are not predictive of class labels. Recent generative models successfully learn low-dimensional representations using auto-encoding and have been argued to preserve better visual features. Here we leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2404.14728.pdf' target='_blank'>https://arxiv.org/pdf/2404.14728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jay Lee, Dai-Yan Ji, Yuan-Ming Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14728">Novel Topological Machine Learning Methodology for Stream-of-Quality Modeling in Smart Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a topological analytics approach within the 5-level Cyber-Physical Systems (CPS) architecture for the Stream-of-Quality assessment in smart manufacturing. The proposed methodology not only enables real-time quality monitoring and predictive analytics but also discovers the hidden relationships between quality features and process parameters across different manufacturing processes. A case study in additive manufacturing was used to demonstrate the feasibility of the proposed methodology to maintain high product quality and adapt to product quality variations. This paper demonstrates how topological graph visualization can be effectively used for the real-time identification of new representative data through the Stream-of-Quality assessment.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2404.13299.pdf' target='_blank'>https://arxiv.org/pdf/2404.13299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Fang, Weigang Wang, Xiaoxin Lv, Jun Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13299">PCQA: A Strong Baseline for AIGC Quality Assessment Based on Prompt Condition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of Large Language Models (LLM) and Diffusion Models brings the boom of Artificial Intelligence Generated Content (AIGC). It is essential to build an effective quality assessment framework to provide a quantifiable evaluation of different images or videos based on the AIGC technologies. The content generated by AIGC methods is driven by the crafted prompts. Therefore, it is intuitive that the prompts can also serve as the foundation of the AIGC quality assessment. This study proposes an effective AIGC quality assessment (QA) framework. First, we propose a hybrid prompt encoding method based on a dual-source CLIP (Contrastive Language-Image Pre-Training) text encoder to understand and respond to the prompt conditions. Second, we propose an ensemble-based feature mixer module to effectively blend the adapted prompt and vision features. The empirical study practices in two datasets: AIGIQA-20K (AI-Generated Image Quality Assessment database) and T2VQA-DB (Text-to-Video Quality Assessment DataBase), which validates the effectiveness of our proposed method: Prompt Condition Quality Assessment (PCQA). Our proposed simple and feasible framework may promote research development in the multimodal generation field.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2404.11243.pdf' target='_blank'>https://arxiv.org/pdf/2404.11243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Gabriel Vinholi, Marco Chini, Anis Amziane, Renato Machado, Danilo Silva, Patrick Matgen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11243">Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comparing images captured by disparate sensors is a common challenge in remote sensing. This requires image translation -- converting imagery from one sensor domain to another while preserving the original content. Denoising Diffusion Implicit Models (DDIM) are potential state-of-the-art solutions for such domain translation due to their proven superiority in multiple image-to-image translation tasks in computer vision. However, these models struggle with reproducing radiometric features of large-scale multi-patch imagery, resulting in inconsistencies across the full image. This renders downstream tasks like Heterogeneous Change Detection impractical. To overcome these limitations, we propose a method that leverages denoising diffusion for effective multi-sensor optical image translation over large areas. Our approach super-resolves large-scale low spatial resolution images into high-resolution equivalents from disparate optical sensors, ensuring uniformity across hundreds of patches. Our contributions lie in new forward and reverse diffusion processes that address the challenges of large-scale image translation. Extensive experiments using paired Sentinel-II (10m) and Planet Dove (3m) images demonstrate that our approach provides precise domain adaptation, preserving image content while improving radiometric accuracy and feature representation. A thorough image quality assessment and comparisons with the standard DDIM framework and five other leading methods are presented. We reach a mean Learned Perceptual Image Patch Similarity (mLPIPS) of 0.1884 and a FrÃ©chet Inception Distance (FID) of 45.64, expressively outperforming all compared methods, including DDIM, ShuffleMixer, and SwinIR. The usefulness of our approach is further demonstrated in two Heterogeneous Change Detection tasks.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2404.00470.pdf' target='_blank'>https://arxiv.org/pdf/2404.00470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hassanuzzaman, Nurul Akhtar Hasan, Mohammad Abdullah Al Mamun, Khawza I Ahmed, Ahsan H Khandoker, Raqibul Mostafa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00470">Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Congenital anomalies arising as a result of a defect in the structure of the heart and great vessels are known as congenital heart diseases or CHDs. A PCG can provide essential details about the mechanical conduction system of the heart and point out specific patterns linked to different kinds of CHD. This study aims to investigate the minimum signal duration required for the automatic classification of heart sounds. This study also investigated the optimum signal quality assessment indicator (Root Mean Square of Successive Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral coefficients (MFCCs) based feature is used as an input to build a Transformer-Based residual one-dimensional convolutional neural network, which is then used for classifying the heart sound. The study showed that 0.4 is the ideal threshold for getting suitable signals for the RMSSD and ZCR indicators. Moreover, a minimum signal length of 5s is required for effective heart sound classification. It also shows that a shorter signal (3 s heart sound) does not have enough information to categorize heart sounds accurately, and the longer signal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is obtained for the 5s signal to distinguish the heart sound.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2403.06915.pdf' target='_blank'>https://arxiv.org/pdf/2403.06915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Campagnaro, Matin Ghalkhani, Riccardo Tumiati, Federico Marin, Matteo Del Grande, Alessandro Pozzebon, Davide De Battisti, Roberto Francescon, Michele Zorzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06915">Monitoring the Venice Lagoon: an IoT Cloud-Based Sensor Nerwork Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring the coastal area of the Venice Lagoon is of significant importance. While the impact of global warming is felt worldwide, coastal and littoral regions bear the brunt more prominently. These areas not only face the threat of rising sea levels but also contend with the escalating occurrence of seaquakes and floods. Additionally, the intricate ecosystems of rivers, seas, and lakes undergo profound transformations due to climate change and pollutants.
  Employing devices like the SENSWICH floating wireless sensor presented in this article and similar measurement instruments proves invaluable to automate environmental monitoring, hence eliminating the need for manual sampling campaigns. The utilization of wireless measurement devices offers cost-effectiveness, real-time analysis, and a reduction in human resource requirements. Storing data in cloud services further enhances the ability to monitor parameter changes over extended time intervals.
  In this article, we present an enhanced sensing device aimed at automating water quality assessment, while considering power consumption and reducing circuit complexity. Specifically, we will introduce the new schematic and circuit of SENSWICH which had changes in circuit and electronic aspects. Furthermore, we outline the methodology for aggregating data in a cloud service environment, such as Amazon Web Service (AWS), and using Grafana for visualization.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2403.03661.pdf' target='_blank'>https://arxiv.org/pdf/2403.03661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura MartÃ­n, Luis SÃ¡nchez, Jorge Lanza, Pablo Sotres
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03661">Development and evaluation of Artificial Intelligence techniques for IoT data quality assessment and curation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, data is becoming the new fuel for economic wealth and creation of novel and profitable business models. Multitude of technologies are contributing to an abundance of information sources which are already the baseline for multi-millionaire services and applications. Internet of Things (IoT), is probably the most representative one. However, for an economy of data to actually flourish there are still several critical challenges that have to be overcome. Among them, data quality can become an issue when data come from heterogeneous sources or have different formats, standards and scale. Improving data quality is of utmost importance for any domain since data are the basis for any decision-making system and decisions will not be accurate if they are based on inadequate low-quality data. In this paper we are presenting a solution for assessing several quality dimensions of IoT data streams as they are generated. Additionally, the solution described in the paper actually improves the quality of data streams by curating them through the application of Artificial Intelligence techniques. The approach followed in our work has been to append data quality information as metadata linked to each individual piece of curated data. We have leveraged linked-data principles and integrated the developed AI-based IoT data curation mechanisms within a Data Enrichment Toolchain (DET) that employs the NGSI-LD standard to harmonize and enrich heterogeneous data sources. Furthermore, we have evaluated our design under experimental research conditions, achieving a robust compromise between functionality and overhead. Besides, it demonstrates a stable and scalable performance.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2403.03648.pdf' target='_blank'>https://arxiv.org/pdf/2403.03648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura MartÃ­n, Jorge Lanza, VÃ­ctor GonzÃ¡lez, Juan RamÃ³n Santana, Pablo Sotres, Luis SÃ¡nchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03648">A Connector for Integrating NGSI-LD Data into Open Data Portals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, there are plenty of data sources generating massive amounts of information that, combined with novel data analytics frameworks, are meant to support optimisation in many application domains. Nonetheless, there are still shortcomings in terms of data discoverability, accessibility and interoperability. Open Data portals have emerged as a shift towards openness and discoverability. However, they do not impose any condition to the data itself, just stipulate how datasets have to be described. Alternatively, the NGSI-LD standard pursues harmonisation in terms of data modelling and accessibility. This paper presents a solution that bridges these two domains (i.e., Open Data portals and NGSI-LD-based data) in order to keep benefiting from the structured description of datasets offered by Open Data portals, while ensuring the interoperability provided by the NGSI-LD standard. Our solution aggregates the data into coherent datasets and generate high-quality descriptions, ensuring comprehensiveness, interoperability and accessibility. The proposed solution has been validated through a real-world implementation that exposes IoT data in NGSI-LD format through the European Data Portal (EDP). Moreover, the results from the Metadata Quality Assessment that the EDP implements, show that the datasets' descriptions generated achieve excellent ranking in terms of the Findability, Accessibility, Interoperability and Reusability (FAIR) data principles.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2402.08749.pdf' target='_blank'>https://arxiv.org/pdf/2402.08749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marina Manso Jimeno, Keerthi Sravan Ravi, Maggie Fung, John Thomas Vaughan,, Sairam Geethanath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08749">Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success. This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN for three-class classification and tested it on publicly available retrospective and prospective datasets. Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model's results. The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets. Additionally, the model's classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion. This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-around, and motion artifacts. This tool automates part of the time-consuming QA process and augments expertise on-site, particularly relevant in low-resource settings where local MR knowledge is scarce.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2401.15448.pdf' target='_blank'>https://arxiv.org/pdf/2401.15448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Liu, Alessandra Mileo, Alan F. Smeaton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15448">A Systematic Review of Available Datasets in Additive Manufacturing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-situ monitoring incorporating data from visual and other sensor technologies, allows the collection of extensive datasets during the Additive Manufacturing (AM) process. These datasets have potential for determining the quality of the manufactured output and the detection of defects through the use of Machine Learning during the manufacturing process. Open and annotated datasets derived from AM processes are necessary for the machine learning community to address this opportunity, which creates difficulties in the application of computer vision-related machine learning in AM. This systematic review investigates the availability of open image-based datasets originating from AM processes that align with a number of pre-defined selection criteria. The review identifies existing gaps among the current image-based datasets in the domain of AM, and points to the need for greater availability of open datasets in order to allow quality assessment and defect detection during additive manufacturing, to develop.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2401.07022.pdf' target='_blank'>https://arxiv.org/pdf/2401.07022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Lu, Quan Qi, Huaibin Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07022">Edge-Enabled Anomaly Detection and Information Completion for Social Network Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly advancing information era, various human behaviors are being precisely recorded in the form of data, including identity information, criminal records, and communication data. Law enforcement agencies can effectively maintain social security and precisely combat criminal activities by analyzing the aforementioned data. In comparison to traditional data analysis methods, deep learning models, relying on the robust computational power in cloud centers, exhibit higher accuracy in extracting data features and inferring data. However, within the architecture of cloud centers, the transmission of data from end devices introduces significant latency, hindering real-time inference of data. Furthermore, low-latency edge computing architectures face limitations in direct deployment due to relatively weak computing and storage capacities of nodes. To address these challenges, a lightweight distributed knowledge graph completion architecture is proposed. Firstly, we introduce a lightweight distributed knowledge graph completion architecture that utilizes knowledge graph embedding for data analysis. Subsequently, to filter out substandard data, a personnel data quality assessment method named PDQA is proposed. Lastly, we present a model pruning algorithm that significantly reduces the model size while maximizing performance, enabling lightweight deployment. In experiments, we compare the effects of 11 advanced models on completing the knowledge graph of public security personnel information. The results indicate that the RotatE model outperforms other models significantly in knowledge graph completion, with the pruned model size reduced by 70\%, and hits@10 reaching 86.97\%.}
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2401.06992.pdf' target='_blank'>https://arxiv.org/pdf/2401.06992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiqun Wu, Xiaoling Jiang, Rui Yu, Yonggang Luo, Tian Jiang, Xi Wu, Peng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06992">Progressive Feature Fusion Network for Enhancing Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image compression has been applied in the fields of image storage and video broadcasting. However, it's formidably tough to distinguish the subtle quality differences between those distorted images generated by different algorithms. In this paper, we propose a new image quality assessment framework to decide which image is better in an image group. To capture the subtle differences, a fine-grained network is adopted to acquire multi-scale features. Subsequently, we design a cross subtract block for separating and gathering the information within positive and negative image pairs. Enabling image comparison in feature space. After that, a progressive feature fusion block is designed, which fuses multi-scale features in a novel progressive way. Hierarchical spatial 2D features can thus be processed gradually. Experimental results show that compared with the current mainstream image quality assessment methods, the proposed network can achieve more accurate image quality assessment and ranks second in the benchmark of CLIC in the image perceptual model track.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2312.07101.pdf' target='_blank'>https://arxiv.org/pdf/2312.07101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madalina Olteanu, Fabrice Rossi, Florian Yger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07101">Meta-survey on outlier and anomaly detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impact of outliers and anomalies on model estimation and data  processing is of paramount importance, as evidenced by the extensive  body of research spanning various fields over several decades:  thousands of research papers have been published on the subject.  As  a consequence, numerous reviews, surveys, and textbooks have sought  to summarize the existing literature, encompassing a wide range of  methods from both the statistical and data mining  communities. While these endeavors to organize and summarize the  research are invaluable, they face inherent challenges due to the  pervasive nature of outliers and anomalies in all data-intensive  applications, irrespective of the specific application field or  scientific discipline. As a result, the resulting collection of  papers remains voluminous and somewhat heterogeneous.  To address the need for knowledge organization in this domain, this  paper implements the first systematic meta-survey of general surveys  and reviews on outlier and anomaly detection. Employing a classical  systematic survey approach, the study collects nearly 500 papers  using two specialized scientific search engines. From this  comprehensive collection, a subset of 56 papers that claim to be  general surveys on outlier detection is selected using a snowball  search technique to enhance field coverage. A meticulous quality  assessment phase further refines the selection to a subset of 25  high-quality general surveys.   Using this curated collection, the paper investigates the evolution  of the outlier detection field over a 20-year period, revealing  emerging themes and methods. Furthermore, an analysis of the surveys  sheds light on the survey writing practices adopted by scholars from  different communities who have contributed to this field.    Finally, the paper delves into several topics where consensus has  emerged from the literature. These include taxonomies of outlier  types, challenges posed by high-dimensional data, the importance of  anomaly scores, the impact of learning conditions, difficulties in  benchmarking, and the significance of neural  networks. Non-consensual aspects are also discussed, particularly  the distinction between local and global outliers and the challenges  in organizing detection methods into meaningful taxonomies.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2311.10728.pdf' target='_blank'>https://arxiv.org/pdf/2311.10728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SÃ¶ren Aguirre Reid, Frank Kammer, Jonas-Ian Kuche, Pia-Doreen Ritzke, Markus Siepermann, Max Stephan, Armin Wagenknecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10728">Improving Feedback from Automated Reviews of Student Spreadsheets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2311.04908.pdf' target='_blank'>https://arxiv.org/pdf/2311.04908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan-Hong Yang, Ying-Hui Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04908">Education journal rankings: A diversity-based Author Affiliation Index assessment methodology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the reputation of academic journals is an crucial issue. The Author Affiliation Index (AAI) was proposed as a novel indicator for judging journal quality in many academic disciplines. Nevertheless, the original AAI has several potential limitations, some of which have been discussed and addressed in previous studies. In this paper, we modified the original AAI by incorporating diversity of top-notch institutions, namely the AAID, exploring how institutional diversity is related to journal quality assessment. We further conducted a quality assessment of 263 education journals indexed in the Social Sciences Citation Index (SSCI) by applying the AAID, AAI and weighted AAI. We find that the AAID ranking possesses a low correlation coefficient with the Journal Impact Factor (JIF) and Eigenfactor Score (ES). That is to say, the AAID rating has not reached a good agreement with the most popular ranking indicators JIF and ES for journals in the field of education. Moreover, we analyze the reasons for the highest AAID from the structure of complex networks. Overall, the AAID is an alternative indicator for evaluating the prestige of journals from a new perspective.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2311.00996.pdf' target='_blank'>https://arxiv.org/pdf/2311.00996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Wang, Bowen Liu, Shiyu Liu, Fengyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00996">VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compression-based degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2310.08805.pdf' target='_blank'>https://arxiv.org/pdf/2310.08805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>K M Arefeen Sultan, Benjamin Orkild, Alan Morris, Eugene Kholmovski, Erik Bieging, Eugene Kwan, Ravi Ranjan, Ed DiBella, Shireen Elhabian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08805">Two-Stage Deep Learning Framework for Quality Assessment of Left Atrial Late Gadolinium Enhanced MRI Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate assessment of left atrial fibrosis in patients with atrial fibrillation relies on high-quality 3D late gadolinium enhancement (LGE) MRI images. However, obtaining such images is challenging due to patient motion, changing breathing patterns, or sub-optimal choice of pulse sequence parameters. Automated assessment of LGE-MRI image diagnostic quality is clinically significant as it would enhance diagnostic accuracy, improve efficiency, ensure standardization, and contributes to better patient outcomes by providing reliable and high-quality LGE-MRI scans for fibrosis quantification and treatment planning. To address this, we propose a two-stage deep-learning approach for automated LGE-MRI image diagnostic quality assessment. The method includes a left atrium detector to focus on relevant regions and a deep network to evaluate diagnostic quality. We explore two training strategies, multi-task learning, and pretraining using contrastive learning, to overcome limited annotated data in medical imaging. Contrastive Learning result shows about $4\%$, and $9\%$ improvement in F1-Score and Specificity compared to Multi-Task learning when there's limited data.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2310.05986.pdf' target='_blank'>https://arxiv.org/pdf/2310.05986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Severo, Lucas Theis, Johannes BallÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05986">The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares (WLS) problem, defined at the pixel-level, and solved at inference-time, that can capture global and local image characteristics. The distance in embedding space is used to define a perceptual similarity metric which we call LASI: Linear Autoregressive Similarity Index. Experiments on full-reference image quality assessment datasets show LASI performs competitively with learned deep feature based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020), at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang et al., 2003). We found that increasing the dimensionality of the embedding space consistently reduces the WLS loss while increasing performance on perceptual tasks, at the cost of increasing the computational complexity. LASI is fully differentiable, scales cubically with the number of embedding dimensions, and can be parallelized at the pixel-level. A Maximum Differentiation (MAD) competition (Wang & Simoncelli, 2008) between LASI and LPIPS shows that both methods are capable of finding failure points for the other, suggesting these metrics can be combined.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2309.13464.pdf' target='_blank'>https://arxiv.org/pdf/2309.13464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose A. Miranda, Celia LÃ³pez-Ongil, Javier Andreu-Perez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13464">Personalised and Adjustable Interval Type-2 Fuzzy-Based PPG Quality Assessment for the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of today's wearable technology provides seamless cardiac activity monitoring. Specifically, the vast majority employ Photoplethysmography (PPG) sensors to acquire blood volume pulse information, which is further analysed to extract useful and physiologically related features. Nevertheless, PPG-based signal reliability presents different challenges that strongly affect such data processing. This is mainly related to the fact of PPG morphological wave distortion due to motion artefacts, which can lead to erroneous interpretation of the extracted cardiac-related features. On this basis, in this paper, we propose a novel personalised and adjustable Interval Type-2 Fuzzy Logic System (IT2FLS) for assessing the quality of PPG signals. The proposed system employs a personalised approach to adapt the IT2FLS parameters to the unique characteristics of each individual's PPG signals.Additionally, the system provides adjustable levels of personalisation, allowing healthcare providers to adjust the system to meet specific requirements for different applications. The proposed system obtained up to 93.72\% for average accuracy during validation. The presented system has the potential to enable ultra-low complexity and real-time PPG quality assessment, improving the accuracy and reliability of PPG-based health monitoring systems at the edge.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2309.09264.pdf' target='_blank'>https://arxiv.org/pdf/2309.09264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mosleh Mahamud, Isak Samsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09264">Code quality assessment using transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatically evaluate the correctness of programming assignments is rather straightforward using unit and integration tests. However, programming tasks can be solved in multiple ways, many of which, although correct, are inelegant. For instance, excessive branching, poor naming or repetitiveness make the code hard to understand and maintain. These subjective qualities of code are hard to automatically assess using current techniques. In this work we investigate the use of CodeBERT to automatically assign quality score to Java code. We experiment with different models and training paradigms. We explore the accuracy of the models on a novel dataset for code quality assessment. Finally, we assess the quality of the predictions using saliency maps. We find that code quality to some extent is predictable and that transformer based models using task adapted pre-training can solve the task more efficiently than other techniques.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2308.14940.pdf' target='_blank'>https://arxiv.org/pdf/2308.14940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vikram Mohanty, Kurt Luther
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14940">DoubleCheck: Designing Community-based Assessability for Historical Person Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Historical photos are valuable for their cultural and economic significance, but can be difficult to identify accurately due to various challenges such as low-quality images, lack of corroborating evidence, and limited research resources. Misidentified photos can have significant negative consequences, including lost economic value, incorrect historical records, and the spread of misinformation that can lead to perpetuating conspiracy theories. To accurately assess the credibility of a photo identification (ID), it may be necessary to conduct investigative research, use domain knowledge, and consult experts. In this paper, we introduce DoubleCheck, a quality assessment framework for verifying historical photo IDs on Civil War Photo Sleuth (CWPS), a popular online platform for identifying American Civil War-era photos using facial recognition and crowdsourcing. DoubleCheck focuses on improving CWPS's user experience and system architecture to display information useful for assessing the quality of historical photo IDs on CWPS. In a mixed-methods evaluation of DoubleCheck, we found that users contributed a wide diversity of sources for photo IDs, which helped facilitate the community's assessment of these IDs through DoubleCheck's provenance visualizations. Further, DoubleCheck's quality assessment badges and visualizations supported users in making accurate assessments of photo IDs, even in cases involving ID conflicts.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2308.12077.pdf' target='_blank'>https://arxiv.org/pdf/2308.12077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12077">Analysis of XLS-R for Speech Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2307.02730.pdf' target='_blank'>https://arxiv.org/pdf/2307.02730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng-Lan Liu, Yu-Ning Ding, Gang Yan, Si-Fan Zhang, Jin-Rong Zhang, Wen-Yue Chen, Xue-Hai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02730">Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fine-grained action analysis of the existing action datasets is challenged by insufficient action categories, low fine granularities, limited modalities, and tasks. In this paper, we propose a Multi-modality and Multi-task dataset of Figure Skating (MMFS) which was collected from the World Figure Skating Championships. MMFS, which possesses action recognition and action quality assessment, captures RGB, skeleton, and is collected the score of actions from 11671 clips with 256 categories including spatial and temporal labels. The key contributions of our dataset fall into three aspects as follows. (1) Independently spatial and temporal categories are first proposed to further explore fine-grained action recognition and quality assessment. (2) MMFS first introduces the skeleton modality for complex fine-grained action quality assessment. (3) Our multi-modality and multi-task dataset encourage more action analysis models. To benchmark our dataset, we adopt RGB-based and skeleton-based baseline methods for action recognition and action quality assessment.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2304.13625.pdf' target='_blank'>https://arxiv.org/pdf/2304.13625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafal K. Mantiuk, Dounia Hammou, Param Hanji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13625">HDR-VDP-3: A multi-metric for predicting image differences, quality and contrast distortions in high dynamic range and regular content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-Dynamic-Range Visual-Difference-Predictor version 3, or HDR-VDP-3, is a visual metric that can fulfill several tasks, such as full-reference image/video quality assessment, prediction of visual differences between a pair of images, or prediction of contrast distortions. Here we present a high-level overview of the metric, position it with respect to related work, explain the main differences compared to version 2.2, and describe how the metric was adapted for the HDR Video Quality Measurement Grand Challenge 2023.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2304.06491.pdf' target='_blank'>https://arxiv.org/pdf/2304.06491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdur Rab Dhruba, Kazi Nabiul Alam, Md. Shakib Khan, Sananda Saha, Mohammad Monirujjaman Khan, Mohammed Baz, Mehedi Masud, Mohammed A. AlZain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06491">IoT-Based Water Quality Assessment System for Industrial Waste WaterHealthcare Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The environment, especially water, gets polluted due to industrialization and urbanization. Pollution due to industrialization and urbanization has harmful effects on both the environment and the lives on Earth. This polluted water can cause food poisoning, diarrhea, short-term gastrointestinal problems, respiratory diseases, skin problems, and other serious health complications. In a developing country like Bangladesh, where ready-made garments sector is one of the major sources of the total Gross Domestic Product (GDP), most of the wastes released from the garment factories are dumped into the nearest rivers or canals. Hence, the quality of the water of these bodies become very incompatible for the living beings, and so, it has become one of the major threats to the environment and human health. In addition, the amount of fish in the rivers and canals in Bangladesh is decreasing day by day as a result of water pollution. Therefore, to save fish and other water animals and the environment, we need to monitor the quality of the water and find out the reasons for the pollution. Real-time monitoring of the quality of water is vital for controlling water pollution. Most of the approaches for controlling water pollution are mainly biological and lab-based, which takes a lot of time and resources. To address this issue, we developed an Internet of Things (IoT)-based real-time water quality monitoring system, integrated with a mobile application. The proposed system in this research measures some of the most important indexes of water, including the potential of hydrogen (pH), total dissolved solids (TDS), and turbidity, and temperature of water. The proposed system results will be very helpful in saving the environment, and thus, improving the health of living creatures on Earth.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2303.10369.pdf' target='_blank'>https://arxiv.org/pdf/2303.10369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaohui Wang, Zhuowei Xu, Mai Xu, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10369">Blind Multimodal Quality Assessment of Low-light Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) aims at automatically and accurately forecasting objective scores for visual signals, which has been widely used to monitor product and service quality in low-light applications, covering smartphone photography, video surveillance, autonomous driving, etc. Recent developments in this field are dominated by unimodal solutions inconsistent with human subjective rating patterns, where human visual perception is simultaneously reflected by multiple sensory information. In this article, we present a unique blind multimodal quality assessment (BMQA) of low-light images from subjective evaluation to objective score. To investigate the multimodal mechanism, we first establish a multimodal low-light image quality (MLIQ) database with authentic low-light distortions, containing image-text modality pairs. Further, we specially design the key modules of BMQA, considering multimodal quality representation, latent feature alignment and fusion, and hybrid self-supervised and supervised learning. Extensive experiments show that our BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark database. In particular, we also build an independent single-image modality Dark-4K database, which is used to verify its applicability and generalization performance in mainstream unimodal applications. Qualitative and quantitative results on Dark-4K show that BMQA achieves superior performance to existing BIQA approaches as long as a pre-trained model is provided to generate text description. The proposed framework and two databases as well as the collected BIQA methods and evaluation metrics are made publicly available on here.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2302.09973.pdf' target='_blank'>https://arxiv.org/pdf/2302.09973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Kirchgasser, Christof Kauba, Georg Wimmer, Andreas Uhl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09973">Advanced Image Quality Assessment for Hand- and Fingervein Biometrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Scene Statistics commonly used in non-reference image quality measures and a deep learning based quality assessment approach are proposed as biometric quality indicators for vasculature images. While NIQE and BRISQUE if trained on common images with usual distortions do not work well for assessing vasculature pattern samples' quality, their variants being trained on high and low quality vasculature sample data behave as expected from a biometric quality estimator in most cases (deviations from the overall trend occur for certain datasets or feature extraction methods). The proposed deep learning based quality metric is capable of assigning the correct quality class to the vaculature pattern samples in most cases, independent of finger or hand vein patterns being assessed. The experiments were conducted on a total of 13 publicly available finger and hand vein datasets and involve three distinct template representations (two of them especially designed for vascular biometrics). The proposed (trained) quality measures are compared to a several classical quality metrics, with their achieved results underlining their promising behaviour.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2302.05289.pdf' target='_blank'>https://arxiv.org/pdf/2302.05289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abderrazek Azri, CÃ©cile Favre, Nouria Harbi, JÃ©rÃ´me Darmont, Camille NoÃ»s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05289">Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of rumors on social media has become a major concern due to its ability to create a devastating impact. Manually assessing the veracity of social media messages is a very time-consuming task that can be much helped by machine learning. Most message veracity verification methods only exploit textual contents and metadata. Very few take both textual and visual contents, and more particularly images, into account. Moreover, prior works have used many classical machine learning models to detect rumors. However, although recent studies have proven the effectiveness of ensemble machine learning approaches, such models have seldom been applied. Thus, in this paper, we propose a set of advanced image features that are inspired from the field of image quality assessment, and introduce the Multimodal fusiON framework to assess message veracIty in social neTwORks (MONITOR), which exploits all message features by exploring various machine learning models. Moreover, we demonstrate the effectiveness of ensemble learning algorithms for rumor detection by using five metalearning models. Eventually, we conduct extensive experiments on two real-world datasets. Results show that MONITOR outperforms state-of-the-art machine learning baselines and that ensemble models significantly increase MONITOR's performance.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2302.04796.pdf' target='_blank'>https://arxiv.org/pdf/2302.04796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ak, Emin Zerman, Maurice Quach, Aladine Chetouani, Aljosa Smolic, Giuseppe Valenzise, Patrick Le Callet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04796">BASICS: Broad quality Assessment of Static point clouds In Compression Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point clouds have become increasingly prevalent in representing 3D scenes within virtual environments, alongside 3D meshes. Their ease of capture has facilitated a wide array of applications on mobile devices, from smartphones to autonomous vehicles. Notably, point cloud compression has reached an advanced stage and has been standardized. However, the availability of quality assessment datasets, which are essential for developing improved objective quality metrics, remains limited. In this paper, we introduce BASICS, a large-scale quality assessment dataset tailored for static point clouds. The BASICS dataset comprises 75 unique point clouds, each compressed with four different algorithms including a learning-based method, resulting in the evaluation of nearly 1500 point clouds by 3500 unique participants. Furthermore, we conduct a comprehensive analysis of the gathered data, benchmark existing point cloud quality assessment metrics and identify their limitations. By publicly releasing the BASICS dataset, we lay the foundation for addressing these limitations and fostering the development of more precise quality metrics.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2209.04699.pdf' target='_blank'>https://arxiv.org/pdf/2209.04699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raluca Jalaboi, Ole Winther, Alfiia Galimzianova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.04699">Explainable Image Quality Assessments in Teledermatological Photography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality is a crucial factor in the effectiveness and efficiency of teledermatological consultations. However, up to 50% of images sent by patients have quality issues, thus increasing the time to diagnosis and treatment. An automated, easily deployable, explainable method for assessing image quality is necessary to improve the current teledermatological consultation flow. We introduce ImageQX, a convolutional neural network for image quality assessment with a learning mechanism for identifying the most common poor image quality explanations: bad framing, bad lighting, blur, low resolution, and distance issues. ImageQX was trained on 26,635 photographs and validated on 9,874 photographs, each annotated with image quality labels and poor image quality explanations by up to 12 board-certified dermatologists. The photographic images were taken between 2017 and 2019 using a mobile skin disease tracking application accessible worldwide. Our method achieves expert-level performance for both image quality assessment and poor image quality explanation. For image quality assessment, ImageQX obtains a macro F1-score of 0.73 +- 0.01, which places it within standard deviation of the pairwise inter-rater F1-score of 0.77 +- 0.07. For poor image quality explanations, our method obtains F1-scores of between 0.37 +- 0.01 and 0.70 +- 0.01, similar to the inter-rater pairwise F1-score of between 0.24 +- 0.15 and 0.83 +- 0.06. Moreover, with a size of only 15 MB, ImageQX is easily deployable on mobile devices. With an image quality detection performance similar to that of dermatologists, incorporating ImageQX into the teledermatology flow can enable a better, faster flow for remote consultations.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2203.13856.pdf' target='_blank'>https://arxiv.org/pdf/2203.13856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme C. Oliveira, Gustavo H. Rosa, Daniel C. G. Pedronette, JoÃ£o P. Papa, Himeesh Kumar, Leandro A. Passos, Dinesh Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.13856">Robust deep learning for eye fundus images: Bridging real and synthetic data for enhancing generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning applications for assessing medical images are limited because the datasets are often small and imbalanced. The use of synthetic data has been proposed in the literature, but neither a robust comparison of the different methods nor generalizability has been reported. Our approach integrates a retinal image quality assessment model and StyleGAN2 architecture to enhance Age-related Macular Degeneration (AMD) detection capabilities and improve generalizability. This work compares ten different Generative Adversarial Network (GAN) architectures to generate synthetic eye-fundus images with and without AMD. We combined subsets of three public databases (iChallenge-AMD, ODIR-2019, and RIADD) to form a single training and test set. We employed the STARE dataset for external validation, ensuring a comprehensive assessment of the proposed approach. The results show that StyleGAN2 reached the lowest Frechet Inception Distance (166.17), and clinicians could not accurately differentiate between real and synthetic images. ResNet-18 architecture obtained the best performance with 85% accuracy and outperformed the two human experts (80% and 75%) in detecting AMD fundus images. The accuracy rates were 82.8% for the test set and 81.3% for the STARE dataset, demonstrating the model's generalizability. The proposed methodology for synthetic medical image generation has been validated for robustness and accuracy, with free access to its code for further research and development in this field.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2202.03879.pdf' target='_blank'>https://arxiv.org/pdf/2202.03879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nisar Ahmed, Shahzad Asif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.03879">BIQ2021: A Large-Scale Blind Image Quality Assessment Database</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The assessment of the perceptual quality of digital images is becoming increasingly important as a result of the widespread use of digital multimedia devices. Smartphones and high-speed internet are just two examples of technologies that have multiplied the amount of multimedia content available. Thus, obtaining a representative dataset, which is required for objective quality assessment training, is a significant challenge. The Blind Image Quality Assessment Database, BIQ2021, is presented in this article. By selecting images with naturally occurring distortions and reliable labeling, the dataset addresses the challenge of obtaining representative images for no-reference image quality assessment. The dataset consists of three sets of images: those taken without the intention of using them for image quality assessment, those taken with intentionally introduced natural distortions, and those taken from an open-source image-sharing platform. It is attempted to maintain a diverse collection of images from various devices, containing a variety of different types of objects and varying degrees of foreground and background information. To obtain reliable scores, these images are subjectively scored in a laboratory environment using a single stimulus method. The database contains information about subjective scoring, human subject statistics, and the standard deviation of each image. The dataset's Mean Opinion Scores (MOS) make it useful for assessing visual quality. Additionally, the proposed database is used to evaluate existing blind image quality assessment approaches, and the scores are analyzed using Pearson and Spearman's correlation coefficients. The image database and MOS are freely available for use and benchmarking.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2003.02321.pdf' target='_blank'>https://arxiv.org/pdf/2003.02321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason L. Granstedt, Weimin Zhou, Mark A. Anastasio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.02321">Approximating the Hotelling Observer with Autoencoder-Learned Efficient Channels for Binary Signal Detection Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective assessment of image quality (IQ) has been advocated for the analysis and optimization of medical imaging systems. One method of obtaining such IQ metrics is through a mathematical observer. The Bayesian ideal observer is optimal by definition for signal detection tasks, but is frequently both intractable and non-linear. As an alternative, linear observers are sometimes used for task-based image quality assessment. The optimal linear observer is the Hotelling observer (HO). The computational cost of calculating the HO increases with image size, making a reduction in the dimensionality of the data desirable. Channelized methods have become popular for this purpose, and many competing methods are available for computing efficient channels. In this work, a novel method for learning channels using an autoencoder (AE) is presented. AEs are a type of artificial neural network (ANN) that are frequently employed to learn concise representations of data to reduce dimensionality. Modifying the traditional AE loss function to focus on task-relevant information permits the development of efficient AE-channels. These AE-channels were trained and tested on a variety of signal shapes and backgrounds to evaluate their performance. In the experiments, the AE-learned channels were competitive with and frequently outperformed other state-of-the-art methods for approximating the HO. The performance gains were greatest for the datasets with a small number of training images and noisy estimates of the signal image. Overall, AEs are demonstrated to be competitive with state-of-the-art methods for generating efficient channels for the HO and can have superior performance on small datasets.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/1903.00473.pdf' target='_blank'>https://arxiv.org/pdf/1903.00473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liqun Lin, Shiqi Yu, Tiesong Zhao, Zhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1903.00473">PEA265: Perceptual Assessment of Video Compression Artifacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most widely used video encoders share a common hybrid coding framework that includes block-based motion estimation/compensation and block-based transform coding. Despite their high coding efficiency, the encoded videos often exhibit visually annoying artifacts, denoted as Perceivable Encoding Artifacts (PEAs), which significantly degrade the visual Qualityof- Experience (QoE) of end users. To monitor and improve visual QoE, it is crucial to develop subjective and objective measures that can identify and quantify various types of PEAs. In this work, we make the first attempt to build a large-scale subjectlabelled database composed of H.265/HEVC compressed videos containing various PEAs. The database, namely the PEA265 database, includes 4 types of spatial PEAs (i.e. blurring, blocking, ringing and color bleeding) and 2 types of temporal PEAs (i.e. flickering and floating). Each containing at least 60,000 image or video patches with positive and negative labels. To objectively identify these PEAs, we train Convolutional Neural Networks (CNNs) using the PEA265 database. It appears that state-of-theart ResNeXt is capable of identifying each type of PEAs with high accuracy. Furthermore, we define PEA pattern and PEA intensity measures to quantify PEA levels of compressed video sequence. We believe that the PEA265 database and our findings will benefit the future development of video quality assessment methods and perceptually motivated video encoders.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2511.08186.pdf' target='_blank'>https://arxiv.org/pdf/2511.08186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhui Zhu, Buliao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08186">Pixel-level Quality Assessment for Oriented Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern oriented object detectors typically predict a set of bounding boxes and select the top-ranked ones based on estimated localization quality. Achieving high detection performance requires that the estimated quality closely aligns with the actual localization accuracy. To this end, existing approaches predict the Intersection over Union (IoU) between the predicted and ground-truth (GT) boxes as a proxy for localization quality. However, box-level IoU prediction suffers from a structural coupling issue: since the predicted box is derived from the detector's internal estimation of the GT box, the predicted IoU--based on their similarity--can be overestimated for poorly localized boxes. To overcome this limitation, we propose a novel Pixel-level Quality Assessment (PQA) framework, which replaces box-level IoU prediction with the integration of pixel-level spatial consistency. PQA measures the alignment between each pixel's relative position to the predicted box and its corresponding position to the GT box. By operating at the pixel level, PQA avoids directly comparing the predicted box with the estimated GT box, thereby eliminating the inherent similarity bias in box-level IoU prediction. Furthermore, we introduce a new integration metric that aggregates pixel-level spatial consistency into a unified quality score, yielding a more accurate approximation of the actual localization quality. Extensive experiments on HRSC2016 and DOTA demonstrate that PQA can be seamlessly integrated into various oriented object detectors, consistently improving performance (e.g., +5.96% AP$_{50:95}$ on Rotated RetinaNet and +2.32% on STD).
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2511.05611.pdf' target='_blank'>https://arxiv.org/pdf/2511.05611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuaikang Zhu, Yang Yang, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05611">Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose serves as a cornerstone of action quality assessment (AQA), where subtle spatial-temporal variations in pose often distinguish excellence from mediocrity. In high-level competitions, these nuanced differences become decisive factors in scoring. In this paper, we propose a novel multi-level motion parsing framework for AQA based on enhanced spatial-temporal pose features. On the first level, the Action-Unit Parser is designed with the help of pose extraction to achieve precise action segmentation and comprehensive local-global pose representations. On the second level, Motion Parser is used by spatial-temporal feature learning to capture pose changes and appearance details for each action-unit. Meanwhile, some special conditions other than body-related will impact action scoring, like water splash in diving. In this work, we design an additional Condition Parser to offer users more flexibility in their choices. Finally, Weight-Adjust Scoring Module is introduced to better accommodate the diverse requirements of various action types and the multi-scale nature of action-units. Extensive evaluations on large-scale diving sports datasets demonstrate that our multi-level motion parsing framework achieves state-of-the-art performance in both action segmentation and action scoring tasks.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2511.04133.pdf' target='_blank'>https://arxiv.org/pdf/2511.04133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel E. Andres, Vadim Fedorov, Rida Sadek, Enric Spagnolo-Arrizabalaga, Nadescha Trudel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04133">Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voice AI agents are rapidly transitioning to production deployments, yet systematic methods for ensuring testing reliability remain underdeveloped. Organizations cannot objectively assess whether their testing approaches (internal tools or external platforms) actually work, creating a critical measurement gap as voice AI scales to billions of daily interactions. We present the first systematic framework for evaluating voice AI testing quality through human-centered benchmarking. Our methodology addresses the fundamental dual challenge of testing platforms: generating realistic test conversations (simulation quality) and accurately evaluating agent responses (evaluation quality). The framework combines established psychometric techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence intervals, and permutation tests) with rigorous statistical validation to provide reproducible metrics applicable to any testing approach. To validate the framework and demonstrate its utility, we conducted comprehensive empirical evaluation of three leading commercial platforms focused on Voice AI Testing using 21,600 human judgments across 45 simulations and ground truth validation on 60 conversations. Results reveal statistically significant performance differences with the proposed framework, with the top-performing platform, Evalion, achieving 0.92 evaluation quality measured as f1-score versus 0.73 for others, and 0.61 simulation quality using a league based scoring system (including ties) vs 0.43 for other platforms. This framework enables researchers and organizations to empirically validate the testing capabilities of any platform, providing essential measurement foundations for confident voice AI deployment at scale. Supporting materials are made available to facilitate reproducibility and adoption.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2511.00698.pdf' target='_blank'>https://arxiv.org/pdf/2511.00698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taifour Yousra, Beghdadi Azeddine, Marie Luong, Zuheng Ming
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00698">Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2508.19492.pdf' target='_blank'>https://arxiv.org/pdf/2508.19492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Can Yavuz, Humza Gohar Kabir, Aylin Ãzkan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19492">Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objectivity in journalism has long been contested, oscillating between ideals of neutral, fact-based reporting and the inevitability of subjective framing. With the advent of large language models (LLMs), these tensions are now mediated by algorithmic systems whose training data and design choices may themselves embed cultural or ideological biases. This study investigates geopolitical parallax-systematic divergence in news quality and subjectivity assessments-by comparing article-level embeddings from Chinese-origin (Qwen, BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate both on a human-annotated news quality benchmark spanning fifteen stylistic, informational, and affective dimensions, and on parallel corpora covering politically sensitive topics, including Palestine and reciprocal China-United States coverage. Using logistic regression probes and matched-topic evaluation, we quantify per-metric differences in predicted positive-class probabilities between model families. Our findings reveal consistent, non-random divergences aligned with model origin. In Palestine-related coverage, Western models assign higher subjectivity and positive emotion scores, while Chinese models emphasize novelty and descriptiveness. Cross-topic analysis shows asymmetries in structural quality metrics Chinese-on-US scoring notably lower in fluency, conciseness, technicality, and overall quality-contrasted by higher negative emotion scores. These patterns align with media bias theory and our distinction between semantic, emotional, and relational subjectivity, and extend LLM bias literature by showing that geopolitical framing effects persist in downstream quality assessment tasks. We conclude that LLM-based media evaluation pipelines require cultural calibration to avoid conflating content differences with model-induced bias.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2508.19289.pdf' target='_blank'>https://arxiv.org/pdf/2508.19289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tai Inui, Steven Oh, Magdeline Kuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19289">Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an unsupervised slide-quality assessment pipeline that combines seven expert-inspired visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate presentation slides. Trained on 12k professional lecture slides and evaluated on six academic talks (115 slides), our method achieved Pearson correlations up to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual ratings, discriminant validity against speaker-delivery scores, and exploratory alignment with overall impressions. Our results show that augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling scalable, objective feedback in real time.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2508.19083.pdf' target='_blank'>https://arxiv.org/pdf/2508.19083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo BaÃ¹, Luca Perbellini, Samuele Grillo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19083">A Principled Framework to Evaluate Quality of AC-OPF Datasets for Machine Learning: Benchmarking a Novel, Scalable Generation Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several methods have been proposed in the literature to improve the quality of AC optimal power flow (AC-OPF) datasets used in machine learning (ML) models. Yet, scalability to large power systems remains unaddressed and comparing generation approaches is still hindered by the absence of widely accepted metrics quantifying AC-OPF dataset quality. In this work, we tackle both these limitations. We provide a simple heuristic that samples load setpoints uniformly in total load active power, rather than maximizing volume coverage, and solves an AC-OPF formulation with load slack variables to improve convergence. For quality assessment, we formulate a multi-criteria framework based on three metrics, measuring variability in the marginal distributions of AC-OPF primal variables, diversity in constraint activation patterns among AC-OPF instances and activation frequency of variable bounds. By comparing four open-source methods based on these metrics, we show that our heuristic consistently outperforms uniform random sampling, whether independent or constrained to a convex polytope, scoring as best in terms of balance between dataset quality and scalability.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2508.12506.pdf' target='_blank'>https://arxiv.org/pdf/2508.12506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>E. Ulises Moya-SÃ¡nchez, Abraham SÃ¡nchez-Perez, RaÃºl Nanclares Da Veiga, Alejandro Zarate-MacÃ­as, Edgar Villareal, Alejandro SÃ¡nchez-Montes, Edtna Jauregui-Ulloa, HÃ©ctor Moreno, Ulises CortÃ©s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12506">Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age individuals. Early detection of DR can reduce the risk of vision loss by up to 95%, but a shortage of retinologists and challenges in timely examination complicate detection. Artificial Intelligence (AI) models using retinal fundus photographs (RFPs) offer a promising solution. However, adoption in clinical settings is hindered by low-quality data and biases that may lead AI systems to learn unintended features. To address these challenges, we developed RAIS-DR, a Responsible AI System for DR screening that incorporates ethical principles across the AI lifecycle. RAIS-DR integrates efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated significant improvements, with F1 scores increasing by 5-12%, accuracy by 6-19%, and specificity by 10-20%. Additionally, fairness metrics such as Disparate Impact and Equal Opportunity Difference indicated equitable performance across demographic subgroups, underscoring RAIS-DR's potential to reduce healthcare disparities. These results highlight RAIS-DR as a robust and ethically aligned solution for DR screening in clinical settings. The code, weights of RAIS-DR are available at https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with RAIL.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2508.10397.pdf' target='_blank'>https://arxiv.org/pdf/2508.10397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibin Sun, Xinghui Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10397">PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2508.08962.pdf' target='_blank'>https://arxiv.org/pdf/2508.08962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Liang, Fredrik Cumlin, Victor Ungureanu, Chandan K. A. Reddy, Christian Schuldt, Saikat Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08962">Selection of Layers from Self-supervised Learning Models for Predicting Mean-Opinion-Score of Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) models like Wav2Vec2, HuBERT, and WavLM have been widely used in speech processing. These transformer-based models consist of multiple layers, each capturing different levels of representation. While prior studies explored their layer-wise representations for efficiency and performance, speech quality assessment (SQA) models predominantly rely on last-layer features, leaving intermediate layers underexamined. In this work, we systematically evaluate different layers of multiple SSL models for predicting mean-opinion-score (MOS). Features from each layer are fed into a lightweight regression network to assess effectiveness. Our experiments consistently show early-layers features outperform or match those from the last layer, leading to significant improvements over conventional approaches and state-of-the-art MOS prediction models. These findings highlight the advantages of early-layer selection, offering enhanced performance and reduced system complexity.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2508.04595.pdf' target='_blank'>https://arxiv.org/pdf/2508.04595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan A. Zak, Christian WeiÃenfels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04595">Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Resistance spot welding is the dominant joining process for the body-in-white in the automotive industry, where the weld nugget diameter is the key quality metric. Its measurement requires destructive testing, limiting the potential for efficient quality control. Physics-informed neural networks were investigated as a promising tool to reconstruct internal process states from experimental data, enabling model-based and non-invasive quality assessment in aluminum spot welding. A major challenge is the integration of real-world data into the network due to competing optimization objectives. To address this, we introduce two novel training strategies. First, experimental losses for dynamic displacement and nugget diameter are progressively included using a fading-in function to prevent excessive optimization conflicts. We also implement a custom learning rate scheduler and early stopping based on a rolling window to counteract premature reduction due to increased loss magnitudes. Second, we introduce a conditional update of temperature-dependent material parameters via a look-up table, activated only after a loss threshold is reached to ensure physically meaningful temperatures. An axially symmetric two-dimensional model was selected to represent the welding process accurately while maintaining computational efficiency. To reduce computational burden, the training strategies and model components were first systematically evaluated in one dimension, enabling controlled analysis of loss design and contact models. The two-dimensional network predicts dynamic displacement and nugget growth within the experimental confidence interval, supports transferring welding stages from steel to aluminum, and demonstrates strong potential for fast, model-based quality control in industrial applications.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2508.01511.pdf' target='_blank'>https://arxiv.org/pdf/2508.01511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Parab, A. Lamelas, A. Hassan, P. Bhote
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01511">Canoe Paddling Quality Assessment Using Smart Devices: Preliminary Machine Learning Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over 22 million Americans participate in paddling-related activities annually, contributing to a global paddlesports market valued at 2.4 billion US dollars in 2020. Despite its popularity, the sport has seen limited integration of machine learning (ML) and remains hindered by the cost of coaching and specialized equipment. This study presents a novel AI-based coaching system that uses ML models trained on motion data and delivers stroke feedback via a large language model (LLM). Participants were recruited through a collaboration with the NYU Concrete Canoe Team. Motion data were collected across two sessions, one with suboptimal form and one with corrected technique, using Apple Watches and smartphones secured in sport straps. The data underwent stroke segmentation and feature extraction. ML models, including Support Vector Classifier, Random Forest, Gradient Boosting, and Extremely Randomized Trees, were trained on both raw and engineered features. A web based interface was developed to visualize stroke quality and deliver LLM-based feedback. Across four participants, eight trials yielded 66 stroke samples. The Extremely Randomized Tree model achieved the highest performance with an F score of 0.9496 under five fold cross validation. The web interface successfully provided both quantitative metrics and qualitative feedback. Sensor placement near the wrists improved data quality. Preliminary results indicate that smartwatches and smartphones can enable low cost, accessible alternatives to traditional paddling instruction. While limited by sample size, the study demonstrates the feasibility of using consumer devices and ML to support stroke refinement and technique improvement.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2507.21945.pdf' target='_blank'>https://arxiv.org/pdf/2507.21945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wang, Peng-Jie Li, Yuan-Yuan Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21945">Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term action quality assessment (AQA) focuses on evaluating the quality of human activities in videos lasting up to several minutes. This task plays an important role in the automated evaluation of artistic sports such as rhythmic gymnastics and figure skating, where both accurate motion execution and temporal synchronization with background music are essential for performance assessment. However, existing methods predominantly fall into two categories: unimodal approaches that rely solely on visual features, which are inadequate for modeling multimodal cues like music; and multimodal approaches that typically employ simple feature-level contrastive fusion, overlooking deep cross-modal collaboration and temporal dynamics. As a result, they struggle to capture complex interactions between modalities and fail to accurately track critical performance changes throughout extended sequences. To address these challenges, we propose the Long-term Multimodal Attention Consistency Network (LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to explicitly align multimodal features, enabling stable integration of visual and audio information and enhancing feature representations. Specifically, we introduce a multimodal local query encoder module to capture temporal semantics and cross-modal relations, and use a two-level score evaluation for interpretable results. In addition, attention-based and regression-based losses are applied to jointly optimize multimodal alignment and score fusion. Experiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net significantly outperforms existing methods, validating the effectiveness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2507.17934.pdf' target='_blank'>https://arxiv.org/pdf/2507.17934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Guo, Siyan Liang, Yachao Cui, Juxiang Zhou, Lei Wang, Han Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17934">Multimodal Fine-grained Reasoning for Post Quality Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately assessing post quality requires complex relational reasoning to capture nuanced topic-post relationships. However, existing studies face three major limitations: (1) treating the task as unimodal categorization, which fails to leverage multimodal cues and fine-grained quality distinctions; (2) introducing noise during deep multimodal fusion, leading to misleading signals; and (3) lacking the ability to capture complex semantic relationships like relevance and comprehensiveness. To address these issues, we propose the Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework, which mimics human cognitive processes. MFTRR reframes post-quality assessment as a ranking task and incorporates multimodal data to better capture quality variations. It consists of two key modules: (1) the Local-Global Semantic Correlation Reasoning Module, which models fine-grained semantic interactions between posts and topics at both local and global levels, enhanced by a maximum information fusion mechanism to suppress noise; and (2) the Multi-Level Evidential Relational Reasoning Module, which explores macro- and micro-level relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on three newly constructed multimodal topic-post datasets and the public Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3 improvement over the best unimodal method on the Art History dataset.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2507.17182.pdf' target='_blank'>https://arxiv.org/pdf/2507.17182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linghe Meng, Jiarun Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17182">Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quality assessment of AI-generated content (AIGC) faces multi-dimensional challenges, that span from low-level visual perception to high-level semantic understanding. Existing methods generally rely on single-level visual features, limiting their ability to capture complex distortions in AIGC images. To address this limitation, a multi-level visual representation paradigm is proposed with three stages, namely multi-level feature extraction, hierarchical fusion, and joint aggregation. Based on this paradigm, two networks are developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net) is designed for the perceptual quality assessment, extracting complementary local and global features via dual CNN and Transformer visual backbones. The Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image correspondence by embedding prompt semantics into the visual feature fusion process at each feature level. The fused multi-level features are then aggregated for final evaluation. Experiments on benchmarks demonstrate outstanding performance on both tasks, validating the effectiveness of the proposed multi-level visual assessment paradigm.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2507.15680.pdf' target='_blank'>https://arxiv.org/pdf/2507.15680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkang Hou, Jiarun Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15680">Visual-Language Model Knowledge Distillation Method for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image Quality Assessment (IQA) is a core task in computer vision. Multimodal methods based on vision-language models, such as CLIP, have demonstrated exceptional generalization capabilities in IQA tasks. To address the issues of excessive parameter burden and insufficient ability to identify local distorted features in CLIP for IQA, this study proposes a visual-language model knowledge distillation method aimed at guiding the training of models with architectural advantages using CLIP's IQA knowledge. First, quality-graded prompt templates were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned to enhance its capabilities in IQA tasks. Finally, a modality-adaptive knowledge distillation strategy is proposed to achieve guidance from the CLIP teacher model to the student model. Our experiments were conducted on multiple IQA datasets, and the results show that the proposed method significantly reduces model complexity while outperforming existing IQA methods, demonstrating strong potential for practical deployment.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2507.11636.pdf' target='_blank'>https://arxiv.org/pdf/2507.11636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Fan, Donald Williamson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11636">JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment (SQA) is often used to learn a mapping from a high-dimensional input space to a scalar that represents the mean opinion score (MOS) of the perceptual speech quality. Learning such a mapping is challenging for many reasons, but largely because MOS exhibits high levels of inherent variance due to perceptual and experimental-design differences. Many solutions have been proposed, but many approaches do not properly incorporate perceptual factors into their learning algorithms (beyond the MOS label), which could lead to unsatisfactory results. To this end, we propose JSQA, a two-stage framework that pretrains an audio encoder using perceptually-guided contrastive learning on just noticeable difference (JND) pairs, followed by fine-tuning for MOS prediction. We first generate pairs of audio data within JND levels, which are then used to pretrain an encoder to leverage perceptual quality similarity information and map it into an embedding space. The JND pairs come from clean LibriSpeech utterances that are mixed with background noise from CHiME-3, at different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with audio samples from the NISQA dataset for MOS prediction. Experimental results suggest that perceptually-inspired contrastive pretraining significantly improves the model performance evaluated by various metrics when compared against the same network trained from scratch without pretraining. These findings suggest that incorporating perceptual factors into pretraining greatly contributes to the improvement in performance for SQA.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2507.10222.pdf' target='_blank'>https://arxiv.org/pdf/2507.10222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhi Xu, Yizhe Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10222">Spatial Lifting for Dense Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Spatial Lifting (SL), a novel methodology for dense prediction tasks. SL operates by lifting standard inputs, such as 2D images, into a higher-dimensional space and subsequently processing them using networks designed for that higher dimension, such as a 3D U-Net. Counterintuitively, this dimensionality lifting allows us to achieve good performance on benchmark tasks compared to conventional approaches, while reducing inference costs and significantly lowering the number of model parameters. The SL framework produces intrinsically structured outputs along the lifted dimension. This emergent structure facilitates dense supervision during training and enables robust, near-zero-additional-cost prediction quality assessment at test time. We validate our approach across 19 benchmark datasets (13 for semantic segmentation and 6 for depth estimation), demonstrating competitive dense prediction performance while reducing the model parameter count by over 98% (in the U-Net case) and lowering inference costs. Spatial Lifting introduces a new vision modeling paradigm that offers a promising path toward more efficient, accurate, and reliable deep networks for dense prediction tasks in vision.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2507.02628.pdf' target='_blank'>https://arxiv.org/pdf/2507.02628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irena Girshovitz, Atai Ambus, Moni Shahar, Ran Gilad-Bachrach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02628">Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: The use of Electronic Health Records (EHRs) for epidemiological studies and artificial intelligence (AI) training is increasing rapidly. The reliability of the results depends on the accuracy and completeness of EHR data. However, EHR data often contain significant quality issues, including misrepresentations of subpopulations, biases, and systematic errors, as they are primarily collected for clinical and billing purposes. Existing quality assessment methods remain insufficient, lacking systematic procedures to assess data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit testing and coverage concepts from software engineering to identify data quality concerns. We demonstrate our approach using the Medical Data Pecking Tool (MDPT), which consists of two main components: (1) an automated test generator that uses large language models and grounding techniques to create a test suite from data and study descriptions, and (2) a data testing framework that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and SyntheticMass, generating 55-73 tests per cohort across four conditions. These tests correctly identified 20-43 non-aligned or non-conforming data issues. We present a detailed analysis of the LLM-generated test suites in terms of reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable context-sensitive data quality testing as part of the data analysis workflow to improve the validity of its outcomes. Our approach tackles these challenges from a quality assurance perspective, laying the foundation for further development such as additional data modalities and improved grounding methods.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2507.00046.pdf' target='_blank'>https://arxiv.org/pdf/2507.00046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshansh Mishra, Eyob Mesele Sefene, Shivraman Thapliyal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00046">Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2506.21362.pdf' target='_blank'>https://arxiv.org/pdf/2506.21362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Yixin Wang, Moontae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21362">Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient access to high-quality information is vital for online platforms. To promote more useful information, users not only create new content but also evaluate existing content, often through helpfulness voting. Although aggregated votes help service providers rank their user content, these votes are often biased by disparate accessibility per position and the cascaded influence of prior votes. For a fairer assessment of information quality, we propose the Counterfactual Voting Adjustment (CVA), a causal framework that accounts for the context in which individual votes are cast. Through preliminary and semi-synthetic experiments, we show that CVA effectively models the position and herding biases, accurately recovering the predefined content quality. In a real experiment, we demonstrate that reranking content based on the learned quality by CVA exhibits stronger alignment with both user sentiment and quality evaluation assessed by GPT-4o, outperforming system rankings based on aggregated votes and model-based rerankings without causal inference. Beyond the individual quality inference, our embeddings offer comparative insights into the behavioral dynamics of expert user groups across 120 major StackExchange communities.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2506.16116.pdf' target='_blank'>https://arxiv.org/pdf/2506.16116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ignacio HernÃ¡ndez Montilla, Alfonso Medela, Paola Pasquali, Andy Aguilar, Taig Mac Carthy, Gerardo FernÃ¡ndez, Antonio Martorell, Enrique Onieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16116">Enhanced Dermatology Image Quality Assessment via Cross-Domain Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teledermatology has become a widely accepted communication method in daily clinical practice, enabling remote care while showing strong agreement with in-person visits. Poor image quality remains an unsolved problem in teledermatology and is a major concern to practitioners, as bad-quality images reduce the usefulness of the remote consultation process. However, research on Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage the latest advances in non-dermatology IQA, such as using larger image databases with ratings from large groups of human observers. In this work, we propose cross-domain training of IQA models, combining dermatology and non-dermatology IQA datasets. For this purpose, we created a novel dermatology IQA database, Legit.Health-DIQA-Artificial, using dermatology images from several sources and having them annotated by a group of human observers. We demonstrate that cross-domain training yields optimal performance across domains and overcomes one of the biggest limitations in dermatology IQA, which is the small scale of data, and leads to models trained on a larger pool of image distortions, resulting in a better management of image quality in the teledermatology process.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2506.15718.pdf' target='_blank'>https://arxiv.org/pdf/2506.15718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Guo, Hongji Fang, Tianyu Fang, Zhe Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15718">BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of artificial intelligence, the automatic generation of building-scale 3-D objects has become an active research topic, yet training such models still demands large, clean and richly annotated datasets. We introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors) buildings (about 10 GB) produced by a shape-grammar-driven pipeline that encodes established building-design principles. Every sample consists of a geometrically exact B-rep solid-covering floors, walls, slabs and rule-based openings-together with a fast-loading .npy metadata file that records detailed per-floor parameters. The generator incorporates constraints on spatial scale, daylight optimisation and interior layout, and the resulting objects pass multi-stage filters that remove Boolean failures, undersized rooms and extreme aspect ratios, ensuring compliance with architectural standards. To verify the dataset's learnability we trained two lightweight PointNet baselines. (i) Multi-attribute regression. A single encoder predicts storey count, total rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100 unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same backbone we classify GOOD versus DEFECT; on a balanced 100-model set the network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \% precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K is learnable yet non-trivial for both geometric regression and topological quality assessment
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2506.14297.pdf' target='_blank'>https://arxiv.org/pdf/2506.14297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Alves, Carla Bezerra, Ivan Machado, Larissa Rocha, TÃ¡ssio VirgÃ­nio, Publio Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14297">Quality Assessment of Python Tests Generated by Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The manual generation of test scripts is a time-intensive, costly, and error-prone process, indicating the value of automated solutions. Large Language Models (LLMs) have shown great promise in this domain, leveraging their extensive knowledge to produce test code more efficiently. This study investigates the quality of Python test code generated by three LLMs: GPT-4o, Amazon Q, and LLama 3.3. We evaluate the structural reliability of test suites generated under two distinct prompt contexts: Text2Code (T2C) and Code2Code (C2C). Our analysis includes the identification of errors and test smells, with a focus on correlating these issues to inadequate design patterns. Our findings reveal that most test suites generated by the LLMs contained at least one error or test smell. Assertion errors were the most common, comprising 64% of all identified errors, while the test smell Lack of Cohesion of Test Cases was the most frequently detected (41%). Prompt context significantly influenced test quality; textual prompts with detailed instructions often yielded tests with fewer errors but a higher incidence of test smells. Among the evaluated LLMs, GPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C), whereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For test smells, Amazon Q had fewer detections in the C2C context (9%), while LLama 3.3 performed best in the T2C context (10%). Additionally, we observed a strong relationship between specific errors, such as assertion or indentation issues, and test case cohesion smells. These findings demonstrate opportunities for improving the quality of test generation by LLMs and highlight the need for future research to explore optimized generation scenarios and better prompt engineering strategies.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2506.09833.pdf' target='_blank'>https://arxiv.org/pdf/2506.09833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Sherif, Ali Hamdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09833">Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective rehabilitation assessment is essential for monitoring patient progress, particularly in home-based settings. Existing systems often face challenges such as data imbalance and difficulty detecting subtle movement errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method that generates synthetic skeleton data by simulating clinically relevant movement mistakes. Unlike standard augmentation techniques, EGPA targets biomechanical errors observed in rehabilitation. Combined with an attention-based graph convolutional network, EGPA improves performance across multiple evaluation metrics. Experiments demonstrate reductions in mean absolute error of up to 27.6 percent and gains in error classification accuracy of 45.8 percent. Attention visualizations show that the model learns to focus on clinically significant joints and movement phases, enhancing both accuracy and interpretability. EGPA offers a promising approach for improving automated movement quality assessment in both clinical and home-based rehabilitation contexts.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2506.02082.pdf' target='_blank'>https://arxiv.org/pdf/2506.02082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02082">SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU).
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2505.16485.pdf' target='_blank'>https://arxiv.org/pdf/2505.16485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Wei, Muhammad Usman, Hazrat Bilal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16485">InspectionV3: Enhancing Tobacco Quality Assessment with Deep Convolutional Neural Networks for Automated Workshop Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problems that tobacco workshops encounter include poor curing, inconsistencies in supplies, irregular scheduling, and a lack of oversight, all of which drive up expenses and worse quality. Large quantities make manual examination costly, sluggish, and unreliable. Deep convolutional neural networks have recently made strides in capabilities that transcend those of conventional methods. To effectively enhance them, nevertheless, extensive customization is needed to account for subtle variations in tobacco grade. This study introduces InspectionV3, an integrated solution for automated flue-cured tobacco grading that makes use of a customized deep convolutional neural network architecture. A scope that covers color, maturity, and curing subtleties is established via a labelled dataset consisting of 21,113 images spanning 20 quality classes. Expert annotators performed preprocessing on the tobacco leaf images, including cleaning, labelling, and augmentation. Multi-layer CNN factors use batch normalization to describe domain properties like as permeability and moisture spots, and so account for the subtleties of the workshop. Its expertise lies in converting visual patterns into useful information for enhancing workflow. Fast notifications are made possible by real-time, on-the-spot grading that matches human expertise. Images-powered analytics dashboards facilitate the tracking of yield projections, inventories, bottlenecks, and the optimization of data-driven choices. More labelled images are assimilated after further retraining, improving representational capacities and enabling adaptations for seasonal variability. Metrics demonstrate 97% accuracy, 95% precision and recall, 96% F1-score and AUC, 95% specificity; validating real-world viability.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2505.13875.pdf' target='_blank'>https://arxiv.org/pdf/2505.13875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanlan Kang, Jian Wang, Jian QIn, Yiqin Liang, Yongjun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13875">Automated Quality Evaluation of Cervical Cytopathology Whole Slide Images Based on Content Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ThinPrep Cytologic Test (TCT) is the most widely used method for cervical cancer screening, and the sample quality directly impacts the accuracy of the diagnosis. Traditional manual evaluation methods rely on the observation of pathologist under microscopes. These methods exhibit high subjectivity, high cost, long duration, and low reliability. With the development of computer-aided diagnosis (CAD), an automated quality assessment system that performs at the level of a professional pathologist is necessary. To address this need, we propose a fully automated quality assessment method for Cervical Cytopathology Whole Slide Images (WSIs) based on The Bethesda System (TBS) diagnostic standards, artificial intelligence algorithms, and the characteristics of clinical data. The method analysis the context of WSIs to quantify quality evaluation metrics which are focused by TBS such as staining quality, cell counts and cell mass proportion through multiple models including object detection, classification and segmentation. Subsequently, the XGBoost model is used to mine the attention paid by pathologists to different quality evaluation metrics when evaluating samples, thereby obtaining a comprehensive WSI sample score calculation model. Experimental results on 100 WSIs demonstrate that the proposed evaluation method has significant advantages in terms of speed and consistency.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2505.06692.pdf' target='_blank'>https://arxiv.org/pdf/2505.06692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Pastrello, Diego Cecchin, Gabriele Santin, Francesco Marchetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06692">Tuning Butterworth filter's parameters in SPECT reconstructions via kernel-based Bayesian optimization with a no-reference image evaluation metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Single Photon Emission Computed Tomography (SPECT), the image reconstruction process involves many tunable parameters that have a significant impact on the quality of the resulting clinical images. Traditional image quality evaluation often relies on expert judgment and full-reference metrics such as MSE and SSIM. However, these approaches are limited by their subjectivity or the need for a ground-truth image. In this paper, we investigate the usage of a no-reference image quality assessment method tailored for SPECT imaging, employing the Perception-based Image QUality Evaluator (PIQUE) score. Precisely, we propose a novel application of PIQUE in evaluating SPECT images reconstructed via filtered backprojection using a parameter-dependent Butterworth filter. For the optimization of filter's parameters, we adopt a kernel-based Bayesian optimization framework grounded in reproducing kernel Hilbert space theory, highlighting the connections to recent greedy approximation techniques. Experimental results in a concrete clinical setting for SPECT imaging show the potential of this optimization approach for an objective and quantitative assessment of image quality, without requiring a reference image.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2504.21605.pdf' target='_blank'>https://arxiv.org/pdf/2504.21605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Gwozdz, Andreas Both
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21605">RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2504.09527.pdf' target='_blank'>https://arxiv.org/pdf/2504.09527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Guo, Bo Tang, Jiayuan Xu, Qingyi Li, Yuyuan Qin, Xinghua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09527">A Secure Communication Protocol for Remote Keyless Entry System with Adaptive Adjustment of Transmission Parameters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Keyless Entry (RKE) systems have become a standard feature in modern vehicles, yet their unidirectional fixed-frequency radio communication renders them vulnerable to replay attacks, impersonation attacks, cryptanalysis, and intentional interference. Existing cryptographic authentication methods enhance security but often fail to address real-world constraints such as computational efficiency and radio interference. To mitigate these threats, we designed the Adaptive Frequency-Hopping Algorithm and the Adaptive TXP and PHY Mode Control Algorithm that can dynamically optimize channel selection, transmission power, and PHY modes based on real-time channel quality assessment. To enhance the security and reliability of RKE systems, we propose the Lightweight Vehicle-Key Authentication Protocol. In addition, a prototype of the proposed scheme was implemented to verify its effectiveness in mitigating interference and preventing unauthorized access.Experimental results show that our scheme significantly enhances communication security and reliability while maintaining low computational overhead. Under mild interference conditions, the packet delivery rate (PDR) of the adaptive scheme increases from 93% to 99.23%, and under strong interference, it improves from 85% to 99.01%. Additionally, the scheme effectively prevents replay and impersonation attacks, ensuring secure vehicle access control by dynamically optimizing communication parameters to maintain stable and reliable transmission.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2504.00023.pdf' target='_blank'>https://arxiv.org/pdf/2504.00023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Rottmayer, Claudia Redenbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00023">A Novel Distance-Based Metric for Quality Assessment in Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The assessment of segmentation quality plays a fundamental role in the development, optimization, and comparison of segmentation methods which are used in a wide range of applications. With few exceptions, quality assessment is performed using traditional metrics, which are based on counting the number of erroneous pixels but do not capture the spatial distribution of errors. Established distance-based metrics such as the average Hausdorff distance are difficult to interpret and compare for different methods and datasets. In this paper, we introduce the Surface Consistency Coefficient (SCC), a novel distance-based quality metric that quantifies the spatial distribution of errors based on their proximity to the surface of the structure. Through a rigorous analysis using synthetic data and real segmentation results, we demonstrate the robustness and effectiveness of SCC in distinguishing errors near the surface from those further away. At the same time, SCC is easy to interpret and comparable across different structural contexts.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2503.18799.pdf' target='_blank'>https://arxiv.org/pdf/2503.18799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vivek Vekariya, Mojdeh Golagha, Andrea Stocco, Alexander Pretschner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18799">Latent Space Class Dispersion: Effective Test Data Quality Assessment for DNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality test datasets are crucial for assessing the reliability of Deep Neural Networks (DNNs). Mutation testing evaluates test dataset quality based on their ability to uncover injected faults in DNNs as measured by mutation score (MS). At the same time, its high computational cost motivates researchers to seek alternative test adequacy criteria. We propose Latent Space Class Dispersion (LSCD), a novel metric to quantify the quality of test datasets for DNNs. It measures the degree of dispersion within a test dataset as observed in the latent space of a DNN. Our empirical study shows that LSCD reveals and quantifies deficiencies in the test dataset of three popular benchmarks pertaining to image classification tasks using DNNs. Corner cases generated using automated fuzzing were found to help enhance fault detection and improve the overall quality of the original test sets calculated by MS and LSCD. Our experiments revealed a high positive correlation (0.87) between LSCD and MS, significantly higher than the one achieved by the well-studied Distance-based Surprise Coverage (0.25). These results were obtained from 129 mutants generated through pre-training mutation operators, with statistical significance and a high validity of corner cases. These observations suggest that LSCD can serve as a cost-effective alternative to expensive mutation testing, eliminating the need to generate mutant models while offering comparably valuable insights into test dataset quality for DNNs.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2503.16957.pdf' target='_blank'>https://arxiv.org/pdf/2503.16957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Risha, Mohamed Elsaadany, Paul Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16957">Uncertainty-Driven Modeling of Microporosity and Permeability in Clastic Reservoirs Using Random Forest</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting microporosity and permeability in clastic reservoirs is a challenge in reservoir quality assessment, especially in formations where direct measurements are difficult or expensive. These reservoir properties are fundamental in determining a reservoir's capacity for fluid storage and transmission, yet conventional methods for evaluating them, such as Mercury Injection Capillary Pressure (MICP) and Scanning Electron Microscopy (SEM), are resource-intensive. The aim of this study is to develop a cost-effective machine learning model to predict complex reservoir properties using readily available field data and basic laboratory analyses. A Random Forest classifier was employed, utilizing key geological parameters such as porosity, grain size distribution, and spectral gamma-ray (SGR) measurements. An uncertainty analysis was applied to account for natural variability, expanding the dataset, and enhancing the model's robustness. The model achieved a high level of accuracy in predicting microporosity (93%) and permeability levels (88%). By using easily obtainable data, this model reduces the reliance on expensive laboratory methods, making it a valuable tool for early-stage exploration, especially in remote or offshore environments. The integration of machine learning with uncertainty analysis provides a reliable and cost-effective approach for evaluating key reservoir properties in siliciclastic formations. This model offers a practical solution to improve reservoir quality assessments, enabling more informed decision-making and optimizing exploration efforts.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2503.02897.pdf' target='_blank'>https://arxiv.org/pdf/2503.02897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Lu, Yali Bian, Rahul C. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02897">ClipGrader: Leveraging Vision-Language Models for Robust Label Quality Assessment in Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality annotations are essential for object detection models, but ensuring label accuracy - especially for bounding boxes - remains both challenging and costly. This paper introduces ClipGrader, a novel approach that leverages vision-language models to automatically assess the accuracy of bounding box annotations. By adapting CLIP (Contrastive Language-Image Pre-training) to evaluate both class label correctness and spatial precision of bounding box, ClipGrader offers an effective solution for grading object detection labels. Tested on modified object detection datasets with artificially disturbed bounding boxes, ClipGrader achieves 91% accuracy on COCO with a 1.8% false positive rate. Moreover, it maintains 87% accuracy with a 2.1% false positive rate when trained on just 10% of the COCO data. ClipGrader also scales effectively to larger datasets such as LVIS, achieving 79% accuracy across 1,203 classes. Our experiments demonstrate ClipGrader's ability to identify errors in existing COCO annotations, highlighting its potential for dataset refinement. When integrated into a semi-supervised object detection (SSOD) model, ClipGrader readily improves the pseudo label quality, helping achieve higher mAP (mean Average Precision) throughout the training process. ClipGrader thus provides a scalable AI-assisted tool for enhancing annotation quality control and verifying annotations in large-scale object detection datasets.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2503.00503.pdf' target='_blank'>https://arxiv.org/pdf/2503.00503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Giannitrapani, Elio D. Di Claudio, Giovanni Jacovitti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00503">BELE: Blur Equivalent Linearized Estimator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Full-Reference Image Quality Assessment context, Mean Opinion Score values represent subjective evaluations based on retinal perception, while objective metrics assess the reproduced image on the display. Bridging these subjective and objective domains requires parametric mapping functions, which are sensitive to the observer's viewing distance. This paper introduces a novel parametric model that separates perceptual effects due to strong edge degradations from those caused by texture distortions. These effects are quantified using two distinct quality indices. The first is the Blur Equivalent Linearized Estimator, designed to measure blur on strong and isolated edges while accounting for variations in viewing distance. The second is a Complex Peak Signal-to-Noise Ratio, which evaluates distortions affecting texture regions. The first-order effects of the estimator are directly tied to the first index, for which we introduce the concept of \emph{focalization}, interpreted as a linearization term. Starting from a Positional Fisher Information loss model applied to Gaussian blur distortion in natural images, we demonstrate how this model can generalize to linearize all types of distortions. Finally, we validate our theoretical findings by comparing them with several state-of-the-art classical and deep-learning-based full-reference image quality assessment methods on widely used benchmark datasets.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2502.17929.pdf' target='_blank'>https://arxiv.org/pdf/2502.17929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonalika Subudhi, Alok Kumar Pati, Sephali Bose, Subhasmita Sahoo, Avipsa Pattanaik, Biswa Mohan Acharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17929">Integrating Boosted learning with Differential Evolution (DE) Optimizer: A Prediction of Groundwater Quality Risk Assessment in Odisha</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Groundwater is eventually undermined by human exercises, such as fast industrialization, urbanization, over-extraction, and contamination from agrarian and urban sources. From among the different contaminants, the presence of heavy metals like cadmium (Cd), chromium (Cr), arsenic (As), and lead (Pb) proves to have serious dangers when present in huge concentrations in groundwater. Long-term usage of these poisonous components may lead to neurological disorders, kidney failure and different sorts of cancer. To address these issues, this study developed a machine learning-based predictive model to evaluate the Groundwater Quality Index (GWQI) and identify the main contaminants which are affecting the water quality. It has been achieved with the help of a hybrid machine learning model i.e. LCBoost Fusion . The model has undergone several processes like data preprocessing, hyperparameter tuning using Differential Evolution (DE) optimization, and evaluation through cross-validation. The LCBoost Fusion model outperforms individual models (CatBoost and LightGBM), by achieving low RMSE (0.6829), MSE (0.5102), MAE (0.3147) and a high R$^2$ score of 0.9809. Feature importance analysis highlights Potassium (K), Fluoride (F) and Total Hardness (TH) as the most influential indicators of groundwater contamination. This research successfully demonstrates the application of machine learning in assessing groundwater quality risks in Odisha. The proposed LCBoost Fusion model offers a reliable and efficient approach for real-time groundwater monitoring and risk mitigation. These findings will help the environmental organizations and the policy makers to map out targeted places for sustainable groundwater management. Future work will focus on using remote sensing data and developing an interactive decision-making system for groundwater quality assessment.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2502.17639.pdf' target='_blank'>https://arxiv.org/pdf/2502.17639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Horst K. Hahn, Matthias S. May, Volker Dicken, Michael Walz, Rainer EÃeling, Bianca Lassen-Schmidt, Robert Rischen, Jens Vogel-Claussen, Konstantin Nikolaou, JÃ¶rg Barkhausen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17639">Requirements for Quality Assurance of AI Models for Early Detection of Lung Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung cancer is the second most common cancer and the leading cause of cancer-related deaths worldwide. Survival largely depends on tumor stage at diagnosis, and early detection with low-dose CT can significantly reduce mortality in high-risk patients. AI can improve the detection, measurement, and characterization of pulmonary nodules while reducing assessment time. However, the training data, functionality, and performance of available AI systems vary considerably, complicating software selection and regulatory evaluation. Manufacturers must specify intended use and provide test statistics, but they can choose their training and test data, limiting standardization and comparability. Under the EU AI Act, consistent quality assurance is required for AI-based nodule detection, measurement, and characterization.
  This position paper proposes systematic quality assurance grounded in a validated reference dataset, including real screening cases plus phantom data to verify volume and growth rate measurements. Regular updates shall reflect demographic shifts and technological advances, ensuring ongoing relevance. Consequently, ongoing AI quality assurance is vital. Regulatory challenges are also adressed. While the MDR and the EU AI Act set baseline requirements, they do not adequately address self-learning algorithms or their updates. A standardized, transparent quality assessment - based on sensitivity, specificity, and volumetric accuracy - enables an objective evaluation of each AI solution's strengths and weaknesses. Establishing clear testing criteria and systematically using updated reference data lay the groundwork for comparable performance metrics, informing tenders, guidelines, and recommendations.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2502.15758.pdf' target='_blank'>https://arxiv.org/pdf/2502.15758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelantonio Castelli, Georgios Christos Chouliaras, Dmitri Goldenberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15758">Maturity Framework for Enhancing Machine Learning Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid integration of Machine Learning (ML) in business applications and processes, it is crucial to ensure the quality, reliability and reproducibility of such systems. We suggest a methodical approach towards ML system quality assessment and introduce a structured Maturity framework for governance of ML. We emphasize the importance of quality in ML and the need for rigorous assessment, driven by issues in ML governance and gaps in existing frameworks. Our primary contribution is a comprehensive open-sourced quality assessment method, validated with empirical evidence, accompanied by a systematic maturity framework tailored to ML systems. Drawing from applied experience at Booking.com, we discuss challenges and lessons learned during large-scale adoption within organizations. The study presents empirical findings, highlighting quality improvement trends and showcasing business outcomes. The maturity framework for ML systems, aims to become a valuable resource to reshape industry standards and enable a structural approach to improve ML maturity in any organization.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2502.11620.pdf' target='_blank'>https://arxiv.org/pdf/2502.11620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arindam Sharma, Cristina David
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11620">Assessing Correctness in LLM-Based Code Generation via Uncertainty Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we explore uncertainty estimation as a proxy for correctness in LLM-generated code. To this end, we adapt two state-of-the-art techniques from natural language generation -- one based on entropy and another on mutual information -- to the domain of code generation. Given the distinct semantic properties of code, we introduce modifications, including a semantic equivalence check based on symbolic execution. Our findings indicate a strong correlation between the uncertainty computed through these techniques and correctness, highlighting the potential of uncertainty estimation for quality assessment. Additionally, we propose a simplified version of the entropy-based method that assumes a uniform distribution over the LLM's responses, demonstrating comparable effectiveness. Using these techniques, we develop an abstention policy that prevents the model from making predictions when uncertainty is high, reducing incorrect outputs to near zero. Our evaluation on the LiveCodeBench shows that our approach significantly outperforms a baseline relying solely on LLM-reported log-probabilities.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2502.08540.pdf' target='_blank'>https://arxiv.org/pdf/2502.08540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengqian Ma, Zhengyi Shi, Zhiqiang Lu, Shenghao Xie, Fei Chao, Yao Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08540">A Survey on Image Quality Assessment: Insights, Analysis, and Future Outlook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) represents a pivotal challenge in image-focused technologies, significantly influencing the advancement trajectory of image processing and computer vision. Recently, IQA has witnessed a notable surge in innovative research efforts, driven by the emergence of novel architectural paradigms and sophisticated computational techniques. This survey delivers an extensive analysis of contemporary IQA methodologies, organized according to their application scenarios, serving as a beneficial reference for both beginners and experienced researchers. We analyze the advantages and limitations of current approaches and suggest potential future research pathways. The survey encompasses both general and specific IQA methodologies, including conventional statistical measures, machine learning techniques, and cutting-edge deep learning models such as convolutional neural networks (CNNs) and Transformer models. The analysis within this survey highlights the necessity for distortion-specific IQA methods tailored to various application scenarios, emphasizing the significance of practicality, interpretability, and ease of implementation in future developments.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2502.05356.pdf' target='_blank'>https://arxiv.org/pdf/2502.05356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Stahl, Hannes Gamper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05356">Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate distillation and pruning methods to reduce model size for non-intrusive speech quality assessment based on self-supervised representations. Our experiments build on XLS-R-SQA, a speech quality assessment model using wav2vec 2.0 XLS-R embeddings. We retrain this model on a large compilation of mean opinion score datasets, encompassing over 100,000 labeled clips. For distillation, using this model as a teacher, we generate pseudo-labels on unlabeled degraded speech signals and train student models of varying sizes. For pruning, we use a data-driven strategy. While data-driven pruning performs better at larger model sizes, distillation on unlabeled data is more effective for smaller model sizes. Distillation can halve the gap between the baseline's correlation with ground-truth MOS labels and that of the XLS-R-based teacher model, while reducing model size by two orders of magnitude compared to the teacher model.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2502.03127.pdf' target='_blank'>https://arxiv.org/pdf/2502.03127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pandu Ranga Reddy Konala, Vimal Kumar, David Bainbridge, Junaid Haseeb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03127">A Framework for Measuring the Quality of Infrastructure-as-Code Scripts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrastructure as Code (IaC) has become integral to modern software development, enabling automated and consistent configuration of computing environments. The rapid proliferation of IaC scripts has highlighted the need for better code quality assessment methods. This paper proposes a new IaC code quality framework specifically showcased for Ansible repositories as a foundation. By analyzing a comprehensive dataset of repositories from Ansible Galaxy, we applied our framework to evaluate code quality across multiple attributes. The analysis of our code quality metrics applied to Ansible Galaxy repositories reveal trends over time indicating improvements in areas such as metadata and error handling, while highlighting declines in others such as sophistication and automation. The framework offers practitioners a systematic tool for assessing and enhancing IaC scripts, fostering standardization and facilitating continuous improvement. It also provides a standardized foundation for further work into IaC code quality.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2501.19323.pdf' target='_blank'>https://arxiv.org/pdf/2501.19323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debora Firmino de Souza, Sonia Sousa, Kadri Kristjuhan-Ling, Olga Dunajeva, Mare Roosileht, Avar Pentel, Mati MÃµttus, Mustafa Can Ãzdemir, Å½anna GratÅ¡jova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19323">Trust and Trustworthiness from Human-Centered Perspective in HRI -- A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Industry 5.0 transition highlights EU efforts to design intelligent devices that can work alongside humans to enhance human capabilities, and such vision aligns with user preferences and needs to feel safe while collaborating with such systems take priority. This demands a human-centric research vision and requires a societal and educational shift in how we perceive technological advancements. To better understand this perspective, we conducted a systematic literature review focusing on understanding how trust and trustworthiness can be key aspects of supporting this move towards Industry 5.0. This review aims to overview the most common methodologies and measurements and collect insights about barriers and facilitators for fostering trustworthy HRI. After a rigorous quality assessment following the Systematic Reviews and Meta-Analyses guidelines, using rigorous inclusion criteria and screening by at least two reviewers, 34 articles were included in the review. The findings underscores the significance of trust and safety as foundational elements for promoting secure and trustworthy human-machine cooperation. Confirm that almost 30% of the revised articles do not present a definition of trust, which can be problematic as this lack of conceptual clarity can undermine research efforts in addressing this problem from a central perspective. It highlights that the choice of domain and area of application should influence the choice of methods and approaches to fostering trust in HRI, as those choices can significantly affect user preferences and their perceptions and assessment of robot capabilities. Additionally, this lack of conceptual clarity can be a potential barrier to fostering trust in HRI and explains the sometimes contradictory findings or choice of methods and instruments used to investigate trust in robots and other autonomous systems in the literature.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2501.08809.pdf' target='_blank'>https://arxiv.org/pdf/2501.08809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sida Tian, Can Zhang, Wei Yuan, Wei Tan, Wenjie Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08809">XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2501.01067.pdf' target='_blank'>https://arxiv.org/pdf/2501.01067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Safarzadeh, Mohammad Reza Jamali, Behzad Moshiri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01067">Enhancing Precision of Automated Teller Machines Network Quality Assessment: Machine Learning and Multi Classifier Fusion Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring reliable ATM services is essential for modern banking, directly impacting customer satisfaction and the operational efficiency of financial institutions. This study introduces a data fusion approach that utilizes multi-classifier fusion techniques, with a special focus on the Stacking Classifier, to enhance the reliability of ATM networks. To address class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was applied, enabling balanced learning for both frequent and rare events. The proposed framework integrates diverse classification models - Random Forest, LightGBM, and CatBoost - within a Stacking Classifier, achieving a dramatic reduction in false alarms from 3.56 percent to just 0.71 percent, along with an outstanding overall accuracy of 99.29 percent. This multi-classifier fusion method synthesizes the strengths of individual models, leading to significant cost savings and improved operational decision-making. By demonstrating the power of machine learning and data fusion in optimizing ATM status detection, this research provides practical and scalable solutions for financial institutions aiming to enhance their ATM network performance and customer satisfaction.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2412.12642.pdf' target='_blank'>https://arxiv.org/pdf/2412.12642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijin Liu, Xiang Zhao, You Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12642">RDPI: A Refine Diffusion Probability Generation Method for Spatiotemporal Data Imputation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatiotemporal data imputation plays a crucial role in various fields such as traffic flow monitoring, air quality assessment, and climate prediction. However, spatiotemporal data collected by sensors often suffer from temporal incompleteness, and the sparse and uneven distribution of sensors leads to missing data in the spatial dimension. Among existing methods, autoregressive approaches are prone to error accumulation, while simple conditional diffusion models fail to adequately capture the spatiotemporal relationships between observed and missing data. To address these issues, we propose a novel two-stage Refined Diffusion Probability Impuation (RDPI) framework based on an initial network and a conditional diffusion model. In the initial stage, deterministic imputation methods are used to generate preliminary estimates of the missing data. In the refinement stage, residuals are treated as the diffusion target, and observed values are innovatively incorporated into the forward process. This results in a conditional diffusion model better suited for spatiotemporal data imputation, bridging the gap between the preliminary estimates and the true values. Experiments on multiple datasets demonstrate that RDPI not only achieves state-of-the-art imputation accuracy but also significantly reduces sampling computational costs.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2412.11981.pdf' target='_blank'>https://arxiv.org/pdf/2412.11981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheikh Junaid Fayaz, Nestor Montiel-Bohorquez, Shashank Bishnoi, Matteo Romano, Manuele Gatti, N. M. Anoop Krishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11981">Industrial-scale Prediction of Cement Clinker Phases using Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnes of CO2 annually, faces critical challenges in quality control and process optimization. While traditional process models for cement manufacturing are confined to steady-state conditions with limited predictive capability for mineralogical phases, modern plants operate under dynamic conditions that demand real-time quality assessment. Here, exploiting a comprehensive two-year operational dataset from an industrial cement plant, we present a machine learning framework that accurately predicts clinker mineralogy from process data. Our model achieves unprecedented prediction accuracy for major clinker phases while requiring minimal input parameters, demonstrating robust performance under varying operating conditions. Through post-hoc explainable algorithms, we interpret the hierarchical relationships between clinker oxides and phase formation, providing insights into the functioning of an otherwise black-box model. This digital twin framework can potentially enable real-time optimization of cement production, thereby providing a route toward reducing material waste and ensuring quality while reducing the associated emissions under real plant conditions. Our approach represents a significant advancement in industrial process control, offering a scalable solution for sustainable cement manufacturing.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2412.05683.pdf' target='_blank'>https://arxiv.org/pdf/2412.05683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Jalil Piran, Nguyen H. Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05683">Enhancing Research Methodology and Academic Publishing: A Structured Framework for Quality and Integrity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following a brief introduction to research, research processes, research types, papers, reviews, and evaluations, this paper presents a structured framework for addressing inconsistencies in research methodology, technical writing, quality assessment, and publication standards across academic disciplines. Using a four-dimensional evaluation model that focuses on 1) technical content, 2) structural coherence, 3) writing precision, and 4) ethical integrity, this framework not only standardizes review and publication processes but also serves as a practical guide for authors in preparing high-quality manuscripts. Each of these four dimensions cannot be compromised for the sake of another. Following that, we discuss the components of a research paper adhering to the four-dimensional evaluation model in detail by providing guidelines and principles. By aligning manuscripts with journal standards, reducing review bias, and enhancing transparency, the framework contributes to more reliable and reproducible research results. Moreover, by strengthening cross-disciplinary credibility, improving publication consistency, and fostering public trust in academic literature, this initiative is expected to positively influence both research quality and scholarly publishing's reputation.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2411.19522.pdf' target='_blank'>https://arxiv.org/pdf/2411.19522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sria Biswas, Balasubramanyam Appina, Priyanka Kokil, Sumohana S Channappayya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19522">Subjective and Objective Quality Assessment Methods of Stereoscopic Videos with Visibility Affecting Distortions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present two major contributions in this work: 1) we create a full HD resolution stereoscopic (S3D) video dataset comprised of 12 reference and 360 distorted videos. The test stimuli are produced by simulating the five levels of fog and haze ambiances on the pristine left and right video sequences. We perform subjective analysis on the created video dataset with 24 viewers and compute Difference Mean Opinion Scores (DMOS) as quality representative of the dataset, 2) an Opinion Unaware (OU) and Distortion Unaware (DU) video quality assessment model is developed for S3D videos. We construct cyclopean frames from the individual views of an S3D video and partition them into nonoverlapping blocks. We analyze the Natural Scene Statistics (NSS) of all patches of pristine and test videos, and empirically model the NSS features with Univariate Generalized Gaussian Distribution (UGGD). We compute UGGD model parameters (Î±, \b{eta}) at multiple spatial scales and multiple orientations of spherical steerable pyramid decomposition and show that the UGGD parameters are distortion discriminable. Further, we perform Multivariate Gaussian (MVG) modeling on the pristine and distorted video feature sets and compute the corresponding mean vectors and covariance matrices of MVG fits. We compute the Bhattacharyya distance measure between mean vectors and covariance matrices to estimate the perceptual deviation of a test video from pristine video set. Finally, we pool both distance measures to estimate the overall quality score of an S3D video. The performance of the proposed objective algorithm is verified on the popular S3D video datasets such as IRCCYN, LFOVIAS3DPh1, LFOVIAS3DPh2 and the proposed VAD stereo dataset. The algorithm delivers consistent performance across all datasets and shows competitive performance against off-the-shelf 2D and 3D image and video quality assessment algorithms.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2411.03114.pdf' target='_blank'>https://arxiv.org/pdf/2411.03114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mads Svanborg Peters, Mads Juul AhlebÃ¦k, Mads Toudal Frandsen, Bjarke JÃ¸rgensen, Christian Hald Jessen, Andreas Krogh Carlsen, Wei-Chih Huang, RenÃ© Lynge Eriksen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03114">Investigating the Applicability of a Snapshot Computed Tomography Imaging Spectrometer for the Prediction of Brix and pH of Grapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a recently developed snapshot hyperspectral imaging (HSI) system based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to determine Brix and pH values in Sheegene 20 table grapes through Partial Least Squares Regression (PLSR) modeling. The performance of the CTIS system is compared with that of a state-of-the-art line scan HSI system by imaging 100 grapes across both platforms. Reference measurements of Brix and pH values are obtained directly using a refractometer and a pH meter, as these parameters are essential for assessing the quality of table and wine grapes. The findings indicate that the spectra captured by the CTIS camera correlate well with the reference measurements, despite the system's narrower spectral range. The CTIS camera's advantages, including its lower cost, portability, and reduced susceptibility to motion errors, highlight its potential for promising in-field applications in grape quality assessment.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2410.22566.pdf' target='_blank'>https://arxiv.org/pdf/2410.22566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharath Narayan Shakya, Parimala Kancharla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22566">Deep Priors for Video Quality Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we designed a completely blind video quality assessment algorithm using the deep video prior. This work mainly explores the utility of deep video prior in estimating the visual quality of the video. In our work, we have used a single distorted video and a reference video pair to learn the deep video prior. At inference time, the learned deep prior is used to restore the original videos from the distorted videos. The ability of learned deep video prior to restore the original video from the distorted video is measured to quantify distortion in the video. Our hypothesis is that the learned deep video prior fails in restoring the highly distorted videos. The restoring ability of deep video prior is proportional to the distortion present in the video. Therefore, we propose to use the distance between the distorted video and the restored video as the perceptual quality of the video. Our algorithm is trained using a single video pair and it does not need any labelled data. We show that our proposed algorithm outperforms the existing unsupervised video quality assessment algorithms in terms of LCC and SROCC on a synthetically distorted video quality assessment dataset.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2410.14692.pdf' target='_blank'>https://arxiv.org/pdf/2410.14692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcelo Valentim Silva, Hannes Herrmann, Valerie Maxville
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14692">Attribute-Based Semantic Type Detection and Data Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reliance on data-driven decision-making across sectors highlights the critical need for high-quality data; despite advancements, data quality issues persist, significantly impacting business strategies and scientific research. Current data quality methods fail to leverage the semantic richness embedded in words inside attribute labels (or column names/headers in tables) across diverse datasets and domains, leaving a crucial gap in comprehensive data quality evaluation. This research addresses this gap by introducing an innovative methodology centered around Attribute-Based Semantic Type Detection and Data Quality Assessment. By leveraging semantic information within attribute labels, combined with rule-based analysis and comprehensive Formats and Abbreviations dictionaries, our approach introduces a practical semantic type classification system comprising approximately 23 types, including numerical non-negative, categorical, ID, names, strings, geographical, temporal, and complex formats like URLs, IP addresses, email, and binary values plus several numerical bounded types, such as age and percentage. A comparative analysis with Sherlock, a state-of-the-art Semantic Type Detection system, shows the advantages of our approach in terms of classification robustness and applicability to data quality assessment tasks. Our research focuses on well-known data quality issues and their corresponding data quality dimension violations, grounding our methodology in a robust academic framework. Detailed analysis of fifty distinct datasets from the UCI Machine Learning Repository showcases our method's proficiency in identifying potential data quality issues. Compared to established tools like YData Profiling, our method exhibits superior accuracy, detecting 81 missing values across 922 attributes where YData identified only one.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2410.13883.pdf' target='_blank'>https://arxiv.org/pdf/2410.13883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirna Al-Shetairy, Hanan Hindy, Dina Khattab, Mostafa M. Aref
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13883">Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, interest in vision-language tasks has grown, especially those involving chart interactions. These tasks are inherently multimodal, requiring models to process chart images, accompanying text, underlying data tables, and often user queries. Traditionally, Chart Understanding (CU) relied on heuristics and rule-based systems. However, recent advancements that have integrated transformer architectures significantly improved performance. This paper reviews prominent research in CU, focusing on State-of-The-Art (SoTA) frameworks that employ transformers within End-to-End (E2E) solutions. Relevant benchmarking datasets and evaluation techniques are analyzed. Additionally, this article identifies key challenges and outlines promising future directions for advancing CU solutions. Following the PRISMA guidelines, a comprehensive literature search is conducted across Google Scholar, focusing on publications from Jan'20 to Jun'24. After rigorous screening and quality assessment, 32 studies are selected for in-depth analysis. The CU tasks are categorized into a three-layered paradigm based on the cognitive task required. Recent advancements in the frameworks addressing various CU tasks are also reviewed. Frameworks are categorized into single-task or multi-task based on the number of tasks solvable by the E2E solution. Within multi-task frameworks, pre-trained and prompt-engineering-based techniques are explored. This review overviews leading architectures, datasets, and pre-training tasks. Despite significant progress, challenges remain in OCR dependency, handling low-resolution images, and enhancing visual reasoning. Future directions include addressing these challenges, developing robust benchmarks, and optimizing model efficiency. Additionally, integrating explainable AI techniques and exploring the balance between real and synthetic data are crucial for advancing CU research.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2410.13357.pdf' target='_blank'>https://arxiv.org/pdf/2410.13357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JosÃ© Giraldo, MartÃ­ Llopart-Font, Alex PeirÃ³-Lilja, Carme Armentano-Oller, Gerard Sant, Baybars KÃ¼lebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13357">Enhancing Crowdsourced Audio for Text-to-Speech Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality audio data is a critical prerequisite for training robust text-to-speech models, which often limits the use of opportunistic or crowdsourced datasets. This paper presents an approach to overcome this limitation by implementing a denoising pipeline on the Catalan subset of Commonvoice, a crowd-sourced corpus known for its inherent noise and variability. The pipeline incorporates an audio enhancement phase followed by a selective filtering strategy. We developed an automatic filtering mechanism leveraging Non-Intrusive Speech Quality Assessment (NISQA) models to identify and retain the highest quality samples post-enhancement. To evaluate the efficacy of this approach, we trained a state of the art diffusion-based TTS model on the processed dataset. The results show a significant improvement, with an increase of 0.4 in the UTMOS Score compared to the baseline dataset without enhancement. This methodology shows promise for expanding the utility of crowdsourced data in TTS applications, particularly for mid to low resource languages like Catalan.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2410.13182.pdf' target='_blank'>https://arxiv.org/pdf/2410.13182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anurag Kumar, Andrew Perrault, Donald S. Williamson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13182">Using RLHF to align speech enhancement approaches to mean-opinion quality scores</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective speech quality measures are typically used to assess speech enhancement algorithms, but it has been shown that they are sub-optimal as learning objectives because they do not always align well with human subjective ratings. This misalignment often results in noticeable distortions and artifacts that cause speech enhancement to be ineffective. To address these issues, we propose a reinforcement learning from human feedback (RLHF) framework to fine-tune an existing speech enhancement approach by optimizing performance using a mean-opinion score (MOS)-based reward model. Our results show that the RLHF-finetuned model has the best performance across different benchmarks for both objective and MOS-based speech quality assessment metrics on the Voicebank+DEMAND dataset. Through ablation studies, we show that both policy gradient loss and supervised MSE loss are important for balanced optimization across the different metrics.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2410.06161.pdf' target='_blank'>https://arxiv.org/pdf/2410.06161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaanathi Sundaresan, Nicola K Dinsdale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06161">Automated quality assessment using appearance-based simulations and hippocampus segmentation on low-field paediatric brain MR images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the structural growth of paediatric brains is a key step in the identification of various neuro-developmental disorders. However, our knowledge is limited by many factors, including the lack of automated image analysis tools, especially in Low and Middle Income Countries from the lack of high field MR images available. Low-field systems are being increasingly explored in these countries, and, therefore, there is a need to develop automated image analysis tools for these images. In this work, as a preliminary step, we consider two tasks: 1) automated quality assurance and 2) hippocampal segmentation, where we compare multiple approaches. For the automated quality assurance task a DenseNet combined with appearance-based transformations for synthesising artefacts produced the best performance, with a weighted accuracy of 82.3%. For the segmentation task, registration of an average atlas performed the best, with a final Dice score of 0.61. Our results show that although the images can provide understanding of large scale pathologies and gross scale anatomical development, there still remain barriers for their use for more granular analyses.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2409.06853.pdf' target='_blank'>https://arxiv.org/pdf/2409.06853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sepehr Kazemi Ranjbar, Emad Fatemizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06853">ExIQA: Explainable Image Quality Assessment Using Distortion Attributes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Image Quality Assessment (BIQA) aims to develop methods that estimate the quality scores of images in the absence of a reference image. In this paper, we approach BIQA from a distortion identification perspective, where our primary goal is to predict distortion types and strengths using Vision-Language Models (VLMs), such as CLIP, due to their extensive knowledge and generalizability. Based on these predicted distortions, we then estimate the quality score of the image. To achieve this, we propose an explainable approach for distortion identification based on attribute learning. Instead of prompting VLMs with the names of distortions, we prompt them with the attributes or effects of distortions and aggregate this information to infer the distortion strength. Additionally, we consider multiple distortions per image, making our method more scalable. To support this, we generate a dataset consisting of 100,000 images for efficient training. Finally, attribute probabilities are retrieved and fed into a regressor to predict the image quality score. The results show that our approach, besides its explainability and transparency, achieves state-of-the-art (SOTA) performance across multiple datasets in both PLCC and SRCC metrics. Moreover, the zero-shot results demonstrate the generalizability of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2408.14010.pdf' target='_blank'>https://arxiv.org/pdf/2408.14010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohin Sood, Kevin Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14010">Improving Water Quality Time-Series Prediction in Hong Kong using Sentinel-2 MSI Data and Google Earth Engine Cloud Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective water quality monitoring in coastal regions is crucial due to the progressive deterioration caused by pollution and human activities. To address this, this study develops time-series models to predict chlorophyll-a (Chl-a), suspended solids (SS), and turbidity using Sentinel-2 satellite data and Google Earth Engine (GEE) in the coastal regions of Hong Kong. Leveraging Long Short-Term Memory (LSTM) Recurrent Neural Networks, the study incorporates extensive temporal datasets to enhance prediction accuracy. The models utilize spectral data from Sentinel-2, focusing on optically active components, and demonstrate that selected variables closely align with the spectral characteristics of Chl-a and SS. The results indicate improved predictive performance over previous methods, highlighting the potential for remote sensing technology in continuous and comprehensive water quality assessment.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2408.11349.pdf' target='_blank'>https://arxiv.org/pdf/2408.11349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chingis Oinar, Miao Cao, Shanshan Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11349">Image Score: Learning and Evaluating Human Preferences for Mercari Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mercari is the largest C2C e-commerce marketplace in Japan, having more than 20 million active monthly users. Search being the fundamental way to discover desired items, we have always had a substantial amount of data with implicit feedback. Although we actively take advantage of that to provide the best service for our users, the correlation of implicit feedback for such tasks as image quality assessment is not trivial. Many traditional lines of research in Machine Learning (ML) are similarly motivated by the insatiable appetite of Deep Learning (DL) models for well-labelled training data. Weak supervision is about leveraging higher-level and/or noisier supervision over unlabeled data. Large Language Models (LLMs) are being actively studied and used for data labelling tasks. We present how we leverage a Chain-of-Thought (CoT) to enable LLM to produce image aesthetics labels that correlate well with human behavior in e-commerce settings. Leveraging LLMs is more cost-effective compared to explicit human judgment, while significantly improving the explainability of deep image quality evaluation which is highly important for customer journey optimization at Mercari. We propose a cost-efficient LLM-driven approach for assessing and predicting image quality in e-commerce settings, which is very convenient for proof-of-concept testing. We show that our LLM-produced labels correlate with user behavior on Mercari. Finally, we show our results from an online experimentation, where we achieved a significant growth in sales on the web platform.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2408.04749.pdf' target='_blank'>https://arxiv.org/pdf/2408.04749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Wyss, Gabriela Morgenshtern, Amanda Hirsch-HÃ¼sler, JÃ¼rgen Bernard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04749">DaedalusData: Exploration, Knowledge Externalization and Labeling of Particles in Medical Manufacturing -- A Design Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In medical diagnostics of both early disease detection and routine patient care, particle-based contamination of in-vitro diagnostics consumables poses a significant threat to patients. Objective data-driven decision-making on the severity of contamination is key for reducing patient risk, while saving time and cost in quality assessment. Our collaborators introduced us to their quality control process, including particle data acquisition through image recognition, feature extraction, and attributes reflecting the production context of particles. Shortcomings in the current process are limitations in exploring thousands of images, data-driven decision making, and ineffective knowledge externalization. Following the design study methodology, our contributions are a characterization of the problem space and requirements, the development and validation of DaedalusData, a comprehensive discussion of our study's learnings, and a generalizable framework for knowledge externalization. DaedalusData is a visual analytics system that enables domain experts to explore particle contamination patterns, label particles in label alphabets, and externalize knowledge through semi-supervised label-informed data projections. The results of our case study and user study show high usability of DaedalusData and its efficient support of experts in generating comprehensive overviews of thousands of particles, labeling of large quantities of particles, and externalizing knowledge to augment the dataset further. Reflecting on our approach, we discuss insights on dataset augmentation via human knowledge externalization, and on the scalability and trade-offs that come with the adoption of this approach in practice.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2408.01105.pdf' target='_blank'>https://arxiv.org/pdf/2408.01105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DÃ­az-MuÃ±oz Ana, Cruz-Lemus JosÃ© A., RodrÃ­guez MoisÃ©s, Piattini Mario, Baldassarre Maria Teresa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01105">Validation of an Analysability Model in Hybrid Quantum Software</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of quantum-classical hybrid computing, evaluating analysability, which is the ease of understanding and modifying software, presents significant challenges due to the complexity and novelty of quantum algorithms. Although advances have been made in quantum software development, standard software quality evaluation methods do not fully address the specifics of quantum components, resulting in a gap in the ability to ensure and maintain the quality of hybrid software products. In this registered report proposal, we intend to validate a quality model focused on the analysability of hybrid software through an international collab orative approach involving academic institutions from Italy and Spain through a controlled experiment. This approach allows for a more detailed analysis and validation methodology and establishes a framework for future research and developments in software quality assessment in quantum computing.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2407.20766.pdf' target='_blank'>https://arxiv.org/pdf/2407.20766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoheng Tan, Jiabin Zhang, Yuhui Quan, Jing Li, Yajing Wu, Zilin Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20766">Highly Efficient No-reference 4K Video Quality Assessment with Full-Pixel Covering Sampling and Training Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Video Quality Assessment (VQA) methods have shown impressive high-performance capabilities. Notably, no-reference (NR) VQA methods play a vital role in situations where obtaining reference videos is restricted or not feasible. Nevertheless, as more streaming videos are being created in ultra-high definition (e.g., 4K) to enrich viewers' experiences, the current deep VQA methods face unacceptable computational costs. Furthermore, the resizing, cropping, and local sampling techniques employed in these methods can compromise the details and content of original 4K videos, thereby negatively impacting quality assessment. In this paper, we propose a highly efficient and novel NR 4K VQA technology. Specifically, first, a novel data sampling and training strategy is proposed to tackle the problem of excessive resolution. This strategy allows the VQA Swin Transformer-based model to effectively train and make inferences using the full data of 4K videos on standard consumer-grade GPUs without compromising content or details. Second, a weighting and scoring scheme is developed to mimic the human subjective perception mode, which is achieved by considering the distinct impact of each sub-region within a 4K frame on the overall perception. Third, we incorporate the frequency domain information of video frames to better capture the details that affect video quality, consequently further improving the model's generalizability. To our knowledge, this is the first technology for the NR 4K VQA task. Thorough empirical studies demonstrate it not only significantly outperforms existing methods on a specialized 4K VQA dataset but also achieves state-of-the-art performance across multiple open-source NR video quality datasets.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2407.18938.pdf' target='_blank'>https://arxiv.org/pdf/2407.18938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shun Ito, Hisashi Kashima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18938">Mitigating Cognitive Biases in Multi-Criteria Crowd Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowdsourcing is an easy, cheap, and fast way to perform large scale quality assessment; however, human judgments are often influenced by cognitive biases, which lowers their credibility. In this study, we focus on cognitive biases associated with a multi-criteria assessment in crowdsourcing; crowdworkers who rate targets with multiple different criteria simultaneously may provide biased responses due to prominence of some criteria or global impressions of the evaluation targets. To identify and mitigate such biases, we first create evaluation datasets using crowdsourcing and investigate the effect of inter-criteria cognitive biases on crowdworker responses. Then, we propose two specific model structures for Bayesian opinion aggregation models that consider inter-criteria relations. Our experiments show that incorporating our proposed structures into the aggregation model is effective to reduce the cognitive biases and help obtain more accurate aggregation results.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2407.11767.pdf' target='_blank'>https://arxiv.org/pdf/2407.11767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Pons-SuÃ±er, Laura Arnal, J. RamÃ³n Navarro-CerdÃ¡n, FranÃ§ois Signol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11767">ITI-IQA: a Toolbox for Heterogeneous Univariate and Multivariate Missing Data Imputation Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Missing values are a major challenge in most data science projects working on real data. To avoid losing valuable information, imputation methods are used to fill in missing values with estimates, allowing the preservation of samples or variables that would otherwise be discarded. However, if the process is not well controlled, imputation can generate spurious values that introduce uncertainty and bias into the learning process. The abundance of univariate and multivariate imputation techniques, along with the complex trade-off between data reliability and preservation, makes it difficult to determine the best course of action to tackle missing values. In this work, we present ITI-IQA (Imputation Quality Assessment), a set of utilities designed to assess the reliability of various imputation methods, select the best imputer for any feature or group of features, and filter out features that do not meet quality criteria. Statistical tests are conducted to evaluate the suitability of every tested imputer, ensuring that no new biases are introduced during the imputation phase. The result is a trainable pipeline of filters and imputation methods that streamlines the process of dealing with missing data, supporting different data types: continuous, discrete, binary, and categorical. The toolbox also includes a suite of diagnosing methods and graphical tools to check measurements and results during and after handling missing data.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2406.14489.pdf' target='_blank'>https://arxiv.org/pdf/2406.14489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahime Yilmaz, Feza Buzluca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14489">A Fuzzy Logic-Based Quality Model For Identifying Microservices With Low Maintainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Microservice Architecture (MSA) is a popular architectural style that offers many advantages regarding quality attributes, including maintainability and scalability. Developing a system as a set of microservices with expected benefits requires a quality assessment strategy that is established on the measurements of the system's properties. This paper proposes a hierarchical quality model based on fuzzy logic to measure and evaluate the maintainability of MSAs considering ISO/IEC 250xy SQuaRE (System and Software Quality Requirements and Evaluation) standards. Since the qualitative bounds of low-level quality attributes are inherently ambiguous, we use a fuzzification technique to transform crisp values of code metrics into fuzzy levels and apply them as inputs to our quality model. The model generates fuzzy values for the quality sub-characteristics of the maintainability, i.e., modifiability and testability, converted to numerical values through defuzzification. In the last step, using the values of the sub-characteristics, we calculate numerical scores indicating the maintainability level of each microservice in the examined software system. This score was used to assess the quality of the microservices and decide whether they need refactoring. We evaluated our approach by creating a test set with the assistance of three developers, who reviewed and categorized the maintainability levels of the microservices in an open-source project based on their knowledge and experience. They labeled microservices as low, medium, or high, with low indicating the need for refactoring. Our method for identifying low-labeled microservices in the given test set achieved 94% accuracy, 78% precision, and 100% recall. These results indicate that our approach can assist designers in evaluating the maintainability quality of microservices.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2406.08582.pdf' target='_blank'>https://arxiv.org/pdf/2406.08582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08582">Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are various methods for adapting LLMs to different domains. The most common methods are prompting, finetuning, and RAG. In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA. The experiment aims to simulate human responses based on their interviews. The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2406.01071.pdf' target='_blank'>https://arxiv.org/pdf/2406.01071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Lippemeier, Stefanie Hittmeyer, Oliver NiehÃ¶rster, Markus Lange-Hegermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01071">Visual Car Brand Classification by Implementing a Synthetic Image Dataset Creation Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning, particularly in deep learning and object detection, have significantly improved performance in various tasks, including image classification and synthesis. However, challenges persist, particularly in acquiring labeled data that accurately represents specific use cases. In this work, we propose an automatic pipeline for generating synthetic image datasets using Stable Diffusion, an image synthesis model capable of producing highly realistic images. We leverage YOLOv8 for automatic bounding box detection and quality assessment of synthesized images. Our contributions include demonstrating the feasibility of training image classifiers solely on synthetic data, automating the image generation pipeline, and describing the computational requirements for our approach. We evaluate the usability of different modes of Stable Diffusion and achieve a classification accuracy of 75%.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2405.04997.pdf' target='_blank'>https://arxiv.org/pdf/2405.04997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kirillov Alexey, Andrey Moskalenko, Dmitriy Vatolin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04997">Bridging the Gap Between Saliency Prediction and Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA). However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks. IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations. On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest. Thus, we believe that saliency plays a crucial role in human perception. In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter. Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods. All supplementary code and data will be available at the time of publication.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2405.00670.pdf' target='_blank'>https://arxiv.org/pdf/2405.00670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Chubarau, Hyunjin Yoo, Tara Akhavan, James Clark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00670">Adapting Pretrained Networks for Image Quality Assessment on High Dynamic Range Displays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional image quality metrics (IQMs), such as PSNR and SSIM, are designed for perceptually uniform gamma-encoded pixel values and cannot be directly applied to perceptually non-uniform linear high-dynamic-range (HDR) colors. Similarly, most of the available datasets consist of standard-dynamic-range (SDR) images collected in standard and possibly uncontrolled viewing conditions. Popular pre-trained neural networks are likewise intended for SDR inputs, restricting their direct application to HDR content. On the other hand, training HDR models from scratch is challenging due to limited available HDR data. In this work, we explore more effective approaches for training deep learning-based models for image quality assessment (IQA) on HDR data. We leverage networks pre-trained on SDR data (source domain) and re-target these models to HDR (target domain) with additional fine-tuning and domain adaptation. We validate our methods on the available HDR IQA datasets, demonstrating that models trained with our combined recipe outperform previous baselines, converge much quicker, and reliably generalize to HDR inputs.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2404.18851.pdf' target='_blank'>https://arxiv.org/pdf/2404.18851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Corrales-Astorgano, David Escudero-Mancebo, Lourdes Aguilar, Valle Flores-Lucas, ValentÃ­n CardeÃ±oso-Payo, Carlos Vivaracho-Pascual, CÃ©sar GonzÃ¡lez-Ferreras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18851">A Comprehensive Rubric for Annotating Pathological Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rubrics are a commonly used tool for labeling voice corpora in speech quality assessment, although their application in the context of pathological speech remains relatively limited. In this study, we introduce a comprehensive rubric based on various dimensions of speech quality, including phonetics, fluency, and prosody. The objective is to establish standardized criteria for identifying errors within the speech of individuals with Down syndrome, thereby enabling the development of automated assessment systems. To achieve this objective, we utilized the Prautocal corpus. To assess the quality of annotations using our rubric, two experiments were conducted, focusing on phonetics and fluency. For phonetic evaluation, we employed the Goodness of Pronunciation (GoP) metric, utilizing automatic segmentation systems and correlating the results with evaluations conducted by a specialized speech therapist. While the obtained correlation values were not notably high, a positive trend was observed. In terms of fluency assessment, deep learning models like wav2vec were used to extract audio features, and we employed an SVM classifier trained on a corpus focused on identifying fluency issues to categorize Prautocal corpus samples. The outcomes highlight the complexities of evaluating such phenomena, with variability depending on the specific type of disfluency detected.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2404.16471.pdf' target='_blank'>https://arxiv.org/pdf/2404.16471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panagiotis Sapoutzoglou, Georgios Giapitzakis, Georgios Floros, George Terzakis, Maria Pateraki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16471">COBRA -- COnfidence score Based on shape Regression Analysis for method-independent quality assessment of object pose estimation from single images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a generic procedure for assessing 6D object pose estimates. Our approach relies on the evaluation of discrepancies in the geometry of the observed object, in particular its respective estimated back-projection in 3D, against a putative functional shape representation comprising mixtures of Gaussian Processes, that act as a template. Each Gaussian Process is trained to yield a fragment of the object's surface in a radial fashion with respect to designated reference points. We further define a pose confidence measure as the average probability of pixel back-projections in the Gaussian mixture. The goal of our experiments is two-fold. a) We demonstrate that our functional representation is sufficiently accurate as a shape template on which the probability of back-projected object points can be evaluated, and, b) we show that the resulting confidence scores based on these probabilities are indeed a consistent quality measure of pose.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2404.13555.pdf' target='_blank'>https://arxiv.org/pdf/2404.13555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmood Saeedi kelishami, Amin Saeidi Kelishami, Sajjad Saeedi Kelishami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13555">Cell Phone Image-Based Persian Rice Detection and Classification Using Deep Learning Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces an innovative approach to classifying various types of Persian rice using image-based deep learning techniques, highlighting the practical application of everyday technology in food categorization. Recognizing the diversity of Persian rice and its culinary significance, we leveraged the capabilities of convolutional neural networks (CNNs), specifically by fine-tuning a ResNet model for accurate identification of different rice varieties and employing a U-Net architecture for precise segmentation of rice grains in bulk images. This dual-methodology framework allows for both individual grain classification and comprehensive analysis of bulk rice samples, addressing two crucial aspects of rice quality assessment. Utilizing images captured with consumer-grade cell phones reflects a realistic scenario in which individuals can leverage this technology for assistance with grocery shopping and meal preparation. The dataset, comprising various rice types photographed under natural conditions without professional lighting or equipment, presents a challenging yet practical classification problem. Our findings demonstrate the feasibility of using non-professional images for food classification and the potential of deep learning models, like ResNet and U-Net, to adapt to the nuances of everyday objects and textures. This study contributes to the field by providing insights into the applicability of image-based deep learning in daily life, specifically for enhancing consumer experiences and knowledge in food selection. Furthermore, it opens avenues for extending this approach to other food categories and practical applications, emphasizing the role of accessible technology in bridging the gap between sophisticated computational methods and everyday tasks.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2404.10454.pdf' target='_blank'>https://arxiv.org/pdf/2404.10454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meriam Zribi, Paolo Pagliuca, Francesca Pitolli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10454">A Computer Vision-Based Quality Assessment Technique for the automatic control of consumables for analytical laboratories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of the Industry 4.0 paradigm is increasing the pressure to develop effective automated monitoring systems. Artificial Intelligence (AI) is a convenient tool to improve the efficiency of industrial processes while reducing errors and waste. In fact, it allows the use of real-time data to increase the effectiveness of monitoring systems, minimize errors, make the production process more sustainable, and save costs. In this paper, a novel automatic monitoring system is proposed in the context of production process of plastic consumables used in analysis laboratories, with the aim to increase the effectiveness of the control process currently performed by a human operator. In particular, we considered the problem of classifying the presence or absence of a transparent anticoagulant substance inside test tubes. Specifically, a hand-designed deep network model is used and compared with some state-of-the-art models for its ability to categorize different images of vials that can be either filled with the anticoagulant or empty. Collected results indicate that the proposed approach is competitive with state-of-the-art models in terms of accuracy. Furthermore, we increased the complexity of the task by training the models on the ability to discriminate not only the presence or absence of the anticoagulant inside the vial, but also the size of the test tube. The analysis performed in the latter scenario confirms the competitiveness of our approach. Moreover, our model is remarkably superior in terms of its generalization ability and requires significantly fewer resources. These results suggest the possibility of successfully implementing such a model in the production process of a plastic consumables company.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2404.05764.pdf' target='_blank'>https://arxiv.org/pdf/2404.05764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anantha Prabhu, David Pratap, Narayana Darapeni, Anwesh P R
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05764">Study of the effect of Sharpness on Blind Video Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduction: Video Quality Assessment (VQA) is one of the important areas of study in this modern era, where video is a crucial component of communication with applications in every field. Rapid technology developments in mobile technology enabled anyone to create videos resulting in a varied range of video quality scenarios. Objectives: Though VQA was present for some time with the classical metrices like SSIM and PSNR, the advent of machine learning has brought in new techniques of VQAs which are built upon Convolutional Neural Networks (CNNs) or Deep Neural Networks (DNNs). Methods: Over the past years various research studies such as the BVQA which performed video quality assessment of nature-based videos using DNNs exposed the powerful capabilities of machine learning algorithms. BVQA using DNNs explored human visual system effects such as content dependency and time-related factors normally known as temporal effects. Results: This study explores the sharpness effect on models like BVQA. Sharpness is the measure of the clarity and details of the video image. Sharpness typically involves analyzing the edges and contrast of the image to determine the overall level of detail and sharpness. Conclusion: This study uses the existing video quality databases such as CVD2014. A comparative study of the various machine learning parameters such as SRCC and PLCC during the training and testing are presented along with the conclusion.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2403.18622.pdf' target='_blank'>https://arxiv.org/pdf/2403.18622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankur Nahar, Koustav Kumar Mondal, Debasis Das, Rajkumar Buyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18622">qIoV: A Quantum-Driven Internet-of-Vehicles-Based Approach for Environmental Monitoring and Rapid Response Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research addresses the critical necessity for advanced rapid response operations in managing a spectrum of environmental hazards. We propose a novel framework, qIoV that integrates quantum computing with the Internet-of-Vehicles (IoV) to leverage the computational efficiency, parallelism, and entanglement properties of quantum mechanics. Our approach involves the use of environmental sensors mounted on vehicles for precise air quality assessment. These sensors are designed to be highly sensitive and accurate, leveraging the principles of quantum mechanics to detect and measure environmental parameters. A salient feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system designed to dynamically adjust the quantum network topology in accordance with vehicular movements. This capability is critical to maintaining the integrity of quantum states against environmental and vehicular disturbances, thereby ensuring reliable data transmission and processing. Moreover, our methodology is further augmented by the incorporation of a variational quantum classifier (VQC) with advanced quantum entanglement techniques. This integration offers a significant reduction in latency for hazard alert transmission, thus enabling expedited communication of crucial data to emergency response teams and the public. Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system, revealed significant advancements in pair plot analysis, achieving over 90% in precision, recall, and F1-Score metrics and an 83% increase in the speed of toxic gas detection compared to conventional methods.Additionally, theoretical analyses validate the efficiency of quantum rotation, teleportation protocols, and the fidelity of quantum entanglement, further underscoring the potential of quantum computing in enhancing analytical performance.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2402.12043.pdf' target='_blank'>https://arxiv.org/pdf/2402.12043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qunyue Huang, Bin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12043">A Lightweight Parallel Framework for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer. In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2402.07118.pdf' target='_blank'>https://arxiv.org/pdf/2402.07118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Srikanth, Jayang Gurung, N Satya Deepika, Vineet Joshi, Lopamudra Giri, Pravin Vaddavalli, Soumya Jana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07118">Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2402.05295.pdf' target='_blank'>https://arxiv.org/pdf/2402.05295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alaiz-Rodriguez, R., Parnell, A. C
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05295">An information theoretic approach to quantify the stability of feature selection and ranking algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the disagreements that appear at the top of the list. Moreover, it possesses desirable properties including correction for change, upper lower bounds and conditions for a deterministic selection. We illustrate the use of this stability metric with data generated in a fully controlled way and compare it with popular metrics including the Spearmans rank correlation and the Kunchevas index on feature ranking and selection outcomes, respectively. Additionally, experimental validation of the proposed approach is carried out on a real-world problem of food quality assessment showing its potential to quantify stability from different perspectives.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2401.13956.pdf' target='_blank'>https://arxiv.org/pdf/2401.13956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanchao Ma, Yanlin Jiang, Hongyan Liu, Chengxu Zhou, Ke Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13956">A New Image Quality Database for Multiple Industrial Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a broader range of applications of image processing technologies in multiple industrial processes, such as smoke detection, security monitoring, and workpiece inspection. Different kinds of distortion types and levels must be introduced into an image during the processes of acquisition, compression, transmission, storage, and display, which might heavily degrade the image quality and thus strongly reduce the final display effect and clarity. To verify the reliability of existing image quality assessment methods, we establish a new industrial process image database (IPID), which contains 3000 distorted images generated by applying different levels of distortion types to each of the 50 source images. We conduct the subjective test on the aforementioned 3000 images to collect their subjective quality ratings in a well-suited laboratory environment. Finally, we perform comparison experiments on IPID database to investigate the performance of some objective image quality assessment algorithms. The experimental results show that the state-of-the-art image quality assessment methods have difficulty in predicting the quality of images that contain multiple distortion types.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2312.15239.pdf' target='_blank'>https://arxiv.org/pdf/2312.15239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Therdpong Daengsi, Pongpisit Wuttidittachotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15239">QoE modeling for Voice over IP: Simplified E-model Enhancement Utilizing the Subjective MOS Prediction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research proposes an enhanced measurement method for VoIP quality assessment which provides an improvement to accuracy and reliability. To improve the objective measurement tool called the simplified E-model for the selected codec, G.729, it has been enhanced by utilizing a subjective MOS prediction model based on native Thai users, who use the Thai-tonal language. Then, the different results from the simplified E-model and subjective MOS prediction model were used to create the Bias function, before adding to the simplified E-model. Finally, it has been found that the outputs from the enhanced simplified E-model for the G.729 codec shows better accuracy when compared to the original simplified E-model, specially, after the enhanced model has been evaluated with 4 test sets. The major contribution of this enhancement is that errors are reduced by 58.87 % when compared to the generic simplified E-model. That means the enhanced simplified E-model as proposed in this study can provide improvement beyond the original simplified one significantly.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2312.09799.pdf' target='_blank'>https://arxiv.org/pdf/2312.09799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Han Sun, Chiang Lo-Hsuan Lee, Tian-Sheuan Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09799">IQNet: Image Quality Assessment Guided Just Noticeable Difference Prefiltering For Versatile Video Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image prefiltering with just noticeable distortion (JND) improves coding efficiency in a visual lossless way by filtering the perceptually redundant information prior to compression. However, real JND cannot be well modeled with inaccurate masking equations in traditional approaches or image-level subject tests in deep learning approaches. Thus, this paper proposes a fine-grained JND prefiltering dataset guided by image quality assessment for accurate block-level JND modeling. The dataset is constructed from decoded images to include coding effects and is also perceptually enhanced with block overlap and edge preservation. Furthermore, based on this dataset, we propose a lightweight JND prefiltering network, IQNet, which can be applied directly to different quantization cases with the same model and only needs 3K parameters. The experimental results show that the proposed approach to Versatile Video Coding could yield maximum/average bitrate savings of 41\%/15\% and 53\%/19\% for all-intra and low-delay P configurations, respectively, with negligible subjective quality loss. Our method demonstrates higher perceptual quality and a model size that is an order of magnitude smaller than previous deep learning methods.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2312.09680.pdf' target='_blank'>https://arxiv.org/pdf/2312.09680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Q. Mazouni, H. Spieker, A. Gotlieb, M. Acher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09680">A Review of Validation and Verification of Neural Network-based Policies for Sequential Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In sequential decision making, neural networks (NNs) are nowadays commonly used to represent and learn the agent's policy. This area of application has implied new software quality assessment challenges that traditional validation and verification practises are not able to handle. Subsequently, novel approaches have emerged to adapt those techniques to NN-based policies for sequential decision making. This survey paper aims at summarising these novel contributions and proposing future research directions. We conducted a literature review of recent research papers (from 2018 to beginning of 2023), whose topics cover aspects of the test or verification of NN-based policies. The selection has been enriched by a snowballing process from the previously selected papers, in order to relax the scope of the study and provide the reader with insight into similar verification challenges and their recent solutions. 18 papers have been finally selected. Our results show evidence of increasing interest for this subject. They highlight the diversity of both the exact problems considered and the techniques used to tackle them.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2311.10617.pdf' target='_blank'>https://arxiv.org/pdf/2311.10617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Parisot, Pierrick Bruneau, Patrik Hitzelberger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10617">Astronomical Images Quality Assessment with Automated Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electronically Assisted Astronomy consists in capturing deep sky images with a digital camera coupled to a telescope to display views of celestial objects that would have been invisible through direct observation. This practice generates a large quantity of data, which may then be enhanced with dedicated image editing software after observation sessions. In this study, we show how Image Quality Assessment can be useful for automatically rating astronomical images, and we also develop a dedicated model by using Automated Machine Learning.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2311.01241.pdf' target='_blank'>https://arxiv.org/pdf/2311.01241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Ribeiro, Andreas Uhl, Fernando Alonso-Fernandez, Reuben A. Farrugia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01241">Exploring Deep Learning Image Super-Resolution for Iris Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we test the ability of deep learning methods to provide an end-to-end mapping between low and high resolution images applying it to the iris recognition problem. Here, we propose the use of two deep learning single-image super-resolution approaches: Stacked Auto-Encoders (SAE) and Convolutional Neural Networks (CNN) with the most possible lightweight structure to achieve fast speed, preserve local information and reduce artifacts at the same time. We validate the methods with a database of 1.872 near-infrared iris images with quality assessment and recognition experiments showing the superiority of deep learning approaches over the compared algorithms.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2310.19687.pdf' target='_blank'>https://arxiv.org/pdf/2310.19687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura E. M. Ayravainen, Joanne Hinds, Brittany I. Davidson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19687">Sentiment Analysis in Digital Spaces: An Overview of Reviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sentiment analysis (SA) is commonly applied to digital textual data, revealing insight into opinions and feelings. Many systematic reviews have summarized existing work, but often overlook discussions of validity and scientific practices. Here, we present an overview of reviews, synthesizing 38 systematic reviews, containing 2,275 primary studies. We devise a bespoke quality assessment framework designed to assess the rigor and quality of systematic review methodologies and reporting standards. Our findings show diverse applications and methods, limited reporting rigor, and challenges over time. We discuss how future research and practitioners can address these issues and highlight their importance across numerous applications.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2310.09388.pdf' target='_blank'>https://arxiv.org/pdf/2310.09388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranay Manocha, Donald Williamson, Adam Finkelstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09388">CORN: Co-Trained Full- And No-Reference Speech Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceptual evaluation constitutes a crucial aspect of various audio-processing tasks. Full reference (FR) or similarity-based metrics rely on high-quality reference recordings, to which lower-quality or corrupted versions of the recording may be compared for evaluation. In contrast, no-reference (NR) metrics evaluate a recording without relying on a reference. Both the FR and NR approaches exhibit advantages and drawbacks relative to each other. In this paper, we present a novel framework called CORN that amalgamates these dual approaches, concurrently training both FR and NR models together. After training, the models can be applied independently. We evaluate CORN by predicting several common objective metrics and across two different architectures. The NR model trained using CORN has access to a reference recording during training, and thus, as one would expect, it consistently outperforms baseline NR models trained independently. Perhaps even more remarkable is that the CORN FR model also outperforms its baseline counterpart, even though it relies on the same training data and the same model architecture. Thus, a single training regime produces two independently useful models, each outperforming independently trained models
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2310.02235.pdf' target='_blank'>https://arxiv.org/pdf/2310.02235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Miguel MoÃ¡s, Carla Teixeira Lopes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02235">Automatic Quality Assessment of Wikipedia Articles -- A Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wikipedia is the world's largest online encyclopedia, but maintaining article quality through collaboration is challenging. Wikipedia designed a quality scale, but with such a manual assessment process, many articles remain unassessed. We review existing methods for automatically measuring the quality of Wikipedia articles, identifying and comparing machine learning algorithms, article features, quality metrics, and used datasets, examining 149 distinct studies, and exploring commonalities and gaps in them. The literature is extensive, and the approaches follow past technological trends. However, machine learning is still not widely used by Wikipedia, and we hope that our analysis helps future researchers change that reality.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2309.12362.pdf' target='_blank'>https://arxiv.org/pdf/2309.12362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kashif Ishaq, Atif Alvi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12362">Personalization, Cognition, and Gamification-based Programming Language Learning: A State-of-the-Art Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Programming courses in computing science are important because they are often the first introduction to computer programming for many students. Many university students are overwhelmed with the information they must learn for an introductory course. The current teacher-lecturer model of learning commonly employed in university lecture halls often results in a lack of motivation and participation in learning. Personalized gamification is a pedagogical approach that combines gamification and personalized learning to motivate and engage students while addressing individual differences in learning. This approach integrates gamification and personalized learning strategies to inspire and involve students while addressing their unique learning needs and differences. A comprehensive literature search was conducted by including 81 studies that were analyzed based on their research design, intervention, outcome measures, and quality assessment. The findings suggest that personalized gamification can enhance student cognition in programming courses by improving motivation, engagement, and learning outcomes. However, the effectiveness of personalized gamification varies depending on various factors, such as the type of gamification elements used, the degree of personalization, and the characteristics of the learners. This paper provides insights into designing and implementing effective personalized gamification interventions in programming courses. The findings could inform educational practitioners and researchers in programming education about the potential benefits of personalized gamification and its implications for educational practice.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2309.10424.pdf' target='_blank'>https://arxiv.org/pdf/2309.10424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan M. GarcÃ­a-GÃ³mez, Vicent Blanes-Selva, JosÃ© Carlos de BartolomÃ© Cenzano, Jaime Cebolla-Cornejo, AscensiÃ³n DoÃ±ate-MartÃ­nez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10424">Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Directorate General for Parliamentary Research Services of the European Parliament has prepared a report to the Members of the European Parliament where they enumerate seven main risks of Artificial Intelligence (AI) in medicine and healthcare: patient harm due to AI errors, misuse of medical AI tools, bias in AI and the perpetuation of existing inequities, lack of transparency, privacy and security issues, gaps in accountability, and obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems may implement to reduce the risks associated with their medical purpose: AI passport, User management, Regulation check, Academic use only disclaimer, data quality assessment, Clinicians double check, Continuous performance evaluation, Audit trail, Continuous usability test, Review of retrospective/simulated cases, Bias check, eXplainable AI, Encryption and use of field-tested libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of technical solutions to ensure continuous good performance and use of AI systems to benefit patients in compliance with the future EU regulatory framework.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2309.07156.pdf' target='_blank'>https://arxiv.org/pdf/2309.07156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivam Sharma, Suvadeep Maiti, S. Mythirayee, Srijithesh Rajendran, Raju Surampudi Bapi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07156">Transparency in Sleep Staging: Deep Learning Method for EEG Sleep Stage Classification with Model Interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated Sleep stage classification using raw single channel EEG is a critical tool for sleep quality assessment and disorder diagnosis. However, modelling the complexity and variability inherent in this signal is a challenging task, limiting their practicality and effectiveness in clinical settings. To mitigate these challenges, this study presents an end-to-end deep learning (DL) model which integrates squeeze and excitation blocks within the residual network to extract features and stacked Bi-LSTM to understand complex temporal dependencies. A distinctive aspect of this study is the adaptation of GradCam for sleep staging, marking the first instance of an explainable DL model in this domain with alignment of its decision-making with sleep expert's insights. We evaluated our model on the publically available datasets (SleepEDF-20, SleepEDF-78, and SHHS), achieving Macro-F1 scores of 82.5, 78.9, and 81.9, respectively. Additionally, a novel training efficiency enhancement strategy was implemented by increasing stride size, leading to 8x faster training times with minimal impact on performance. Comparative analyses underscore our model outperforms all existing baselines, indicating its potential for clinical usage.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2308.15822.pdf' target='_blank'>https://arxiv.org/pdf/2308.15822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Aiyub Ali, Md. Shakhawat Hossain, Md. Kawar Hossain, Subhadra Soumi Sikder, Sharun Akter Khushbu, Mirajul Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15822">AMDNet23: A combined deep Contour-based Convolutional Neural Network and Long Short Term Memory system to diagnose Age-related Macular Degeneration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In light of the expanding population, an automated framework of disease detection can assist doctors in the diagnosis of ocular diseases, yields accurate, stable, rapid outcomes, and improves the success rate of early detection. The work initially intended the enhancing the quality of fundus images by employing an adaptive contrast enhancement algorithm (CLAHE) and Gamma correction. In the preprocessing techniques, CLAHE elevates the local contrast of the fundus image and gamma correction increases the intensity of relevant features. This study operates on a AMDNet23 system of deep learning that combined the neural networks made up of convolutions (CNN) and short-term and long-term memory (LSTM) to automatically detect aged macular degeneration (AMD) disease from fundus ophthalmology. In this mechanism, CNN is utilized for extracting features and LSTM is utilized to detect the extracted features. The dataset of this research is collected from multiple sources and afterward applied quality assessment techniques, 2000 experimental fundus images encompass four distinct classes equitably. The proposed hybrid deep AMDNet23 model demonstrates to detection of AMD ocular disease and the experimental result achieved an accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and F1-score 96.49.0%. The system achieves state-of-the-art findings on fundus imagery datasets to diagnose AMD ocular disease and findings effectively potential of our method.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2307.10695.pdf' target='_blank'>https://arxiv.org/pdf/2307.10695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaekyun Ko, Sanghwan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10695">Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets. This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2305.12188.pdf' target='_blank'>https://arxiv.org/pdf/2305.12188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Narayan Subramanian, Atharva Joshi, Daksh Bagga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12188">Transparent and Traceable Food Supply Chain Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The food supply chain has a number of challenges, including a lack of transparency and disengagement among stakeholders. By providing a transparent and traceable digital ledger of transactions and movements for all supply chain actors, blockchain technology can provide a resolution to these problems. We propose a blockchain-based system for tracking a product's full path, from its raw components to the finished item in the store. Many advantages of the offered system include improved quality assessment, increased product transparency and traceability, and sophisticated fraud detection capabilities. By reinventing the way transactions are carried out and enabling stakeholders to obtain a complete record of each product's journey, the system has the potential to completely alter the food supply chain. Also, by minimising inefficiencies, waste, and fraudulent activities that have a negative influence on the supply chain, the deployment of this system can remove limits imposed by the current supply chain. Overall, the suggested blockchain-based system has the potential to significantly increase the efficiency, transparency, and traceability of the food supply chain.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2305.09368.pdf' target='_blank'>https://arxiv.org/pdf/2305.09368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Min Hyun, Tae-Geun Kim, Kyounghun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09368">Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, based on the residual between the input sequence and its neural representation with a cut--off value determined from the two-sigma rule of thumb over the training set. Our experimental observations validated two claims: (i) in the learning environment of label-absence, assessment performance is achievable at a competitive level to the supervised setting, and (ii) the contextual information across a time series of CVS is advantageous for effectively realizing motion-induced unrealistic distortions in signal amplitude and morphology. We also investigated the capability as a pseudo-labeling tool to minimize human-craft annotation by preemptively providing strong candidates for motion-induced anomalies. Empirical evidence has shown that machine-guided annotation can reduce inevitable human-errors during manual assessment while minimizing cumbersome and time-consuming processes.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2305.09141.pdf' target='_blank'>https://arxiv.org/pdf/2305.09141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nisar Ahmed, H. M. Shahzad Asif, Abdul Rauf Bhatti, Atif Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09141">Deep Ensembling for Perceptual Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment is a challenging task particularly due to the unavailability of reference information. Training a deep neural network requires a large amount of training data which is not readily available for image quality. Transfer learning is usually opted to overcome this limitation and different deep architectures are used for this purpose as they learn features differently. After extensive experiments, we have designed a deep architecture containing two CNN architectures as its sub-units. Moreover, a self-collected image database BIQ2021 is proposed with 12,000 images having natural distortions. The self-collected database is subjectively scored and is used for model training and validation. It is demonstrated that synthetic distortion databases cannot provide generalization beyond the distortion types used in the database and they are not ideal candidates for general-purpose image quality assessment. Moreover, a large-scale database of 18.75 million images with synthetic distortions is used to pretrain the model and then retrain it on benchmark databases for evaluation. Experiments are conducted on six benchmark databases three of which are synthetic distortion databases (LIVE, CSIQ and TID2013) and three are natural distortion databases (LIVE Challenge Database, CID2013 and KonIQ-10 k). The proposed approach has provided a Pearson correlation coefficient of 0.8992, 0.8472 and 0.9452 subsequently and Spearman correlation coefficient of 0.8863, 0.8408 and 0.9421. Moreover, the performance is demonstrated using perceptually weighted rank correlation to indicate the perceptual superiority of the proposed approach. Multiple experiments are conducted to validate the generalization performance of the proposed model by training on different subsets of the databases and validating on the test subset of BIQ2021 database.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2305.08408.pdf' target='_blank'>https://arxiv.org/pdf/2305.08408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ding-Jiun Huang, Yu-Ting Kao, Tieh-Hung Chuang, Ya-Chun Tsai, Jing-Kai Lou, Shuen-Huei Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08408">SB-VQA: A Stack-Based Video Quality Assessment Framework for Video Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, several video quality assessment (VQA) methods have been developed, achieving high performance. However, these methods were not specifically trained for enhanced videos, which limits their ability to predict video quality accurately based on human subjective perception. To address this issue, we propose a stack-based framework for VQA that outperforms existing state-of-the-art methods on VDPVE, a dataset consisting of enhanced videos. In addition to proposing the VQA framework for enhanced videos, we also investigate its application on professionally generated content (PGC). To address copyright issues with premium content, we create the PGCVQ dataset, which consists of videos from YouTube. We evaluate our proposed approach and state-of-the-art methods on PGCVQ, and provide new insights on the results. Our experiments demonstrate that existing VQA algorithms can be applied to PGC videos, and we find that VQA performance for PGC videos can be improved by considering the plot of a play, which highlights the importance of video semantic understanding.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2304.07745.pdf' target='_blank'>https://arxiv.org/pdf/2304.07745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laurent Kloeker, Chenghua Liu, Chao Wei, Lutz Eckstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07745">Framework for Quality Evaluation of Smart Roadside Infrastructure Sensors for Automated Driving Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of smart roadside infrastructure sensors is highly relevant for future applications of connected and automated vehicles. External sensor technology in the form of intelligent transportation system stations (ITS-Ss) can provide safety-critical real-time information about road users in the form of a digital twin. The choice of sensor setups has a major influence on the downstream function as well as the data quality. To date, there is insufficient research on which sensor setups result in which levels of ITS-S data quality. We present a novel approach to perform detailed quality assessment for smart roadside infrastructure sensors. Our framework is multimodal across different sensor types and is evaluated on the DAIR-V2X dataset. We analyze the composition of different lidar and camera sensors and assess them in terms of accuracy, latency, and reliability. The evaluations show that the framework can be used reliably for several future ITS-S applications.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2304.03766.pdf' target='_blank'>https://arxiv.org/pdf/2304.03766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcelin Tworski, StÃ©phane LathuiliÃ¨re
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03766">Test your samples jointly: Pseudo-reference for image quality evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the well-known image quality assessment problem but in contrast from existing approaches that predict image quality independently for every images, we propose to jointly model different images depicting the same content to improve the precision of quality estimation. This proposal is motivated by the idea that multiple distorted images can provide information to disambiguate image features related to content and quality. To this aim, we combine the feature representations from the different images to estimate a pseudo-reference that we use to enhance score prediction. Our experiments show that at test-time, our method successfully combines the features from multiple images depicting the same new content, improving estimation quality.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2304.03156.pdf' target='_blank'>https://arxiv.org/pdf/2304.03156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sri Charan Kattamuru, Kshitij Agrawal, Shyam Prasad Adhikari, Abhishek Bose, Hemant Misra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03156">Patch-wise Features for Blur Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Images captured through smartphone cameras often suffer from degradation, blur being one of the major ones, posing a challenge in processing these images for downstream tasks. In this paper we propose low-compute lightweight patch-wise features for image quality assessment. Using our method we can discriminate between blur vs sharp image degradation. To this end, we train a decision-tree based XGBoost model on various intuitive image features like gray level variance, first and second order gradients, texture features like local binary patterns. Experiments conducted on an open dataset show that the proposed low compute method results in 90.1% mean accuracy on the validation set, which is comparable to the accuracy of a compute-intensive VGG16 network with 94% mean accuracy fine-tuned to this task. To demonstrate the generalizability of our proposed features and model we test the model on BHBID dataset and an internal dataset where we attain accuracy of 98% and 91%, respectively. The proposed method is 10x faster than the VGG16 based model on CPU and scales linearly to the input image size making it suitable to be implemented on low compute edge devices.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2303.14479.pdf' target='_blank'>https://arxiv.org/pdf/2303.14479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caner Ozer, Arda Guler, Aysel Turkvatan Cansever, Ilkay Oksuz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14479">Explainable Image Quality Assessment for Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image quality assessment is an important aspect of image acquisition, as poor-quality images may lead to misdiagnosis. Manual labelling of image quality is a tedious task for population studies and can lead to misleading results. While much research has been done on automated analysis of image quality to address this issue, relatively little work has been done to explain the methodologies. In this work, we propose an explainable image quality assessment system and validate our idea on two different objectives which are foreign object detection on Chest X-Rays (Object-CXR) and Left Ventricular Outflow Tract (LVOT) detection on Cardiac Magnetic Resonance (CMR) volumes. We apply a variety of techniques to measure the faithfulness of the saliency detectors, and our explainable pipeline relies on NormGrad, an algorithm which can efficiently localise image quality issues with saliency maps of the classifier. We compare NormGrad with a range of saliency detection methods and illustrate its superior performance as a result of applying these methodologies for measuring the faithfulness of the saliency detectors. We see that NormGrad has significant gains over other saliency detectors by reaching a repeated Pointing Game score of 0.853 for Object-CXR and 0.611 for LVOT datasets.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2303.09097.pdf' target='_blank'>https://arxiv.org/pdf/2303.09097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hitoshi Matsuyama, Nobuo Kawaguchi, Brian Y. Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09097">IRIS: Interpretable Rubric-Informed Segmentation for Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-driven Action Quality Assessment (AQA) of sports videos can mimic Olympic judges to help score performances as a second opinion or for training. However, these AI methods are uninterpretable and do not justify their scores, which is important for algorithmic accountability. Indeed, to account for their decisions, instead of scoring subjectively, sports judges use a consistent set of criteria - rubric - on multiple actions in each performance sequence. Therefore, we propose IRIS to perform Interpretable Rubric-Informed Segmentation on action sequences for AQA. We investigated IRIS for scoring videos of figure skating performance. IRIS predicts (1) action segments, (2) technical element score differences of each segment relative to base scores, (3) multiple program component scores, and (4) the summed final score. In a modeling study, we found that IRIS performs better than non-interpretable, state-of-the-art models. In a formative user study, practicing figure skaters agreed with the rubric-informed explanations, found them useful, and trusted AI judgments more. This work highlights the importance of using judgment rubrics to account for AI decisions.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2303.07151.pdf' target='_blank'>https://arxiv.org/pdf/2303.07151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Parisot, Thomas Tamisier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07151">AmÃ©lioration de la qualitÃ© d'images avec un algorithme d'optimisation inspirÃ©e par la nature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reproducible images preprocessing is important in the field of computer vision, for efficient algorithms comparison or for new images corpus preparation. In this paper, we propose a method to obtain an explicit and ordered sequence of transformations that improves a given image: the computation is performed via a nature-inspired optimization algorithm based on quality assessment techniques. Preliminary tests show the impact of the approach on different state-of-the-art data sets.
  --
  L'application de prÃ©traitements explicites et reproductibles est fondamentale dans le domaine de la vision par ordinateur, pour pouvoir comparer efficacement des algorithmes ou pour prÃ©parer un nouveau corpus d'images. Dans cet article, nous proposons une mÃ©thode pour obtenir une sÃ©quence reproductible de transformations qui amÃ©liore une image donnÃ©e: le calcul est rÃ©alisÃ© via un algorithme d'optimisation inspirÃ©e par la nature et basÃ© sur des techniques d'Ã©valuation de la qualitÃ©. Des tests montrent l'impact de l'approche sur diffÃ©rents ensembles d'images de l'Ã©tat de l'art.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2303.02753.pdf' target='_blank'>https://arxiv.org/pdf/2303.02753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Viqar, Athar A. Moinuddin, Ekram Khan, M. Ghanbari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02753">Frequency-domain Blind Quality Assessment of Blurred and Blocking-artefact Images using Gaussian Process Regression model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most of the standard image and video codecs are block-based and depending upon the compression ratio the compressed images/videos suffer from different distortions. At low ratios, blurriness is observed and as compression increases blocking artifacts occur. Generally, in order to reduce blockiness, images are low-pass filtered which leads to more blurriness. Also, in bokeh mode images they are commonly seen: blurriness as a result of intentional blurred background while blocking artifact and global blurriness arising due to compression. Therefore, such visual media suffer from both blockiness and blurriness distortions. Along with this, noise is also commonly encountered distortion. Most of the existing works on quality assessment quantify these distortions individually. This paper proposes a methodology to blindly measure overall quality of an image suffering from these distortions, individually as well as jointly. This is achieved by considering the sum of absolute values of low and high-frequency Discrete Frequency Transform (DFT) coefficients defined as sum magnitudes. The number of blocks lying in specific ranges of sum magnitudes including zero-valued AC coefficients and mean of 100 maximum and 100 minimum values of these sum magnitudes are used as feature vectors. These features are then fed to the Machine Learning (ML) based Gaussian Process Regression (GPR) model, which quantifies the image quality. The simulation results show that the proposed method can estimate the quality of images distorted with the blockiness, blurriness, noise and their combinations. It is relatively fast compared to many state-of-art methods, and therefore is suitable for real-time quality monitoring applications.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2303.02446.pdf' target='_blank'>https://arxiv.org/pdf/2303.02446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitesh Kumar Singam, Benny LÃ¶vstrÃ¶m, Wlodek J. Kulesza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02446">Comparative Studies of Unsupervised and Supervised Learning Methods based on Multimedia Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the mobile communication field, some of the video applications boosted the interest of robust methods for video quality assessment. Out of all existing methods, We Preferred, No Reference Video Quality Assessment is the one which is most needed in situations where the handiness of reference video is partially available. Our research interest lies in formulating and melding effective features into one model based on human visualizing characteristics. Our work explores comparative study between Supervised and unsupervised learning methods. Therefore, we implemented support vector regression algorithm as NR-based Video Quality Metric(VQM) for quality estimation with simplified input features. We concluded that our proposed model exhibited sparseness even after dimension reduction for objective scores of SSIM quality metric.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2301.12891.pdf' target='_blank'>https://arxiv.org/pdf/2301.12891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyong You, Yuan Lin, Jari Korhonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12891">Half of an image is enough for quality assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep networks have demonstrated promising results in the field of Image Quality Assessment (IQA). However, there has been limited research on understanding how deep models in IQA work. This study introduces a novel positional masked transformer for IQA and provides insights into the contribution of different regions of an image towards its overall quality. Results indicate that half of an image may play a trivial role in determining image quality, while the other half is critical. This observation is extended to several other CNN-based IQA models, revealing that half of the image regions can significantly impact the overall image quality. To further enhance our understanding, three semantic measures (saliency, frequency, and objectness) were derived and found to have high correlation with the importance of image regions in IQA.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2301.10455.pdf' target='_blank'>https://arxiv.org/pdf/2301.10455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengqian Ma, Zhiqiang Wu, Chunlei Cai, Pengwei Zhang, Yi Wang, Long Zheng, Chao Chen, Quan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10455">Rate-Perception Optimized Preprocessing for Video Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the past decades, lots of progress have been done in the video compression field including traditional video codec and learning-based video codec. However, few studies focus on using preprocessing techniques to improve the rate-distortion performance. In this paper, we propose a rate-perception optimized preprocessing (RPP) method. We first introduce an adaptive Discrete Cosine Transform loss function which can save the bitrate and keep essential high frequency components as well. Furthermore, we also combine several state-of-the-art techniques from low-level vision fields into our approach, such as the high-order degradation model, efficient lightweight network design, and Image Quality Assessment model. By jointly using these powerful techniques, our RPP approach can achieve on average, 16.27% bitrate saving with different video encoders like AVC, HEVC, and VVC under multiple quality metrics. In the deployment stage, our RPP method is very simple and efficient which is not required any changes in the setting of video encoding, streaming, and decoding. Each input frame only needs to make a single pass through RPP before sending into video encoders. In addition, in our subjective visual quality test, 87% of users think videos with RPP are better or equal to videos by only using the codec to compress, while these videos with RPP save about 12% bitrate on average. Our RPP framework has been integrated into the production environment of our video transcoding services which serve millions of users every day.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2301.09190.pdf' target='_blank'>https://arxiv.org/pdf/2301.09190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyong You, Zheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09190">Apples and Oranges? Assessing Image Quality over Content Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image recognition and quality assessment are two important viewing tasks, while potentially following different visual mechanisms. This paper investigates if the two tasks can be performed in a multitask learning manner. A sequential spatial-channel attention module is proposed to simulate the visual attention and contrast sensitivity mechanisms that are crucial for content recognition and quality assessment. Spatial attention is shared between content recognition and quality assessment, while channel attention is solely for quality assessment. Such attention module is integrated into Transformer to build a uniform model for the two viewing tasks. The experimental results have demonstrated that the proposed uniform model can achieve promising performance for both quality assessment and content recognition tasks.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2301.01182.pdf' target='_blank'>https://arxiv.org/pdf/2301.01182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyi Pan, Ning Guo, Letu Qingge, Jingyi Zhang, Pei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01182">PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) remains challenging due to the diversity of distortion and image content variation, which complicate the distortion patterns crossing different scales and aggravate the difficulty of the regression problem for BIQA. However, existing BIQA methods often fail to consider multi-scale distortion patterns and image content, and little research has been done on learning strategies to make the regression model produce better performance. In this paper, we propose a simple yet effective Progressive Multi-Task Image Quality Assessment (PMT-IQA) model, which contains a multi-scale feature extraction module (MS) and a progressive multi-task learning module (PMT), to help the model learn complex distortion patterns and better optimize the regression issue to align with the law of human learning process from easy to hard. To verify the effectiveness of the proposed PMT-IQA model, we conduct experiments on four widely used public datasets, and the experimental results indicate that the performance of PMT-IQA is superior to the comparison approaches, and both MS and PMT modules improve the model's performance.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2211.15397.pdf' target='_blank'>https://arxiv.org/pdf/2211.15397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Girish Sundaram, Daniel Berleant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15397">Automating Systematic Literature Reviews with Natural Language Processing and Text Mining: a Systematic Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objectives: An SLR is presented focusing on text mining based automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained.
  Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed.
  Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research.
  Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2211.14420.pdf' target='_blank'>https://arxiv.org/pdf/2211.14420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Guo, Charlie Ruan, Claire Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14420">Photo Rater: Photographs Auto-Selector with Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photo Rater is a computer vision project that uses neural networks to help photographers select the best photo among those that are taken based on the same scene. This process is usually referred to as "culling" in photography, and it can be tedious and time-consuming if done manually. Photo Rater utilizes three separate neural networks to complete such a task: one for general image quality assessment, one for classifying whether the photo is blurry (either due to unsteady hands or out-of-focusness), and one for assessing general aesthetics (including the composition of the photo, among others). After feeding the image through each neural network, Photo Rater outputs a final score for each image, ranking them based on this score and presenting it to the user.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2211.12904.pdf' target='_blank'>https://arxiv.org/pdf/2211.12904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erez Shalom, Ayelet Goldstein, Roni Wais, Maya Slivanova, Nogah Melamed Cohen, Yuval Shahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12904">Implementation and Evaluation of a System for Assessment of The Quality of Long-Term Management of Patients at a Geriatric Hospital</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background
  The use of a clinical decision support system for assessing the quality of care, based on computerized clinical guidelines (GLs), is likely to improve care, reduce costs, save time, and enhance the staff's capabilities.
  Objectives
  Implement and evaluate a system for assessment of the quality of the care, in the domain of management of pressure ulcers, by investigating the level of compliance of the staff to the GLs.
  Methods
  Using data for 100 random patients from the local EMR system we performed a technical evaluation, checking the applicability and usability, followed by a functional evaluation of the system investigating the quality metrics given to the compliance of the medical's staff to the protocol. We compared the scores given by the nurse when supported by the system, to the scores given by the nurse without the system's support, and to the scores given by the system. We also measured the time taken to perform the assessment with and without the system's support.
  Results
  There were no significant differences in the scores of most measures given by the nurse using the system, compared to the scores given by the system. There were also no significant differences across the values of most quality measures given by the nurse without support compared to the values given by the nurse with support. Using the system, however, significantly reduced the nurse's average assessment time.
  Conclusions
  Using an automated quality-assessment system, may enable a senior nurse, to quickly and accurately assess the quality of care. In addition to its accuracy, the system considerably reduces the time taken to assess the various quality measures.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2210.17106.pdf' target='_blank'>https://arxiv.org/pdf/2210.17106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, H. Anthony Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.17106">Intelligent Painter: Picture Composition With Resampling Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Have you ever thought that you can be an intelligent painter? This means that you can paint a picture with a few expected objects in mind, or with a desirable scene. This is different from normal inpainting approaches for which the location of specific objects cannot be determined. In this paper, we present an intelligent painter that generate a person's imaginary scene in one go, given explicit hints. We propose a resampling strategy for Denoising Diffusion Probabilistic Model (DDPM) to intelligently compose unconditional harmonized pictures according to the input subjects at specific locations. By exploiting the diffusion property, we resample efficiently to produce realistic pictures. Experimental results show that our resampling method favors the semantic meaning of the generated output efficiently and generates less blurry output. Quantitative analysis of image quality assessment shows that our method produces higher perceptual quality images compared with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2209.01760.pdf' target='_blank'>https://arxiv.org/pdf/2209.01760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingheng Li, Fushuo Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01760">REQA: Coarse-to-fine Assessment of Image Quality to Alleviate the Range Effect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind image quality assessment (BIQA) of user generated content (UGC) suffers from the range effect which indicates that on the overall quality range, mean opinion score (MOS) and predicted MOS (pMOS) are well correlated; focusing on a particular range, the correlation is lower. The reason for the range effect is that the predicted deviations both in a wide range and in a narrow range destroy the uniformity between MOS and pMOS. To tackle this problem, a novel method is proposed from coarse-grained metric to fine-grained prediction. Firstly, we design a rank-and-gradient loss for coarse-grained metric. The loss keeps the order and grad consistency between pMOS and MOS, thereby reducing the predicted deviation in a wide range. Secondly, we propose multi-level tolerance loss to make fine-grained prediction. The loss is constrained by a decreasing threshold to limite the predicted deviation in narrower and narrower ranges. Finally, we design a feedback network to conduct the coarse-to-fine assessment. On the one hand, the network adopts feedback blocks to process multi-scale distortion features iteratively and on the other hand, it fuses non-local context feature to the output of each iteration to acquire more quality-aware feature representation. Experimental results demonstrate that the proposed method can alleviate the range effect compared to the state-of-the-art methods effectively.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2208.14008.pdf' target='_blank'>https://arxiv.org/pdf/2208.14008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. Di, Y. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.14008">Prediction of Red Wine Quality Using One-dimensional Convolutional Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an alcoholic beverage, wine has remained prevalent for thousands of years, and the quality assessment of wines has been significant in wine production and trade. Scholars have proposed various deep learning and machine learning algorithms for wine quality prediction, such as Support vector machine (SVM), Random Forest (RF), K-nearest neighbors (KNN), Deep neural network (DNN), and Logistic regression (LR). However, these methods ignore the inner relationship between the physical and chemical properties of the wine, for example, the correlations between pH values, fixed acidity, citric acid, and so on. To fill the gap, this paper conducts the Pearson correlation analysis, PCA analysis, and Shapiro-Wilk test on those properties and incorporates 1D-CNN architecture to capture the correlations among neighboring features. In addition, it implemented dropout and batch normalization techniques to improve the robustness of the proposed model. Massive experiments have shown that our method can outperform baseline approaches in wine quality prediction. Moreover, ablation experiments also demonstrate the effectiveness of incorporating the 1-D CNN module, Dropout, and normalization techniques.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2203.07809.pdf' target='_blank'>https://arxiv.org/pdf/2203.07809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Segrey Kastryulin, Jamil Zakirov, Nicola Pezzotti, Dmitry V. Dylov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.07809">Image Quality Assessment for Magnetic Resonance Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) algorithms aim to reproduce the human's perception of the image quality. The growing popularity of image enhancement, generation, and recovery models instigated the development of many methods to assess their performance. However, most IQA solutions are designed to predict image quality in the general domain, with the applicability to specific areas, such as medical imaging, remaining questionable. Moreover, the selection of these IQA metrics for a specific task typically involves intentionally induced distortions, such as manually added noise or artificial blurring; yet, the chosen metrics are then used to judge the output of real-life computer vision models. In this work, we aspire to fill these gaps by carrying out the most extensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date (14,700 subjective scores). We use outputs of neural network models trained to solve problems relevant to MRI, including image reconstruction in the scan acceleration, motion correction, and denoising. Our emphasis is on reflecting the radiologist's perception of the reconstructed images, gauging the most diagnostically influential criteria for the quality of MRI scans: signal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts. Seven trained radiologists assess these distorted images, with their verdicts then correlated with 35 different image quality metrics (full-reference, no-reference, and distribution-based metrics considered). The top performers -- DISTS, HaarPSI, VSI, and FID-VGG16 -- are found to be efficient across three proposed quality criteria, for all considered anatomies and the target tasks.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2202.02616.pdf' target='_blank'>https://arxiv.org/pdf/2202.02616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Allison H. Baker, Alexander Pinard, Dorit M. Hammerling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.02616">DSSIM: a structural similarity index for floating-point data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data visualization is a critical component in terms of interacting with floating-point output data from large model simulation codes. Indeed, postprocessing analysis workflows on simulation data often generate a large number of images from the raw data, many of which are then compared to each other or to specified reference images. In this image-comparison scenario, image quality assessment (IQA) measures are quite useful, and the Structural Similarity Index (SSIM) continues to be a popular choice. However, generating large numbers of images can be costly, and plot-specific (but data independent) choices can affect the SSIM value. A natural question is whether we can apply the SSIM directly to the floating-point simulation data and obtain an indication of whether differences in the data are likely to impact a visual assessment, effectively bypassing the creation of a specific set of images from the data. To this end, we propose an alternative to the popular SSIM that can be applied directly to the floating point data, which we refer to as the Data SSIM (DSSIM). While we demonstrate the usefulness of the DSSIM in the context of evaluating differences due to lossy compression on large volumes of simulation data from a popular climate model, the DSSIM may prove useful for many other applications involving simulation or image data.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2202.02397.pdf' target='_blank'>https://arxiv.org/pdf/2202.02397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yana NehmÃ©, Johanna Delanoy, Florent Dupont, Jean-Philippe Farrugia, Patrick Le Callet, Guillaume LavouÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.02397">Textured Mesh Quality Assessment: Large-Scale Dataset and Deep Learning-based Quality Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, 3D graphics have become highly detailed to mimic the real world, exploding their size and complexity. Certain applications and device constraints necessitate their simplification and/or lossy compression, which can degrade their visual quality. Thus, to ensure the best Quality of Experience (QoE), it is important to evaluate the visual quality to accurately drive the compression and find the right compromise between visual quality and data size. In this work, we focus on subjective and objective quality assessment of textured 3D meshes. We first establish a large-scale dataset, which includes 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity, and corrupted by combinations of 5 types of compression-based distortions applied on the geometry, texture mapping and texture image of the meshes. This dataset contains over 343k distorted stimuli. We propose an approach to select a challenging subset of 3000 stimuli for which we collected 148929 quality judgments from over 4500 participants in a large-scale crowdsourced subjective experiment. Leveraging our subject-rated dataset, a learning-based quality metric for 3D graphics was proposed. Our metric demonstrates state-of-the-art results on our dataset of textured meshes and on a dataset of distorted meshes with vertex colors. Finally, we present an application of our metric and dataset to explore the influence of distortion interactions and content characteristics on the perceived quality of compressed textured meshes.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2201.08684.pdf' target='_blank'>https://arxiv.org/pdf/2201.08684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Sawicki, MichaÅ Burdukiewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.08684">VisQualdex -- the comprehensive guide to good data visualization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid influx of low-quality data visualisations is one of the main challenges in today's communication. Misleading, unreadable, or confusing visualisations spread misinformation, failing to fulfill their purpose. The lack of proper tooling further heightens the problem of the quality assessment process. Therefore, we propose VisQualdex, a systematic set of guidelines isnpired by the Grammar of Graphics for evaluating the quality of data visualisations. To increase the practical impact of VisQualdex, we make these guidelines available in the form of the web server, visqual.info.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2112.15411.pdf' target='_blank'>https://arxiv.org/pdf/2112.15411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqian Ruan, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.15411">Disjoint Contrastive Regression Learning for Multi-Sourced Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator, with the assumption that the ranking of samples from the same annotator is unanimous. Secondly, we apply the gradient reversal layer to learn robust representations that are invariant to different annotators. Experiments on the facial expression prediction task, as well as the image quality assessment task, verify the effectiveness of our proposed framework.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2110.01661.pdf' target='_blank'>https://arxiv.org/pdf/2110.01661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pit Schneider, Yves Maurer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.01661">Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Iterating with new and improved OCR solutions enforces decision making when it comes to targeting the right candidates for reprocessing. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those targeting decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. Through extension of this technique, a regression model, that is able to take into account the enhancement potential of a new OCR engine, is also presented. They both mark promising approaches, especially for cultural institutions dealing with historical data of lower quality.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2109.10134.pdf' target='_blank'>https://arxiv.org/pdf/2109.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Usman, Nauman bin Ali, Claes Wohlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.10134">A Quality Assessment Instrument for Systematic Literature Reviews in Software Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context: Systematic literature reviews (SLRs) have become standard practise as part of software engineering research, although their quality varies. To build on the reviews, both for future research and industry practice, they need to be of high quality. Objective: To assess the quality of SLRs in software engineering, we put forward an appraisal instrument for SLRs. The instrument is intended for use by appraisers of reviews, but authors may also use it as a checklist when designing and documenting their reviews. Method: A well-established appraisal instrument from research in healthcare was used as a starting point to develop a quality assessment instrument. It is adapted to software engineering using guidelines, checklists, and experiences from software engineering. As a validation step, the first version was reviewed by four external experts on SLRs in software engineering and updated based on their feedback. Results: The outcome of the research is an appraisal instrument for the quality assessment of SLRs in software engineering. The instrument intends to support the appraiser in assessing the quality of an SLR. The instrument includes 16 items with different options to capture the quality. The item is assessed on a two or three-grade scale, depending on the item. The instrument also supports consolidating the items into groups, which are then used to assess the overall quality of a systematic literature review. Conclusion: It is concluded that the presented instrument may be helpful support for an appraiser in assessing the quality of SLRs in software engineering.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2005.11269.pdf' target='_blank'>https://arxiv.org/pdf/2005.11269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Rukundo, Samuel E. Schmidt, Olaf T von Ramm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2005.11269">Software Implementation of Optimized Bicubic Interpolated Scan Conversion in Echocardiography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach leveraging objective image quality assessment (IQA) metrics to optimize the outcomes of traditional bicubic (BIC) image interpolation and interpolated scan conversion algorithms. Specifically, feature selection through line chart data visualization and computing the IQA metrics scores are used to estimate the IQA-guided coefficient-k that up-dates the traditional BIC algorithm weighting function. The resulting optimized bicubic (OBIC) algorithm was subjectively and objectively evaluated using natural and ultrasound images. Results showed that the overall performance of the OBIC algorithm was equivalent to 92.22% of 180 occurrences when compared to the BIC algorithm, while it was 57.22% of 180 occurrences when compared to other algorithms. On top of that, the OBIC interpolated scan conversion algorithm generally produced crisper and better contrast cropped ultrasound sectored images than the BIC algorithm, as well as other interpolated scan conversion algorithms mentioned.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/1308.3052.pdf' target='_blank'>https://arxiv.org/pdf/1308.3052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wufeng Xue, Lei Zhang, Xuanqin Mou, Alan C. Bovik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1308.3052">Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is an important task to faithfully evaluate the perceptual quality of output images in many applications such as image compression, image restoration and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between the reference and distorted images combined with a novel pooling strategy the standard deviation of the GMS map can predict accurately perceptual image quality. The resulting GMSD algorithm is much faster than most state-of-the-art IQA methods, and delivers highly competitive prediction accuracy.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2511.08600.pdf' target='_blank'>https://arxiv.org/pdf/2511.08600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08600">Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2511.01194.pdf' target='_blank'>https://arxiv.org/pdf/2511.01194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minmin Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01194">A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2510.21774.pdf' target='_blank'>https://arxiv.org/pdf/2510.21774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21774">OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present OCR-Quality, a comprehensive human-annotated dataset designed for evaluating and developing OCR quality assessment methods. The dataset consists of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse real-world scenarios, including academic papers, textbooks, e-books, and multilingual documents. Each document has been processed using state-of-the-art Vision-Language Models (VLMs) and manually annotated with quality scores using a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset includes detailed source information, annotation guidelines, and representative cases across various difficulty levels. OCR-Quality addresses the critical need for reliable OCR quality assessment in real-world applications and provides a valuable benchmark for training and evaluating OCR verification systems. The dataset is publicly available at https://huggingface.co/datasets/Aslan-mingye/OCR-Quality .
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2510.13898.pdf' target='_blank'>https://arxiv.org/pdf/2510.13898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Misam Abbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13898">Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2510.06743.pdf' target='_blank'>https://arxiv.org/pdf/2510.06743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Levchenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06743">Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2510.03376.pdf' target='_blank'>https://arxiv.org/pdf/2510.03376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanjukta Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03376">Visual Language Model as a Judge for Object Detection in Industrial Diagrams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are essential for the design, operation, and maintenance of industrial plants. Converting these diagrams into digital form is an important step toward building digital twins and enabling intelligent industrial automation. A central challenge in this digitalization process is accurate object detection. Although recent advances have significantly improved object detection algorithms, there remains a lack of methods to automatically evaluate the quality of their outputs. This paper addresses this gap by introducing a framework that employs Visual Language Models (VLMs) to assess object detection results and guide their refinement. The approach exploits the multimodal capabilities of VLMs to identify missing or inconsistent detections, thereby enabling automated quality assessment and improving overall detection performance on complex industrial diagrams.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2509.25213.pdf' target='_blank'>https://arxiv.org/pdf/2509.25213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Varun Kodathala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25213">Six Sigma For Neural Networks: Taguchi-based optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The optimization of hyperparameters in convolutional neural networks (CNNs) remains a challenging and computationally expensive process, often requiring extensive trial-and-error approaches or exhaustive grid searches. This study introduces the application of Taguchi Design of Experiments methodology, a statistical optimization technique traditionally used in quality engineering, to systematically optimize CNN hyperparameters for professional boxing action recognition. Using an L12(211) orthogonal array, eight hyperparameters including image size, color mode, activation function, learning rate, rescaling, shuffling, vertical flip, and horizontal flip were systematically evaluated across twelve experimental configurations. To address the multi-objective nature of machine learning optimization, five different approaches were developed to simultaneously optimize training accuracy, validation accuracy, training loss, and validation loss using Signal-to-Noise ratio analysis. The study employed a novel logarithmic scaling technique to unify conflicting metrics and enable comprehensive multi-quality assessment within the Taguchi framework. Results demonstrate that Approach 3, combining weighted accuracy metrics with logarithmically transformed loss functions, achieved optimal performance with 98.84% training accuracy and 86.25% validation accuracy while maintaining minimal loss values. The Taguchi analysis revealed that learning rate emerged as the most influential parameter, followed by image size and activation function, providing clear guidance for hyperparameter prioritization in CNN optimization.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2509.05394.pdf' target='_blank'>https://arxiv.org/pdf/2509.05394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zoltan Toth-Czifra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05394">Reverse Browser: Vector-Image-to-Code Generator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating the conversion of user interface design into code (image-to-code or image-to-UI) is an active area of software engineering research. However, the state-of-the-art solutions do not achieve high fidelity to the original design, as evidenced by benchmarks. In this work, I approach the problem differently: I use vector images instead of bitmaps as model input. I create several large datasets for training machine learning models. I evaluate the available array of Image Quality Assessment (IQA) algorithms and introduce a new, multi-scale metric. I then train a large open-weights model and discuss its limitations.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2508.08824.pdf' target='_blank'>https://arxiv.org/pdf/2508.08824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Frias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08824">A Parametric Bi-Directional Curvature-Based Framework for Image Artifact Classification and Quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel framework for No-Reference Image Quality Assessment (NR-IQA) founded on the analysis of directional image curvature. Within this framework, we define a measure of Anisotropic Texture Richness (ATR), which is computed at the pixel level using two tunable thresholds -- one permissive and one restrictive -- that quantify orthogonal texture suppression. When its parameters are optimized for a specific artifact, the resulting ATR score serves as a high-performance quality metric, achieving Spearman correlations with human perception of approximately -0.93 for Gaussian blur and -0.95 for white noise on the LIVE dataset. The primary contribution is a two-stage system that leverages the differential response of ATR to various distortions. First, the system utilizes the signature from two specialist ATR configurations to classify the primary artifact type (blur vs. noise) with over 97% accuracy. Second, following classification, it employs a dedicated regression model mapping the relevant ATR score to a quality rating to quantify the degradation. On a combined dataset, the complete system predicts human scores with a coefficient of determination (R2) of 0.892 and a Root Mean Square Error (RMSE) of 5.17 DMOS points. This error corresponds to just 7.4% of the dataset's total quality range, demonstrating high predictive accuracy. This establishes our framework as a robust, dual-purpose tool for the classification and subsequent quantification of image degradation.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2508.05724.pdf' target='_blank'>https://arxiv.org/pdf/2508.05724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massimiliano Romiti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05724">A Graph-Based Framework for Exploring Mathematical Patterns in Physics: A Proof of Concept</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vast corpus of physics equations forms an implicit network of mathematical relationships that traditional analysis cannot fully explore. This work introduces a graph-based framework combining neural networks with symbolic analysis to systematically discover and validate mathematical patterns across physics domains. Starting from 659 equations, we performed rigorous semantic disambiguation to resolve notational polysemy affecting 213 equations, then focused on 400 advanced physics equations by excluding elementary mechanics to emphasize inter-branch connections of modern physics. This corpus was represented as a weighted knowledge graph where a Graph Attention Network achieved 97.4% AUC in link prediction, significantly outperforming classical baselines. The framework's primary value emerges from its dual capability: generating hypotheses and auditing knowledge. First, it functions as a hypothesis generator, producing hundreds of candidate cross-domain connections, from blackbody radiation coupled with Navier-Stokes equations to radioactive decay linked with electromagnetic induction. Second, through symbolic analysis of 30 equation clusters, it serves as a computational auditor that verified established theory consistencies, synthesized the Magnetic Reynolds Number from electromagnetic-fluid coupling, and revealed how even parsing errors could potentially point toward legitimate research like analog gravity. This proof-of-concept intentionally over-generates candidates to ensure comprehensive exploration of mathematical possibility space. Even tautologies and errors serve scientific purposes: redundancy identification and knowledge base quality assessment. The system transforms the intractable combinatorial space into a filtered stream of mathematical patterns for human interpretation.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2508.00317.pdf' target='_blank'>https://arxiv.org/pdf/2508.00317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Chin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00317">Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech quality assessment (SQA) refers to the evaluation of speech quality, and developing an accurate automatic SQA method that reflects human perception has become increasingly important, in order to keep up with the generative AI boom. In recent years, SQA has progressed to a point that researchers started to faithfully use automatic SQA in research papers as a rigorous measurement of goodness for speech generation systems. We believe that the scientific challenges and open-source activities of late have stimulated the growth in this field. In this paper, we review recent challenges as well as open-source implementations and toolkits for SQA, and highlight the importance of maintaining such activities to facilitate the development of not only SQA itself but also generative AI for speech.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2507.23226.pdf' target='_blank'>https://arxiv.org/pdf/2507.23226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanming Xiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23226">Toward Safe, Trustworthy and Realistic Augmented Reality User Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As augmented reality (AR) becomes increasingly integrated into everyday life, ensuring the safety and trustworthiness of its virtual content is critical. Our research addresses the risks of task-detrimental AR content, particularly that which obstructs critical information or subtly manipulates user perception. We developed two systems, ViDDAR and VIM-Sense, to detect such attacks using vision-language models (VLMs) and multimodal reasoning modules. Building on this foundation, we propose three future directions: automated, perceptually aligned quality assessment of virtual content; detection of multimodal attacks; and adaptation of VLMs for efficient and user-centered deployment on AR devices. Overall, our work aims to establish a scalable, human-aligned framework for safeguarding AR experiences and seeks feedback on perceptual modeling, multimodal AR content implementation, and lightweight model adaptation.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2506.10987.pdf' target='_blank'>https://arxiv.org/pdf/2506.10987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoyi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10987">Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have become vital tools for software development, but they often require verbose intermediate reasoning for complex code tasks, leading to high latency and costs. This research extends the Chain of Draft (CoD) method to software engineering, designing and evaluating multiple CoD variants tailored for code tasks. Through comprehensive experiments on all 300 samples from the SWE-bench benchmark, we found that all CoD variants used significantly fewer tokens than Chain of Thought (CoT), with Baseline CoD being most efficient at 55.4% of CoT's tokens. While this represents substantial efficiency gains - translating to approximately 45% reduction in processing time and API costs - it differs from the extreme 7.6% reported in the original CoD paper for mathematical reasoning. This difference stems from the inherent complexity and context-dependency of software tasks, which require more detailed reasoning to maintain solution quality. Our multi-dimensional quality assessment revealed that CoD variants maintain over 90% of CoT's code quality across key metrics including correctness, compatibility, and maintainability, making them practical alternatives for real-world development scenarios where efficiency matters. This research demonstrates how domain-specific characteristics influence prompting strategy effectiveness and provides a framework for balancing efficiency with solution quality in software engineering applications. Our findings offer practical guidance for optimizing LLM-based development workflows through appropriate prompting strategy selection based on project requirements.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2506.06327.pdf' target='_blank'>https://arxiv.org/pdf/2506.06327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06327">Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reproducible wine-quality assessment is critical for production control yet remains dominated by subjective, labour-intensive tasting panels. We present the first unified benchmark of five ensemble learners (Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho Verde red- and white-wine datasets (1,599 and 4,898 instances, 11 physicochemical attributes). Our leakage-free workflow employs an 80:20 stratified train-test split, five-fold StratifiedGroupKFold within the training set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost weighting, Optuna hyper-parameter search (120-200 trials per model) and a two-stage feature-selection refit. Final scores on untouched test sets are reported with weighted F1 as the headline metric. Gradient Boosting achieves the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016 for white), followed within three percentage points by Random Forest and XGBoost. Limiting each model to its five top-ranked variables lowers dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage points for red and 3.0 percentage points for white, indicating that alcohol, volatile acidity, sulphates, free SO2 and chlorides capture most predictive signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We therefore recommend Random Forest as the most cost-effective production model, XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as the accuracy ceiling for offline benchmarking. The fully documented pipeline and metric set provide a reproducible baseline for future work on imbalanced multi-class wine-quality prediction.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2505.02001.pdf' target='_blank'>https://arxiv.org/pdf/2505.02001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineesh Kumar Reddy Mondem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02001">Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional image quality assessment metrics like Mean Squared Error and Structural Similarity Index often fail to reflect perceptual quality under complex distortions. We propose the Hybrid Image Resolution Quality Metric (HIRQM), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. HIRQM combines three components: Probability Density Function for local pixel distribution analysis, Multi-scale Feature Similarity for structural integrity across resolutions, and Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic alignment with human perception. A dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. Our contributions include a unified metric and dynamic weighting for better perceptual alignment. Evaluated on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations of 0.92 and 0.90, outperforming traditional metrics. It excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2503.05802.pdf' target='_blank'>https://arxiv.org/pdf/2503.05802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Selcuk Yazar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05802">Illuminant and light direction estimation using Wasserstein distance method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Illumination estimation remains a pivotal challenge in image processing, particularly for robotics, where robust environmental perception is essential under varying lighting conditions. Traditional approaches, such as RGB histograms and GIST descriptors, often fail in complex scenarios due to their sensitivity to illumination changes. This study introduces a novel method utilizing the Wasserstein distance, rooted in optimal transport theory, to estimate illuminant and light direction in images. Experiments on diverse images indoor scenes, black-and-white photographs, and night images demonstrate the method's efficacy in detecting dominant light sources and estimating their directions, outperforming traditional statistical methods in complex lighting environments. The approach shows promise for applications in light source localization, image quality assessment, and object detection enhancement. Future research may explore adaptive thresholding and integrate gradient analysis to enhance accuracy, offering a scalable solution for real-world illumination challenges in robotics and beyond.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2411.18656.pdf' target='_blank'>https://arxiv.org/pdf/2411.18656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JÃ©rÃ©mie Sublime
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18656">The Return of Pseudosciences in Artificial Intelligence: Have Machine Learning and Deep Learning Forgotten Lessons from Statistics and History?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's world, AI programs powered by Machine Learning are ubiquitous, and have achieved seemingly exceptional performance across a broad range of tasks, from medical diagnosis and credit rating in banking, to theft detection via video analysis, and even predicting political or sexual orientation from facial images. These predominantly deep learning methods excel due to their extraordinary capacity to process vast amounts of complex data to extract complex correlations and relationship from different levels of features.
  In this paper, we contend that the designers and final users of these ML methods have forgotten a fundamental lesson from statistics: correlation does not imply causation. Not only do most state-of-the-art methods neglect this crucial principle, but by doing so they often produce nonsensical or flawed causal models, akin to social astrology or physiognomy. Consequently, we argue that current efforts to make AI models more ethical by merely reducing biases in the training data are insufficient. Through examples, we will demonstrate that the potential for harm posed by these methods can only be mitigated by a complete rethinking of their core models, improved quality assessment metrics and policies, and by maintaining humans oversight throughout the process.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2411.07556.pdf' target='_blank'>https://arxiv.org/pdf/2411.07556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07556">Multi-task Feature Enhancement Network for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the scarcity of labeled samples in Image Quality Assessment (IQA) datasets, numerous recent studies have proposed multi-task based strategies, which explore feature information from other tasks or domains to boost the IQA task. Nevertheless, multi-task strategies based No-Reference Image Quality Assessment (NR-IQA) methods encounter several challenges. First, existing methods have not explicitly exploited texture details, which significantly influence the image quality. Second, multi-task methods conventionally integrate features through simple operations such as addition or concatenation, thereby diminishing the network's capacity to accurately represent distorted features. To tackle these challenges, we introduce a novel multi-task NR-IQA framework. Our framework consists of three key components: a high-frequency extraction network, a quality estimation network, and a distortion-aware network. The high-frequency extraction network is designed to guide the model's focus towards high-frequency information, which is highly related to the texture details. Meanwhile, the distortion-aware network extracts distortion-related features to distinguish different distortion types. To effectively integrate features from different tasks, a feature fusion module is developed based on an attention mechanism. Empirical results from five standard IQA databases confirm that our method not only achieves high performance but also exhibits robust generalization ability.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2410.10488.pdf' target='_blank'>https://arxiv.org/pdf/2410.10488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Gonzalo Antonel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10488">A Novel No-Reference Image Quality Metric For Assessing Sharpness In Satellite Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel no-reference image quality metric aimed at assessing image sharpness. Designed to be robust against variations in noise, exposure, contrast, and image content, it measures the normalized decay rate of gradients along pronounced edges, offering an objective method for sharpness evaluation without reference images. Primarily developed for satellite imagery to align with human visual perception of sharpness, this metric supports monitoring and quality characterization of satellite fleets. It demonstrates significant utility and superior performance in consistency with human perception across various image types and operational conditions. Unlike conventional metrics, this heuristic approach provides a way to score images from lower to higher sharpness, making it a reliable and versatile tool for enhancing quality assessment processes without the need for pristine or ground truth comparison. Additionally, this metric is computationally efficient compared to deep learning analysis, ensuring faster and more resource-effective sharpness evaluations.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2409.10139.pdf' target='_blank'>https://arxiv.org/pdf/2409.10139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Djibril Sarr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10139">Towards Explainable Automated Data Quality Enhancement without Domain Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of big data, ensuring the quality of datasets has become increasingly crucial across various domains. We propose a comprehensive framework designed to automatically assess and rectify data quality issues in any given dataset, regardless of its specific content, focusing on both textual and numerical data. Our primary objective is to address three fundamental types of defects: absence, redundancy, and incoherence. At the heart of our approach lies a rigorous demand for both explainability and interpretability, ensuring that the rationale behind the identification and correction of data anomalies is transparent and understandable. To achieve this, we adopt a hybrid approach that integrates statistical methods with machine learning algorithms. Indeed, by leveraging statistical techniques alongside machine learning, we strike a balance between accuracy and explainability, enabling users to trust and comprehend the assessment process. Acknowledging the challenges associated with automating the data quality assessment process, particularly in terms of time efficiency and accuracy, we adopt a pragmatic strategy, employing resource-intensive algorithms only when necessary, while favoring simpler, more efficient solutions whenever possible. Through a practical analysis conducted on a publicly provided dataset, we illustrate the challenges that arise when trying to enhance data quality while keeping explainability. We demonstrate the effectiveness of our approach in detecting and rectifying missing values, duplicates and typographical errors as well as the challenges remaining to be addressed to achieve similar accuracy on statistical outliers and logic errors under the constraints set in our work.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2408.14575.pdf' target='_blank'>https://arxiv.org/pdf/2408.14575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Y. Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14575">EVINCE: Optimizing Multi-LLM Dialogues Using Conditional Statistics and Information Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>EVINCE (Entropy and Variation IN Conditional Exchanges) is a novel framework for optimizing multi-LLM dialogues using conditional statistics and information theory. It addresses limitations in multi-agent debate (MAS) frameworks, where multiple LLMs ``chat'' without behavior modulation or mutual information quality assessment. Using dual entropy optimization to balance perspective diversity and prior knowledge, $\EVINCE$ provides quantitative tools to dynamically regulate LLM linguistic behaviors. When mutual information is low and both cross-entropy and Wasserstein distance are high, EVINCE promotes contentious dialogues to expose diverse perspectives and uncover inconsistencies. Conversely, as cross-entropy decreases and mutual information stabilizes, it transitions discussions into a conciliatory phase, encouraging compromise and acknowledgment of valid points. Using information-theoretic metrics and optimizing mutual information, $\EVINCE$ emerges as a structured and highly effective framework for multi-LLM collaboration.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2405.16196.pdf' target='_blank'>https://arxiv.org/pdf/2405.16196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Makgotso Jacqueline Maotwana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16196">Maintaining and Managing Road Quality:Using MLP and DNN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poor roads are a major issue for cars, drivers, and pedestrians since they are a major cause of vehicle damage and can occasionally be quite dangerous for both groups of people (pedestrians and drivers), this makes road surface condition monitoring systems essential for traffic safety, reducing accident rates ad also protecting vehicles from getting damaged. The primary objective is to develop and evaluate machine learning models that can accurately classify road conditions into four categories: good, satisfactory, poor, and very poor, using a Kaggle dataset of road images. To address this, we implemented a variety of machine learning approaches. Firstly, a baseline model was created using a Multilayer Perceptron (MLP) implemented from scratch. Secondly, a more sophisticated Deep Neural Network (DNN) was constructed using Keras. Additionally, we developed a Logistic Regression model from scratch to compare performance. Finally, a wide model incorporating extensive feature engineering was built using the K-Nearest Neighbors (KNN) algorithm with sklearn.The study compared different models for image-based road quality assessment. Deep learning models, the DNN with Keras achieved the best accuracy, while the baseline MLP provided a solid foundation. The Logistic Regression although it is simpler, but it provided interpretability and insights into important features. The KNN model, with the help of feature engineering, achieved the best results. The research shows that machine learning can automate road condition monitoring, saving time and money on maintenance. The next step is to improve these models and test them in real cities, which will make our cities better managed and safer.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2405.15253.pdf' target='_blank'>https://arxiv.org/pdf/2405.15253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leif Bergerhoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15253">Seeing the World through an Antenna's Eye: Reception Quality Visualization Using Incomplete Technical Signal Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We come up with a novel application for image analysis methods in the context of direction dependent signal characteristics. For this purpose, we describe an inpainting approach adding benefit to technical signal information which are typically only used for monitoring and control purposes in ground station operations. Recalling the theoretical properties of the employed inpainting technique and appropriate modeling allow us to demonstrate the usefulness of our approach for satellite data reception quality assessment. In our application, we show the advantages of inpainting products over raw data as well as the rich potential of the visualization of technical signal information.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2405.09177.pdf' target='_blank'>https://arxiv.org/pdf/2405.09177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>PÃ©ter KirÃ¡ly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09177">Shacl4Bib: custom validation of library data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Shapes Constraint Language (SHACL) is a formal language for validating RDF graphs against a set of conditions. Following this idea and implementing a subset of the language, the Metadata Quality Assessment Framework provides Shacl4Bib: a mechanism to define SHACL-like rules for data sources in non-RDF based formats, such as XML, CSV and JSON. QA catalogue extends this concept further to MARC21, UNIMARC and PICA data. The criteria can be defined either with YAML or JSON configuration files or with Java code. Libraries can validate their data against criteria expressed in a unified language, that improves the clarity and the reusability of custom validation processes.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2405.04311.pdf' target='_blank'>https://arxiv.org/pdf/2405.04311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04311">Cross-IQA: Unsupervised Learning for Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic perception of image quality is a challenging problem that impacts billions of Internet and social media users daily. To advance research in this field, we propose a no-reference image quality assessment (NR-IQA) method termed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA method can learn image quality features from unlabeled image data. We construct the pretext task of synthesized image reconstruction to unsupervised extract the image quality information based ViT block. The pretrained encoder of Cross-IQA is used to fine-tune a linear regression model for score prediction. Experimental results show that Cross-IQA can achieve state-of-the-art performance in assessing the low-frequency degradation information (e.g., color change, blurring, etc.) of images compared with the classical full-reference IQA and NR-IQA under the same datasets.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2405.03870.pdf' target='_blank'>https://arxiv.org/pdf/2405.03870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Widad Elouataoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03870">AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of big data has ushered in a new era of data-driven decision-making, transforming numerous industries and sectors. However, the efficacy of these decisions hinges on the quality of the underlying data. Poor data quality can result in inaccurate analyses and deceptive conclusions. Managing the vast volume, velocity, and variety of data sources presents significant challenges, heightening the importance of addressing big data quality issues. While there has been increased attention from both academia and industry, current approaches often lack comprehensiveness and universality. They tend to focus on limited metrics, neglecting other dimensions of data quality. Moreover, existing methods are often context-specific, limiting their applicability across different domains. There is a clear need for intelligent, automated approaches leveraging artificial intelligence (AI) for advanced data quality corrections.
  To bridge these gaps, this Ph.D. thesis proposes a novel set of interconnected frameworks aimed at enhancing big data quality comprehensively. Firstly, we introduce new quality metrics and a weighted scoring system for precise data quality assessment. Secondly, we present a generic framework for detecting various quality anomalies using AI models. Thirdly, we propose an innovative framework for correcting detected anomalies through predictive modeling. Additionally, we address metadata quality enhancement within big data ecosystems. These frameworks are rigorously tested on diverse datasets, demonstrating their efficacy in improving big data quality. Finally, the thesis concludes with insights and suggestions for future research directions.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2312.16551.pdf' target='_blank'>https://arxiv.org/pdf/2312.16551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaohui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16551">Blind Image Quality Assessment: A Brief Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blind Image Quality Assessment (BIQA) is essential for automatically evaluating the perceptual quality of visual signals without access to the references. In this survey, we provide a comprehensive analysis and discussion of recent developments in the field of BIQA. We have covered various aspects, including hand-crafted BIQAs that focus on distortion-specific and general-purpose methods, as well as deep-learned BIQAs that employ supervised and unsupervised learning techniques. Additionally, we have explored multimodal quality assessment methods that consider interactions between visual and audio modalities, as well as visual and text modalities. Finally, we have offered insights into representative BIQA databases, including both synthetic and authentic distortions. We believe this survey provides valuable understandings into the latest developments and emerging trends for the visual quality community.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2311.11175.pdf' target='_blank'>https://arxiv.org/pdf/2311.11175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo C. Garrido-Merchan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11175">Best uses of ChatGPT and Generative AI for computer science research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Artificial Intelligence (AI), particularly tools like OpenAI's popular ChatGPT, is reshaping the landscape of computer science research. Used wisely, these tools can boost the productivity of a computer research scientist. This paper provides an exploration of the diverse applications of ChatGPT and other generative AI technologies in computer science academic research, making recommendations about the use of Generative AI to make more productive the role of the computer research scientist, with the focus of writing new research papers. We highlight innovative uses such as brainstorming research ideas, aiding in the drafting and styling of academic papers and assisting in the synthesis of state-of-the-art section. Further, we delve into using these technologies in understanding interdisciplinary approaches, making complex texts simpler, and recommending suitable academic journals for publication. Significant focus is placed on generative AI's contributions to synthetic data creation, research methodology, and mentorship, as well as in task organization and article quality assessment. The paper also addresses the utility of AI in article review, adapting texts to length constraints, constructing counterarguments, and survey development. Moreover, we explore the capabilities of these tools in disseminating ideas, generating images and audio, text transcription, and engaging with editors. We also describe some non-recommended uses of generative AI for computer science research, mainly because of the limitations of this technology.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2308.13094.pdf' target='_blank'>https://arxiv.org/pdf/2308.13094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takamichi Miyata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13094">Interpretable Image Quality Assessment via CLIP with Multiple Antonym-Prompt Pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No reference image quality assessment (NR-IQA) is a task to estimate the perceptual quality of an image without its corresponding original image. It is even more difficult to perform this task in a zero-shot manner, i.e., without task-specific training. In this paper, we propose a new zero-shot and interpretable NRIQA method that exploits the ability of a pre-trained visionlanguage model to estimate the correlation between an image and a textual prompt. The proposed method employs a prompt pairing strategy and multiple antonym-prompt pairs corresponding to carefully selected descriptive features corresponding to the perceptual image quality. Thus, the proposed method is able to identify not only the perceptual quality evaluation of the image, but also the cause on which the quality evaluation is based. Experimental results show that the proposed method outperforms existing zero-shot NR-IQA methods in terms of accuracy and can evaluate the causes of perceptual quality degradation.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2307.15464.pdf' target='_blank'>https://arxiv.org/pdf/2307.15464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge Martinez-Gil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15464">Framework to Automatically Determine the Quality of Open Data Catalogs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data catalogs play a crucial role in modern data-driven organizations by facilitating the discovery, understanding, and utilization of diverse data assets. However, ensuring their quality and reliability is complex, especially in open and large-scale data environments. This paper proposes a framework to automatically determine the quality of open data catalogs, addressing the need for efficient and reliable quality assessment mechanisms. Our framework can analyze various core quality dimensions, such as accuracy, completeness, consistency, scalability, and timeliness, offer several alternatives for the assessment of compatibility and similarity across such catalogs as well as the implementation of a set of non-core quality dimensions such as provenance, readability, and licensing. The goal is to empower data-driven organizations to make informed decisions based on trustworthy and well-curated data assets. The source code that illustrates our approach can be downloaded from https://www.github.com/jorge-martinez-gil/dataq/.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2306.16728.pdf' target='_blank'>https://arxiv.org/pdf/2306.16728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Mante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16728">IoT Data Processing for Smart City and Semantic Web Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The world has been experiencing rapid urbanization over the last few decades, putting a strain on existing city infrastructure such as waste management, water supply management, public transport and electricity consumption. We are also seeing increasing pollution levels in cities threatening the environment, natural resources and health conditions. However, we must realize that the real growth lies in urbanization as it provides many opportunities to individuals for better employment, healthcare and better education. However, it is imperative to limit the ill effects of rapid urbanization through integrated action plans to enable the development of growing cities. This gave rise to the concept of a smart city in which all available information associated with a city will be utilized systematically for better city management.
  The proposed system architecture is divided in subsystems and is discussed in individual chapters. The first chapter introduces and gives overview to the reader of the complete system architecture. The second chapter discusses the data monitoring system and data lake system based on the oneM2M standards. DMS employs oneM2M as a middleware layer to achieve interoperability, and DLS uses a multi-tenant architecture with multiple logical databases, enabling efficient and reliable data management. The third chapter discusses energy monitoring and electric vehicle charging systems developed to illustrate the applicability of the oneM2M standards. The fourth chapter discusses the Data Exchange System based on the Indian Urban Data Exchange framework. DES uses IUDX standard data schema and open APIs to avoid data silos and enable secure data sharing. The fifth chapter discusses the 5D-IoT framework that provides uniform data quality assessment of sensor data with meaningful data descriptions.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2003.06885.pdf' target='_blank'>https://arxiv.org/pdf/2003.06885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Rukundo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.06885">Evaluation of Rounding Functions in Nearest-Neighbor Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel evaluation study of the most appropriate round function for nearest-neighbor (NN) image interpolation is presented. Evaluated rounding functions are selected among the five rounding rules defined by the Institute of Electrical and Electronics Engineers (IEEE) 754-2008 standard. Both full- and no-reference image quality assessment (IQA) metrics are used to study and evaluate the influence of rounding functions on NN interpolation image quality. The concept of achieved occurrences over targeted occurrences is used to determine the percentage of achieved occurrences based on the number of test images used. Inferential statistical analysis is applied to deduce from a small number of images and draw a conclusion of the behavior of each rounding function on a bigger number of images. Under the normal distribution and at the level of confidence equals to 95%, the maximum and minimum achievable occurrences by each evaluated rounding function are both provided based on the inferential analysis-based experiments.
